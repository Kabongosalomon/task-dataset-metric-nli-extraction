<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memorize, Factorize, or be Na?ve: Learning Optimal Feature Interaction Methods for CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Lyu</surname></persName>
							<email>fuyuan.lyu@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 www.ruizhang.info</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Tang</surname></persName>
							<email>xing.tang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 www.ruizhang.info</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<email>huifeng.guo@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 www.ruizhang.info</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<email>tangruiming@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<email>hexiuqiang1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 www.ruizhang.info</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 www.ruizhang.info</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Liu</surname></persName>
							<email>xueliu@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memorize, Factorize, or be Na?ve: Learning Optimal Feature Interaction Methods for CTR Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Click-through Rate Prediction</term>
					<term>Feature Interac- tion</term>
					<term>Recommendation</term>
					<term>Neural Architecture Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate prediction is one of the core tasks in commercial recommender systems. It aims to predict the probability of a user clicking a particular item given user and item features. As feature interactions bring in non-linearity, they are widely adopted to improve the performance of CTR prediction models. Therefore, effectively modelling feature interactions has attracted much attention in both the research and industry field. The current approaches can generally be categorized into three classes: (i) na?ve methods, which do not model feature interactions and only use original features; (ii) memorized methods, which memorize feature interactions by explicitly viewing them as new features and assigning trainable embeddings; (iii) factorized methods, which learn latent vectors for original features and implicitly model feature interactions through factorization functions. Studies have shown that modelling feature interactions by one of these methods alone are suboptimal due to the unique characteristics of different feature interactions. To address this issue, we first propose a general framework called OptInter which finds the most suitable modelling method for each feature interaction. Different state-of-the-art deep CTR models can be viewed as instances of OptInter. To realize the functionality of OptInter, we also introduce a learning algorithm that automatically searches for the optimal modelling method. We conduct extensive experiments on four large datasets, including three public and one private. Experimental results demonstrate the effectiveness of OptInter. Because our OptInter finds the optimal modelling method for each feature interaction, our experiments show that OptInter improves the best performed state-of-the-art baseline deep CTR models by up to 2.21%. Compared to the memorized method, which also outperforms baselines, we reduce up to 91% parameters. In addition, we conduct several ablation studies to investigate the influence of different components of OptInter. Finally, we provide interpretable discussions on the results of OptInter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Click-through rate prediction is one of the core tasks in commercial recommender systems. It aims to predict the probability of a user clicking a particular item given user and item features. As feature interactions bring in non-linearity, they are widely adopted to improve the performance of CTR prediction models. Therefore, effectively modelling feature interactions has attracted much attention in both the research and industry field. The current approaches can generally be categorized into three classes: (i) na?ve methods, which do not model feature interactions and only use original features; (ii) memorized methods, which memorize feature interactions by explicitly viewing them as new features and assigning trainable embeddings; (iii) factorized methods, which learn latent vectors for original features and implicitly model feature interactions through factorization functions. Studies have shown that modelling feature interactions by one of these methods alone are suboptimal due to the unique characteristics of different feature interactions. To address this issue, we first propose a general framework called OptInter which finds the most suitable modelling method for each feature interaction. Different state-of-the-art deep CTR models can be viewed as instances of OptInter. To realize the functionality of OptInter, we also introduce a learning algorithm that automatically searches for the optimal modelling method. We conduct extensive experiments on four large datasets, including three public and one private. Experimental results demonstrate the effectiveness of OptInter. Because our OptInter finds the optimal modelling method for each feature interaction, our experiments show that OptInter improves the best performed state-of-the-art baseline deep CTR models by up to 2.21%. Compared to the memorized method, which also outperforms baselines, we reduce up to 91% parameters. In addition, we conduct several ablation studies to investigate the influence of different components of OptInter. Finally, we provide interpretable discussions on the results of OptInter.</p><p>Index Terms-Click-through Rate Prediction, Feature Interaction, Recommendation, Neural Architecture Search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The Click-through rate (CTR) prediction task is crucial for recommender systems, which aims to predict the probability of a certain user clicking on a recommended item (e.g. movie, ?This work is done when Fuyuan Lyu worked as an intern at Huawei Noah's Ark Lab.</p><p>*Co-first author with equal contribution. ?Corresponding authors. advertisement) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Many recommendations can therefore be performed based on the result of CTR prediction. For instance, to maximize the number of clicks, the items returned to a user can be ranked by predicted CTR (pCTR). Due to the powerful featur e representation learning ability, the mainstream of CTR prediction research is dominated by deep learning models. As an important research direction to improve deep CTR models, many methods of modelling effective feature interactions are proposed, such as <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>The simplest way of modelling feature interactions is feeding original features in Multi-layer Perceptron (MLP). Shown as an example in <ref type="figure" target="#fig_0">Figure 1</ref>(a), FNN <ref type="bibr" target="#b4">[5]</ref> directly feeds original features into MLP and relies on the capability of MLP to model feature interactions. The universal approximation rule has proved that MLP can mimic arbitrary functions given enough data and computation power <ref type="bibr" target="#b5">[6]</ref>. However, it is challenging for MLP to model low-rank feature interactions solely based on original features <ref type="bibr" target="#b6">[7]</ref>. Such a way of modelling feature interactions by MLP directly is referred to as na?ve methods in our paper. Another alternative to model feature interactions is memorizing them explicitly as new features, named as memorized methods. These methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref> which memorize all secondorder feature interactions as new features and feed them in shallow models (as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, feature interactions are treated as individual features and fed into the wide component), achieve superior performance than na?ve methods. The reason is that some feature interactions are served as strong signals such that memorizing them as new features makes correlated patterns much easier to capture. However, memorized methods are prone to overfitting as the new features (generated by feature interactions) are more sparse and have lower frequency than original features in the training set.</p><p>The final method is to model feature interactions via a factorization function, named as factorized method. It is originated from Factorization Machine (FM) <ref type="bibr" target="#b8">[9]</ref> and its variants <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Factorized methods implicitly model all second-order feature interactions by learning latent vectors of original features and aggregating them using a specific func-tion (e.g., inner-product) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> or learnable factorization function <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, as shown in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. Factorized methods alleviate the feature sparsity issue and are widely adopted by the mainstream deep CTR models <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. However, the latent vectors are used in both representation learning and feature interaction modelling, which tends to conflict with each other.</p><p>The above three methods (namely, na?ve, memorized, factorized methods) model all possible feature interactions in an identical way. However, as stated in <ref type="bibr" target="#b14">[15]</ref>, modelling feature interactions in the same way may lead to a suboptimal solution because the characteristics (e.g., complexity) of each feature interaction may not be identical. Hence, AutoML methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> are introduced to find an appropriate modelling method for each feature interaction <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. For instance, AutoFIS <ref type="bibr" target="#b14">[15]</ref> aims to select which feature interactions should be factorized and which ones should be ignored. In other words, AutoFIS makes a choice between factorized and na?ve adaptively for each individual feature interaction, while the option of memorized is neglected.</p><p>With the limitations of prior research observed, a datadriven strategy to automatically find an optimal method from na?ve, memorized, factorized methods for each feature interaction is required. This motivates us to propose a general framework called OptInter. For each feature interaction, OptInter selects the optimal modelling method from na?ve, memorized, factorized methods adaptively and automatically. Inspired by DARTS <ref type="bibr" target="#b15">[16]</ref> and previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, to efficiently search the optimal method for each feature interaction, we devise a two-stage learning algorithm. In the search stage, we aim to select the optimal method for each feature interaction from the search space na?ve, memorized, factorized, where the selection is treated as a neural architecture search problem. However, such a selection is discrete and makes the overall framework not differentiable. Therefore, instead of searching over three candidate modelling methods, we relax the choices to be continuous by approximating with Gumbel-softmax tricks <ref type="bibr" target="#b18">[19]</ref> via a set of architecture parameters (one for each modelling method with respect to a feature interaction). Then, the architecture parameters can be learned by gradient descent, which is jointly optimized with neural network weights. In the re-train stage, we select the modelling method with the largest probability for each feature interaction and re-train the model from scratch. The neural network weights obtained from the search stage are discarded to avoid the influence of suboptimal modelling methods.</p><p>Extensive experiments are conducted on four large-scale datasets, including three public datasets and one private dataset. Experimental results demonstrate OptInter consistently performs well on all datasets. Specifically, memorized method improves the best performed deep CTR model by 0.1%-2.2% in terms of the AUC score with the help of 18 times more parameters. Moreover, compared to the memorized method, OptInter achieves further improvement of AUC by 0.01%-0.25% while reduces about 18%-91% parameters. The results demonstrate the effectiveness of introducing memorized method in our search space and the efficiency of OptInter by selecting suitable modelling methods for individual feature interactions. Our ablation studies also show that our proposed search algorithm yields a more effective architecture than other search algorithms. Last but not least, we analyze the search architecture of OptInter from the perspective of information theory and provide interpretability. To sum up, the main contributions of this paper are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) We propose a novel deep CTR prediction framework</head><p>OptInter including na?ve, memorized, factorized feature interaction methods. To our best knowledge, OptInter is the first work to introduce memorized method in deep CTR models. Moreover, some mainstream deep CTR methods can be viewed as instances of OptInter. 2) As a part of OptInter, we propose a two-stage learning algorithm to select the optimal method for each feature interaction automatically. In the search stage, OptInter can learn the relative importance of each feature interaction method via architecture parameters. In the re-train stage, with the resulting optimal methods, we re-train the model from scratch to guarantee the neural network is not influenced by suboptimal methods. 3) Comprehensive experiments are conducted on three public datasets and one private dataset and show that OptInter outperforms the state-of-the-art deep CTR prediction models. The results demonstrate that OptInter is both effective and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In this section, we first formulate the problem of CTR prediction task and feature interaction modelling methods in Section II-A. Then we describe the proposed framework OptInter in Section II-B. Finally, we elaborate details of the learning algorithm for OptInter in Section II-C. To make our framework easier to understand, we list all the notations used in <ref type="table" target="#tab_0">Table I.</ref> A. Problem Formulation 1) CTR Prediction: A dataset for training CTR models consists of instances (X o , y), where y ? {0, 1} is the ground truth label and X o is a multi-field data instance including M original features</p><formula xml:id="formula_0">X o = [x o 1 , x o 2 , ..., x o M ],<label>(1)</label></formula><p>where x o i is the one-hot encoded vector for the feature value in the i-th original feature. The problem of CTR prediction is to predict the probability of a user clicking a certain item according to original features X o . Formally, a machine learning model to estimate the probability is defined as follows,</p><formula xml:id="formula_1">P(y = 1|X o ) = f (X o ; ?),<label>(2)</label></formula><p>where f is the model, ? indicates all the model parameters, and P is the conditional probability.  </p><formula xml:id="formula_2">factorized embedding for (x o i , x o j ) x m (i,j) cross-product transformed features for (x o i , x o j ) e m (i,j) memorized embedding for (x o i , x o j ) e n null embedding e b (i,j) optimal embedding for (x o i , x o j ) M number of original feature K searchable modelling space D dataset ? cross product ? model parameter ? architecture<label>parameter</label></formula><p>2) Feature Interaction: Feature interactions capture the correlation between different features and induce non-linearity to the model. As shown in existing works, it is crucial to model feature interactions to boost the model performance <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this section, we formally define the feature interaction and introduce three modelling methods.</p><formula xml:id="formula_3">Definition 1. A h-th order (1 ? h ? M ) feature interaction H is defined as a multivariate group (x o c1 , x o c2 , ..., x o c h ), where each feature x o ci is selected from original feature X o .</formula><p>Generally, there are three methods to define feature interaction:</p><p>(i) Factorized method: Given the latent vectors for original features E o , H can be modelled as <ref type="bibr" target="#b19">[20]</ref> </p><formula xml:id="formula_4">e f H = o (h?1) (...(o (1) (e o c1 , e o c2 )), ..., e o c h ), e o ci = E o x o ci .<label>(3)</label></formula><p>Namely, the factorized embedding e f H is generated by the utilizing h?1 operators o (1) (?), o (2) (?), .., o (h?1) (?) in order to aggregate h latent vectors e o c1 , e o c2 , ..., e o c h . Here the operators could be FM, inner product or etc.</p><p>(ii) Memorized method: Memorized embedding e m H can be defined as <ref type="bibr" target="#b20">[21]</ref> </p><formula xml:id="formula_5">e m H = E m x m H , x m H = onehot(x o c1 ?x o c2 ? ... ?x o c h ),<label>(4)</label></formula><p>wherex o ci is zero-padded original feature x o ci (zero-padding is introduced to align the dimension), ? is the cross-product operation, x m H is the cross-product transformed feature for H, E m is the embedding table for cross-product transformed features.</p><p>(iii) Na?ve method: the na?ve embedding e n H = e n can be defined as a zero vector with arbitrary length. Note that for different H, the na?ve embedding e n remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OptInter Framework</head><p>In this section, we elaborate the OptInter framework in detail, which is presented in <ref type="figure" target="#fig_1">Figure 2</ref>. The OptInter framework consists of four components: (i) the input layer (Section II-B1), (ii) the embedding layer (Section II-B2), (iii) the feature interaction layer (Section II-B3) and (iv) the classifier (Section II-B4). The feature interaction layer plays a central part in OptInter and greatly influences the performance, where the combination block searches the optimal method for each individual feature interaction. In OptInter, Equation 2 is specified to estimate the probability for input X o as follows,</p><formula xml:id="formula_6">y = f (X o |?, ?),<label>(5)</label></formula><p>where f (?) indicates the OptInter framework, ? indicates all the neural network parameters and ? indicates the architecture parameters. Such architecture parameters in combination block decide which modelling method to choose for each feature interaction. 1) Input Layer: In OptInter, the input layer contains a cross-product transformation block, which takes original features X o as input and generates one-hot encoded representation X m according to <ref type="bibr">Equation 4</ref>. Note that for M original features, there are C h M h-th order feature interactions. In Opt-Inter, we only consider the second-order feature interactions (namely, h = 2) for two reasons: (i) second-order feature interactions have been demonstrated to be the most important for prediction <ref type="bibr" target="#b17">[18]</ref> and (ii) modelling higher-order feature interactions exponentially increases the size of the embedding table and makes the current hardware difficult to train such large models. Note that although we only consider secondorder feature interactions in this paper, our methods could easily be extended to higher-order. The embeddings of all the second-order feature interactions are concatenated to form</p><formula xml:id="formula_7">X m = [x m (1,2) , x m (1,3) , ..., x m (M ?1,M ) ].<label>(6)</label></formula><p>The final input includes both original feature X o and crossproduct transformed features X m for feature interactions.</p><p>2) Embedding Layer: The embedding layer is to transform the one-hot encoded features into continuous embeddings. After going through input layer, the features contains two parts: (i) original features X o and (ii) cross-product transformed features X m . Both X o and X m are one-hot encoded and multi-field. All the features are in categorical form, where features in the numerical form are usually transformed into categorical form by bucketing <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. For univalent features (e.g., "Gender=Male"), we embed the one-hot encoding of each feature separately to a continuous vector using a linear embedding, i.e., the embedding e o i of feature i is</p><formula xml:id="formula_8">e o i = E o x o i , where E o is the embedding table for original features.</formula><p>For multivalent features (e.g., "Interest=Football, Basketball"), we keep the same procedure as <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> where all the embeddings of individual feature values are aggregated by mean pooling. The embeddings of the original features are concatenated to form</p><formula xml:id="formula_9">e o = [e o 1 , e o 2 , ..., e o M ].<label>(7)</label></formula><p>The cross-product transformed features X m for all feature interactions are also embedded to a continuous vector in the same way. For the cross-product transformed feature between original feature x o i and x o j , its embedding e m (i,j) = E m x m (i,j) , where E m is the embedding table for cross-product transformed features. All e m (i,j) s are concatenated to form</p><formula xml:id="formula_10">e m = [e m (1,2) , e m (1,3) , ..., e m (M ?1,M ) ].<label>(8)</label></formula><p>3) Feature Interaction Layer: The feature interaction layer is the nucleus of OptInter. As observed in <ref type="figure" target="#fig_1">Figure 2</ref>, we introduce a combination block to search for the optimal modelling method for each feature interaction. The combination block takes both the original feature embedding e o and the cross-product transformed feature embedding e m as inputs and generates embeddings e b for all the feature interactions with the methods searched by OptInter. For each feature interaction, we search its optimal modelling method from (i) memorized method with the memorized cross-product embedding e m ; (ii) factorized method with the original feature embedding e o and factorization function; and (iii) na?ve method which indicates this feature interaction is useless and does not need to modelling. We will elaborate the learning algorithm used to search the optimal method for each feature interaction in Section II-C. 4) Classifier: In the feature interaction layer, e o and e b are concatenated into a single vector e = [e o , e b ]. Similar to previous works <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, such vector e is fed into MLP, in which layer normalization and ReLU are applied. One such layer in MLP is defined as</p><formula xml:id="formula_11">a (l+1) = LN(relu(W (l) a (l) + b (l) )), a 0 = e,<label>(9)</label></formula><p>where a (l) , W (l) and b (l) are the input, model weight, and bias of the l-th layer. Activation ReLU is defined as</p><formula xml:id="formula_12">relu(z) = max(0, z),<label>(10)</label></formula><p>and layer normalization is defined as <ref type="bibr" target="#b21">[22]</ref> </p><formula xml:id="formula_13">LN(z) = z ? E[z] Var[x] + * ? + ?.<label>(11)</label></formula><p>Note that MLP(a (0) ) = a (h) , where h is the depth of MLP. Finally, the output of MLP, a (h) , is fed into a sigmoid unit, in order to re-scale the prediction value to a probability. Formally, the final predicted result i?</p><formula xml:id="formula_14">y = sigmoid(a (h) ) = 1 1 + e ?a (h) ? (0, 1),<label>(12)</label></formula><p>which indicates the probability of a specific user clicking on the item. To train our CTR prediction model, we use the crossentropy loss (i.e., log-loss) function</p><formula xml:id="formula_15">L(D|?, ?) = ? 1 |D| (x,y)?D CE(y,?), CE(y,?) = y log(?) + (1 ? y) log(1 ??),<label>(13)</label></formula><p>where D indicates the training dataset, ? includes all the neural network weights {W (l) , b (l) |1 ? l ? h} and embedding tables {E o , E m }, ? is the set of architecture parameters used to search the optimal feature interaction. ? will be discussed in Section II-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning Algorithm for OptInter</head><p>To find the optimal feature interaction method, we need to define a search space and devise an efficient search algorithm. In this section, we first define our search space consisting of memorized, factorized, and na?ve methods for each feature interaction. Then we introduce our search algorithm to find the optimal methods efficiently. At last, we present the re-train stage for final training. Our search space consists of these three methods to model feature interaction. Note that there exist a wide variety of factorization functions, such as Hadamard Product ?, Pointwise-Addition ? and Generalized-Product . In this paper, as we focus on searching the optimal modelling method (namely, memorized, factorized and na?ve) for each feature interaction, we take Hadamard Product ? as the representative for factorized methods, instead of considering all possible product operations. Our framework can be extended easily to taking multiple operations into account as factorized methods. Hadamard Product is element-wise multiplication over each element in the vectors. Formally, the Hadamard Product for feature e i and e j is</p><formula xml:id="formula_16">e f (i,j) = e o i ? e o j = [e 1 i ? e 1 j , e 2 i ? e 2 j , ..., e s i ? e s j ],<label>(14)</label></formula><p>where e t i is the t-th element of the embedding for feature i, s is the length of embedding. For each feature interation, we make the choice from 3 options. There are C 2 M = M (M ? 1)/2 second-order feature interactions in total. Therefore the total search space of OptInter is O(3 M (M ?1)/2 ), which is an incredibly huge space to search over.</p><p>The combination block is visualized in <ref type="figure" target="#fig_2">Figure 3</ref>. The final output e b (i,j) is chosen from: memorized cross-product embed-ding e m (i,j) , factorized embedding e f (i,j) or na?ve embedding e n (which is actually empty embedding) Update architecture parameter ? by descending gradient ? L(D train |?, ?) 7: end while 2) Search Algorithm: In this section, we propose a search algorithm for exploring the huge search space efficiently. Instead of searching over a discrete (categorical) set of selections on the candidate methods, which will make the whole framework not end-to-end differentiable, we approximate the discrete sampling via introducing the Gumbel-softmax operation <ref type="bibr" target="#b18">[19]</ref> in this work. The Gumbel-softmax operation provides a differentiable sampling, which makes the architecture parameters learnable by Adam optimizer.</p><formula xml:id="formula_17">e b (i,j) ? {e m (i,j) , e f (i,j) , e n }.<label>(15</label></formula><p>To be specific, suppose architecture parameters {? k (i,j) |k ? K} are the class probability over different feature interaction methods, K indicates the searchable space over feature interaction methods (namely, memorized, factorized, na?ve). Then a discrete selection z can be drawn via the gumbel-softmax trick <ref type="bibr" target="#b22">[23]</ref> as</p><formula xml:id="formula_18">z = onehot arg max k?K [log ? k (i,j) + g (i,j) ] , g (i,j) = ? log(? log(u (i,j) )), u (i,j) ? Uniform(0, 1).<label>(16)</label></formula><p>The gumbel noise g (i,j) is i.i.d. sampled, which aims to perturb the log term log ? k (i,j) and makes the argmax operation equivalent to drawing a sample by {? k (i,j) |k ? K} weights. However, the argmax operation makes this trick nondifferentiable. To deal with this problem, we replace the argmax operation with the softmax function</p><formula xml:id="formula_19">p k (i,j) = exp( log(? k (i,j) ) ? ) k?K exp( log(? k (i,j) ) ? ) ,<label>(17)</label></formula><p>where ? is the temperature parameter to control the smoothness of the Gumbel-softmax operation. When ? approximates to zero, the Gumbel-softmax operation approximately outputs a one-hot vector. With this softmax function, p k (i,j) is the probability of selecting the method k to model the feature interaction between feature i and j. The candidate embeddings for a feature interaction are {e f (i,j) , e m (i,j) , e n } ({f, m, n} indicates factorize, memorize and na?ve methods respectively). The output of combination module is formalized as the weighted sum over all the candidate embeddings of the current feature interaction</p><formula xml:id="formula_20">e b (i,j) = k?K p k (i,j) ? e k (i,j) = p m (i,j) ? e m (i,j) + p f (i,j) ? e f (i,j) + p n (i,j) ? e n .<label>(18)</label></formula><p>Then the feature interaction embeddings e b (i,j) are fed into the MLP classifier so that all these parameters can be optimized via gradient descent. To summarize, with the Gumbel-softmax tricks, the search procedure becomes end-toend differentiable.</p><p>To make the presentation more clear, we summarize the pseudo-code of the search algorithm in Algorithm 1. The parameters that need to be optimized during the search period are in two categories: (i) ?, the model parameters of OptInter, including the parameters of both Embedding tables and the MLP classifier; (ii) ?, the architecture parameters selecting the optimal feature interaction methods from the given search space.</p><p>Following previous research work <ref type="bibr" target="#b14">[15]</ref>, we update both model parameters ? and architecture parameters ? simultaneously instead of alternately. This is because, in CTR prediction, the predicted label? is highly sensitive towards the embedding table. Suppose the model parameters ? and architecture parameters ? are trained alternately. In that case, the overall framework is hard to converge (and therefore resulted in suboptimal performance) because a small disturb in ? leads to a significant change in ?. Moreover, we empirically compare the result of updating both model parameter ? and architecture parameter ? simultaneously and alternately in Section III-E, which demonstrates that our learning algorithm is more effective.</p><p>3) Re-train: In the search stage, architecture parameters also influence the model training. Re-training model with fixed architecture parameters can eliminate such influences of suboptimal modelling methods during the search stage. Hence, we introduce the re-train stage to fully train the model with the optimal method for each feature interaction that is found in the search stage.</p><p>During the re-train stage, the gumbel-softmax operation is no longer used. We select the optimal modelling method for each feature interaction with the largest weight, based on the learned parameter ?. This is formalized as Generate the predicted labels? via OptInter with current model parameter ? and the optimal architecture parameter ? * given Equation <ref type="formula" target="#formula_0">12</ref> 4:</p><formula xml:id="formula_21">e b (i,j) = e k * (i,j) , s.t. k * = arg max k?K ? k (i,j) .<label>(19)</label></formula><p>Calculate the cross-entropy loss L(D train |?, ? * ) over the mini-batch given Equation <ref type="formula" target="#formula_0">13</ref> 5:</p><p>Update model parameter ? by descending gradient</p><formula xml:id="formula_22">? L(D train |?, ? * ) 6: end while</formula><p>In all, we re-train the model following Algorithm 2 after obtaining the optimal modelling method for each feature interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Discussion</head><p>In this section, we discuss the relationship of OptInter with mainstream CTR models. We distinguish these models by the method and factorization function in feature interaction layer. As summarized in <ref type="table" target="#tab_0">Table III</ref>, all the models can be viewed as instances of our framework. Furthermore, some detailed conclusions can be observed as follows:</p><p>? According to the modelling methods they use in the feature interaction layer, these methods are grouped into four categorizes: na?ve methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>; memorized methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>; factorized methods [2]- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>; and hybrid methods <ref type="bibr" target="#b14">[15]</ref> (including OptInter). ? All the models can be viewd as instances of our OptInter framework. For instance, FNN <ref type="bibr" target="#b4">[5]</ref> is a deep model via the na?ve feature interaction method. IPNN <ref type="bibr" target="#b2">[3]</ref> factorizes feature interaction with inner-product function. ? Most of the deep CTR models adopt factorized methods, which only differ in the factorization functions. Our Opt-Inter is the first to introduce memorized method in search space for deep CTR models. We empirically demonstrate the benefits of introducing memorized method in deep CTR models. ? AutoFIS <ref type="bibr" target="#b14">[15]</ref> searches suitable feature interactions to be factorized, therefore it utilizes a hybrid modelling method over {factorized, na?ve}. The search space of OptInter is a superset of AutoFIS. Note that both OptInter and AutoFIS can be flexibly adopted to any factorzation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, to comprehensively evaluate our proposed framework, we design experiments to answer the following research questions:  A. Experiment Setup 1) Datasets: We conduct our experiments on three public datasets and one private dataset. The statistics of all datasets are given in <ref type="table" target="#tab_0">Table II</ref>. We describe all these datasets and the pre-processing steps below.</p><p>Criteo 1 dataset was used in a competition on click-through rate prediction jointly hosted by Criteo and Kaggle in 2014. 80% of the randomly shuffled data is used for training and validation, with 20% for testing. Both categorical features and cross-product transformed features with less than 20 times of appearance are set as a dummy feature out-of-vocabulary 1 https://labs.criteo.com/2013/12/download-terabyte-click-logs/ (OOV). Following <ref type="bibr" target="#b1">[2]</ref>, the continuous features are first normalized into [0, 1] via min-max normalization technique</p><formula xml:id="formula_23">x ? x ? x min x max ? x min<label>(20)</label></formula><p>and then multiplied with corresponding embeddings. Avazu 2 dataset was released as a part of a click-through rate prediction challenge jointly hosted by Avazu and Kaggle in 2014. 80% of the randomly shuffled data is used for training and validation, with 20% for testing. Categorical features with less than five times of appearance are replaced by OOV.</p><p>iPinYou 3 dataset was released as a part of the RTB Bidding Algorithm Competition, 2013. We only use the click data from seasons 2 and 3 because of the same data schema. We follow the previous data processing <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b3">4</ref> and remove "user tags" to prevent leakage.</p><p>Private dataset is collected from Huawei App Store, which samples from user behaviour logs in eight consecutive days. We select the first seven days as training and validation set and the last day as testing set. This dataset contains app features (e.g., App ID, category), user features (e.g., user's behaviour history) and context features.</p><p>2) Metrics: Following the previous works <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, we use the common evaluation metrics for CTR prediction, AUC (Area Under ROC) and Log loss (cross-entropy). To measure the model size, we take the number of parameters in the model as the metric. Note that in CTR prediction task, 0.1% improvement in terms of AUC is considered significant <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>3) Baseline Models: We select the most representative and state-of-the-art methods as our baselines: LR <ref type="bibr" target="#b23">[24]</ref> (logistic regression), FM [9] (factorization machine), Poly-2 <ref type="bibr" target="#b7">[8]</ref> (logistic regression with all second-order feature interaction), FNN <ref type="bibr" target="#b4">[5]</ref> (deep neural network), IPNN <ref type="bibr" target="#b2">[3]</ref> (deep neural network using inner-product as factorization function), DeepFM <ref type="bibr" target="#b1">[2]</ref> (deep neural network combined with factorization machine), PIN <ref type="bibr" target="#b3">[4]</ref> (deep neural network using sub-neural network as factorization function) and AutoFIS <ref type="bibr" target="#b14">[15]</ref> (deep neural network with automatically selected second-order feature interaction). As discussed in Section II-D, all these models can be viewed as instances of OptInter.</p><p>We also compare another two instances of our OptInter: OptInter-M and OptInter-F. OptInter-F only models feature interaction by factorizing latent vectors through the Hadamard product. OptInter-M models feature interactions only by memorizing them as new features. 4) Parameter Setup: To enable other researchers to reproduce our experiment results, we summarize all the hyperparameters for each model in <ref type="table" target="#tab_0">Table IV</ref>. We will make the source code publicly available upon the acceptance of this paper.</p><p>For Criteo, Avazu and iPinYou datasets, the parameters of baseline models are set following <ref type="bibr" target="#b3">[4]</ref>. For OptInter-F and OptInter-M models, we grid search for the optimal hyperparameters. Following <ref type="bibr" target="#b3">[4]</ref>, Adam optimizer and Xavier initialization <ref type="bibr" target="#b25">[26]</ref> are adopted. Xavier initialises the weights in the model such that their values are subjected to a uniform distribution between [? 6/(n in + n out ), 6/(n in + n out )] with n in and n out being the input and output sizes of a hidden layer. Such initialization has been proven to be able to stabilize activations and gradients in the early stage of training <ref type="bibr" target="#b3">[4]</ref>. Layer normalization <ref type="bibr" target="#b21">[22]</ref> has been applied to each fully connected layer to avoid the internal covariate shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Significance Test: Following previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we calculate the p-values for OptInter and the best baseline by repeating the experiments ten times and performing a twotailed pairwise t-test. 6) Platform: All experiments are conducted on a Linux server with 18 Intel Xeon Gold 6154 cores, 128 GB memory and one Nvidia-V100 GPU with PCIe connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Performance (RQ1 &amp; RQ2)</head><p>The overall performance of our OptInter and the baselines on four datasets are reported in <ref type="table" target="#tab_6">Table V</ref>. We also report the details of selected architectures in <ref type="table" target="#tab_0">Table VI</ref>. Based on our previous taxonomy in Section II-D, we group these models into four categories: na?ve, factorized, memorized and hybrid. We can make the following observations from <ref type="table" target="#tab_6">Table V</ref>.</p><p>First, our OptInter is both effective and efficient by automatically searching the optimal feature interaction methods. On Criteo and Avazu dataset, it achieves better performance than the best baseline (OptInter-M) but requires only approximately 50% to 80% of the model parameters. On iPinYou dataset, it significantly improves the model performance compared with the best baseline model OptInter-M with less than 10% of the model parameters. Finally, on the Private dataset, it significantly improves the model performance compared with the best baseline model OptInter-M with around 40% of the model parameters.</p><p>Secondly, the memorized method (namely, OptInter-M) is effective in the deep CTR model. On all the four datasets, OptInter-M is the best performed baseline. This verifies the necessity of involving memorized method in the search space of OptInter.</p><p>Finally, we observe that simply memorized all feature interaction results in a very large model. Particularly, the number of parameters for OptInter-M increases 10? ? 20? compared with other baseline models. Across different datasets, our OptInter selects suitable and necessary feature interactions to be memorized. According to Table VI, OptInter selects only 36%, 40% and 20% feature interactions to be memorized on three public datasets respectively. OptInter is hence 2?, 1.22? and 11.11? smaller than OptInter-M in terms of model size. With smaller model size, the performance of OptInter is still higher than (or at least comparable to) OptInter-M, which demonstrates that OptInter is able to find suitable and necessary feature interactions to be memorized. Careful readers may find that OptInter memorizes 40% of the feature interactions but reduces only 20% model size compared to OptInter-M, on Avazu dataset. The reason is that the feature Device ID on Avazu dataset includes many distinct feature values, such that the feature interactions involving Device ID have significantly more unique values than other feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed comparison with na?ve and factorized methods (RQ3)</head><p>From <ref type="table" target="#tab_6">Table V</ref>, OptInter achieves much better performance than na?ve and factorized methods, at the cost of more parameters on Criteo and Avazu datasets. In this section, we compare represents the number of parameters for this model. * denotes statistically significant improvement (measured by t-test with p-value &lt; 0.005) over the best baseline. We group all the models into four categories: na?ve methods, factorize methods, memorize methods and hybrid methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE VI METHOD SELECTION FOR DIFFERENT FEATURE INTERACTIONS</head><p>OptInter with na?ve and factorized models given roughly the same amount of model parameters.</p><p>We conduct this comparison experiments on Criteo and Avazu datasets, as OptInter maintains comparable number of parameters as na?ve and factorized methods on iPinYou dataset. The number of parameters of baseline na?ve and factorized methods is increases by enlarging the embedding size (20? and 17.5? for Criteo and Avazu).</p><p>The experimental result of such comparison is reported in <ref type="table" target="#tab_0">Table VII</ref>. As can be observed, enlarging embedding size does not bring significant performance improvement for na?ve and factorized methods. Moreover, the model performance is even worse with a larger embedding size (e.g., FM and DeepFM) due to overfitting. With the same amount of parameters, OptInter still significantly outperforms na?ve and factorized methods. The phenomenon indicates that when more space resource is available, enlarging embedding size is not always a good way to utilize such extra space to improve model performance. Instead, the experimental result demonstrates that increasing the number of parameters by searching suitable and necessary feature interactions to memorize as few features (as OptInter does) is more effective to boost performance when extra space resource is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed comparison with memorized methods (RQ3)</head><p>As demonstrated in Section III-B, OptInter-M is the best baseline. In this section, we further compare OptInter and OptInter-M in terms of effectiveness (measured by AUC) and efficiency (measured by the number of parameters) on Criteo and Avazu datasets. As is shown in <ref type="figure" target="#fig_3">Figure 4</ref>, three models are compared: OptInter-M(10) sets the embedding size for memorized embeddings to 10, OptInter(10) and OptInter(5) sets the embedding size for memorized embeddings to 10 and 5. In <ref type="figure" target="#fig_3">Figure 4</ref>(a), AUC varies from 0.8091 to 0.8101 and the number of parameters varies from 50M to 225M. In <ref type="figure" target="#fig_3">Figure  4</ref>(b), AUC varies from 0.8045 to 0.8065 and the number of parameters varies from 367M to 1012M. From <ref type="figure" target="#fig_3">Figure 4</ref>, the following observations can be made. First, OptInter outperforms OptInter-M under many cases with much fewer parameters. This is because OptInter finds suitable and necessary feature interactions to memorize, instead of memorizing all of them as by OptInter-M. Second, the curves of OptInter indicate that its performance degrades dramatically when the number of parameters shrinks below a certain threshold. This reason is that some feature interactions are strong signals that must be memorized to guarantee good performance (some such example feature interactions will be illustrated in Section III-G2). Third, reducing the embedding size for cross-product transformed features can significantly decrease the number of parameters, with only a slight performance drop. This phenomenon suggests that when the space resource is constrained, reducing the embedding size of memorized embeddings is better than throwing away identified feature interactions to be memorized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation study on the search stage (RQ4)</head><p>In this section, we investigate how the search stage affects model performance on public datasets. We compare two search algorithms: Bi-level and OptInter. Bi-level search algorithm updates model parameters and architecture parameters alternatively while the search algorithm in OptInter optimizes these two families of parameters jointly. Bi-level has been widely adopted in neural architecture search domain <ref type="bibr" target="#b15">[16]</ref>. However, as discussed in Section II-C, it may be suboptimal to update both model parameters and architecture parameters alternately as a minor updating in architecture parameters leads to a significant change in the model parameters (as also stated in <ref type="bibr" target="#b14">[15]</ref>). As a baseline, we also report the result of performing a random search (namely random). With random search, we randomly assign each feature interaction with one of the three modelling methods. Then the mean values of all the metrics are reported by repeating the random search ten times.</p><p>Observed from <ref type="table" target="#tab_0">Table VIII</ref>, both two search algorithms, Bilevel and OptInter, outperform randomly generated architecture, which demonstrates the effectiveness of the search stage. The OptInter performs better than Bi-level method, which verifies our previous discussions on how on optimize model parameters and architecture parameters.</p><p>Note that Bi-level optimization requires approximate ? 2? GPU memory compared with our search algorithm. Therefore, its experiment on the Avazu dataset cannot be accomplished due to the limits of GPU memory.</p><p>F. Ablation study on the re-train stage (RQ4)</p><p>In this section, we investigate how the re-train stage affects the model performance. We compare OptInter performance under different settings with and without re-train stage, of which the result is presented in <ref type="table" target="#tab_0">Table IX</ref>. It is observed that the re-train stage significantly improves the model performance. Without re-training, the candidate methods to model feature interactions may influence each other, which makes the neural network parameters ? suboptimal before the learning process of architecture parameters converges. Re-training makes neural network parameters optimal according to the suitable modelling method for each feature interaction decided in the search stage. These two sub-sections together demonstrate the effectiveness of the two-stage learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Interpretable Discussions (RQ5)</head><p>In this section, we investigate which kind of feature interactions are selected by OptInter for each method (namely, memorize, factorize and na?ve methods) on Criteo and Avazu. We distinguish feature interactions with mutual information scores between feature interactions and labels. For feature interactions {H = (x o i , x o j )} and ground truth labels y (y ? y), the mutual information between them is defined as MI({H}, y) = ? P(y) log P(y)</p><formula xml:id="formula_24">+ P(H, y) log P(y|H),<label>(21)</label></formula><p>where the first term is the marginal entropy and the second term is the conditional entropy of ground truth labels y given feature interaction H = (x o i , x o j ). Note that feature interactions with high mutual information scores are more informative (hence more important) to the prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Overall Comparison:</head><p>We group feature interactions according to each individual method in OptInter. We calculate and compare the mean mutual information scores of the feature interactions modelled by each method, as presented in <ref type="figure" target="#fig_4">Figure 5</ref>. Three conclusions can be observed. (i) OptInter tends to memorize feature interactions with higher mutual information scores, which is reasonable because treating informative feature interactions as new features makes learning them better. (ii) OptInter removes the feature interactions with low mutual information scores, which helps remove uninformative and noisy feature interactions. (iii) The characteristics of factorized feature interactions in OptInter varies with respect to different datasets. To summarize, the overall trend of feature interactions selected by different methods is consistent with their mutual information scores, which explains the intuition of OptInter. However, it is hard to assign a modelling method to each feature interaction based on heuristics as it may not be optimal, which motivates an automatic framework to do so.</p><p>2) Case Study: As a case study, we investigate the selected method for each feature interaction OptInter method on the Avazu dataset, which is shown in <ref type="figure">Figure 6</ref>. <ref type="figure">Figure 6(a)</ref> shows the heat map of mutual information scores of all the feature interactions, which represents how informative each feature interaction is in predicting the label. <ref type="figure">Figure 6(b)</ref> shows the searched method for each feature interaction. As can be seen, these two maps are positively correlated to each other, which indicates that OptInter obtains the optimal method for each feature interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Interaction in CTR Prediction</head><p>Feature interaction is one of the core problems in CTR prediction <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Feature interaction is one of the core problems in CTR prediction <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Generalized linear models like Logistic Regression (LR) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref> model feature interaction naively, i.e., they do not model feature interactions. One way to model all feature interactions is based on degree-2 polynomial mappings <ref type="bibr" target="#b7">[8]</ref>. It increases model performance, but it also leads to a combinatorial explosion. One possible solution is to design feature interactions manually. However, this requires experts' experience and is hard to generalize. Other methods like FM <ref type="bibr" target="#b8">[9]</ref> and its variants <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> models the low order feature interactions by factorizing them with latent vectors.</p><p>With the success of deep learning in computer vision and natural language processes, the deep CTR prediction model has gained tremendous attention in recent years. Compared with the linear model, deep models can predict CTR more effectively. Usually, deep CTR models choose to model feature interaction into three classes: na?ve <ref type="bibr" target="#b4">[5]</ref>, memorized <ref type="bibr" target="#b0">[1]</ref>, and factorized [2]- <ref type="bibr" target="#b3">[4]</ref>. FNN <ref type="bibr" target="#b4">[5]</ref>, as an example of na?ve modelling, introduces multi-layer perceptron to predict CTR scores with original features. However, such modelling method usually leads to lower model capacity (compared to the other two methods) because it cannot learn low-rank feature interactions well <ref type="bibr" target="#b6">[7]</ref>.</p><p>Memorized methods model feature interaction by memorizing them explicitly as new features. Wide&amp;Deep <ref type="bibr" target="#b0">[1]</ref>, as an example, indicates that the memorized method is a promising alternative for strong feature interactions. More specifically, Wide&amp;Deep [1] memorizes a manually selected set of feature interactions and feeds it into its wide component. In the experiments of <ref type="bibr" target="#b0">[1]</ref>, the model structure for Google Play selects the feature interaction (User Installed App, Impression App) as new feature for its wide component. This is a powerful indicator for a user downloading an app, as the installation history indicates user preference. However, this requires experts' experience and is hard to generalize. Moreover, memorized method induces a severe feature sparsity problem because the occurrence of many new features (generated by feature interactions) is relatively less than original features in the training set. The severe feature sparsity problem makes these infrequent new features difficult to learn and degrades the model performance.</p><p>Due to this reason, factorized methods are proposed to model feature interactions via a factorization function. Mainstream deep CTR models <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> usually adopt factorized method to model feature interactions. For example, IPNN and OPNN <ref type="bibr" target="#b2">[3]</ref> use inner-product and outer-product as the factorization function, respectively. DeepFM <ref type="bibr" target="#b1">[2]</ref> borrows the FM layer from Factorization Machine <ref type="bibr" target="#b8">[9]</ref> and uses that as a factorization function. PIN <ref type="bibr" target="#b3">[4]</ref>, improving on IPNN and OPNN <ref type="bibr" target="#b2">[3]</ref>, uses a small MLP component to model as a learnable factorization function. Factorized methods alleviate feature sparsity issue as they assign learnable parameters to original features instead of assigning them to new features (generated by feature interactions) with a much larger feature space. However, these methods have their inherited drawbacks. As the feature interactions are modelled by the latent vectors of original features, the latent vectors take the responsibility of both representation learning and feature interaction modelling. These two tasks may conflict with each other so that the model performance is bounded, as observed in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>(a) Heatmap of mutual information between each feature interaction and the label (b) Obtained optimal methods for feature interactions <ref type="figure">Fig. 6</ref>. An example of interpretability on Avazu Dataset. In both subfigures, the number indicates the field ID. In subfigure (a), the color indicates the strength of feature interaction in predicting label. In subfigure (b), different colors indicate three modelling methods searched by OptInter.</p><p>Recently CAN <ref type="bibr" target="#b13">[14]</ref> highlights the importance of memorized method. It manually selects a subset of feature interactions to memorize and prove its effectiveness through extensive experiments. But their model design still makes CAN be a factorized method.</p><p>OptInter is the first work introducing the memorized method in deep CTR models (although Wide&amp;Deep uses memorized method, it applies memorized method to its wide component only). Furthermore, OptInter is general enough to unify mainstream deep CTR models under its framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural architecture search (NAS) and its application in Recommender System</head><p>Neural architecture search (NAS) aims at automatically finding an optimal architecture for specific tasks and datasets, which avoids manual design <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>. These methods could be used to find optimal network structures, loss functions or hyper-parameters, significantly reducing human intervention in the ML pipeline. These methods can be categorized into three classes: (i) reinforcement learning-based methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> train an external controller (like RNN or reinforcement learning agent) to design cell structure for a specific model architecture; (ii) evolutionary algorithms <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref> that try to evolve an optimal architecture by mutating the top-k architectures and explore new potential models; (iii) gradient-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref> relax the search space to be continuous instead of searching on a discrete set of architectures. Such relaxation allows the searching process to be more efficient based on a gradient descent optimizer.</p><p>Recently, NAS methods have attracted much attention in the CTR prediction task. AutoFIS <ref type="bibr" target="#b14">[15]</ref> utilizes GRDA Optimizer <ref type="bibr" target="#b24">[25]</ref> to select proper feature interactions. AutoFeature <ref type="bibr" target="#b17">[18]</ref> adopts a tree of Naive Bayes classifiers to find suitable feature interaction functions for various fields. Au-toPI <ref type="bibr" target="#b19">[20]</ref> extends the search space to computational graph and feature interaction functions to achieve higher generalization. Many research works <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> aim to select suitable dimensions for various fields. AutoDis <ref type="bibr" target="#b34">[35]</ref> focuses on modelling continuous feature and proposes a framework to select optimal discretization methods. AutoFT <ref type="bibr" target="#b35">[36]</ref> proposes an end-to-end transfer learning framework to automatically determine transfer policy in CTR prediction via Gumbelsoftmax tricks <ref type="bibr" target="#b18">[19]</ref>. AutoLoss <ref type="bibr" target="#b36">[37]</ref> proposes a framework that learn sample-specific loss function via bi-level optimization.</p><p>OptInter has its uniqueness compared with other research works that apply NAS techniques into the CTR prediction task. Compared with the works which focus on modelling feature interaction <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, OptInter introduces memorized methods into its search space and searches within a broader space than existing works.</p><p>V. CONCLUSION In this paper, we proposed a novel deep CTR prediction framework named OptInter, which is the first to introduce the memorized feature interaction method in deep CTR models.</p><p>OptInter can search and identify the optimal method to model the feature interactions from na?ve, memorized and factorized. To achieve this, we first proposed a deep framework that unifies mainstream deep CTR models. Then, as a part of OptInter, a two-stage learning algorithm is proposed. In the search stage, the search process is modelled as an architecture search problem solved efficiently by neural architecture search techniques. During the re-train stage, the model is re-trained from scratch to achieve better performance. Extensive experiments on four large-scale datasets demonstrate the superior performance of OptInter. Several ablation studies show our method is effective in improving prediction performance and efficient in model size. Moreover, we also explain obtained results in the view of mutual information, which further highlights our method learns the optimal feature interaction methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of modeling feature interactions: Na?ve, Memorized and Factorized</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The OptInter CTR Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed illustration figure about combination block 1) Search Space: As stated in Section II-D, we categorize feature interaction methods into three classes: (i) memorized methods treat each feature interaction as a new feature and explicitly assign trainable weights or embedding. (ii) factorized methods model a feature interaction via factorization methods on latent vectors of original features. (iii) na?ve methods feed the original features into MLP to model their interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of efficiency-effectiveness trade-off for different datasets. Efficiency is measured by the size of parameters, shown in X-axis. Effectiveness is measured by the AUC score, shown in Y-axis. Here OptInter-M(X) indicates the embedding size of memorized embedding in OptInter-M model being X, OptInter(Y) indicates the embedding size of memorized embedding in OptInter model being Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Overall mutual information score of each method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>SELECTED NOTATIONS IN THIS PAPER</cell></row><row><cell cols="2">Notations Descriptions</cell></row><row><cell>y</cell><cell>ground truth label</cell></row><row><cell>y</cell><cell>predicted result</cell></row><row><cell>X o</cell><cell>original features</cell></row><row><cell>X m</cell><cell>cross-product transformed features</cell></row><row><cell>x o i e o i e f (i,j)</cell><cell>original feature i embedding for original feature i</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Algorithm 1 The Optimization of Search StageInput: Training dataset D consists of original features (x 1 , .., x M ) and ground-truth labels y Output: the searched optimal architecture parameter ? * 1: while not converge do</figDesc><table><row><cell>2:</cell><cell>Sample a mini-batch of training data D train</cell></row><row><cell>3:</cell><cell>Generate the predicted labels? via OptInter with</cell></row><row><cell></cell><cell>model parameter ? and architecture parameter ?</cell></row><row><cell></cell><cell>given Equation 12</cell></row><row><cell>4:</cell><cell>Calculate the cross-entropy loss L(D train |?, ?) over</cell></row><row><cell></cell><cell>the mini-batch given Equation 13</cell></row><row><cell>5:</cell><cell>Update model parameter ? by descending gradient</cell></row></table><note>? L(D train |?, ?)6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 2The Optimization of Re-train Process Input: Training dataset D and searched optimal architecture parameter ? * Output: the well-trained model parameter ? * 1: while not converge do</figDesc><table><row><cell>2:</cell><cell>Sample a mini-batch of training data D train</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc>? 10 5 7.1 ? 10 7 0.17 Note: #cont refers to the number of the continuous original feature, #cate refers to the number of the categorical original feature, #cross refers to the number of the cross-product transformed features, #orig value refers to the number of unique values for original features, #cross value refers to the number of unique values for cross-product transformed features, pos ratio refers to the positive ratio.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">DATASET STATISTICS</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>#samples</cell><cell cols="6">#cont #cate #cross #orig value #cross value pos ratio</cell></row><row><cell>Criteo</cell><cell>4.6 ? 10 7</cell><cell>13</cell><cell>26</cell><cell>325</cell><cell>5.1 ? 10 5</cell><cell>3.7 ? 10 7</cell><cell>0.23</cell></row><row><cell>Avazu</cell><cell>4.0 ? 10 7</cell><cell>0</cell><cell>24</cell><cell>276</cell><cell>1.2 ? 10 6</cell><cell>2.4 ? 10 8</cell><cell>0.17</cell></row><row><cell>iPinYou</cell><cell>1.9 ? 10 7</cell><cell>0</cell><cell>16</cell><cell>120</cell><cell>9.4 ? 10 5</cell><cell>6.8 ? 10 7</cell><cell>0.0008</cell></row><row><cell>Private</cell><cell>8.0 ? 10 8</cell><cell>0</cell><cell>9</cell><cell>36</cell><cell>4.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">MODEL DISCUSSION</cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>Model</cell><cell cols="2">Feature Interaction Layer Method Func.</cell><cell>Classifier</cell></row><row><cell>na?ve</cell><cell>LR [24] FNN [5]</cell><cell>{n} {n}</cell><cell>--</cell><cell>Shallow Deep</cell></row><row><cell>memorized</cell><cell>Poly2 [8] Wide&amp;Deep [1]</cell><cell>{m} {m}</cell><cell>--</cell><cell>Shallow S&amp;D</cell></row><row><cell></cell><cell>FM [9] FwFM [11]</cell><cell>{f } {f }</cell><cell>e o i , e o j i , e o e o j w (i,j)</cell><cell>Shallow Shallow</cell></row><row><cell>factorized</cell><cell>FmFM [12] IPNN [3] OPNN [3] DeepFM [2] PIN [4]</cell><cell>{f } {f } {f } {f } {f }</cell><cell>e o i W (i,j) e o j e o i , e o j e o i , e o j ? e o i , e o j net(e o i , e o j ) T</cell><cell>Shallow Deep Deep Deep Deep</cell></row><row><cell>hybrid</cell><cell>AutoFIS [15] OptInter</cell><cell>{n, f } {n, m, f }</cell><cell>flexible flexible</cell><cell>Deep Deep</cell></row></table><note>Note: Category refers to which category does the model belong to, which is determined by how it model feature interaction. All models can generally be categorized into four classes: na?ve, memorized, factorized and hybrid. Method denotes the potential methods, with n, m, f denoting na?ve, memorized, factorized methods respectively. The Method is fixed unless the model belongs to Hybrid category. Func. refers to the factoriza- tion function, which is only meaningful for factorized method. net refers to a neural network. S&amp;D is short for shallow and deep.? RQ2: How effective is the memorized method in deep CTR models?? RQ3: Is the good performance achieved by OptInter due to the increased amount of parameters?? RQ4: How effective is two-stage learning algorithm in OptInter (namely, search and re-train)?? RQ5: What kind of feature interactions is selected by each modeling method in OptInter?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">PARAMETER SETUP</cell><cell></cell></row><row><cell>Params</cell><cell>Criteo</cell><cell>Avazu</cell><cell>iPinYou</cell></row><row><cell></cell><cell>bs=2000</cell><cell>bs=2000</cell><cell>bs=2000</cell></row><row><cell></cell><cell>opt=Adam</cell><cell>opt=Adam</cell><cell>opt=Adam</cell></row><row><cell>General</cell><cell>lr=5e-4</cell><cell>lr=5e-4</cell><cell>lr o=1e-5</cell></row><row><cell></cell><cell>l2 o=0.0</cell><cell>l2 o=0.0</cell><cell>l2 o=1e-6</cell></row><row><cell></cell><cell>eps=1e-8</cell><cell>eps=1e-8</cell><cell>eps=1e-4</cell></row><row><cell>LR</cell><cell>-</cell><cell>-</cell><cell>lr o=1e-4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>s1=20</cell></row><row><cell>FM</cell><cell>s1=20</cell><cell>s1=40</cell><cell>lr o=1e-4 l2 o=1e-9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>eps=1e-6</cell></row><row><cell>Poly-2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FNN IPNN</cell><cell>s1=20 net=[700 ? 5] LN=true</cell><cell>s1=40 net=[500 ? 5] LN=true</cell><cell>s1=20 net=[300 ? 3] LN=true</cell></row><row><cell></cell><cell>s1=20</cell><cell>s1=40</cell><cell>s1=20</cell></row><row><cell>DeepFM</cell><cell>lr=5e-4 net=[700 ? 5]</cell><cell>lr=5e-4 net=[500 ? 5]</cell><cell>l2 o=1e-7 net=[300 ? 3]</cell></row><row><cell></cell><cell>LN=true</cell><cell>LN=true</cell><cell>LN=true</cell></row><row><cell>PIN</cell><cell>s1=20 net=[700 ? 5] sub-net=[40,5] LN=true</cell><cell>s1=40 net=[500 ? 5] sub-net=[40,5] LN=true</cell><cell>s1=20 net=[300 ? 3] sub-net=[40,5] LN=true l2 o=1e-9</cell></row><row><cell>AutoFIS</cell><cell cols="3">mu=0.8, c=5e-4 mu=0.8, c=5e-4 mu=0.535, c=5e-3</cell></row><row><cell></cell><cell>s1=20, s2=10</cell><cell>s1=40, s2=4</cell><cell>s1=20, s2=2</cell></row><row><cell>OptInter-M OptInter-F</cell><cell>lr o=lr c=1e-4 l2 c=3e-8 net=[700 ? 5]</cell><cell>lr o=lr c=1e-4 l2 c=3e-8 net=[500 ? 5]</cell><cell>lr c=1e-6 l2 c=3e-8 net=[300 ? 3]</cell></row><row><cell></cell><cell>LN=true</cell><cell>LN=true</cell><cell>LN=true</cell></row><row><cell>OptInter</cell><cell>lr a=3e-5</cell><cell>lr a=3e-5</cell><cell>lr a=1e-3</cell></row><row><cell cols="4">Note: bs=batch size, opt=optimizer, lr o=learning rate for original feature</cell></row><row><cell cols="4">embedding table and neural network parameter, lr c=learning rate for</cell></row><row><cell cols="4">feature combination embedding table, lr a=learning rate for architec-</cell></row><row><cell cols="4">ture parameters, l2 o=l 2 regularization on original feature embedding</cell></row><row><cell cols="4">table, l2 c=l 2 regularization on feature combination embedding table,</cell></row><row><cell cols="4">s1=embedding size for original feature, s2=embedding size for cross-</cell></row><row><cell cols="4">product transformed feature, net=MLP structure, LN=layer normalization,</cell></row><row><cell cols="3">mu and c are parameters in GRDA optimizer [25].</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">OVERALL PERFORMANCE COMPARISON</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>Criteo</cell><cell></cell><cell></cell><cell>Avazu</cell><cell></cell><cell></cell><cell>iPinYou</cell><cell></cell><cell></cell><cell>Private</cell><cell></cell></row><row><cell>Metric</cell><cell>AUC</cell><cell cols="2">Log loss Param.</cell><cell>AUC</cell><cell>Log loss</cell><cell>Param.</cell><cell>AUC</cell><cell>Log loss</cell><cell>Param.</cell><cell>AUC</cell><cell cols="2">log Loss Param.</cell></row><row><cell>LR</cell><cell>0.7785</cell><cell>0.4708</cell><cell>0.5M</cell><cell>0.7685</cell><cell>0.3862</cell><cell>1.2M</cell><cell>0.7554</cell><cell>0.005689</cell><cell>0.9M</cell><cell>0.7690</cell><cell>0.3836</cell><cell>0.4M</cell></row><row><cell>FNN</cell><cell>0.7995</cell><cell>0.4514</cell><cell>13M</cell><cell>0.7858</cell><cell>0.3761</cell><cell>51M</cell><cell>0.7780</cell><cell>0.005622</cell><cell>19M</cell><cell>0.8348</cell><cell>0.3353</cell><cell>32M</cell></row><row><cell>FM</cell><cell>0.7845</cell><cell>0.4681</cell><cell>10M</cell><cell>0.7826</cell><cell>0.3790</cell><cell>49M</cell><cell>0.7776</cell><cell>0.005573</cell><cell>19M</cell><cell>0.8304</cell><cell>0.3406</cell><cell>32M</cell></row><row><cell>IPNN</cell><cell>0.8005</cell><cell>0.4504</cell><cell>13M</cell><cell>0.7885</cell><cell>0.3745</cell><cell>51M</cell><cell>0.7784</cell><cell>0.005628</cell><cell>19M</cell><cell>0.8410</cell><cell>0.3303</cell><cell>32M</cell></row><row><cell>DeepFM</cell><cell>0.7997</cell><cell>0.4511</cell><cell>13M</cell><cell>0.7860</cell><cell>0.3760</cell><cell>51M</cell><cell>0.7791</cell><cell>0.005617</cell><cell>19M</cell><cell>0.8383</cell><cell>0.3325</cell><cell>32M</cell></row><row><cell>PIN</cell><cell>0.8016</cell><cell>0.4510</cell><cell>17M</cell><cell>0.7826</cell><cell>0.3790</cell><cell>52M</cell><cell>0.7782</cell><cell>0.005624</cell><cell>20M</cell><cell>0.8331</cell><cell>0.3365</cell><cell>33M</cell></row><row><cell>OptInter-F</cell><cell>0.8003</cell><cell>0.4507</cell><cell>21M</cell><cell>0.7860</cell><cell>0.3761</cell><cell>56M</cell><cell>0.7762</cell><cell>0.005688</cell><cell>23M</cell><cell>0.8380</cell><cell>0.3325</cell><cell>37M</cell></row><row><cell>Poly2</cell><cell>0.7827</cell><cell>0.4751</cell><cell>22M</cell><cell>0.7860</cell><cell>0.3795</cell><cell>241M</cell><cell>0.7740</cell><cell>0.005578</cell><cell>69M</cell><cell>0.8307</cell><cell>0.3390</cell><cell>71M</cell></row><row><cell>OptInter-M</cell><cell>0.8094</cell><cell>0.4423</cell><cell>225M</cell><cell>0.8060</cell><cell>0.3638</cell><cell>1012M</cell><cell>0.7800</cell><cell>0.005640</cell><cell>296M</cell><cell>0.8415</cell><cell>0.3265</cell><cell>738M</cell></row><row><cell>AutoFIS</cell><cell>0.8014</cell><cell>0.4514</cell><cell>22M</cell><cell>0.7861</cell><cell>0.3758</cell><cell>51M</cell><cell>0.7790</cell><cell>0.005618</cell><cell>19M</cell><cell>0.8413</cell><cell>0.3299</cell><cell>32M</cell></row><row><cell>OptInter</cell><cell>0.8101  *</cell><cell>0.4417  *</cell><cell>100M</cell><cell>0.8062  *</cell><cell>0.3637  *</cell><cell>827M</cell><cell>0.7825  *</cell><cell>0.005604  *</cell><cell>26M</cell><cell>0.8425  *</cell><cell>0.3256</cell><cell></cell></row></table><note>* 302M Param.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>COMPARISON WITH na?ve AND factorized MODELS UTILIZING THE SAME AMOUNT OF PARAMETERS Orig.E. stands for embedding size of original features. Cross.E. stands for embedding size of cross-product transformed features. Param. represents the number of model parameters. * denotes statistically significant improvement (measured by t-test with p-value &lt; 0.005 over the best baseline.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>Criteo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avazu</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>FM</cell><cell>FNN</cell><cell>IPNN</cell><cell>DeepFM</cell><cell>OptInter</cell><cell>FNN</cell><cell>FM</cell><cell>IPNN</cell><cell>DeepFM</cell><cell>OptInter</cell></row><row><cell>AUC</cell><cell cols="3">0.7543 0.7990 0.8014</cell><cell>0.7678</cell><cell>0.8101  *</cell><cell cols="3">0.7677 0.7848 0.7923</cell><cell>0.7691</cell><cell>0.8062  *</cell></row><row><cell>log loss</cell><cell cols="3">0.5192 0.4516 0.4495</cell><cell>0.5075</cell><cell>0.4417  *</cell><cell cols="3">0.3947 0.3768 0.3723</cell><cell>0.3934</cell><cell>0.3637  *</cell></row><row><cell>Orig.E.</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>20</cell><cell>700</cell><cell>700</cell><cell>700</cell><cell>700</cell><cell>40</cell></row><row><cell>Cross.E.</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell></row><row><cell>Param.</cell><cell>103M</cell><cell>109M</cell><cell>109M</cell><cell>109M</cell><cell>100M</cell><cell>860M</cell><cell>953M</cell><cell>954M</cell><cell>953M</cell><cell>827M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>Arch stands for the searched architecture. [x,y,z] indicates the number of feature interactions that are selected to perform memorized, factorized and na?ve methods. Param. represents the number of parameters for this model. Bi-level refers to the result of architectures searched by bilevel optimization. For Random, the mean performance of ten randomly generated architectures are reported here.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">BETWEEN DIFFERENT SEARCH ALGORITHM</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>AUC</cell><cell>log loss</cell><cell>Arch</cell><cell>Param.</cell></row><row><cell></cell><cell>Random</cell><cell>0.8089</cell><cell>0.3764</cell><cell>-</cell><cell>84M</cell></row><row><cell>Criteo</cell><cell>Bi-level</cell><cell>0.8099</cell><cell>0.3741</cell><cell>[114,109,104]</cell><cell>95M</cell></row><row><cell></cell><cell>OptInter</cell><cell>0.8101</cell><cell>0.3760</cell><cell>[117,98,110]</cell><cell>100M</cell></row><row><cell></cell><cell>Random</cell><cell>0.8030</cell><cell>0.3658</cell><cell>-</cell><cell>418M</cell></row><row><cell>Avazu</cell><cell>Bi-level</cell><cell></cell><cell cols="2">Out of Memory</cell><cell></cell></row><row><cell></cell><cell>OptInter</cell><cell>0.8062</cell><cell>0.3637</cell><cell>[107,73,96]</cell><cell>827M</cell></row><row><cell></cell><cell>Random</cell><cell cols="2">0.7781 0.005734</cell><cell>[36,38,46]</cell><cell>108M</cell></row><row><cell>iPinYou</cell><cell>Bi-level</cell><cell cols="2">0.7796 0.005620</cell><cell>[34,16,70]</cell><cell>31M</cell></row><row><cell></cell><cell>OptInter</cell><cell cols="2">0.7825 0.005606</cell><cell>[25,12,83]</cell><cell>26M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX</head><label>IX</label><figDesc>stands for re-train stage after search stage with fixed architecture parameters. w.o. stands for without re-train stage after search stage.</figDesc><table><row><cell cols="5">PERFORMANCE COMPARISON BETWEEN WITH OR WITHOUT RE-TRAIN</cell></row><row><cell></cell><cell cols="2">STAGE</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Criteo</cell><cell></cell><cell>Avazu</cell><cell></cell></row><row><cell>Metric</cell><cell>w.</cell><cell>w.o.</cell><cell>w.</cell><cell>w.o.</cell></row><row><cell>AUC</cell><cell cols="2">0.8101 0.7953</cell><cell cols="2">0.8062 0.7772</cell></row><row><cell cols="3">log loss 0.3760 0.4558</cell><cell cols="2">0.3637 0.3829</cell></row><row><cell>w.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? RQ1: Could OptInter achieve superior performance, compared with mainstream CTR prediction models?</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.kaggle.com/c/avazu-ctr-prediction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://contest.ipinyou.com/ 4 https://github.com/wnzhang/make-ipinyou-data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<editor>A. Karatzoglou, B. Hidasi, D. Tikk, O. S. Shalom, H. Roitman, B. Shapira, and L. Rokach</editor>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems<address><addrLine>DLRS@RecSys; Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-09-15" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepfm: A factorizationmachine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<editor>C. Sierra</editor>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Data Mining, ICDM 2016</title>
		<editor>F. Bonchi, J. Domingo-Ferrer, R. Baeza-Yates, Z. Zhou, and X. Wu</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction over multifield categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data --A case study on user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -38th European Conference on IR Research</title>
		<editor>N. Ferro, F. Crestani, M. Moens, J. Mothe, F. Silvestri, G. M. D. Nunzio, C. Hauff, and G. Silvello</editor>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-03-20" />
			<biblScope unit="volume">9626</biblScope>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
	<note>Proceedings, ser. Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018</title>
		<editor>Y. Chang, C. Zhai, Y. Liu, and Y. Maarek</editor>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training and testing low-degree polynomial data mappings via linear SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1471" to="1490" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th IEEE International Conference on Data Mining</title>
		<editor>I. Webb, B. Liu, C. Zhang, D. Gunopulos, and X. Wu</editor>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>ICDM 2010</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<editor>S. Sen, W. Geyer, J. Freyne, and P. Castells</editor>
		<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Field-weighted factorization machines for click-through rate prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<editor>P. Champin, F. Gandon, M. Lalmas, and P. G. Ipeirotis</editor>
		<meeting>the 2018 World Wide Web Conference on World Wide Web<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23" />
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fm 2 : Field-matrixed factorization machines for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flores</surname></persName>
		</author>
		<idno>abs/2102.12994</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An elementary view on factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
		<editor>P. Cremonesi, F. Ricci, S. Berkovsky, and A. Tuzhilin</editor>
		<meeting>the Eleventh ACM Conference on Recommender Systems<address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-27" />
			<biblScope unit="page" from="179" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CAN: revisiting feature co-action for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>R. Gupta, Y. Liu, J. Tang, and B. A. Prakash</editor>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1806.09055</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Research, D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
	<note>ser. Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autofeature: Searching for feature interactions and their architectures for clickthrough rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<editor>M. d&apos;Aquin, S. Dietze, C. Hauff, E. Curry, and P. Cudr?-Mauroux</editor>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general method for automatic discovery of powerful interactions in click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2105.10484</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive feature generation via learning adjacency tensor of feature graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>G?rel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<imprint>
			<date type="published" when="1945" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<editor>C. L. Williamson, M. E. Zurko, P. F. Patel-Schneider, and P. J. Shenoy</editor>
		<meeting>the 16th International Conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-05-08" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Directional pruning of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010</title>
		<editor>ser. JMLR Proceedings, Y. W. Teh and D. M. Titterington</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<editor>E. Saka, D. Shen, K. Lee, and Y. Li</editor>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising<address><addrLine>New York City; New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08-24" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Montr?al, Canada, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memory-efficient embedding for recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autoemb: Automated embedding dimensionality search in streaming recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated embedding size search in deep recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
		<editor>J. Huang, Y. Chang, X. Cheng, J. Kamps, V. Murdock, J. Wen, and Y. Liu</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2307" to="2316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autodis: Automatic discretization for embedding numerical features in CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2012.08986</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autoft: Automatic fine-tune for parameters transfer learning in click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2106.04873</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autoloss: Automated loss function search in recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2106.06713</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
