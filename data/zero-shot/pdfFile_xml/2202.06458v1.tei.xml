<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faster hyperspectral image classification based on selective kernel mechanism using deep convolutional networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunju</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Faster hyperspectral image classification based on selective kernel mechanism using deep convolutional networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperspectral imagery is rich in spatial and spectral information. Using 3D-CNN can simultaneously acquire features of spatial and spectral dimensions to facilitate classification of features, but hyperspectral image information spectral dimensional information redundancy. The use of continuous 3D-CNN will result in a high amount of parameters, and the computational power requirements of the device are high, and the training takes too long. This letter designed the Faster selective kernel mechanism network (FSKNet), FSKNet can balance this problem. It designs 3D-CNN and 2D-CNN conversion modules, using 3D -CNN to complete feature extraction while reducing the dimensionality of spatial and spectrum. However, such a model is not lightweight enough. In the converted 2D-CNN, a selective kernel mechanism is proposed, which allows each neuron to adjust the receptive field size based on the twoway input information scale. Under the Selective kernel mechanism, it mainly includes two components, se module and variable convolution. Se acquires channel dimensional attention and variable convolution to obtain spatial dimension deformation information of ground objects. The model is more accurate, faster, and less computationally intensive. FSKNet achieves high accuracy on the IN, UP, Salinas, and Botswana data sets with very small parameters.</p><p>Index Terms-Convolutional neural networks (CNNs), selective kernel mechanism, combining 3D-CNN and 2D-CNN conversion modules,faster hyperspectral image (HSI) classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past few decades, the classification of HSI has been very active. Feature extraction means include artificially designed feature extraction techniques and learning-based feature extraction techniques. The use of hand-designed feature descriptions <ref type="bibr" target="#b1">[2]</ref> includes several HSI classification methods. <ref type="bibr" target="#b2">[3]</ref> - <ref type="bibr" target="#b3">[4]</ref> propose sparse representations or patchbased sparse representations as spatial features. A composite kernel is used to combine spatial and spectral information for HSI classification <ref type="bibr" target="#b4">[5]</ref>. Later, the academic community proposed a number of spatial-spectral correlation models <ref type="bibr" target="#b5">[6]</ref> - <ref type="bibr" target="#b6">[7]</ref> to improve classification accuracy. In <ref type="bibr" target="#b7">[8]</ref>, based on the spatial information of the SVM classifier, multi-scale superpixel segmentation is used to model the distribution of the class. In <ref type="bibr" target="#b8">[9]</ref>, three kernels are used to take advantage of spatial spectral information and combine them for classification.</p><p>In recent years, deep learning technology has made great progress in the field of remote sensing research, especially convolutional neural networks have great advantages over manual extraction features. Zhao et al. <ref type="bibr" target="#b9">[10]</ref> proposed using principal component analysis to reduce the original hyperspectral image, using 2D-CNN to extract spatial features on the reduced-dimensional image, and using balanced local discriminant embedding to extract spectral Information, and finally combine spatial and spectral information into the classifier to improve classification accuracy. Chen et al. <ref type="bibr" target="#b10">[11]</ref> proposed a hyperspectral feature extraction framework based on CNN, and discussed the extraction effects of 1D-CNN, 2D-CNN and 3D-CNN. Based on 3D-CNN, Zhong et al. <ref type="bibr" target="#b11">[12]</ref> proposed using the supervised spatial-spectral residual network SSRN, design spatial and spectral residual module to extract spatial and spectral information respectively, which is a powerful extension of 3D -CNN using residual structure. Li et al. <ref type="bibr" target="#b12">[13]</ref> proposed the use of 3D-CNN combined with DenseNet's dense connection structure to achieve deep extraction of HSI features. <ref type="bibr" target="#b13">[14]</ref> proposed a dual path network combining residual and dense connections. <ref type="bibr" target="#b14">[15]</ref> proposed HybirdSN to explore the feature extraction method of 3D-2D CNN for hyperspectral image classification, but the accuracy is general and the model parameters are very high.</p><p>It is obvious from the literature that some of the methods used for classification only use 2D-CNN, do not make full use of the correlation between the spatial-spectral information, and some use 3D-CNN, simultaneously sampling in the spatial and spectral dimensions to obtain good accuracy. However, the HSI information spectral dimension information is redundant. The use of continuous 3D-CNN will generate a lot of parameter quantities, and the computational power requirement for the device is high, and the training takes too long. This letter is used to solve the above two problems. First, combining 3D-CNN and 2D-CNN, using 3D-CNN to complete feature extraction while reducing the dimensionality of spatial and spectral. Secondly, whether it is based on 3D-CNN or 2D-CNN models, it tends to deepen the network structure and contribute to deep feature extraction. But such a model is not lightweight enough. We proposed a selective kernel mechanism in the converted 2D-CNN, which allows each neuron to adjust the receptive field size based on two-way input information scale adaptively. Under the Selective kernel mechanism, it mainly includes two components, SE module <ref type="bibr" target="#b15">[16]</ref> and deformable convolution <ref type="bibr" target="#b16">[17]</ref>. SE acquires channel dimensional attention and deformable convolution to obtain spatial dimension ground object deformation information.Feature extraction is performed spatially using deformable convolutions with two inconsistent kernel sizes. Two-way information is passed through the SE module to achieve spatial and dimensional feature association. The selective kernel mechanism dynamically selects the region of the receptive field and attention, and maximizes the extraction of useful information with higher accuracy, faster speed, and less computational power. We call the Faster selective kernel mechanism network (FSKNet). FSKNet achieves high accuracy on the IN, UP, Salinas, and Botswana data sets with very small parameter quantities. For HSI classification, 2D-CNN only applies convolution on the spatial dimension, however we prefer to capture spectral information encoded in multiple frequency bands and spatial information. The 3D-CNN kernel can extract band and spatial feature representations from HSI, but at the cost of increased computational complexity and the model becomes very slow. In order to make full use of the 2D and 3D CNN automatic learning features, we propose a hybrid feature learning module combining 3D CNN and 2D CNN. HSI spectral dimension information redundancy, in order to eliminate spectral redundancy, first apply the traditional PCA to the original HSI data along the spectral band. PCA reduces the number of spectral bands while maintaining the same spatial size, retaining more important information, but PCA also suffers loss of information while performing dimensionality reduction. We believe that the ideal dimension reduction method should be to reduce the size of the data input to the 3d cube while extracting the spatial and spectral features. As shown in <ref type="figure">Figure x</ref>, the first three layers are 3D stride convolution layers. At the same time of spatial dimensionality reduction, we chose a larger dimensionality reduction dimension in the spectral dimension, so that the spatial and spectral simultaneous sampling and synchronous dimensionality reduction are realized in the 3D-CNN framework. Unlike PCA, which reduces the dimension before input, our dimension reduction method is hidden in feature extraction. As the convolution layer increases, the feature map becomes smaller,but the number of filters growssignificantly. In order to reduce the amount of parameters, we use separable convolution in the fourth layer. The most important part of the conversion mechanism is the Reshape layer. Since each feature vector is convolved from the same raw data with different convolution kernels, which makes these vectors have different representations of particular features, there is a strong correlation between these vectors. Reshape all original vectors and integrate channel and spectral dimensions. After the four-layer convolution, the spectral dimension has been reduced to a small value (we reduced the spectral dimension to 1 by controlling the stride size), and we can think that the model has learned a wealth of spectral knowledge, at this time (shape <ref type="bibr" target="#b0">[1]</ref>, shape <ref type="bibr" target="#b1">[2]</ref>, shape <ref type="bibr" target="#b2">[3]</ref>, shape <ref type="bibr" target="#b3">[4]</ref>) becomes (shape <ref type="bibr" target="#b0">[1]</ref>, shape <ref type="bibr" target="#b1">[2]</ref>, shape <ref type="bibr" target="#b2">[3]</ref>xshape <ref type="bibr" target="#b3">[4]</ref>). After the reshaping operation, the data can be input to our selective kernel module as a normal 2D image classification, thereby converting the 3D data to the 2D data in this way. This conversion mechanism can achieve good performance and avoid excessive Fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Selective kernel mechanism</head><p>After using the combination of 3D-CNN and 2D-CNN conversion modules, we get 2D input image. In order to enable the neurons to be adaptive to the size of the receptive field, we proposed selective kernel mechanism for automatic selection operations in two kernel branches with different kernel sizes. The mechanism consists of two major structures. The first structure consists of two deformable convolutions with different kernel sizes. The use of deformable convolution is more effective in spatially efficient extraction of spectral images. From the conversion of 3D to 2D-CNN, there is no loss of spatial information. The use of two different branches of the kernel is to allow the model to automatically adjust the regional receptive field. This can be used to deal with the characteristics of sparse features in HSI. In order to keep the output feature size of the first structure consistent, we used a detailed convolution kernel in the 5x5 kernel branch. The second structure is a SE module, a lightweight gating mechanism designed to simulate channel relationships in a computationally efficient manner. It is designed to enhance the representation of basic modules throughout the network. Applying an attention mechanism to HSI, it biases the allocation of valid sample resources available for processing to the region with the richest input signal. It accelerates the understanding of the type of features by the deep learning model and facilitates the final classification. The SE module can suppress invalid information, activate valid information, and pass back the weighted output information. The SE module consists of two operations, the first is squeeze, the global information is compressed into a channel descriptor using globalaveragepooooling, the second is excitation, adaptive recalibration, using the sigmoid function to obtain the normalized weight between 0-1, and then normalized weights are weighted to the characteristics of each channel by a scale operation. These two branches get a feature map of the mixed receptive field, and the network can adaptively adjust the receptive field size according to the learned data content. We use this gate mechanism to control the flow of information from two branches carrying different information scales to the next neuron. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network architecture</head><p>The overall network structure is as shown in the figure. Input 3D HSI, obtain 2D image through conversion layer, complete feature extraction and dimensionality reduction of spatial spectral dimension, and then perform multi-scale feature correlation through the selective kernel module to dynamically select the kernel. In the structure, you can add multiple selective kernel modules to enhance feature association. But in fact, on the four HSI datasets tested, a selective kernel can achieve high accuracy, and the parameter of the model is much lower than the state of the art. Finally, through the gobalavgeragepool to get the input softmax vector, we do not use the fc layer, fc parameter is high, global pooling can be a good substitute for fc.  </p><formula xml:id="formula_0">III EXPERIMENTS 3.1 Datasets</formula><p>The first data set is the Indian Pines dataset. It was collected in June 1992 by the AVIRIS spectral imager at the Indiana sPine Forest Experimental Area in Northwest Indiana. It measures 145 x 145 pixels with a spatial resolution of 20 meters and a wavelength range of 0.4-2.5 microns for the 220 band.</p><p>The second data set is Pavia University dataset. It was collected by the ROSIS spectral imager in 2001 in the Pavia region of northern Italy. The image size is 610 ? 340 pixels, thespatial resolution is 13 m, contains 115 bands, and the wavelength range is 0.43-0.86 ?m for the 103 band.</p><p>The third data set is Salinas dataset. It was collected by AVIRIS sensors located in the Salinas Valley, California, with 224 bands and a high spatial resolution of 3.7 m pixels. The pixel space size is 512?217 pixels.</p><p>The fourth data set is the Botswana dataset. The NASA EO-1 satellite acquired a series of data from the Okavango Delta in Botswana from 2001 to 2004. The Hyperion sensor on EO-1 acquires data of 30 m pixel resolution over a 7.7 km strip covering 242 bands of the 400-2500 nm spectral portion in a 10 nm window.</p><p>In our experiments, we used OA, AA and Kappa as the main indicators to evaluate the accuracy of the model. We used training and test time, flops as the main indicator to evaluate the speed of the model.  datasets, the accuracy is preferably 5:1:4. In the Salinas dataset, the highest accuracy is achieved when the training set ratio is 4:1:5. In Botswana dataset, when the training set ratio is 3:1:6, the accuracy is 1., and the dataset is also the most prominent data set of FSKNet.   In this letter, we present the current real-time network structure for processing HSIs. Based on the 3D-2D CNN conversion module and the selective kernel mechanism, FSKNet completes the spectral dimension reduction when the spatial spectral features are jointly extracted. The selective kernel module allows each neuron to adapt to the field size based on two-way input information. FSKNet maximizes the extraction of useful information with higher precision, faster speed and less computational power. It achieves state of the art with excellent precision and speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Combining 3D-CNN and 2D-CNN conversion module II. PROPOSED METHOD 2.1 Combining 3D-CNN and 2D-CNN conversion modules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Selective kernel mechanism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the architecture of the FSKNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 . 2</head><label>32</label><figDesc>Discussion parameters 3.2.1The effect of the size of neighboring pixel blocks on the classification accuracy of the model neighboring pixel blocks are important factors in the size of the HSI input into the model. We discussed the changes in OA, AA, and Kappa for FSKNet on four datasets when changing from 15-23, as shown in the table. On the IN, UP, Botswana and Salinas dataset, the neighboring pixel blocks were selected to be 23, 15, 15, and 17.Due to hardware limitations, only the 15 and 17 groups were selected on the salinas dataset. The training set ratio of FSKNet is 5:1:4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I</head><label>I</label><figDesc>is a structural table of the model on the IN data set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STRUCTURE</head><label>I</label><figDesc>TABLE OF THE MODEL ON THE IN DATA SET</figDesc><table><row><cell>reshape_2 (Reshape)</cell><cell>(None, 1, 1, 64)</cell><cell>global_average_pooling2d_1</cell><cell>0</cell></row><row><cell>dense_1 (Dense)</cell><cell>(None, 1, 1, 4)</cell><cell>reshape_2</cell><cell>256</cell></row><row><cell>dense_2 (Dense)</cell><cell>(None, 1, 1, 64)</cell><cell>dense_1</cell><cell>256</cell></row><row><cell>multiply_1 (Multiply)</cell><cell>(None, 11, 11, 64)</cell><cell>batch_normalization_5 dense_2</cell><cell>0</cell></row><row><cell>multiply_2 (Multiply)</cell><cell>(None, 11, 11, 64)</cell><cell>batch_normalization_6dense_2</cell><cell>0</cell></row><row><cell>add_2 (Add)</cell><cell>(None, 11, 11, 64)</cell><cell>multiply_1/multiply_2</cell><cell>0</cell></row><row><cell>separable_conv2d_1 (SeparableConv2D)</cell><cell>(None, 9, 9, 64)</cell><cell>add_2</cell><cell>4672</cell></row><row><cell>separable_conv2d_2 (SeparableConv2D)</cell><cell>(None, 7, 7, 128)</cell><cell>separable_conv2d_1</cell><cell>8768</cell></row><row><cell>global_average_pooling2d_2 (GlobalAveragePooling2D)</cell><cell>(None, 128)</cell><cell>separable_conv2d_2</cell><cell>0</cell></row><row><cell>dense_3 (Dense)</cell><cell>(None, 16)</cell><cell>global_average_pooling2d_2</cell><cell>2064</cell></row><row><cell></cell><cell>Total params: 215,808</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Trainable params: 215,264</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Non-trainable params: 544</cell><cell></cell><cell></cell></row><row><cell>Layers</cell><cell>Output Size</cell><cell>Connected to</cell><cell>Param</cell></row><row><cell>input_1 (InputLayer)</cell><cell>(None, 19, 19, 200, 1)</cell><cell></cell><cell>0</cell></row><row><cell>conv3d_1 (Conv3D)</cell><cell>(None, 17, 17, 28, 16)</cell><cell>input_1</cell><cell>1008</cell></row><row><cell>batch_normalization_1 (BatchNormalization)</cell><cell>(None, 17, 17, 28, 16)</cell><cell>conv3d_1</cell><cell>64</cell></row><row><cell>conv3d_2 (Conv3D)</cell><cell>(None, 15, 15, 5, 32)</cell><cell>batch_normalization_1</cell><cell>23040</cell></row><row><cell>batch_normalization_2 (BatchNormalization)</cell><cell>(None, 15, 15, 5, 32)</cell><cell>conv3d_2</cell><cell>128</cell></row><row><cell>conv3d_3 (Conv3D)</cell><cell>(None, 13, 13, 1, 64)</cell><cell>batch_normalization_2</cell><cell>55296</cell></row><row><cell>batch_normalization_3 (BatchNormalization)</cell><cell>(None, 13, 13, 1, 64)</cell><cell>conv3d_3</cell><cell>256</cell></row><row><cell>separable_conv3d_1 (SeparableConv3D)</cell><cell>(None, 11, 11, 1, 128)</cell><cell>batch_normalization_3</cell><cell>8768</cell></row><row><cell>reshape_1 (Reshape)</cell><cell>(None, 11, 11, 128)</cell><cell>separable_conv3d_1</cell><cell>0</cell></row><row><cell>conv2d_1 (Conv2D)</cell><cell>(None, 11, 11, 32)</cell><cell>reshape_1</cell><cell>4096</cell></row><row><cell>batch_normalization_4 (BatchNormalization)</cell><cell>(None, 11, 11, 32)</cell><cell>conv2d_1</cell><cell>128</cell></row><row><cell>deformableconv _1 (Deformableconv2D)</cell><cell>(None, 11, 11, 64)</cell><cell>batch_normalization_4</cell><cell>36864</cell></row><row><cell>deformableconv _2 (Deformableconv2D)</cell><cell>(None, 11, 11, 64)</cell><cell>batch_normalization_4</cell><cell>69632</cell></row><row><cell>batch_normalization_3 (BatchNormalization)</cell><cell>(None, 11, 11, 64)</cell><cell>deformableconv _1</cell><cell>256</cell></row><row><cell>batch_normalization_4 (BatchNormalization)</cell><cell>(None, 11, 11, 64)</cell><cell>deformableconv _2</cell><cell>256</cell></row><row><cell>add_1 (Add)</cell><cell>(None, 11, 11, 64)</cell><cell>batch_normalization_3</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>batch_normalization_4</cell><cell></cell></row><row><cell>global_average_pooling2d_1 (GlobalAveragePooling2D)</cell><cell>(None, 64)</cell><cell>add_1</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II INFLUENCE</head><label>II</label><figDesc>OF NEIGHOURHOOD PIXEL BLOCKS ON THE PRECISION OF IN, UP, SALINAS AND BOTWANA DATA SETS The effect of the size of training ratio on the classification accuracy of the model After choosing the best neighboring pixel block parameters, we discussed the impact of the training set size. On the IN and UP</figDesc><table><row><cell>Neignboring</cell><cell></cell><cell>IN</cell><cell></cell><cell></cell><cell>UP</cell><cell></cell><cell></cell><cell>Botswana</cell><cell></cell><cell></cell><cell>Salinas</cell><cell></cell></row><row><cell>pixel</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell></row><row><cell>15</cell><cell>99.58</cell><cell>99.20</cell><cell>99.53</cell><cell>99.96</cell><cell>99.94</cell><cell>99.95</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>99.85</cell><cell>99.70</cell><cell>99.84</cell></row><row><cell>17</cell><cell>99.68</cell><cell>99.27</cell><cell>99.64</cell><cell>99.96</cell><cell>99.89</cell><cell>99.94</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>99.94</cell><cell>99.94</cell><cell>99.94</cell></row><row><cell>19</cell><cell>99.63</cell><cell>99.22</cell><cell>99.58</cell><cell>99.88</cell><cell>99.70</cell><cell>99.85</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>21</cell><cell>99.78</cell><cell>99.68</cell><cell>99.75</cell><cell>99.92</cell><cell>99.84</cell><cell>99.89</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>23</cell><cell>99.83</cell><cell>99.73</cell><cell>99.81</cell><cell>99.81</cell><cell>99.57</cell><cell>99.74</cell><cell>99.85</cell><cell>99.86</cell><cell>99.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3.2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>EFFECT OF TRAINING SET SCALE ON PRECISION ON IN, UP, SALINAS, BOTWANA DATA SETS Experimenal Results In Table IV, the FSKNet parameter quantities and the required flops are significantly lower than the comparison model.In Tables V and VI, we compared the five indicators of OA, AA, Kappa, training time and Test time. FSKNet has achieved a big advantage in all four data sets. On the IN dataset, all five indicators are optimal. On the UP dataset, only the test time is slightly slower. In the Salinas dataset, although the accuracy is behind the SSRN and 3D-DenseNet, the training time and test time are the shortest, especially compared to 3D-DenseNet. The accuracy is optimal on the Botwana dataset.</figDesc><table><row><cell>Training ratio</cell><cell></cell><cell>IN</cell><cell></cell><cell></cell><cell>UP</cell><cell></cell><cell></cell><cell>Botswana</cell><cell></cell><cell></cell><cell>Salinas</cell><cell></cell></row><row><cell></cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell></row><row><cell>2:1:7</cell><cell>99.34</cell><cell>99.22</cell><cell>99.25</cell><cell>99.90</cell><cell>99.89</cell><cell>99.87</cell><cell>99.34</cell><cell>99.43</cell><cell>99.28</cell><cell>99.90</cell><cell>99.87</cell><cell>99.89</cell></row><row><cell>3:1:6</cell><cell>99.48</cell><cell>99.42</cell><cell>99.41</cell><cell>99.93</cell><cell>99.91</cell><cell>99.91</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>99.86</cell><cell>99.73</cell><cell>99.85</cell></row><row><cell>4:1:5</cell><cell>99.63</cell><cell>99.08</cell><cell>99.58</cell><cell>99.80</cell><cell>99.68</cell><cell>99.73</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>99.98</cell><cell>99.98</cell><cell>99.98</cell></row><row><cell>5:1:4</cell><cell>99.83</cell><cell>99.73</cell><cell>99.81</cell><cell>99.96</cell><cell>99.94</cell><cell>99.95</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>99.94</cell><cell>99.94</cell><cell>99.94</cell></row><row><cell>6:1:3</cell><cell>99.71</cell><cell>99.41</cell><cell>99.67</cell><cell>99.90</cell><cell>99.91</cell><cell>99.87</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF PARAMETER QUANTITIES AND FLOPS OF 5 MODELS</figDesc><table><row><cell>Params</cell><cell>16394652</cell><cell>346788</cell><cell>2562452</cell><cell>5503108</cell><cell>215812</cell></row><row><cell>Flops</cell><cell>81959692</cell><cell>1732909</cell><cell>5234500</cell><cell>11005179</cell><cell>893584</cell></row><row><cell></cell><cell>3D-CNN</cell><cell>SSRN</cell><cell>3D-DenseNet</cell><cell>HybridSN</cell><cell>FSKNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>COMPARISON OF 5 METHODS ON IN AND UP DATA SETS</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell>IN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>UP</cell><cell></cell><cell></cell></row><row><cell></cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>Training</cell><cell>Test time</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>Training</cell><cell>Test time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell></row><row><cell>3D-CNN</cell><cell>99.61</cell><cell>98.63</cell><cell>99.55</cell><cell>13884.72</cell><cell>16.09</cell><cell>99.41</cell><cell>99.37</cell><cell>99.22</cell><cell>4203.72</cell><cell>6.25</cell></row><row><cell>SSRN</cell><cell>99.56</cell><cell>93.06</cell><cell>99.50</cell><cell>6336.10</cell><cell>11.10</cell><cell>99.88</cell><cell>99.76</cell><cell>99.85</cell><cell>8389.92</cell><cell>10.98</cell></row><row><cell>3D-DenseNet</cell><cell>99.95</cell><cell>99.66</cell><cell>99.94</cell><cell>32232.29</cell><cell>43.66</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>37216.36</cell><cell>45.81</cell></row><row><cell>HybridSN</cell><cell>99.83</cell><cell>99.30</cell><cell>99.81</cell><cell>5707.83</cell><cell>8.73</cell><cell>99.80</cell><cell>99.69</cell><cell>99.73</cell><cell>5312.18</cell><cell>9.58</cell></row><row><cell>FSKNet</cell><cell>99.83</cell><cell>99.73</cell><cell>99.81</cell><cell>2995.59</cell><cell>6.99</cell><cell>99.96</cell><cell>99.94</cell><cell>99.95</cell><cell>7242.29</cell><cell>10.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">TABLE VI COMPARISON OF 5 METHODS ON SALINAS AND BOTWANA DATASETS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell>Salinas</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Botswana</cell><cell></cell><cell></cell></row><row><cell></cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>Training</cell><cell>Test time</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell><cell>Training</cell><cell>Test time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell></row><row><cell>3D-CNN</cell><cell>99.36</cell><cell>98.86</cell><cell>99.29</cell><cell>30236.66</cell><cell>1200.12</cell><cell>99.90</cell><cell>99.91</cell><cell>99.89</cell><cell>725.83</cell><cell>1.81</cell></row><row><cell>SSRN</cell><cell>99.99</cell><cell>99.99</cell><cell>99.99</cell><cell>17661.24</cell><cell>116.19</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>522.52</cell><cell>1.64</cell></row><row><cell>3D-DenseNet</cell><cell>99.99</cell><cell>99.99</cell><cell>99.99</cell><cell>78055.12</cell><cell>193.26</cell><cell>99.64</cell><cell>99.60</cell><cell>99.61</cell><cell>2289.37</cell><cell>6.62</cell></row><row><cell>HybridSN</cell><cell>99.87</cell><cell>99.94</cell><cell>99.86</cell><cell>14217.54</cell><cell>110.41</cell><cell>99.95</cell><cell>99.95</cell><cell>99.94</cell><cell>351.98</cell><cell>1.25</cell></row><row><cell>FSKNet</cell><cell>99.98</cell><cell>99.98</cell><cell>99.98</cell><cell>13420.11</cell><cell>108.39</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>424.82</cell><cell>1.29</cell></row><row><cell></cell><cell></cell><cell cols="2">IV CONCLUSION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crop Yield Estimation Based on Unsupervised Linear Unmixing of Multidate Hyperspectral Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="173" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification via Multiscale Joint Collaborative Representation With Locally Adaptive Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="112" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification Using Dictionary-Based Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="3973" to="3985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification via Kernel Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2013-01" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="217" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Composite kernels for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munoz-Mari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vila-Frances</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calpe-Maravilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification of Hyperspectral Images by Exploiting Spectral-Spatial Information of Superpixel via Multiple Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6663" to="6674" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse Representation-Based Nearest Neighbor Classifiers for Hyperspectral Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2418" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiscale Superpixel-Level Subspace-Based Support Vector Machines for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pi?urica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2142" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classification of Hyperspectral Images by Exploiting Spectral-Spatial Information of Superpixel via Multiple Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6663" to="6674" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral-Spatial Feature Extraction for Hyperspectral Image Classification: A Dimension Reduction and Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Feature Extraction and Classification of Hyperspectral Images Based on Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Three-dimensional densely connected convolutional network for hyperspectral remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16519</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-Path Network-Based Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="447" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">G</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06701</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
