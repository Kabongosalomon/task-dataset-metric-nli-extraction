<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>&gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-point cloud</term>
					<term>quality assessment</term>
					<term>degradation</term>
					<term>coding</term>
					<term>projection</term>
					<term>recoloring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud coding solutions have been recently standardized to address the needs of multiple application scenarios. The design and assessment of point cloud coding methods require reliable objective quality metrics to evaluate the level of degradation introduced by compression or any other type of processing. Several point cloud objective quality metrics has been recently proposed to reliable estimate human perceived quality, including the socalled projection-based metrics. In this context, this paper proposes a joint geometry and color projection-based point cloud objective quality metric which solves the critical weakness of this type of quality metrics, i.e., the misalignment between the reference and degraded projected images. The best performing 2D quality metrics in the literature are exploited to evaluate the quality of the projected images. The experimental results show that the proposed projection-based quality metric offers the best subjective-objective correlation performance in comparison with other metrics in the literature. The Pearson correlation gains regarding D1-PSNR and D2-PSNR metrics are 17% and 14.2% when data with all coding degradations is considered.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ECENT advances in 3D acquisition and reconstruction technologies have enabled many visual immersive applications, such as virtual and augmented reality, immersive communications, and video gaming. Point cloud (PC) is an emerging 3D visual representation format that is becoming rather popular because of its easy acquisition and capability to realistically represent objects and 3D visual scenes. However, since realistic PCs require a large number of points, a compact representation of PCs is essential for storage and transmission applications and services. PC coding is a rather new and challenging problem due to the unstructured nature of PCs where each point, i.e., filled voxel in voxelized PCs, is associated to a 3D coordinate; moreover, each point has often associated attributes such as color, transparency, reflectance, etc. In recent years, several efforts in PC coding were able to significantly reduce the bitrate while still maintaining the PC quality and fidelity. MPEG has already developed two PC coding standards <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, notably Geometry-based Point Cloud Compression (G-PCC) for static and progressive acquired PCs, and Video-based Point Cloud Compression (V-PCC) for dynamic PCs. In this context, the assessment of PC quality is very important as it plays a significant role in the design and optimization of coding solutions as well as on the validation of the quality of experience offered to the users.</p><p>The best way to reliably measure the PC quality is through subjective quality assessment where a specially designed framework collects opinion scores from a minimum number of subjects. In the literature, there are many subjective PC quality studies available, considering different ways to visualize the PC <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b4">[5]</ref>, several PC rendering methods <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b6">[7]</ref> and types of degradation <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b8">[9]</ref>. However, since subjective quality assessment is expensive and time-consuming, reliable objective quality metrics are critical to facilitate the design of more efficient coding solutions and assess the quality of experience offered to the users. In the literature, the performance of multiple PC objective quality metrics has been assessed through the correlation between corresponding objective and subjective quality scores, notably for different codecs, with different types of degradation <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the literature, a few works have already exploited the idea of measuring the PC quality by projecting the 3D PC into one or more 2D images, i.e., by converting a 3D PC into several 2D images, a more traditional type of data. In the context of PC coding, this type of approach was successfully exploited to achieve efficient compression, as demonstrated by the MPEG V-PCC standard <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. In the context of PC quality assessment, these 2D projected images can be obtained by performing multiple projections into different viewpoints, i.e., using different projection centers. After projection, the most recent, and powerful 2D image quality metrics available can be exploited without any changes, to assess the entire PC quality through the projected images quality. However, the projectionbased metrics available in the literature are not yet showing better subjective-objective correlation performance than the popular point-to-point quality metrics, where correspondences are established in the 3D space and errors/distances in position or color are accounted.</p><p>The most critical weakness of projection-based metrics is caused by the inability of 2D quality metrics to efficiently handle local displacement errors, since pixel-level (or regionlevel) comparisons are usually made. Due to lossy PC coding, geometry distortions (or degradations) cause many displacements and thus a lower correlation performance. Typically, 2D objective quality metrics consider that these pixels/regions have a high distortion when, in fact, the small or medium geometry degradations are perceptually well tolerated, A. Javaheri, C. Brites, Member, IEEE, F. Pereira, Fellow, IEEE, J. Ascenso, Senior Member, IEEE Joint Geometry and Color Projection-based Point Cloud Quality Metric R especially when color is also available as some degree of masking may happen <ref type="bibr" target="#b5">[6]</ref>. For example, small displacement errors in the projected images due to geometry distortions may not be perceived by humans but may lead to high objective distortions when 2D quality metrics are used to assess the quality of the PC projected images. Another critical issue is related to the difference between the number of points in the reference and decoded PCs. This often occurs when the PC coding solution uses planar or triangular approximations of the PC surface, and more points may be recreated at the decoder side when these surfaces are sampled or when the PC coding solution reduces the number of coded points using octree pruning. This implies that one of the projected 2D images, the reference or the degraded one, may have, for some positions, pixels occupied while these pixels are not filled in the other projected 2D image, thus leading to large pixel-based mismatches. In some past works <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref>, these positions are either ignored or an occupied position is compared with a nonoccupied position (usually filled with some background color). However, both these solutions negatively impact the final quality metric correlation performance since, for some cases, these pixels are visually important and should not be ignored; moreover, the quality score should not depend on an arbitrarily selected background color. In this context, this paper proposes a novel joint geometry and color projection-based PC quality metric, which addresses the weaknesses and issues above, thus allowing to achieve a higher objective-subjective correlation performance. The key original ideas underpinning this novel quality metric are twofold:</p><p>? Reference and degraded projected images are compared for two fixed geometry conditions, notably for the reference and degraded geometries. After, these two quality scores are fused, thus implying that the proposed approach implicitly considers geometry and color distortions/degradations. By comparing images created for the same geometry level, reference or degraded, the undesired above misalignments are avoided and there is no difference between the number of points on the reference and degraded PCs for the same geometry condition. ? A padding operation is performed in the 2D domain to avoid assigning an arbitrary, uniform color to the background (i.e., not projected) pixels. Because of the aligned geometries, these pixels are not filled in the reference/degraded projected images and if a uniform background value is assigned, the 2D quality metric would be biased due to these regions for which no distortions would occur. The proposed padding operation mitigates the impact of these background pixels.</p><p>To achieve its objectives, the rest of the paper is organized as follows. Section II briefly reviews the state-of-the-art on PC objective quality metrics. Section III describes the proposed joint geometry and color projection-based PC quality metric. Experimental results are presented and analyzed in Section IV and, finally, conclusions are offered in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND WORK ON PC OBJECTIVE QUALITY ASSESSMENT</head><p>In this section, the state-of-the-art on PC objective quality metrics is briefly reviewed, by addressing first, point-based metrics, followed by feature-based metrics and, finally, projection-based metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Point-based PC Quality Metrics</head><p>A point-based quality metric compares the geometry or attributes of the reference and degraded PCs directly point-bypoint after defining the necessary point correspondences. The most popular point-based geometry quality metrics are the Point-to-Point (Po2Po) <ref type="bibr" target="#b12">[13]</ref> and Point-to-Plane (Po2Pl) <ref type="bibr" target="#b13">[14]</ref> metrics. In a Po2Po metric, for every point in a degraded/reference PC, the nearest neighbor is obtained in the corresponding reference/degraded PC (thus a point correspondence is obtained); after, the Hausdorff distance or the Mean Squared Error (MSE) distance are computed over all pairs of points. The main disadvantage of this type of metrics is that they do not consider that PC points represent the surface of an object(s) in the visual scene. To solve this issue, Point-to-Plane (Po2Pl) metrics have been proposed by Tian et al. <ref type="bibr" target="#b13">[14]</ref>, which model the underlying surface at each point as a plane perpendicular to the normal vector at that point. This type of metrics results into smaller errors for the points closer to the PC surface, here associated to a plane. Currently, the MPEGadopted PC geometry quality metrics are the Po2Po MSE (D1) and Po2Pl MSE (D2) distances/errors and their associated PSNR <ref type="bibr" target="#b14">[15]</ref>. Moreover, a Plane-to-Plane (Pl2Pl) metric has been proposed by Alexiou et al. <ref type="bibr" target="#b15">[16]</ref>, which measures the similarity between the underlying surfaces associated to the corresponding points in the reference and degraded PCs. In this case, tangent planes are estimated for both the reference and degraded points and the associated angular similarity is assessed.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, Javaheri et al. propose a geometry quality metric based on the Generalized Hausdorff distance, which corresponds to the maximum distance for a specific percentage of data rather than the whole data, thus filtering some outlier points. The Generalized Hausdorff distance between two PCs adopted in this quality metric may be computed for both Po2Po and Po2Pl metrics. In <ref type="bibr" target="#b17">[18]</ref>, Javaheri et al. also propose a socalled Point-to-Distribution (Po2D) metric based on the Mahalanobis distance between a point in a PC and its K nearest neighbors in the other PC. The mean and covariance matrix of the corresponding distribution are computed and used to measure the Mahalanobis distance between points in one PC and their corresponding set of nearest neighbors in the other PC. These distances are averaged to compute the final quality score.</p><p>There are not many point-based quality metrics for PC attributes and specifically for color. However, the Po2Po PSNR for color in the YCbCr color space is widely used by MPEG and in the literature to evaluate PC color degradations. This metric works like the Po2Po geometry metrics, with the error corresponding now to the difference between the colors of the points in some PC correspondence. This metric may either be computed only for the luminance (Y-PSNR) or chroma components (Cb/Cr-PSNR) individually, or as a weighted average of all color components (YUV-PSNR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature-based PC Quality Metrics</head><p>A feature-based PC quality metric computes a quality score based on the difference between some local or/and global features extracted from the reference and degraded PCs. <ref type="bibr">Meynet et al.</ref> propose in <ref type="bibr" target="#b19">[20]</ref> the so-called Point Cloud Multi-Scale Distortion metric (PC-MSDM), a structural similaritybased PC geometry quality metric based on local curvature statistics. This metric computes the surface curvature associated to each point and establishes after point-based correspondences. The metric score corresponds to the Gaussian weighted curvature statistics for a set of local neighborhoods.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, Viola et al. design a PC quality metric based on the histogram and correlogram of the luminance component. After, the proposed color quality metric is fused with the Po2Pl MSE geometry metric (D2) using a linear model with a weighting parameter determined using a grid search method.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, Diniz et al. propose the so-called Geotex metric, which is based on Local Binary Pattern (LBP) descriptors adapted to PCs and applied to the luminance component. To apply it on PCs, the LBP descriptor is computed on a local neighborhood corresponding to the K-nearest neighbors of each point in the other PC. Histograms of the extracted feature maps are obtained for both the reference and degraded PCs and used to compute the final quality score using a distance metric such as the f-divergence <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, Diniz et al. extend the Geotex metric by considering multiple distances, notably Po2Pl MSE for geometry and the distance between LBP statistics <ref type="bibr" target="#b21">[22]</ref> for color. In <ref type="bibr" target="#b24">[25]</ref>, Diniz et al. also propose another quality metric, which computes Local Luminance Patterns (LLP) on the Knearest neighbors of each point on the other PC.</p><p>In <ref type="bibr" target="#b25">[26]</ref>, Meynet et al. propose the Point Cloud Quality Metric (PCQM) metric, which combines the geometry features used in <ref type="bibr" target="#b19">[20]</ref> with five color features related to lightness, chroma and hue. PCQM corresponds to the weighted average of the differences for geometry and color features between the reference and degraded PCs. In <ref type="bibr" target="#b26">[27]</ref>, Viola et al. propose the first reduced reference quality metric that jointly evaluates geometry and color. A set of seven statistical features such as the mean and standard deviation are extracted from the reference and degraded PCs in the geometry, texture, and normal vector domain, in a total of 21 features. The reduced quality score is computed as the weighted average of the differences for all these features between the reference and degraded PCs.</p><p>Inspired by the SSIM quality metric for 2D images, Alexiou et al. propose in <ref type="bibr" target="#b27">[28]</ref> a quality metric using local statistical dispersion features. These statistical features are extracted in a local neighborhood around each point in the reference and degraded PCs considering four independent 'attributes', notably geometry, color (luminance), normal and curvature information. The final quality metric is obtained by pooling the feature value differences between associated points in the reference and degraded PCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Projection-based PC Quality Metrics</head><p>A projection-based PC quality metric maps the 3D reference and degraded PCs onto some selected 2D planes and computes the quality score by comparing the projected images using some 2D image quality metric. The first projection-based PC quality metric has been proposed by Queiroz et al. in <ref type="bibr" target="#b10">[11]</ref>. This metric projects the reference and degraded PCs onto the six faces of a bounding cube around the PC, concatenates the corresponding projected images and measures the 2D PSNR between the corresponding degraded and reference concatenated projected images.</p><p>In <ref type="bibr" target="#b11">[12]</ref>, Alexiou et al. propose a rendering software for PC visualization on 2D screens, which performs the orthographic projection of a PC onto the six faces of the PC bounding box. A 2D quality metric is then applied to the reference and degraded projected images resulting from the rendering and the final score is obtained by computing the average over the six pairs of projected images. In <ref type="bibr" target="#b28">[29]</ref>, Alexiou et al. study the impact of the number of projected 2D images (each corresponding to a specific view) on the subjective-objective correlation performance of projection-based quality metrics. It is shown that even a single view may be enough to achieve a reasonable correlation performance. Moreover, a projectionbased PC quality metric weighting the projected images according to the user interactions performed during the subjective test is proposed. In <ref type="bibr" target="#b9">[10]</ref>, the quality metric proposed in <ref type="bibr" target="#b11">[12]</ref> is benchmarked considering different number of views, pooling functions, etc. The best performance is achieved when 2D quality metrics are applied on the projections from 42 different views and pooled with an l1-norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED PROJECTION-BASED PC QUALITY METRIC</head><p>In this section, the architecture and walkthrough of the proposed joint geometry and color projection-based PC quality metric is presented; after the most relevant modules are explained in detail. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the proposed Joint Geometry and Color Projection-based PC Quality Metric architecture, referred from now on as JGC-ProjQM. The key idea behind this metric is that the degraded and reference PCs are processed in two parallel branches, one associated to the reference geometry and the other associated to the degraded geometry, to obtain two intermediate quality scores which are fused at the end. To avoid misalignment errors, before applying the 3D to 2D projection, the reference and degraded PCs are processed to obtain two PCs to be compared with the same geometry:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture and Walkthrough</head><p>? Reference Geometry Branch: In the top branch, the geometry of the reference PC is used; the reference PC geometry is recolored with the color of the degraded PC and the resulting PC is compared with the reference PC (naturally, including the original color). ? Degraded Geometry Branch: In the bottom branch, the geometry of the degraded PC is used; the degraded PC geometry is recolored with the reference PC color and the resulting PC is compared with the degraded PC (naturally, including the degraded color). Naturally, the color attributes assigned to the points in the two PCs with the same geometry in the two branches will be different, notably using the color data with and without coding degradations.</p><p>Before applying the JGC-ProjQM metric, the PCs have to be voxelized to some fixed precision, if they are available in floating point precision. This step is important to perform the 3D (voxels) to 2D (pixels) projection. Nowadays, most available PC data are already in fixed precision, i.e., the PC has been voxelized, and, thus, this step may not be needed; for this reason, it is not included in the architecture in <ref type="figure" target="#fig_0">Fig. 1</ref>. Moreover, both V-PCC and G-PCC standard codecs code fixed precision PCs or perform voxelization as a pre-processing step before coding. The main modules in the proposed JGC-ProjQM metric pipeline are briefly explained in the following walkthrough:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Recoloring:</head><p>Due to the lossy coding of geometry data, the positions of the points in the reference and degraded PCs are not the same. Thus, when a projection is made, the resulting reference and degraded projected 2D images have regions that are not aligned, even when the degraded PC has only slight geometry degradations. This creates a problem for 2D quality metrics which are typically not robust to misalignments (or displacements) and often perform very poorly in this situation. Therefore, to align the reference and degraded projected images, a recoloring step is applied where the degraded (or reference) PC color is mapped onto the reference (or degraded) geometry. In this way, the color degradation is compared for the two geometry conditions (reference and degraded geometry, each at a time), without any misalignments (or different number of points) while still considering both the geometry and color degradations. This is a key technical novelty of the proposed JGC-ProjQM quality metric. This solution also exploits the fact that color degradations typically have a higher perceptual impact than geometry degradations, notably due masking effects <ref type="bibr" target="#b5">[6]</ref>. More details about this module are presented in Section III.B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Projection:</head><p>The reference, degraded and the two recolored PCs obtained in the previous step are orthographically projected onto the six faces of a cube to obtain six projected images for each PC, this means six non-overlapping images for each of the four PCs, i.e., reference, degraded, reference recolored and degraded recolored. Although another type of projection could be used, the low-complexity orthographic projection is enough to assess the quality degradations, especially considering that the PC pairs to compare have now the same geometry. In this process, six binary occupancy maps are also obtained for each projected PC; these occupancy maps serve to signal if a 2D image pixel corresponds (or not) to a point (filled voxel) in the 3D PC. The size of the projected images and occupancy maps only depends on the precision of the PC, thus commonly obtaining a 2 ! ? 2 ! size. More details about this module are presented in Section III.C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cropping:</head><p>After the projection, and depending on the PC size and position, the projected images may have a rather large background area (area without projected pixels around the PC object(s)), notably in comparison with the image area occupied with the PC points. These empty background areas can act as a distractor for the 2D quality metric, notably if the same (uniform) color value is assigned to all background pixels and thus they must be reduced as much as possible with a cropping procedure. More details about this module are presented in Section III.D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Padding:</head><p>If an arbitrary, uniform background value is used for background pixels, i.e., pixels positions on the projected images that have not been filled, the 2D quality metric may be biased, thus lowering the JGC-ProjQM prediction power. Computing the 2D quality metric only on the foreground pixels is not also a good solution as some 2D quality metrics such as SSIM cannot be applied to arbitrarily shaped objects. In this context, the padding module targets the creation of a seamless image, where the background positions are filled with interpolated/padded values, thus obtaining an image that is more suitable for quality assessment. In this process, the background holes inside the PC foreground area are also padded in the same way as the empty areas around the projected PC. More details about this module are presented in Section III.E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">2D Quality Assessment:</head><p>At this stage, a 2D image quality metric is computed between the six reference, padded images and the corresponding degraded, padded images for the same view/projection; this happens for the two architectural branches. The output of this process are six objective quality scores, one for each pair of projected, padded images, corresponding to each projection plane, which must be pooled together. The final JGC-ProjQM metric performance has been studied for several 2D quality metrics, using common pooling functions, e.g., max, min, and weighted average. Since it was found that the final subjective-objective correlation performance is rather similar for the various pooling functions, it was decided to adopt average pooling to obtain a single quality score for each architectural branch of the proposed projection-based PC quality metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Fusion:</head><p>All modules previously described are included in the two architectural branches of the proposed projectionbased PC quality metric to obtain two intermediate quality scores, notably JGC-ProjQMreference and JGC-ProjQMdegraded. These two intermediate quality scores represent the quality associated to the projected images as measured by a 2D quality metric, for two different geometry conditions, i.e., reference and degraded geometries, and must be fused together to obtain the final JGC-ProjQM quality metric. Although different fusion strategies are possible, even applying machine learning techniques, it was found that a simple linear regression was enough to obtain a high subjective-objective correlation performance, without the risk of overfitting. More details about this process are presented in Section III.F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recoloring</head><p>The main challenge with projection-based PC quality metrics is that geometry distortions may cause misalignment errors between the reference and degraded projected 2D images. 2D quality metrics do not typically handle well local displacement errors; for example, when the same pixel location in images projected for reference and degraded geometry is compared, the measured error may not express well the user perceived distortion since the color of different 3D positions in the reference and degraded PCs are used. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the frontal projection for the Egyptian Mask PC before coding (reference geometry and color) and after G-PCC decoding and recoloring with the original/reference color, for the lowest geometry rate. <ref type="figure" target="#fig_1">Fig. 2</ref> also shows a residual image with the difference between the previous reference and decoded, recolored projected images (with some enhancement for better visualization). Although the color in both PCs is the same and the PCs are visually similar, the residual image shows large misalignment errors.</p><p>To overcome this key problem, this paper proposes an original solution involving computing the 2D quality metrics with different color data under two geometry conditions, notably reference and degraded geometries. The idea is to use the PC geometry, reference and degraded, and to perform recoloring to assign the decoded color to the reference geometry (top branch of the architecture) and the reference color to the decoded geometry (bottom branch of the architecture). By using this approach, the projected images are always geometryaligned within each branch, thus avoiding misalignments.</p><p>To recolor PC A, with the color of PC B, each point in PC A will have a color assigned using the color of one or more corresponding points in PC B. In the proposed recoloring algorithm, the color for each point in PC A after recoloring is determined as follows:</p><p>1. For each point in PC A, the nearest neighbor in PC B is found ( " ) and, for each point in PC B, the nearest neighbor in PC A is found ( # ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For each point in PC</head><p>A perform: if point is listed in the nearest neighbors of some points in PC B ( ? # ), its color is the average color of the points in PC B which have point as their nearest neighbor, as defined in <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">$ = ) ? % &amp; %?#,)) ! (%),$ ? 1 %?#,)) ! (%),$<label>(1)</label></formula><p>Otherwise, its color is the color of its nearest neighbor listed in " . In (1) $ and % are the colors at points a in PC A and b in PC B. The denominator counts the number of points in PC B which have point a in PC A as its nearest neighbor. <ref type="figure">Fig. 3</ref> illustrates the recoloring process for the Amphoriskos PC using a point-based rendering solution with cube primitives. <ref type="figure">Fig. 3</ref>(a) shows the reference PC (reference geometry and color) on the left and the recolored PC (reference geometry and decoded color) on the right. <ref type="figure">Fig. 3(b)</ref> shows the degraded PC (decoded geometry and color) on the left and the recolored PC (a) (b) <ref type="figure">Fig. 3</ref>. Amphoriskos PC decoded with G-PCC in Octree geometry coding mode and Lifting color coding mode at the lowest rate: (a) PCs obtained for the reference geometry with the reference (left) and decoded color, after recoloring (right); (b) PCs obtained for the decoded geometry with the decoded color (left) and reference color, after recoloring (right).</p><p>(decoded geometry and reference color) on the right. Each pair of PCs have the same geometry and either the reference or degraded colors; because of the geometry alignment, it is now possible to compare these color values using a simple, direct pixel-to-pixel correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Projection</head><p>This is a core module of the proposed JGC-ProjQM architecture, where 3D PCs are mapped onto six 2D planes from different perspectives, thus creating the projected images. The proposed projection procedure is based on the orthographic projection, an often-used parallel projection that renders objects with suitable shapes and sizes. In this procedure, each PC point is projected to a pixel in a 2D image while considering its visibility (or occlusion) for each specific viewing perspective and, thus, projection plane. The proposed projection procedure considers two main steps:</p><p>? Mapping: The planar (or 2D) images are generated applying the orthographic projection for the different sides (planes) of the precision box, i.e., box surrounding the PC object with a size defined by the coordinates' precision. For each plane, a point is projected onto the plane as long as the point is not occluded by another point closer to the same plane.</p><p>? Filtering: Points which are projected onto a projection plane but do not belong to the PC surface closer to the plane must be removed. Considering a typical rendering process, these points will be occluded due to the use of primitives around each point or due to surface reconstruction techniques. These points are unduly projected when there is some empty space between points in the surface closer to the plane and, therefore, points from the opposite side of the object are unduly projected onto this plane. Since these points are not visible after PC rendering is performed, a filtering technique is used to remove these points from the projected images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Mapping</head><p>The mapping algorithm projects every point visible from the perspective associated to each specific PC precision box plane as follows:</p><p>1. Six images, each corresponding to a projection plane are initialized with a uniform background color, e.g., white. In practice, the background color can be any color (in this case 255 is used) since the background pixels will be later padded and, thus, filled with non-uniform, interpolated values. Six planes are defined, notably PL = { , , ,??,?,?}, the first three with (0,0,0) origin and, the last three, including the opposite point, i.e. (2 ! , 2 ! , 2 ! ), where is the PC coordinates precision; these planes are shown in <ref type="figure">Fig. 4</ref>.</p><p>Initially, these images are set to 255 (as mentioned above) according to <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_1">-. = 7 255 ? 255 ? ? ? 255 ? 255 &lt; &amp; " ?&amp; " ?0<label>(2)</label></formula><p>where IPL is the projected image associated to any of the planes, PL, e.g., <ref type="bibr" target="#b11">12</ref> or 3?5?. <ref type="figure">Fig. 4</ref>. The six precision box planes surrounding a PC object: { , , ,??,?,?} 2. Six binary images corresponding to the occupancy map ( ) for each projected image are initialized with '0', i.e., non-occupied pixel/voxel, as follows:</p><formula xml:id="formula_2">67 = 7 0 ? 0 ? ? ? 0 ? 0 &lt; &amp; " ?&amp; " ?0<label>(3)</label></formula><p>where OMPL is the occupancy associated to any of the planes PL, e.g., <ref type="bibr" target="#b11">12</ref> or 3?5?.</p><p>3. To keep track of the occluded points, for the three coordinates, two depth maps are used to store the minimum projected depth (NearD) and maximum depth (FarD). These depth maps are initialized with 0 and 2 ! , respectively, according to (4).</p><formula xml:id="formula_3">= ( 0 ? 0 ? ? ? 0 ? 0 - ! ! ?! ! , = ( 2 # ? 2 # ? ? ? 2 # ? 2 # - ! ! ?! !<label>(4)</label></formula><p>While NearD keeps record of the depth of the closest projected point to the (??,?,?) planes, FarD keeps record of the depth of the closest projected point to the ( , , ) planes, for every projected position.</p><p>4. For each point 8 = ( 9 , 3 , 5 ) in the PC with color ! , two parallel planes from the set PL will be jointly processed, starting with ( ,??). The following steps are performed:</p><p>i. Define the depth 8 of point 8 as the value for the coordinate 5 since z is the perpendicular axis to both the and ?? planes.</p><p>ii. If 8 is less than or equal to the maximum depth at position ( 9?, 3?) in FarD for this plane, then the point is projected onto position ( 9?, 3?) of image 9?3?. The corresponding pixel in the occupancy map is also set to '1' and the corresponding maximum depth is updated to the depth 8 as follows: iii. If 8 is larger than or equal to the minimum depth at position H 9 , 3 I in NearD for this plane, then the point is projected onto position H 9 , 3 I of image <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3</ref> . The corresponding pixel in the occupancy map is also set to '1' and the corresponding minimum depth is updated to the depth 8 as follows:</p><formula xml:id="formula_4">if 8 ? ( 9?, 3?) then<label>(5)</label></formula><formula xml:id="formula_5">if 8 ? ( 9 , 3 ) then<label>(6)</label></formula><p>9,3 H 9 , 3 I = ! and 9,3 H 9 , 3 I = 1 and</p><formula xml:id="formula_6">H 9 , 3 I = 8</formula><p>Finally, steps i-iii have to be repeated for the other two pairs of planes in PL, i.e., ( ,?) and ( ,?), thus obtaining more four images ( 1: , 9?5?, 2: , 3?5?) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Filtering</head><p>At this stage, every point that is not occluded should have been projected. In this context, it is possible that some points from the surface farther to the projection plane may be visible from it and, thus, projected onto it. This occurs because the surfaces close and farther away to the projected plane may not have the same density and may not be aligned. Thus, some pixel positions may be filled from surfaces that are not even visible after rendering (from the perspective of the projected plane).</p><p>These points should be filtered out by comparing their depth to the depth of their neighboring pixels in a window since the depth of these far away points are significantly different from its projected neighboring closest points. The algorithm for filtering the points from the 'back part' of the PC that are not seen by the users from that perspective, proceeds as follows:</p><p>1. For each occupied pixel ( , ) in the projected image for planes ? { , , }, compute the difference between NearD(u,v) and the average of NearD values for its occupied neighbors in a 2D window with size ? centered at ( , ). If this difference is smaller than a predefined positive threshold , then set that position to unoccupied in the associated occupancy map and set the corresponding projected image position to the background value as follows: </p><p>67 C ( , ) = 255 and ; C ( , ) = 0 The algorithm above is a proximity filtering algorithm that depends on the threshold value that should be set according to the curvature of the objects surface, which is typically not very high. Even if a few PC front points are unduly 'filtered', the impact is small as they will be filled during the padding process performed next. After some experiments, it was found that a fixed filtering threshold of 20 was effective in the filtering of these already projected pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cropping</head><p>The output of the previous step corresponds to six projected images with size 2 ! ? 2 ! , one per projection plane. These images may contain a significant amount of background pixels corresponding to empty areas around the PC object projection. To remove the undesired influence of this background data, the excessive background around the projected PC is cropped out to the minimum size including the occupied pixels after projection. First, a bounding box surrounding the PC object(s) for each projected image is obtained for the reference and degraded PCs, using the occupancy map obtained during the projection step. More precisely, by scanning from top left to bottom right, the positions of the first and last occupied pixel in each occupancy map are used to define the bounding box for the object in the projected map. Note that the reference and degraded bounding boxes for each view are identical as using aligned geometries. After, cropping is performed using the obtained bounding box for the reference/degraded projected images associated to each plane. <ref type="figure" target="#fig_3">Fig. 5</ref> shows an example of the cropping operation for the Longdress PC. While the full projected image is shown in <ref type="figure" target="#fig_3">Fig.  5(a)</ref> with the precision bounding box in green, <ref type="figure" target="#fig_3">Fig. 5(b)</ref> shows the cropped image with a largely reduced background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Padding</head><p>After cropping, the background information is significantly removed. However, background pixels associated to unoccupied points inside the object and some background area around the object still exist. To avoid the negative impact of the background color pixels when computing the 2D quality metric, the color values for the unoccupied pixels (and thus with uniform background values) should be set using some 2D interpolation technique, thus more appropriately filling all empty spaces. This operation aims to replicate the effect of PC rendering, which creates continuous surfaces without holes using appropriate rendering primitives, thus removing any bias on the quality score due to pixels with the same identical value for reference and degraded projected images, i.e., for areas without any distortion. This padding approach allows to apply any 2D quality metric, which uses as input a rectangular image without requiring any adaptations for the 2D quality metric to work with arbitrarily shaped 2D regions, i.e., to consider only foreground pixels. To fill the non-occupied pixels, it is proposed to use an image inpainting technique from the literature called Navier-Strokes <ref type="bibr" target="#b29">[30]</ref>, which has been selected due to its good performance. The occupancy maps created during the projection process are used as a padding mask to guarantee that only the unoccupied pixels are padded. An example of a padded image is shown in <ref type="figure" target="#fig_3">Fig. 5(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Fusion</head><p>Before obtaining the final JGC-ProjQM PC quality metric, two intermediate scores are computed, JGC-ProjQMreference and JGC-ProjQMdegraded, corresponding to the two parallel branches in the architecture, one corresponding to the reference geometry and another to the degraded geometry. To combine the two intermediate quality scores, the following linear model is proposed:</p><p>JGC-ProjQM= JGC-ProjQMreference+ JGC-ProjQMdegraded <ref type="bibr" target="#b8">(9)</ref> In <ref type="formula" target="#formula_0">(14)</ref>, the and parameters are estimated using a least squares linear regression procedure that minimizes the residual sum of the squared difference between the objective scores predicted by the linear approximation, JGC-ProjQM, and the Mean Opinion Scores (MOS) available in some selected dataset. For this paper, the used dataset was the MPEG Point Cloud Compression Dataset (M-PCCD) <ref type="bibr" target="#b30">[31]</ref>. Although more complex models may be selected, they typically require more parameters, bringing the risk of overfitting to the selected dataset. This is rather critical as there are not that many PC datasets available with subjective scores and representative geometry and color degradations, especially compared to image and video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERFORMANCE ASSESSMENT</head><p>The main objective of this section is to evaluate the proposed JGC-ProjQM PC quality metric compared with the best performing PC quality metrics available in the literature, projection-based or not. Moreover, an ablation study is presented to assess the performance impact of each module in the JGC-ProjQM architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Subjective Evaluation Dataset</head><p>To perform the performance assessment, the MPEG Point Cloud Compression Dataset (M-PCCD), publicly available in <ref type="bibr" target="#b30">[31]</ref>, has been selected. This recent dataset includes 232 stimuli where geometry and color are both encoded, which is very suitable to evaluate the proposed JGC-ProjQM performance. The M-PCCD dataset includes both the MOS values as well as the reference and degraded/decoded PCs.</p><p>The test material in this dataset corresponds to nine PCs, including four objects and five human figures. While Longdress, Loot, Redandblack, Soldier, The20smaria and Head are from the MPEG repository <ref type="bibr" target="#b31">[32]</ref>, Romanoillamp and Biplane are from the JPEG repository <ref type="bibr" target="#b32">[33]</ref> and Amphoriskos from Sketchfab dataset <ref type="bibr" target="#b33">[34]</ref>, see <ref type="figure" target="#fig_4">Fig. 6</ref>. Redandback has been used for training the subjects. The PCs are shown in <ref type="figure" target="#fig_4">Fig. 6</ref> and their characteristics listed in <ref type="table" target="#tab_1">Table I.</ref> The PCs have been coded in the following conditions: i) 24 rates for the MPEG G-PCC standard with six different rates for each combination of Octree and TriSoup geometry coding modes with the RAHT and Lifting color coding modes; and ii) five rates for the MPEG V-PCC standard. The rates were selected based on the MPEG Common Test Conditions (CTC) recommendations <ref type="bibr" target="#b14">[15]</ref>.</p><p>The performed subjective quality assessment study has followed the Double Stimulus Impairment Scale (DSIS) methodology and scores have been obtained in two separate labs, each with 20 subjects. A point-based rendering procedure was used, and the PCs were shown side-by-side with an interactive evaluation protocol which allowed subjects to select and modify their viewpoint.  The outlier detection algorithm described in the ITU-R Recommendation BT.500-13 <ref type="bibr" target="#b34">[35]</ref> was performed separately for each laboratory, to exclude subjects whose ratings deviated drastically from the rest of the quality scores. As a result, no outliers were identified, thus, leading to 20 ratings per stimulus at each lab. Then the MOS was computed by averaging all the subject scores for each stimulus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Parameters Overfitting Checking</head><p>The final JGC-ProjQM metric is a linear combination of the intermediate quality metrics associated to the two architectural branches as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The and parameters are estimated using a least squares linear regression procedure which uses the objective and subjective scores for all point clouds and therefore is important to confirm that no overfitting happens. For this purpose, the M-PCCD dataset is randomly split into 75% training data and 25% test data for 100 times. For each iteration, the and parameter values are estimated from the training data split and used to compute the final PC quality metric score for the test data split. The subjective-objective performance is measured with the Pearson Linear Correlation Coefficient (PLCC) which has been computed for each iteration and is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. The average performance for all iterations (i.e., for different splits) is computed as the average PLCC over all iterations and is also shown in <ref type="figure" target="#fig_5">Fig. 7</ref> as a red line. The PLCC performance computed using all data for both training and testing is also shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. The analysis of the results shows that the final JGC-ProjQM values are very close to the average JGC-ProjQM value, thus implying that the obtained performance is not due to overfitting. This overfitting analysis is made for four 2D quality metrics, notably SSIM, MS-SSIM, FSIM and Y-PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results and Analysis</head><p>In this section, the objective-subjective correlation performance of the proposed PC quality metric 1 is evaluated and compared with some relevant benchmarks. In the following, non-linear fitting is applied to all objective quality scores, in this case, using the following logistic function:</p><formula xml:id="formula_8">! = &amp; + E ? &amp; 1 + FG H &amp; FI 5 I 6 J<label>(10)</label></formula><p>1 A python implementation is made available online at https://github.com/AlirezaJav/Projection-based-PC-Quality-Metric where 8 are the objective metric scores and E , ? , K are the regression model parameters. This approach allows to fit the objective metric scores to the perceptual (MOS) scale to obtain the fitted predicted MOS scores. To assess the quality metrics performance, the PLCC, Spearman Rank Order Correlation Coefficient (SROCC) and Root Mean Squared Error (RMSE) are used. When the PLCC and SROCC scores are close to '1', the predicted objective quality scores are highly correlated and have a monotonic relationship with the ground-truth MOS scores. As a measure of monotonicity, SROCC does not depend on the selected fitting function. A wide range of objective PC quality metrics available in the literature were selected as benchmarks to evaluate the proposed PC quality metric more effectively. These metrics are listed in <ref type="table" target="#tab_1">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Projection-based Metrics Performance Comparison</head><p>In this section, the proposed JGC-ProjQM metric performance is compared with the projection-based metrics proposed in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>. For a fair comparison, the same 2D quality metric is used in the JGC-ProjQM quality metric and the selected benchmarks. In <ref type="bibr" target="#b9">[10]</ref>, VIFP, SSIM, MS-SSIM and Y-PSNR are used, while in <ref type="bibr" target="#b10">[11]</ref>, only Y-PSNR is used.  Metric Name Short Description D1and D1-PSNR <ref type="bibr" target="#b14">[15]</ref> Po2Po MSE error and associated PSNR. D2 and D2-PSNR <ref type="bibr" target="#b14">[15]</ref> Po2Pl MSE error and associated PSNR. Hausdorff distance and Hausdorff PSNR <ref type="bibr" target="#b12">[13]</ref> Po2Po and Po2Pl Hausdorff distance and associated PSNR. <ref type="bibr" target="#b14">[15]</ref> MSE between luminance of points and their nearest neighbor and associated PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y-MSE and Y-PSNR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angle-MSE [16]</head><p>Mean squared of angular similarity between underlying surfaces at each point and its nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCQM [26]</head><p>Weighted average of geometry curvature features and lightness, chroma and hue features between reference and decoded PCs.</p><p>PointSSIM <ref type="bibr" target="#b27">[28]</ref> Color features are extracted in a local neighborhood around each point in the reference and degraded PCs using variance as statistical dispersion measure.</p><p>!" # <ref type="bibr" target="#b20">[21]</ref> Computed from the color histogram and correlogram of the luminance component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>$% [21]</head><p>Linear combination of Po2Pl MSE with !" # .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCMRR [27]</head><p>Reduced reference quality metric corresponding to the weighted average of 7 geometry, 7 color and 7 normal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geotex [22]</head><p>Local Binary Pattern descriptors applied to the luminance are extracted and used for quality metric computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proj-Y-PSNR [11]</head><p>Projection-based quality metric using six faces of a cube; the six images are concatenated and Y-PSNR are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proj-Y-MSSIM and Proj-Y-VIFP [10]</head><p>Projection-based metric using 42 views; the final metric is obtained using several 2D metrics and pooled using l1-norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GH-PSNR [17]</head><p>Generalized Hausdorff distance-based PSNR, considering 98% of the distances, using maximum pooling function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RA-PSNR [19]</head><p>Resolution-Adaptive PSNR considering the rendering resolution computed over the 10 nearest neighbors.</p><p>MMD and MMD-PSNR <ref type="bibr" target="#b17">[18]</ref> PSNR based Point-to-Distribution metric where the mean Mahalanobis distances between a point and a distribution of points is computed.</p><p>Table III clearly shows that proposed JGC-ProjQM metric significantly outperforms the already available projectionbased PC quality metrics, for the same 2D quality metric. The maximum JGC-ProjQM gains are 18.0% for PLCC, 18.3% for SROCC and 0.26 for RMSE, comparing to Alexiou et al. <ref type="bibr" target="#b9">[10]</ref>. These gains are rather large and consistent across the used quality metric performance measures, i.e., PLCC, SROCC and RMSE, especially considering that <ref type="bibr" target="#b9">[10]</ref> uses 42 views, which is a much larger (and also more complex) number of views than the six views considered in JGC-ProjQM. Other interesting conclusion is that VIFP is the best 2D quality metric and Y-PSNR leads to the worst correlation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) 2D Quality Assessment Metrics Performance Impact</head><p>As stated in Section III, the proposed JGC-ProjQM metric is flexible enough to accommodate any 2D quality metric. In this section, the performance of the proposed metric is evaluated for several 2D quality metrics. This will allow to identify which 2D quality metric leads to the best correlation performance. In this case, the same 2D quality metric is used in both architectural branches of the proposed quality metric, i.e., reference and degraded, especially because the fusion module works best when the quality range and scale is similar for both the reference and degraded geometry branches. Since the used 2D quality metric can significantly influence the JGC-ProjQM performance, a wide set of available quality 2D metrics are evaluated, notably: 1) Y-PSNR, 2) PSNR-HVS <ref type="bibr" target="#b35">[36]</ref>; 3) PSNR-HVS-M <ref type="bibr" target="#b36">[37]</ref>; 4) Structural Similarity Index Metric (SSIM) <ref type="bibr" target="#b37">[38]</ref>; 5) Multi-Scale Structural Similarity Index Metric (MS-SSIM) <ref type="bibr" target="#b38">[39]</ref>, 6) Visual Information Fidelity Measure (VIFP) <ref type="bibr" target="#b39">[40]</ref>; 7) Feature Similarity Index (FSIM) <ref type="bibr" target="#b40">[41]</ref>; 8) Visual Saliency Index (VSI) <ref type="bibr" target="#b41">[42]</ref>; 9) Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b42">[43]</ref>; 10) Deep Image Structure and Texture Similarity (DISTS) <ref type="bibr" target="#b43">[44]</ref>; and 11) Haar Perceptual Similarity Index (HaarPSI) <ref type="bibr" target="#b44">[45]</ref>. <ref type="table" target="#tab_1">Table IV</ref> shows the JGC-ProjQM correlation performance for a large set of 2D quality metrics, considering all possible coding degradations (all codecs data). The DISTS 2D quality metric has the best correlation performance among all the 2D quality metrics while LPIPS, FSIM, and HaarPSI come in the following positions. Both DISTS and LPIPS are very recent 2D quality metrics that use powerful deep-learning features to perform quality assessment. More specifically, DISTS includes both color and structure similarity components, which are weighted to achieve a better correlation with the perceived quality and to be invariant to small changes in color patches (homogenous regions with repeated elements). LPIPS computes distances between features extracted from the reference and degraded projected images at different layers of a neural network. For both DISTS and LPIPS metrics, a perceptual feature space is used. Typically, these quality metrics weight more general appearance changes than small color changes, where their elements may have different location, size, color and orientation. This fits rather well the projected images obtained by the proposed JGC-ProjQM metric where small color changes may occur due to the recoloring process. The proposed JGC-ProjQM metric with the best four 2D quality metrics, i.e., DISTS, LPIPS, FSIM, VSI, will be used for the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Overall Point Cloud Quality Metrics Performance Comparison</head><p>In this section, the performance of the proposed JGC-ProjQM metric is compared with many state-of-the-art PC quality metrics available in the literature. <ref type="table" target="#tab_5">Table V</ref> shows the SROCC, PLCC and RMSE scores for the many benchmark quality metrics, using all the codecs scores together as well as the V-PCC and G-PCC scores individually. When a large set of metrics is proposed in a reference, only the best variants are included. This separation can be justified by the fact that the PC quality metrics performance may be significantly influenced by the different type of coding artifacts generated by different PC coding solutions (which may be rather different). This split was performed as follows: i) G-PCC decoded PCs, including TriSoup and Octree geometry coding modes as well as RAHT and Lifting color coding modes; ii) V-PCC decoded PCs; and iii) all decoded PCs together. From Table V, where the best results are highlighted in bold and second best in grey shaded cells, the following conclusions may be derived:</p><p>? Overall correlation performance: The proposed JGC-ProjQM metric using DISTS is the best performing metric for PC quality assessment, achieving the best PLCC,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) JGC-ProjQM Metric Ablation Study</head><p>The proposed JGC-ProjQM metric includes several modules that have different impact on the overall correlation performance. To individually assess the impact of each JGC-ProjQM module, an ablation study is performed using the entire dataset. More precisely, the JGC-ProjQM metric performance is measured for all stimuli (all codecs scores) included in the dataset, each time turning off one of the architectural modules while keeping the others, notably recoloring, cropping, and padding. <ref type="table" target="#tab_1">Table VI</ref> shows the PLCC and SROCC results after non-linear logistic fitting for this ablation study.</p><p>The following conclusions can be derived: ? Recoloring: The correlation performance results show the importance of the recoloring module for the JGC-ProjQM performance. For example, for VSI and FSIM, the absence of recoloring leads to losses of 10.8% and 9.7% for PLCC and 10.3% to 8.6% for SROCC. The performance losses after removing recoloring are lower for DISTS and LPIPS, mainly because these two recent 2D quality metrics are robust to geometry distortions and transformations. However, the performance gains by using the recoloring module are up to 10.8% for PLCC and 10.3% for SROCC.</p><p>? Cropping: The cropping module significantly improves the JGC-ProjQM performance by removing background pixels, around the projected PC, that are common to the reference and degraded PCs. These background areas work as a distractor for the 2D quality assessment metric and lower the JGC-ProjQM prediction power, even when the background pixels are padded. The performance gains associated to the cropping module are up to 12.3% for PLCC and 11.4% for SROCC.</p><p>? Padding: The padding module also improves the JGC-ProjQM correlation performance. While most of the excessive background is removed by cropping, some background area around the object remains. Moreover, background pixels that are visible in the object surface, often due to sparse sampling during acquisition or the removal of points during coding, are filled by padding. The performance gains by using the padding module are up to 11.2% for PLCC and 12.4% for SROCC. It is important to note that the correlation performance gains for these modules may be significantly higher when less powerful 2D image metrics, i.e., with lower overall correlation performance, are used, such as MS-SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper proposes a novel joint geometry and color projection-based PC quality metric, JGC-ProjQM, that compares PCs in the 2D domain for two geometry conditions, i.e., reference and degraded geometry. The projection-based PC quality metric applies a projection to obtain six projected images, corresponding to different views over the PC, which are after cropped and padded before performing a 2D quality assessment. After, any 2D quality metric can be applied and the intermediate quality scores for the two geometry conditions are fused to obtain the final JGC-ProjQM quality score. The objective-subjective correlation results show that proposed JGC-ProjQM metric outperforms all the state-of-the-art PC quality metrics and, thus, PC quality can be efficiently measured in the 2D domain, especially when powerful 2D quality metrics are also exploited. As future work, some visual saliency information and attention models could be integrated in the proposed PC quality assessment framework, thus further improving the correlation performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Proposed joint geometry and color projection-based PC quality metric (JGC-ProjQM) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Egyptian Mask projected from front view: (a) Reference PC; (b) Decoded G-PCC geometry for lowest geometry rate, recolored with the original color; (c) Residual image between (a) and (b) after enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>For each occupied pixel ( , ) in the projected image for planes R ? {??,?,?}, compute the difference between FarD (u,v) and the average of FarD values for its neighbors in a 2D window with size ? centered at ( , ). If this difference is smaller than a predefined positive threshold , than set that position to unoccupied in the associated occupancy map and set the corresponding projected image position of the background value as follows:if ; C ( , ) ? ? D$&gt;? #$ , (8,@)?AB #$ ,(8,@)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Frontal view projection for Longdress PC decoded with G-PCC in Octree coding mode and RAHT color coding mode at the medium rate: (a) projected image with the bounding box in green; (b) cropped image; (c) padded image. The padded image looks 'brighter' since the non-padded images still have many light grey background pixels inside the foreground area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Test materials in the M-PCCD dataset. From top left to bottom right: (a) Amphoriskos; (b) Biplane; (c) Head; (d) Romanoillamp; (e) Longdress; (f) Loot; (g) Redandblack; (h) Soldier; (i) The20smaria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>JGC-ProjQM PLCC overfitting analysis for the following 2D quality metrics: (a) SSIM; (b) MS-SSIM; (c) FSIM; (d) Y-PSNR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I M</head><label>I</label><figDesc>-PCCD TEST PCS AND ASSOCIATED CHARACTERISTICS.</figDesc><table><row><cell>Name</cell><cell>Type</cell><cell>Repository</cell><cell cols="2">Precision No. Points</cell></row><row><cell cols="2">Amphoriskos Objects</cell><cell>Sketchfab</cell><cell>10-bit</cell><cell>814.474</cell></row><row><cell cols="3">Romanoillamp Objects JPEG repository</cell><cell>10-bit</cell><cell>636.127</cell></row><row><cell>Biplane</cell><cell cols="2">Objects JPEG repository</cell><cell>10-bit</cell><cell>1.181.016</cell></row><row><cell>Head</cell><cell cols="2">Objects MPEG repository</cell><cell>9-bit</cell><cell>938.112</cell></row><row><cell>Longdress</cell><cell cols="2">People MPEG repository</cell><cell>10-bit</cell><cell>857.966</cell></row><row><cell>Loot</cell><cell cols="2">People MPEG repository</cell><cell>10-bit</cell><cell>805.285</cell></row><row><cell cols="3">Redandblack People MPEG repository</cell><cell>10-bit</cell><cell>757.691</cell></row><row><cell>Soldier</cell><cell cols="2">People MPEG repository</cell><cell>10-bit</cell><cell>1.089.091</cell></row><row><cell cols="3">The20smaria People MPEG repository</cell><cell>10-bit</cell><cell>1.553.937</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PC</head><label>II</label><figDesc>QUALITY METRICS SELECTED AS BENCHMARKS FOR PERFORMANCE COMPARISON.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III OBJECTIVE</head><label>III</label><figDesc>-SUBJECTIVE CORRELATION PERFORMANCE FOR JGC-PROJQM USING DIFFERENT 2D QUALITY METRICS, FROM BEST TO WORST PLCC.</figDesc><table><row><cell>2D Metric</cell><cell cols="3">PLCC SROCC RMSE</cell></row><row><cell>DISTS</cell><cell>94.7</cell><cell>95.6</cell><cell>0.439</cell></row><row><cell>LPIPS</cell><cell>92.3</cell><cell>93.2</cell><cell>0.523</cell></row><row><cell>FSIM</cell><cell>88.2</cell><cell>90.1</cell><cell>0.640</cell></row><row><cell>VSI</cell><cell>85.4</cell><cell>87.6</cell><cell>0.707</cell></row><row><cell>HaarPSI</cell><cell>84.8</cell><cell>87.7</cell><cell>0.721</cell></row><row><cell>VIPF</cell><cell>83.0</cell><cell>85.5</cell><cell>0.758</cell></row><row><cell>SSIM</cell><cell>80.9</cell><cell>81.3</cell><cell>0.800</cell></row><row><cell>MS-SSIM</cell><cell>79.5</cell><cell>82.8</cell><cell>0.825</cell></row><row><cell>PSNR HVS M</cell><cell>78.7</cell><cell>81.3</cell><cell>0.840</cell></row><row><cell>PSNR HVS</cell><cell>78.4</cell><cell>80.5</cell><cell>0.845</cell></row><row><cell>Y-PSNR</cell><cell>77.1</cell><cell>79.1</cell><cell>0.866</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV OBJECTIVEProjQM vs point-based PC quality metrics:</head><label>IV</label><figDesc>-SUBJECTIVE CORRELATION PERFORMANCE COMPARISON OF THE PROPOSED METRIC WITH [10] AND [11] FOR THE SAME 2D QUALITY METRIC. The JGC-ProjQM metric significantly outperforms the pointbased D1-PSNR and D2-PSNR and plane-to-plane quality metrics that are currently used by the MPEG and JPEG standardization groups. The JGC-ProjQM-DISTS gains over these metrics are rather significant, notably up to 32.3% for PLCC and 47.7% for SROCC for all data. Moreover, JGC-ProjQM-DISTS outperforms the best pointbased metric in the literature (Po2Po RA-PSNR) by 5.9% for PLCC and 5.4% for SROCC for all data, and by larger gains for all remaining point-based quality metrics. The JGC-ProjQM-DISTS is the best performing metric for V-PCC decoded data with 86.4% for PLCC and 85.3% for SROCC. The benchmark point-based and projection-based quality metrics fail to reliably assess the V-PCC decoded quality. The JGC-ProjQM-DISTS metric outperforms the best projection-based metric in the literature (Proj-Y-VIFP) by 42.7% for PLCC and 49.7% for SROCC. For the best point-based metric (Po2Pl RA-PSNR), the performance increase is 6.5% and 8.4% for PLCC and SROCC, respectively. Nowadays, the featurebased metrics are the best performing PC quality metrics in the literature and thus the gains are smaller, 3.4% for PLCC and 0.8% for SROCC, when compared to the feature-based PointSSIM metric. Both, JGC-ProjQM-DISTS and PointSSIM show higher performance compared to the other quality metrics.</figDesc><table><row><cell>Projection-based PC quality metric</cell><cell>2D quality metric</cell><cell cols="3">PLCC SROCC RMSE</cell></row><row><cell>JGC-ProjQM</cell><cell></cell><cell>85.5</cell><cell>83.0</cell><cell>0.760</cell></row><row><cell>Alexiou et al. [10]</cell><cell>VIFP</cell><cell>74.2</cell><cell>71.5</cell><cell>0.951</cell></row><row><cell>Gain</cell><cell></cell><cell>11.3</cell><cell>11.5</cell><cell>0.191</cell></row><row><cell>JGC-ProjQM</cell><cell></cell><cell>81.3</cell><cell>80.9</cell><cell>0.800</cell></row><row><cell>Alexiou et al. [10]</cell><cell>SSIM</cell><cell>63.3</cell><cell>62.6</cell><cell>1.061</cell></row><row><cell>Gain</cell><cell></cell><cell>18.0</cell><cell>18.3</cell><cell>0.261</cell></row><row><cell>JGC-ProjQM</cell><cell></cell><cell>82.8</cell><cell>79.5</cell><cell>0.830</cell></row><row><cell>Alexiou et al. [10]</cell><cell>MS-SSIM</cell><cell>75.2</cell><cell>70.9</cell><cell>0.959</cell></row><row><cell>Gain</cell><cell></cell><cell>7.6</cell><cell>8.6</cell><cell>0.129</cell></row><row><cell>JGC-ProjQM</cell><cell></cell><cell>79.1</cell><cell>77.1</cell><cell>0.870</cell></row><row><cell>Alexiou et al. [10]</cell><cell></cell><cell>62.8</cell><cell>66.7</cell><cell>1.013</cell></row><row><cell>Gain</cell><cell>Y-PSNR</cell><cell>16.3</cell><cell>10.4</cell><cell>0.143</cell></row><row><cell>de Queiroz et al. [11]</cell><cell></cell><cell>33.1</cell><cell>43.0</cell><cell>1.228</cell></row><row><cell>Gain</cell><cell></cell><cell>46.0</cell><cell>34.1</cell><cell>0.358</cell></row><row><cell cols="5">SROCC and RMSE scores. This result also confirms that by</cell></row><row><cell cols="5">projecting a PC into several 2D images (which are close to</cell></row><row><cell cols="5">what a user sees) and after exploiting the power of 2D</cell></row><row><cell cols="5">quality metrics, a top correlation performance can be</cell></row><row><cell cols="5">obtained. The proposed JGC-ProjQM with LPIPS and the</cell></row><row><cell cols="5">PointSSIM quality metrics also have a very high correlation</cell></row><row><cell>performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">? JGC-? G-PCC decoded data: The JGC-ProjQM-DISTS metric is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the best performing metric for G-PCC decoded data with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>95.</cell></row></table><note>? JGC-ProjQM vs feature-based PC quality metrics: The overall correlation performance of the best proposed projection-based metric, i.e., JGC-ProjQM-DISTS, is almost 2.1% in PLCC and 3.8% in SROCC higher than the best feature-based quality metric, i.e., PointSSIM. The feature-based quality metrics often come in second place, achieving also rather good performance, especially compared to point-based quality metrics.? V-PCC decoded data:8% for PLCC and 96% for SROCC. Feature-based and point-based metrics (except for Pl2Pl) also show acceptable performance for G-PCC decoded data. However, JGC- ProjQM-DISTS outperforms the best MPEG/JPEG adopted metrics (D2-PSNR) by 12.4% for PLCC and 8.7% for SROCC. The correlation gains against the best feature-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>OBJECTIVE-SUBJECTIVE CORRELATION PERFORMANCE COMPARISON BETWEEN THE PROPOSED JGC-PROJQM METRIC AND BENCHMARKING METRICS. CELLS WITH A DASH (-) SHOW MISSING PERFORMANCE SINCE SOME OF THESE RESULTS WERE OBTAINED FROM THE RELEVANT PAPER.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell>V-PCC</cell><cell></cell><cell></cell><cell>G-PCC</cell><cell></cell></row><row><cell>Type</cell><cell>Metric Name</cell><cell cols="9">PLCC SROCC RMSE PLCC SROCC RMSE PLCC SROCC RMSE</cell></row><row><cell></cell><cell>D1 [15]</cell><cell>84.8</cell><cell>86.8</cell><cell>0.722</cell><cell>46.3</cell><cell>42.0</cell><cell>0.928</cell><cell>88.6</cell><cell>90.0</cell><cell>0.651</cell></row><row><cell></cell><cell>D1-PSNR [15]</cell><cell>77.7</cell><cell>79.7</cell><cell>0.857</cell><cell>30.4</cell><cell>28.2</cell><cell>0.997</cell><cell>82.5</cell><cell>83.9</cell><cell>0.794</cell></row><row><cell></cell><cell>Hausdorff [13]</cell><cell>1.4</cell><cell>37.0</cell><cell>1.360</cell><cell>14.7</cell><cell>-17.5</cell><cell>1.047</cell><cell>5.3</cell><cell>54.4</cell><cell>1.404</cell></row><row><cell>Po2Po</cell><cell>Hausdorff PSNR [13] GH-PSNR [17]</cell><cell>66.1 84.6</cell><cell>36.6 86.9</cell><cell>1.021 0.726</cell><cell>27.1 57.8</cell><cell>-14.9 57.3</cell><cell>1.008 0.854</cell><cell>76.0 88.5</cell><cell>53.3 89.9</cell><cell>0.912 0.653</cell></row><row><cell></cell><cell>RA-PSNR [19]</cell><cell>88.8</cell><cell>90.2</cell><cell>0.626</cell><cell>68.9</cell><cell>67.3</cell><cell>0.759</cell><cell>91.0</cell><cell>91.8</cell><cell>0.584</cell></row><row><cell></cell><cell>Y-MSE [15]</cell><cell>66.3</cell><cell>66.2</cell><cell>1.018</cell><cell>37.9</cell><cell>33.3</cell><cell>0.969</cell><cell>70.3</cell><cell>70.3</cell><cell>0.998</cell></row><row><cell></cell><cell>Y-PSNR [15]</cell><cell>67.1</cell><cell>66.2</cell><cell>1.009</cell><cell>37.6</cell><cell>33.3</cell><cell>0.970</cell><cell>71.4</cell><cell>70.3</cell><cell>0.984</cell></row><row><cell>Point-based</cell><cell>D2 [15]</cell><cell>85.9</cell><cell>88.4</cell><cell>0.695</cell><cell>73.5</cell><cell>68.8</cell><cell>0.710</cell><cell>87.9</cell><cell>90.6</cell><cell>0.669</cell></row><row><cell cols="4">Po2Pl based quality metric (PointSSIM) are 1.4% for PLCC and D2-PSNR [15] 80.5 83.8 Hausdorff [13] 67.2 50.5 Hausdorff PSNR [13] 56.3 49.3 3.1% for SROCC. The benchmark projection-based metrics</cell><cell>0.808 1.007 1.124</cell><cell>60.3 23.8 28.6</cell><cell>55.3 12.8 13.5</cell><cell>0.835 1.017 1.003</cell><cell>83.4 78.4 68.7</cell><cell>87.3 66.3 65.3</cell><cell>0.774 0.871 1.020</cell></row><row><cell cols="4">GH-PSNR [17] do not show an acceptable performance for the quality 84.3 87.9</cell><cell>0.731</cell><cell>75.1</cell><cell>71.2</cell><cell>0.691</cell><cell>87.5</cell><cell>91.0</cell><cell>0.680</cell></row><row><cell cols="2">RA-PSNR [19] assessment of G-PCC decoded PCs.</cell><cell>88.9</cell><cell>89.9</cell><cell>0.622</cell><cell>79.9</cell><cell>76.9</cell><cell>0.629</cell><cell>90.3</cell><cell>91.5</cell><cell>0.604</cell></row><row><cell>Po2D</cell><cell>MMD [18] MMD-PSNR [18]</cell><cell>86.9 86.9</cell><cell>88.7 88.7</cell><cell>0.672 0.673</cell><cell>71.8 71.9</cell><cell>69.0 69.0</cell><cell>0.729 0.728</cell><cell>88.8 88.7</cell><cell>90.3 90.3</cell><cell>0.647 0.648</cell></row><row><cell>Pl2Pl</cell><cell>Angle-MSE [16]</cell><cell>62.4</cell><cell>47.7</cell><cell>1.063</cell><cell>51.6</cell><cell>34.1</cell><cell>0.897</cell><cell>69.0</cell><cell>55.0</cell><cell>1.016</cell></row><row><cell></cell><cell>PCQM [26]</cell><cell>89.9</cell><cell>91.6</cell><cell>0.597</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>$% [21]</cell><cell>90.4</cell><cell>92.0</cell><cell>0.585</cell><cell>75.3</cell><cell>74.0</cell><cell>0.689</cell><cell>92.5</cell><cell>93.9</cell><cell>0.533</cell></row><row><cell>Feature-based</cell><cell>!" # [21] PCMRR [27]</cell><cell>85.3 90.2</cell><cell>88.4 90.7</cell><cell>0.710 0.573</cell><cell>65.7 71.6</cell><cell>68.3 64.8</cell><cell>0.789 0.731</cell><cell>87.9 89.2</cell><cell>91.9 91.0</cell><cell>0.669 0.636</cell></row><row><cell></cell><cell>PointSSIM [28]</cell><cell>92.6</cell><cell>91.8</cell><cell>0.514</cell><cell>83.0</cell><cell>84.5</cell><cell>0.584</cell><cell>94.4</cell><cell>92.9</cell><cell>0.462</cell></row><row><cell></cell><cell>Geotex [22]</cell><cell>-</cell><cell>87.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Proj-Y-MS-SSIM [10]</cell><cell>70.9</cell><cell>75.2</cell><cell>0.959</cell><cell>31.9</cell><cell>35.4</cell><cell>0.992</cell><cell>75.3</cell><cell>50.1</cell><cell>0.924</cell></row><row><cell>Projection-based</cell><cell>Proj-Y-VIFP [10]</cell><cell>71.5</cell><cell>74.2</cell><cell>0.951</cell><cell>43.7</cell><cell>35.6</cell><cell>0.942</cell><cell>75.0</cell><cell>79.2</cell><cell>0.929</cell></row><row><cell></cell><cell>Proj-Y-PSNR [11]</cell><cell>43.0</cell><cell>33.1</cell><cell>1.228</cell><cell>29.7</cell><cell>-10.1</cell><cell>1.000</cell><cell>47.7</cell><cell>35.7</cell><cell>1.234</cell></row><row><cell></cell><cell>JGC-ProjQM-FSIM</cell><cell>88.2</cell><cell>90.1</cell><cell>0.640</cell><cell>71.0</cell><cell>72.1</cell><cell>0.737</cell><cell>90.3</cell><cell>92.1</cell><cell>0.604</cell></row><row><cell>Proposed</cell><cell>JGC-ProjQM-VSI</cell><cell>85.4</cell><cell>87.6</cell><cell>0.707</cell><cell>64.5</cell><cell>63.9</cell><cell>0.800</cell><cell>88.1</cell><cell>90.1</cell><cell>0.664</cell></row><row><cell>Projection-based</cell><cell>JGC-ProjQM-LPIPS</cell><cell>92.3</cell><cell>93.2</cell><cell>0.523</cell><cell>80.7</cell><cell>79.5</cell><cell>0.618</cell><cell>93.5</cell><cell>94.2</cell><cell>0.497</cell></row><row><cell></cell><cell>JGC-ProjQM-DISTS</cell><cell>94.7</cell><cell>95.6</cell><cell>0.439</cell><cell>86.4</cell><cell>85.3</cell><cell>0.526</cell><cell>95.8</cell><cell>96.0</cell><cell>0.402</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI OBJECTIVE</head><label>VI</label><figDesc>-SUBJECTIVE CORRELATION PERFORMANCE FOR THE ABLATION STUDY OF THE PROPOSED JGC-PROJQM METRIC</figDesc><table><row><cell>Proposed</cell><cell>Correlat.</cell><cell>Full</cell><cell></cell><cell cols="2">Removed Module</cell></row><row><cell>JGC-ProjQM</cell><cell>Metric</cell><cell>Performance</cell><cell cols="2">Recoloring Cropping</cell><cell>Padding</cell></row><row><cell>JGC-ProjQM-</cell><cell>PLCC</cell><cell>94.7</cell><cell>93.7</cell><cell>83.7</cell><cell>91.4</cell></row><row><cell>DISTS</cell><cell>SROCC</cell><cell>95.6</cell><cell>94.9</cell><cell>86.7</cell><cell>91.1</cell></row><row><cell>JGC-ProjQM-</cell><cell>PLCC</cell><cell>92.3</cell><cell>89.5</cell><cell>80.0</cell><cell>81.1</cell></row><row><cell>LPIPS</cell><cell>SROCC</cell><cell>93.2</cell><cell>90.7</cell><cell>81.8</cell><cell>80.8</cell></row><row><cell>JGC-ProjQM-</cell><cell>PLCC</cell><cell>88.2</cell><cell>78.5</cell><cell>81.5</cell><cell>86.6</cell></row><row><cell>FSIM</cell><cell>SROCC</cell><cell>90.1</cell><cell>81.5</cell><cell>83.7</cell><cell>87.5</cell></row><row><cell>JGC-ProjQM-</cell><cell>PLCC</cell><cell>85.4</cell><cell>74.6</cell><cell>81.6</cell><cell>82.7</cell></row><row><cell>VSI</cell><cell>SROCC</cell><cell>87.6</cell><cell>77.3</cell><cell>83.9</cell><cell>83.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emerging MPEG Standards for Point Cloud Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emer. Sel. Topics Circ. Sys</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="148" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point-Cloud Compression: Moving Picture Experts Group&apos;s New Standard in 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mekuria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Preda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Cons. Electron. Mag</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="21" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video-Based Point-Cloud-Compression Standard in MPEG: From Evidence Collection to Committee Draft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sign. Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="118" to="123" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note>Standards in a Nutshell</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Impact of Visualization Strategy for Subjective Quality Assessment of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia &amp; Expo Workshops</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Subjective Quality Study and Database of Compressed Point Clouds with 6DoF Headmounted Display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02501</idno>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point Cloud Rendering after Coding: Impacts on Subjective and Objective Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia (Early Access)</title>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Textured Mesh vs Coloured Point Cloud: A Subjective Study for Volumetric Video Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ozcinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Quality of Multimedia Exp</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Subjective and Objective Quality Evaluation of 3D Point Cloud Denoising Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia &amp; Expo Workshops</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quality Evaluation of Static Point Clouds Encoded Using MPEG Codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Proc</title>
		<meeting><address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Comprehensive Study of the Rate-Distortion Performance in MPEG Point Cloud Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Trans. on Sign. and Info. Proc</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion-Compensated Compression of Dynamic Voxelized Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3886" to="3895" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Novel Methodology for Quality Assessment of Voxelized Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Torlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno>107520I</idno>
	</analytic>
	<monogr>
		<title level="j">Applic. of Digital Image Proc. XLI</title>
		<imprint>
			<biblScope unit="volume">10752</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation Criteria for PCC (Point Cloud Compression)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mekuria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tulvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISO/IEC MPEG Doc. N16332</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric Distortion Metrics for Point Cloud Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ochimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Proc</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Common Test Conditions for Point Cloud Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>3dg Group</surname></persName>
		</author>
		<idno>JTC1/SC29/WG11 Doc. N18474</idno>
		<imprint>
			<date type="published" when="2019-03" />
			<pubPlace>Geneva, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Point Cloud Quality Assessment Metric based on Angular Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Multimedia &amp; Expo Workshops</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Generalized Hausdorff Distance Based Quality Metric for Point Cloud Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Quality of Multimedia Exp</title>
		<meeting><address><addrLine>Athlone, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mahalanobis Based Point to Distribution Metric for Point Cloud Geometry Quality Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sign. Proc. Let</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1350" to="1354" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving PSNR-Based Quality Metrics Performance for Point Cloud Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Proc</title>
		<meeting><address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PC-MSDM: A Quality Metric for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meynet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Digne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavou?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Quality of Multimedia Exp</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Color-Based Objective Quality Metric for Point Cloud Contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Quality of Multimedia Exp</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards a Point Cloud Quality Assessment Model Using Local Binary Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Quality of Multimedia Exp</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the F-Divergence and Singularity of Probability Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vadja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Periodica Mathematica Hungarica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="223" to="234" />
			<date type="published" when="1972-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-Distance Point Cloud Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Proc</title>
		<meeting><address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local Luminance Patterns for Point Cloud Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Workshop on Multimedia Signal Proc</title>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PCQM: A Full-Reference Quality Metric for Colored 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meynet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nehm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Digne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavou?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Quality of Multimedia Exp</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Reduced Reference Metric for Visual Quality Evaluation of Point Cloud Contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Let</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1660" to="1664" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a Point Cloud Structural Similarity Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Multimedia &amp; Expo Workshops</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting User Interactivity in Quality Assessment of Point Cloud Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Quality of Multimedia Exp</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Navier-Stokes, Fluid Dynamics, and Image and Video Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conf. on Comput. Vision and Pattern Recogn</title>
		<imprint>
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epfl</forename><surname>Mmspg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;m-Pccd</forename></persName>
		</author>
		<ptr target="https://www.epfl.ch/labs/mmspg/downloads/quality-assessment-for-point-cloud-compression/" />
		<imprint/>
		<respStmt>
			<orgName>MPEG Point Cloud Compression Dataset</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MPEG Point Cloud Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>3dg Group</surname></persName>
		</author>
		<ptr target="http://mpegfs.int-evry.fr/MPEG/PCC/DataSets/pointCloud/CfP/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">JPEG Pleno database -University of Sao Paulo point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jpeg Committee</surname></persName>
		</author>
		<ptr target="http://plenodb.jpeg.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sketchfab</title>
		<ptr target="https://sketchfab.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Methodology for the Subjective Assessment of the Quality of Television Pictures</title>
		<idno>ITU-R Recommendation BT.500-13</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Telecom. Union</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">New Full-reference Quality Metrics based on HVS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Video Proc. and Quality Metrics for Consumer Electro</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On between-Coefficient Contrast Masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Video Proc. and Quality Metrics</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: From Error Visibility to Structural Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiscale Structural Similarity for Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conf. on Sign</title>
		<meeting><address><addrLine>Pacific Grove, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Sys. &amp; Comp</publisher>
			<date type="published" when="2003-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image Information and Visual Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FSIM: A Feature SIMilarity Index for Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">VSI: A Visual Saliency-Induced Index for Perceptual Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Comput. Vision and Pattern Recogn</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: Unifying Structure and Texture Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3045810</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Pattern Anal Mach. Intell</title>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Haar Wavelet-Based Perceptual Similarity Index for Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reisenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kutynio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sign. Proc.: Image Comm</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
