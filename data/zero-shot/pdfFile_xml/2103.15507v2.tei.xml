<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Modeling in 3D Human Pose Estimation: A Unified Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Advanced Innovation Center For Future Visual Entertainment (AICFVE)</orgName>
								<orgName type="institution">Beijing Film Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Adv. Inst. of Info. Tech</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Deepwise AI Lab</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context Modeling in 3D Human Pose Estimation: A Unified Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D human pose from a single image suffers from severe ambiguity since multiple 3D joint configurations may have the same 2D projection. The state-of-the-art methods often rely on context modeling methods such as pictorial structure model (PSM) or graph neural network (GNN) to reduce ambiguity. However, there is no study that rigorously compares them side by side. So we first present a general formula for context modeling in which both PSM and GNN are its special cases. By comparing the two methods, we found that the end-to-end training scheme in GNN and the limb length constraints in PSM are two complementary factors to improve results. To combine their advantages, we propose ContextPose based on attention mechanism that allows enforcing soft limb length constraints in a deep network. The approach effectively reduces the chance of getting absurd 3D pose estimates with incorrect limb lengths and achieves state-of-the-art results on two benchmark datasets. More importantly, the introduction of limb length constraints into deep networks enables the approach to achieve much better generalization performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular 3D human pose estimation has attracted much attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref> because it can benefit many applications such as virtual reality and intelligent video analysis. The task is more difficult than 2D pose estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> because it needs to estimate relative depth between body joints which suffers from severe ambiguity. Psychology experiments <ref type="bibr" target="#b3">[4]</ref> show that context plays an important role in resolving ambiguity in human visual system. Following this idea, body joints can serve as mutual context * denotes equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.Update</head><p>x 2 , e <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2</ref> x 0 , e 1,0</p><formula xml:id="formula_0">y 1 = f(x 1,<label>) 2 1 0</label></formula><p>1.Collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.Aggregation</head><p>?(x 0 , e 1,0 ), ?(x 2 , e 1,2 ),... <ref type="figure">Figure 1</ref>: A general formula of context modeling in the 3D human pose estimation task. To update features of a particular joint, the approach first collects features from its contextual joints (defined by the input graph structure), aggregates the collected features, and uses the features to update the joint of interest.</p><p>to each other in human pose estimation-localizing one facilitates the localization of the other. For example, elbow is more likely to be found at a distance from shoulder depending on the length of upper arm. Some work <ref type="bibr" target="#b11">[12]</ref> also explores surrounding environment as context for joints to further narrow down the space. The success of CNN in 2D pose estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> has promoted a shift from model-based 3D pose estimators <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> to discriminative ones <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. In particular, Martinez et al. <ref type="bibr" target="#b20">[21]</ref> propose to estimate 3D pose from estimated 2D pose by a Fully Connected Network (FCN). It achieves notably smaller error than previous methods due to its strong capability of fitting large amounts of data and improved 2D pose estimation accuracy. But it does not explicitly explore context which may result in poor results in challenging cases <ref type="bibr" target="#b9">[10]</ref>. GNN <ref type="bibr" target="#b10">[11]</ref> computes features for each node by aggregating those of its neighbors. The interaction among nodes makes it suitable for modeling context. For example, Ci et al. <ref type="bibr" target="#b9">[10]</ref> treat each joint as a node and perform feature passing among the nodes to estimate their 3D locations. The method is more robust to inaccurate 2D poses which validates the values of context. But they cannot explicitly model spatial relation between joints such as limb length constraints which is a big limitation-limb length is useful to reduce ambiguity when some joints are occluded.</p><p>PSM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> had been commonly used for both 2D and 3D pose estimation before deep networks dominate the field. The key idea is to determine optimal joint locations by simultaneously considering their appearance and spatial relation. For example, Qiu et al. <ref type="bibr" target="#b27">[28]</ref> divide the 3D motion space by regular voxels and assign each joint to the optimal voxel by minimizing an energy function defined on all joints. The approach may get accurate 3D estimates for occluded joints based on their neighbors. Some works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> also combine PSM with deep learning by first applying CNN to estimate features and then using PSM to do inference on the features. However, the improvement is limited because it cannot be trained end-to-end.</p><p>To our best knowledge, there is no work discussing the pros and cons of PSM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref> and GNN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> since they were developed in different fields. But this is actually very important. To that end, starting from their standard formulation, we develop a general formula for the two methods which allows us to clearly understand their relations and differences. In the meanwhile, we can compare their advantages and disadvantages side by side. The basic idea is sketched in <ref type="figure">Figure 1</ref>. It has three steps: for each joint of interest, it first collects features from its contextual joints which are determined by the input human graph. Then it aggregates the collected features as context which in turn is used to update the features of the joint.</p><p>In particular, we find in our empirical study that the GNNbased methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref> powered by end-to-end learning get more accurate estimates than PSM in general cases. We believe this is mainly because deep neural networks have strong capability to fit a large amount of data. On the other hand, PSM-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> are more robust to occlusion and get better out-of-distribution generalization performance. It is worth noting that PSM is mainly used in the multiview setting. Our experiment in the monocular setting shows that PSM alone gets very bad results because of its limited capability to reduce ambiguity (3D pose estimates may still be inaccurate although their limb lengths are correct). The observation motivates us to combine PSM and GNN in order to benefit from their advantages. Note that the task is non-trivial because PSM requires solving the discrete optimization function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Formula Voxel Based</p><p>End-to-End</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cyclic Graph</head><p>Limb Length Prior PSM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> 1 GNN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> 2 ContextPose (Ours) 4 <ref type="table">Table 1</ref>: Comparison of different context modeling methods. Please refer to Section 3.5 for more details.</p><p>To that end, we present an approach termed as Con-textPose on top of the general formula which is inspired by the attention mechanism <ref type="bibr" target="#b33">[34]</ref>. It is built on the voxel representation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref> and allows enforcing soft limb length constraints by paying more attention to information passed between locations that satisfy limb length constraints. More importantly, the approach avoids solving the discrete optimization problem and can be trained end-to-end. <ref type="table">Table 1</ref> briefly summarizes different methods. <ref type="figure" target="#fig_0">Figure 2</ref> shows how ContextPose is leveraged by the state-of-the-art method <ref type="bibr" target="#b13">[14]</ref> for 3D pose estimation. Given an input image, it first estimates 2D features by a 2D network (CNN). Then it inversely projects them to the 3D voxels using camera parameters and uses a 3D network to estimate 3D heatmaps representing the likelihood of each voxel having each body joint. ContextPose can be inserted into the 3D network to fuse features from different joints at different locations. Specifically, it updates the features of a joint at a voxel by a linear combination of the features of its contextual joints at all voxels. The weights in linear combination are determined by their spatial relation (pairwise attention) and appearance (global attention) of the contextual joints. The bottom section of <ref type="figure" target="#fig_0">Figure 2</ref> shows more details of how we compute global attention and pairwise attention with the knee joint as an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Overview</head><p>In summary, we make three contributions:</p><p>1) We develop a general formula for context modeling methods in 3D human pose estimation which allows us to clearly understand their pros and cons. We also empirically compare them in a rigorous way.</p><p>2) We propose ContextPose on top of the general formula which combines the advantages of PSM and GNN. In particular, it allows leveraging limb length constraints and can be leveraged by 3D pose estimation networks for end-to-end training.</p><p>3) We demonstrate the state-of-the-art performance on two benchmark datasets. More importantly, Con-textPose shows better generalization results on out-ofdistribution data. The code and models will be released in order to inspire more research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Context Modeling: A Unified Perspective</head><p>We first introduce some notations and then reformulate PSM and GNN, respectively. Based on the reformulation, we develop a general formula for context modeling and show that both PSM and GNN are its special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations</head><p>As shown in <ref type="figure">Figure 1</ref>, we represent human body by a graph G = (J , E) where J = {J 0 , J 1 , ? ? ? , J N ?1 } represents N body joints. The set E represents edges that connect pairs of joints. We define the joints that are connected by edges to be contextual joints of each other. The goal of monocular 3D pose estimation is to estimate the 3D locations of the joints from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Reformulate PSM</head><p>PSM is commonly used in multiview 3D pose estimation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. It first divides the 3D space by regular voxels ? with each having a discrete location q ? R 3 . The goal of PSM is to assign each joint to one of the voxels by minimizing an energy function defined on all joints. When the human graph is acyclic, PSM can be optimized by dynamic programming in which messages are sequentially passed from child nodes. In particular, the likelihood of a sub-tree with root joint J u at voxel q is computed as <ref type="bibr" target="#b0">(1)</ref> where child(J u ) denotes the children of J u and x u,q is the confidence of J u at q determined by appearance.</p><formula xml:id="formula_1">yu,q = xu,q ? Jv ?child(Ju) (max k?? {?(q, k, eu,v) ? y v,k }),</formula><p>The formula can be interpreted by three steps: (1) for each non-leaf node J u , it first collects features from each of its children J v by ?(q, k, e u,v )?y v,k where y v,k represents J v 's likelihood of being at k ? R 3 which in turn is determined by its own children. The pairwise term ?(q, k, e u,v ) encodes the limb length constraint measuring whether the distance between q and k satisfies the limb length prior in e u,v . The maximum score over all voxel locations ? represents the message passed from joint J v to J u . This step collects such information from all of its children; (2) then the context features collected from its children are aggregated by ; (3) finally, it updates y u,q by multiplying the aggregated context with the confidence x u,q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Reformulate GNN</head><p>Ci et al. <ref type="bibr" target="#b9">[10]</ref> present a formula which unifies FCN <ref type="bibr" target="#b20">[21]</ref>, GNN <ref type="bibr" target="#b39">[40]</ref>, and LCN <ref type="bibr" target="#b9">[10]</ref>. We further reformulate it such that it has a similar form as PSM</p><formula xml:id="formula_2">y u = f (x u , Jv?J (e u,v ? W u,v x v )),<label>(2)</label></formula><p>where x u ? R Minput represents the features of J u obtained from the previous layer or input, and y u ? R Moutput denotes the updated features of J u . It is important to note that these methods do not discretize the 3D space but directly estimate continuous locations. So we do not compute features for each discrete location q as in Eq. (1). The binary scalar e u,v encodes the pairwise relation between joint J u and J v , and is set to be one if J v is a contextual joint of J u . W u,v ? R Moutput?Minput is a learnable weight matrix. We can also interpret the formula by three steps in a similar way as PSM. It first collects features by e u,v ? W u,v x v from its contextual joints, then aggregates them using the sum operator and finally uses multilayer perceptron (MLP) f to update the joint of interest.</p><p>The difference between FCN, GNN and LCN lies in how to compute e u,v and W u,v . FCN <ref type="bibr" target="#b20">[21]</ref> does not use human graph when collecting features. Instead, e u,v is set to be one for every joint pair (J u , J v ). In contrast, in GNN <ref type="bibr" target="#b39">[40]</ref> and LCN <ref type="bibr" target="#b9">[10]</ref>, e u,v is set with special consideration. Generally, e u,v is non-zero only when the two joints are connected according to the human graph. In other words, they only collect features from contextual joints. So their main difference lies in the collection step. Please refer to <ref type="bibr" target="#b9">[10]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">General Formula</head><p>We introduce a general context modeling formula, which updates features y u of joint J u by</p><formula xml:id="formula_3">y u = f (x u , AGG({ ?(x v , e u,v ) | ?(J u , J v ) ? E}) ), (3)</formula><p>where x u denotes the features of joint J u before updating, and e u,v encodes the spatial relation prior (e.g. limb length) between J u and J v . There are three steps in the formula as will be detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Collection</head><p>For each joint of interest, it collects features from its contextual joints as represented by ?(?, ?) in the formula. This is the most complex step in context modeling which determines where and how to collect features from the graph nodes.</p><p>2. Aggregation This is denoted by AGG(?) in the formula. It is a permutation invariant function, e.g. sum or product function, defined on a set of contextual features. It aims to aggregate the collected features.</p><p>3. Update This is denoted by f (?, ?) in the formula. It updates the feature of a joint by transforming its own as well as the aggregated features.</p><p>It is straightforward to verify that both PSM and GNN can be interpreted by the formula. The advantage of PSM is that it can explicitly enforce limb length constraints while GNN can learn implicit priors from a large amount of data.</p><p>In the following, we present an approach to combine their advantages on top of the general formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ContextPose</head><p>This section introduces the details of ContextPose. We first present an overview of how it can be leveraged by an existing method <ref type="bibr" target="#b13">[14]</ref> to estimate 3D human pose in Section 3.1. Then we dive into the technical and training details of ContextPose in the following three sub-sections. Finally, we discuss the differences between ContextPose and other context modeling methods in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>We adopt the state-of-the-art 3D pose estimator <ref type="bibr" target="#b13">[14]</ref> as our baseline. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, it first constructs a 3D feature volume by inversely projecting image features to the 3D space using camera parameters. Then the feature volume is fed to an encoder-decoder network to estimate 3D heatmaps. In particular, it predicts N scores for each voxel representing the likelihood of N joints. Finally, we compute expectation over the 3D heatmaps of each joint to obtain its 3D location <ref type="bibr" target="#b30">[31]</ref>. ContextPose is inserted between the encoder and decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ContextPose</head><p>Denote the input tensor of ContextPose as V input cont ? R N M ?D?H?W which represents the features of N joints at D?H ?W voxels. We split V input cont into N groups along the channel dimension such that each group corresponds to the features of one joint. Inspired by the attention mechanism <ref type="bibr" target="#b33">[34]</ref>, ContextPose updates the features of a joint J u at voxel q by a linear combination of the features of its contextual joints at all voxels</p><formula xml:id="formula_4">yu,q = xu,q + Jv ?J [ k?? (Gv(x v,k ) ? P (q, k, eu,v) ? Wu,vx v,k )],<label>(4)</label></formula><p>where ? denotes the set of voxels, x v,k ? R M denotes the features of joint J v at voxel k. The global attention G v (x v,k ) and pairwise attention P (q, k, e u,v ) determines the weight in linear combination. W u,v ? R M ?M is a learnable matrix to transform features.</p><p>Global Attention (GA) We estimate a confidence score for each joint J v at a voxel k representing to what extent should this feature contribute to other joints. Intuitively, we expect a lower score for non-person voxels in order to reduce the risk of corrupting good features. In other words, we expect large scores for voxels that are likely to include joint J v . As a result, joint J u can focus on features from high likelihood voxels of joint J v (see <ref type="figure" target="#fig_0">Figure 2</ref> (a) and (b)). The GA for joint J v is defined as</p><formula xml:id="formula_5">G v (x v,k ) ? exp(d T v x v,k ),<label>(5)</label></formula><p>which is normalized such that</p><formula xml:id="formula_6">k?? G v (x v,k ) = 1. d v ? R M is a learnable vector.</formula><p>Pairwise Attention (PA) PA explores spatial relation between a pair of joints. The general idea is to give larger weights to features passed from locations of a joint that satisfy the pre-defined spatial relation. In this work, we focus on limb length constraints. But this can be extended to other priors such as limb orientations. If joint J v is connected to J u by a rigid bone, then their distance in the 3D space is fixed for the same person which is independent of human postures. Offline, we compute the average distance ? u,v and the standard deviation ? u,v in the training set as the limb length distribution prior and let e u,v = (? u,v , ? u,v ) as the limb pre-defined parameters. The pairwise attention for the joint pair is defined as</p><formula xml:id="formula_7">P (q, k, e u,v ) ? exp(? (||q ? k|| 2 ?? u,v ) 2 2?? 2 u,v + ).<label>(6)</label></formula><p>The pairwise attention is normalized over all voxels such that</p><formula xml:id="formula_8">k?? G v (x v,k ) ? P (q, k, e u,v ) = 1.</formula><p>The hyper-parameter ? is used to adjust the tolerance to limb length errors, which is empirically set to be 1500 in this work. The parameter is used to improve numerical robustness. See <ref type="figure" target="#fig_0">Figure 2</ref> (c)-(f). Besides, if joint J v is not connected to J u by a rigid bone, the features from joint J v may also be helpful to J u . For example, left hand may also help the detection of right hand. In this case, we simply set the pairwise term to be P (q, k, e u,v ) = 1 and completely rely on the global attention to determine the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regression of 3D Human Pose</head><p>The decoder network transforms the output of Con-textPose V output cont ? R N M ?D?H?W to 3D heatmaps V output ? R N ?D ?H ?W of N body joints which represents the likelihood of each joint at each location. Then the 3D location J u for joint J u is obtained by computing the expectation of V output u ? R D ?H ?W with the common integral technique <ref type="bibr" target="#b30">[31]</ref> according to the following formula</p><formula xml:id="formula_9">J u = D x=1 H y=1 W z=1 (x, y, z) ? V output u (x, y, z).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The parameters in ContextPose are jointly learned with the 2D CNN and the encoder-decoder network by enforcing two losses:</p><formula xml:id="formula_10">L = L 3D + ?L GA ,<label>(8)</label></formula><p>in which L 3D and L GA are the loss functions enforced on the 3D joint locations and global attention maps, respectively. Same as <ref type="bibr" target="#b13">[14]</ref>, we compute the L 1 loss between the ground-truth 3D pose J gt and the estimated 3D pose J with a weak heatmap regularizer which promotes Gaussian shape distribution for the estimated 3D heatmaps as</p><formula xml:id="formula_11">L 3D = 1 N Ju?J (||J u ? J gt u || 1 ?? ? log(V output u (J gt u ))).</formula><p>(9) In addition, to help the GA focus on the voxels that are likely to have joint J u , we enforce an L 2 loss:</p><formula xml:id="formula_12">L GA = 1 N DHW Ju?J ||G u ? G gt u || 2 2 ,<label>(10)</label></formula><p>where G u ? R D?H?W is the GA map for joint J u and G gt u ? R D?H?W is the ground-truth heatmap generated by applying a 3D Gaussian centered at the ground truth location of the joint J u .</p><p>In our experiment, we set ? and ? to be 10 ?2 and 10 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Comparison of PSM, GNN and ContextPose</head><p>It is easy to verify that PSM, GNN, and ContextPose are all special cases of the general formula Eq. (3). The main difference between them lies in the collection step which includes the structures of human graph G, pairwise relation e u,v , the collection function ?(?, ?), and training scheme. We will compare them side by side from the above aspects hoping to clearly understand their advantages and disadvantages.</p><p>Graph Structures PSM often uses acyclic graphs in order to get optimum solution. In contrast, ContextPose is not subject to this restriction. Cyclic graph offers greater flexibility to represent more powerful and natural context. For example, in ContextPose, we can add connections between left and right shoulders to the human graph and require that they cannot be at the same location which helps solve the "double counting" problem. We can even add connections between joints in neighboring frames to promote smoothness in future work. GNN can also use cyclic graphs but it cannot explicitly express and enforce natural rules on the joints. It is not clear what kind of pairwise relation does GNN learns from data which makes it a black box.</p><p>Pairwise Relation In PSM, the pairwise relation is often implemented as limb length constraints. As discussed in Eq.</p><p>(1), it encourages detections of a pair of joints that satisfy the limb length prior. In GNN, the pairwise term reflects the similarity between the features of two nodes. Although the features also encode some location information, it is hardly possible that GNN will implicitly learn limb length constraints. ContextPose does not enforce hard limb length constraints as PSM. But it encourages pose estimates to have reasonable limb length by focusing on features that are passed between locations that satisfy limb length constraints.</p><p>End-to-End Learning PSM requires solving a discrete optimization problem in order to obtain optimal locations for all joints. In particular, it uses the argmax operator to identify optimal voxels for each joint which makes the approach non-differentiable. In contrast, the GNN-based methods can be trained end-to-end because all operators in the collection, aggregation and update functions are differentiable. ContextPose can also be trained end-to-end which combines the advantages of PSM and GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantization Error</head><p>The PSM-based methods and Con-textPose both work on discrete voxels. So their accuracy depends on the size of each voxel. Using a smaller voxel decreases quantization error but meanwhile increases computation time. In <ref type="bibr" target="#b13">[14]</ref>, the authors propose to compute expectation over the heatmaps to obtain continuous 3D locations which notably decreases the impact of quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M (H36M) <ref type="bibr" target="#b12">[13]</ref> Following <ref type="bibr" target="#b9">[10]</ref>, we use the subjects S1, S5, S6, S7, and S8 for training, and S9, S11 for testing. The Mean Per Joint Position Error (MPJPE) metric is computed under two protocols: Protocol #1 computes MPJPE between the ground-truth (GT) and the estimated 3D poses after aligning their root (mid-hip) joints; Protocol #2 reports MPJPE after the 3D estimate is aligned with the GT via a rigid transformation. Additionally, we present two new metrics to comprehensively measure the quality of the 3D pose estimates: (1) Mean Per Limb Length Error (MPLLE) computes the average limb length error between the GT and estimated poses over 16 limbs (i.e. the purple edges in <ref type="figure">Figure 1</ref>), and (2) Mean Per Limb Angle Error (MPLAE) measures the average limb angle error between the GT and the estimated poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP (3DHP) [22]</head><p>This dataset provides monocular videos of six subjects acting in three different scenes which include green screen indoor scenes, indoor scenes and outdoor scenes. This dataset is often used to evaluate the generalization performance of different models. Following the convention, we directly apply our model trained on the H36M dataset to this dataset without re-training. We report results using two metrics: Percentage of Correctly estimated Keypoints (PCK) <ref type="bibr" target="#b1">[2]</ref> and Area Under the Curve (AUC) <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the state-of-the-art 3D pose estimator <ref type="bibr" target="#b13">[14]</ref> as our baseline to estimate 3D poses. We insert ContextPose between the encoder and decoder networks as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. To reduce GPU memory cost, we decrease the number of layers in the 3D network from five to two. The modification slightly improves the results of the baseline. For the ContextPose network, M is set to be 3. We jointly train the 2D and 3D networks for 30 epochs with the Adam <ref type="bibr" target="#b16">[17]</ref> optimizer. The learning rates are set to be 0.0001 and 0.001 for the 2D and 3D networks, respectively. To prevent from over-fitting to the human appearance in the H36M dataset, we fix the 2D network and train the 3D network for 20 epochs before end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the State-of-the-arts</head><p>Results on the H36M Dataset <ref type="table" target="#tab_1">Table 2</ref> shows the results of the state-of-the-art methods on the H36M dataset. Our approach outperforms the state-of-the-art methods by a notable margin under both protocols. This includes methods that explore temporal information in videos (labeled by * in the table). In particular, our method outperforms PSM <ref type="bibr" target="#b27">[28]</ref>, FCN <ref type="bibr" target="#b20">[21]</ref>, GNN <ref type="bibr" target="#b39">[40]</ref>, and LCN <ref type="bibr" target="#b9">[10]</ref> by an even larger margin which validates the effectiveness of our context modeling strategy. We discover in our experiment that PSM <ref type="bibr" target="#b27">[28]</ref> gets very poor results in the monocular setting. To investigate the reasons, we project the estimated 3D poses back to 2D images and find that, for most cases, the projections perfectly match the 2D people although their 3D estimates are very different from the GT poses. We show an example in <ref type="figure" target="#fig_1">Figure 3</ref>. This is mainly because PSM alone has limited capability to resolve ambiguity. Note that a 3D pose estimate may be inaccurate even when its limb lengths are correct. In contrast, the deep learning-based methods such as GNN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref> have strong capability to reduce ambiguity because they can fit a large amount of data. We will discuss in more details on why our approach gets more accurate estimates than PSM and GNN in the subsequent ablative study. <ref type="table">Table 3</ref> shows the results of different methods on the 3DHP dataset. Our approach achieves significantly better PCK and AUC scores than other methods including FCN, LCN, and PSM for almost all scenes. The result suggests that ContextPose has strong generalization performance which we think is due to the leverage of limb length priors in deep networks. FCN <ref type="bibr" target="#b20">[21]</ref> gets a low accuracy because the dense connections degrade the generalization capability which has already been discussed in <ref type="bibr" target="#b9">[10]</ref>. LCN <ref type="bibr" target="#b9">[10]</ref> gets better results by fusing features of contextual joints but it is still worse than ours. The result validates the importance of combining deep networks and limb length priors.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the 3DHP Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effect of ContextPose We first compare our approach to the baseline w/o ContextPose. The results on the H36M dataset are shown in  <ref type="table">Table 3</ref>: The results of the state-of-the-art methods on the 3DHP dataset. GS represents the green screen background scene. The results of <ref type="bibr" target="#b20">[21]</ref> are taken from <ref type="bibr" target="#b19">[20]</ref>.</p><p>6% meaning that the limb lengths of the estimated 3D poses are more accurate than the baseline. The improvement for S11 in terms of MPJPE is marginal because the baseline is already very accurate. However, we can see that there is still clear improvement in terms of limb lengths and angles. The result of the baseline is different from the number in <ref type="table" target="#tab_1">Table 2</ref> because we use a smaller 3D network in <ref type="table" target="#tab_2">Table 4</ref>   We plot the MPLLE of the baseline and our method for each sample in H36M dataset in <ref type="figure">Figure 4</ref>. We can see that ContextPose gets smaller errors than baseline for about 80% of the test data. In particular, the improvement is larger for hard cases where the baseline gets large errors (see the left side of the figure). It indicates that ContextPose reduces the chance of getting absurd poses by exploring context. There are few cases where ContextPose gets worse results. This usually happens when multiple body joints are occluded which makes estimating global attention a very challenging task. <ref type="table">Table 3</ref> shows the results on the 3DHP dataset. We can see that using ContextPose significantly improves the PCK of the baseline from 71.3% to 80.5%. The result represents that ContextPose is very important to improve the generalization performance of the 3D pose estimator. This is a big advantage for actual deployment. In fact, we can see that our approach even outperforms the methods which use even more training data.</p><p>Effect of GA and PA We report results when we add one of the two modules (GA and PA) to the baseline in <ref type="table" target="#tab_2">Table  4</ref>. Adding only the GA module makes little difference on the ultimate results measured by MPJPE, MPLLE, and MPLAE. In contrast, if we add the PA module, the results are improved by a notable margin which validates the importance of pairwise compatibility in context modeling. <ref type="figure">Figure 5</ref> shows some 3D poses estimated by ContextPose. The last four columns show the predicted weights (i.e. the product of the GA and PA) for some random joints. In the first case of (a), the approach pays more attention to the features around the right knee when estimating the right ankle. Similarly, in the third case of (b), it focuses on features from right elbow when estimating right wrist. We show two failure cases in row (d) and (e). In particular, in (e) our estimate has correct limb lengths but inaccurate limb angles for the left leg. In addition, the projection of the 3D pose is also reasonable. This is a common error for monocular 3D pose estimation because it has severe ambiguity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We first introduce a general formula for context modeling in 3D pose estimation which allows comparing PSM and GNN side by side. Based on the formula, we present ContextPose that combines their advantages which allows enforcing limb length constraints in deep networks. So it can be trained end-to-end on large data. The approach outperforms the state-of-the-art methods on two benchmarks, and more importantly, shows better generalization performance on unseen datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(, *,. )|. ? ?} {) 3 (, 3,. )|. ? ?} , ., 6 7,* )|. ? ?}, ?, {4(5 9 , ., 6 7,* )|. " , ., 6 7,3 )|. ? ?}, ?, {4(5 9 , ., 6 7,3 )|. ? ?}} Global A8en9on : Pairwise Attention An example pipeline of using ContextPose for 3D pose estimation. The bottom shows how ContextPose collects features from contextual joints based on global and pairwise attention. Global attention, for example G 0 in (b), represents the likelihood of J 0 at each voxel k. For each voxel q of joint J 1 , for example q i in (e) or q j in (f), pairwise attention P (q, k, e 1,0 ) traverses every voxel k of joint J 0 and computes a spatial compatibility score between q and k. The product of global attention and pairwise attention gives the weight in linear combination as shown in (i)-(j).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of a 3D pose estimated by PSM [28]. The left figure shows the projection of the estimated 3D pose. The right figure shows the estimated (solid lines) and GT (dashed lines) 3D poses. The estimated 3D pose has correct 2D projection but it is very different from GT 3D pose. It means PSM suffers from severe ambiguity when it is used in the monocular setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>MPLLE (mm) of individual samples. The gray line shows the errors of the baseline. The blue line represents the error difference between ContextPose and baseline (below zero means our method gets smaller error). Example 3D pose estimates. The last four columns show the projected 2D poses and the weights in linear combination for some random joints (highlighted by small blue boxes). Row (d) and (e) show two failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dire. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD Smoke Wait WalkD Walk WalkT Avg Zhou et al. [41] ICCV'17 54.8 60.7 58.2 71.4 62.0 65.5 53.8 55.6 75.2 111.6 64.2 66.1 51.4 63.2 55.3 64.9 Martinez et al. (FCN) [21] ICCV'17 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9 Pavlakos et al. [25] CVPR'18 48.5 54.4 54.4 52.0 59.4 65.3 49.9 52.9 65.8 71.1 56.6 52.9 60.9 44.7 47.8 56.2 Yang et al. [39] CVPR'18 51.5 58.9 50.4 57.0 62.1 65.4 49.8 52.7 69.2 85.2 57.4 58.4 43.6 60.1 47.7 58.6 Zhao et al. (GNN) [40] CVPR'19 47.3 60.7 51.4 60.5 61.1 49.9 47.3 68.1 86.2 55.0 67.8 61.0 42.1 60.6 45.3 57.6 Qiu et al. 32.0 32.2 35.0 37.8 28.6 32.6 40.8 52.0 35.0 31.9 35.6 26.6 28.5 34.6</figDesc><table><row><cell>Protocol #1</cell><cell></cell></row><row><cell>(PSM) [28] ICCV'19</cell><cell>223.1 231.8 273.0 237.3 248.1 243.9 209.0 279.7 280.9 296.3 241.9 234.0 230.8 217.8 220.4 244.8</cell></row><row><cell>Iskakov et al. [14] ICCV'19</cell><cell>41.9 49.2 46.9 47.6 50.7 57.9 41.2 50.9 57.3 74.9 48.6 44.3 41.3 52.8 42.7 49.9</cell></row><row><cell>Wang et al. [36] ICCV'19</cell><cell>44.7 48.9 47.0 49.0 56.4 67.7 48.7 47.0 63.0 78.1 51.1 50.1 54.5 40.1 43.0 52.6</cell></row><row><cell>Ci et al. (LCN) [10] ICCV'19</cell><cell>46.8 52.3 44.7 50.4 52.9 68.9 49.6 46.4 60.2 78.9 51.2 50.0 54.8 40.4 43.3 52.7</cell></row><row><cell>Pavllo* et al. [27] CVPR'19</cell><cell>47.1 50.6 49.0 51.8 53.6 61.4 49.4 47.4 59.3 67.4 52.4 49.5 55.3 39.5 42.7 51.8*</cell></row><row><cell>Cai* et al. [6] ICCV'19</cell><cell>46.5 48.8 47.6 50.9 52.9 61.3 48.3 45.8 59.2 64.4 51.2 48.4 53.5 39.2 41.2 50.6*</cell></row><row><cell>Xu* et al. [38] CVPR'20</cell><cell>40.6 47.1 45.7 46.6 50.7 63.1 45.0 47.7 56.3 63.9 49.4 46.5 51.9 38.1 42.3 49.2*</cell></row><row><cell>Ours</cell><cell>36.3 42.8 39.5 40.0 43.9 48.8 36.7 44.0 51.0 63.1 44.3 40.6 44.4 34.9 36.7 43.4</cell></row><row><cell>Protocol #2</cell><cell>Dire. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD Smoke Wait WalkD Walk WalkT Avg</cell></row><row><cell cols="2">Martinez et al. (FCN) [21] ICCV'17 39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7</cell></row><row><cell>Pavlakos et al. [25] CVPR'18</cell><cell>34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5 41.8</cell></row><row><cell>Yang et al. [39] CVPR'18</cell><cell>26.9 30.9 36.3 39.9 43.9 47.4 28.8 29.4 36.9 58.4 41.5 30.5 29.5 42.5 32.2 37.7</cell></row><row><cell>Qiu et al. (PSM) [28] ICCV'19</cell><cell>117.0 123.2 128.0 121.7 126.1 128.7 105.3 130.1 145.1 170.2 125.1 114.5 128.9 115.3 117.1 126.7</cell></row><row><cell>Wang et al. [36] ICCV'19</cell><cell>33.6 38.1 37.6 38.5 43.4 48.8 36.0 35.7 51.1 63.1 41.0 38.6 40.9 30.3 34.1 40.7</cell></row><row><cell>Ci et al. (LCN) [10] ICCV'19</cell><cell>36.9 41.6 38.0 41.0 41.9 51.1 38.2 37.6 49.1 62.1 43.1 39.9 43.5 32.2 37.0 42.2</cell></row><row><cell>Pavllo* et al. [27] CVPR'19</cell><cell>36.0 38.7 38.0 41.7 40.1 45.9 37.1 35.4 46.8 53.4 41.4 36.9 43.1 30.3 34.8 40.0*</cell></row><row><cell>Cai* et al. [6] ICCV'19</cell><cell>36.8 38.7 38.2 41.7 40.7 46.8 37.9 35.6 47.6 51.7 41.3 36.8 42.7 31.0 34.7 40.2*</cell></row><row><cell>Xu* et al. [38] CVPR'20</cell><cell>33.6 37.4 37.0 37.6 39.2 46.4 34.3 35.4 45.1 52.1 40.1 35.5 42.1 29.8 35.3 38.9*</cell></row><row><cell>Ours</cell><cell>30.5 34.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The MPJPE (mm) of the state-of-the-art methods on the H36M dataset under protocol #1 and protocol #2, respectively.</figDesc><table /><note>* means the method uses temporal information in videos.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>We can see that ContextPose notably decreases MPJPE of the baseline from 54.38mm to 50.24mm on the challenging subject S9. MPLLE decreases by nearly</figDesc><table><row><cell>Method</cell><cell>GS (PCK)</cell><cell>noGS (PCK)</cell><cell>Outdoor (PCK)</cell><cell>ALL (PCK) ?</cell><cell>ALL (AUC) ?</cell></row><row><cell></cell><cell cols="3">Trained on: H36M+MPII [2]</cell><cell></cell><cell></cell></row><row><cell>Zhou et al. [41]</cell><cell>71.1</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell></row><row><cell>Yang et al. [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>32.0</cell></row><row><cell>Wang et al. [36]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.9</cell><cell>35.8</cell></row><row><cell cols="4">Trained on: H36M+MPII+LSP [15]</cell><cell></cell><cell></cell></row><row><cell>Pavlakos et al. [25]</cell><cell>76.5</cell><cell>63.1</cell><cell>77.5</cell><cell>71.9</cell><cell>35.3</cell></row><row><cell></cell><cell cols="2">Trained on: H36M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Martinez et al. (FCN) [21]</cell><cell>49.8</cell><cell>42.5</cell><cell>31.2</cell><cell>42.5</cell><cell>17.0</cell></row><row><cell>Qiu et al. (PSM) [28]</cell><cell>26.4</cell><cell>22.6</cell><cell>19.6</cell><cell>23.3</cell><cell>8.0</cell></row><row><cell>Ci et al. (LCN) [10]</cell><cell>74.8</cell><cell>70.8</cell><cell>77.3</cell><cell>74.0</cell><cell>36.7</cell></row><row><cell>Baseline</cell><cell>75.2</cell><cell>73.3</cell><cell>62.2</cell><cell>71.3</cell><cell>35.0</cell></row><row><cell>Ours</cell><cell>82.6</cell><cell>80.5</cell><cell>77.3</cell><cell>80.5</cell><cell>42.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MPLLE ? MPLAE ? MPJPE ? MPLLE ? MPLAE ?</figDesc><table><row><cell cols="2">Method MPJPE ? Baseline GA PA 54.38</cell><cell>S9 15.03</cell><cell>0.1600</cell><cell>35.12</cell><cell>S11 10.16</cell><cell>0.1250</cell></row><row><cell>Ours w/o PA</cell><cell>52.00</cell><cell>14.58</cell><cell>0.1517</cell><cell>35.16</cell><cell>9.78</cell><cell>0.1240</cell></row><row><cell>Ours w/o GA</cell><cell>52.46</cell><cell>14.16</cell><cell>0.1524</cell><cell>34.98</cell><cell>9.67</cell><cell>0.1224</cell></row><row><cell>Ours</cell><cell>50.24</cell><cell>14.13</cell><cell>0.1509</cell><cell>34.10</cell><cell>9.50</cell><cell>0.1217</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to reduce</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>memory usage as stated in Section 4.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablative study on the global attention and pairwise attention in ContextPose. We show the MPJPE (mm), MPLLE (mm) and MPLAE (radian) on each test subject separately. ContextPose achieves large improvement on the more challenging subject of S9.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by National Key R&amp;D Program of China (2018YFB1403900), NSFC-61625201 and NSFC-62061136001.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene perception: Detecting and judging objects undergoing relational violations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">People tracking using hybrid monte carlo filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiam</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2282" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12.7</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3D human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human upper body pose estimation in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="126" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for markerless 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Covariance scaled sampling for monocular 3d body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>I-I. IEEE</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="197" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Not all parts are created equal: 3d human pose estimation by modeling bidirectional dependencies of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7771" to="7780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling 3d human poses from uncalibrated monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiaolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xiaoyan Hu, and Kostas Daniilidis. 3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
