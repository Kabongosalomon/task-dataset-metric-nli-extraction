<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>MDPI</publisher>
				<availability status="unknown"><p>Copyright MDPI</p>
				</availability>
				<date type="published" when="2022">2022, 22, 1285</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F?rat</forename><surname>Hardala?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Uysal</surname></persName>
							<email>uysal@gazi.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering and Architecture</orgName>
								<orgName type="institution">Kafkas University</orgName>
								<address>
									<postCode>36100</postCode>
									<settlement>Kars</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Peker</surname></persName>
							<email>ozan.peker@gazi.edu.tro.p.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>?i?eklida?</surname></persName>
							<email>muratciceklidag@gazi.edu.trm.?.</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Orthopaedics and Traumatology</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tolunay</surname></persName>
							<email>tolgatolunay@gazi.edu.trt.t.</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Orthopaedics and Traumatology</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nil</forename><surname>Tokg?z</surname></persName>
							<email>nil.tokgoz@gazi.edu.tr</email>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U?urhan</forename><surname>Kutbay</surname></persName>
							<email>ukutbay@gazi.edu.tru.k.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boran</forename><surname>Demirciler</surname></persName>
							<email>boran.demirciler@huawei.comb.d.</email>
							<affiliation key="aff4">
								<orgName type="department">Huawei Turkey R&amp;D Center</orgName>
								<address>
									<postCode>34768</postCode>
									<settlement>?stanbul</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Mert</surname></persName>
							<email>fatih.mert@huawei.comf.m.</email>
							<affiliation key="aff4">
								<orgName type="department">Huawei Turkey R&amp;D Center</orgName>
								<address>
									<postCode>34768</postCode>
									<settlement>?stanbul</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Accepted at Sensors</title>
						<imprint>
							<publisher>MDPI</publisher>
							<date type="published" when="2022">2022, 22, 1285</date>
						</imprint>
					</monogr>
					<note>1 of 30 Accepted at Sensors, MDPI, 2022, 22, 1285. Hardala?, F.; Uysal, F.; Peker, O.; ?i?eklida?, M.; Tolunay, T.; Tokg?z, N.; Kutbay, U.; Demirciler, B.; Mert, F. Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. for citation and final version please click here: https://www.mdpi.com/1424-8220/22/3/1285 Article * Correspondence:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence</term>
					<term>biomedical image processing</term>
					<term>bone fractures</term>
					<term>deep learning</term>
					<term>fracture detection</term>
					<term>object detection</term>
					<term>transfer learning</term>
					<term>wrist</term>
					<term>X-ray</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hospitals, especially their emergency services, receive a high number of wrist fracture cases. For correct diagnosis and proper treatment of these, images obtained from various medical equipment must be viewed by physicians, along with the patient's medical records and physical examination. The aim of this study is to perform fracture detection by use of deep-learning on wrist X-ray images to support physicians in the diagnosis of these fractures, particularly in the emergency services. Using SABL, RegNet, RetinaNet, PAA, Libra R-CNN, FSAF, Faster R-CNN, Dynamic R-CNN and DCN deep-learning-based object detection models with various backbones, 20 different fracture detection procedures were performed on Gazi University Hospital's dataset of wrist X-ray images. To further improve these procedures, five different ensemble models were developed and then used to reform an ensemble model to develop a unique detection model, 'wrist fracture detection-combo (WFD-C).' From 26 different models for fracture detection, the highest detection result obtained was 0.8639 average precision (AP50) in the WFD-C model. Huawei Turkey R&amp;D Center supports this study within the scope of the ongoing cooperation project coded 071813 between Gazi University, Huawei and Medskor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Examination of the parts where bone fractures occur reveal that there are many cases of fractures on various parts of the body, such as wrists, shoulders and arms, especially in the emergency services of hospitals. It is also observed that the fractures that may result from various reasons and can occur as partial or complete fractures of the bones. These are classified as open or closed bone fractures. In an open fracture, also called a compound fracture, the skin is exposed through a deep wound, or the bone pierces the skin, becoming visible. In a closed fracture, also called a simple fracture, the bone is broken, with the skin remaining intact. It was concluded upon examination of the causes of bone fractures that fractures mostly occur when more force is applied to the bone than it can withstand. Based thereon, the causes that may lead to bone fractures are falls, trauma or a direct blow or kick to the body. Stress fractures that are common in athletes are caused by overuse or repetitive motions making the muscles tired, applying more pressure on the bone.</p><p>Moreover, fractures can also be caused by diseases that render the bone weak, such as osteoporosis or cancer in the bones. The symptoms of the bone fractures may include sudden pain, bruises, swelling, obvious deformity, warmth or redness <ref type="bibr">[1]</ref>.</p><p>Within the scope of the steps used in the detection of bone fractures, along with a complete medical history (including inquiry about the manner of occurrence of the fracture) and physical exam, physicians may require tests used for fractures. Mainly, three different devices are used for these tests, which are X-ray, MRI (Magnetic Resonance Imaging) and CT (Computed Tomography) <ref type="bibr">[1]</ref>. The most preferred among these devices is the X-ray device, which is also more cost-efficient compared to the other options. X-ray images obtained from Gazi University Hospital were used for deep-learning based fracture detection in wrist images in this study.</p><p>The anatomy of the wrist consists of radius, ulna and carpal bones. The carpal bones connect the hand to the forearm. <ref type="figure" target="#fig_0">Figure 1</ref> shows a sample image of the anatomy of the wrist. The image in <ref type="figure" target="#fig_0">Figure 1</ref> shows that the wrist is made up of eight small carpal bones in total. These are: scaphoid, lunate, trapezium, trapezoid, capitate, hamate, triquetrum and pisiform.</p><p>? Scaphoid: A long, boat-shaped bone under the thumb. Under these carpal bones are located the radius and ulna bones <ref type="bibr" target="#b12">[2]</ref>. The fractures in the wrist X-ray images used in this study are focused on the radius and ulna bones.</p><p>The main contributions of this study, in which fracture detection is performed on wrist X-ray images using deep-learning-based object detection models, are as follows:</p><p>?</p><p>In this study, fracture detection was performed in the first place, using 10 different deep learning models available in the literature, and the results thereof were compared. ? Many different augmentation methods were tried with these models, and the most compatible augmentation method was identified, on X-ray images in particular, followed by fracture detection procedures again.</p><p>?</p><p>In order to better analyze the results of fracture detection, in addition to the average precision (AP) and average recall (AR) parameters that are widely used in detection procedures in the literature, localization recall precision (LRP), which is another upto-date evaluation parameter, was used in the field of biomedical and in evaluation on bone fracture detection for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Ensemble models were used to improve the results of detection procedures carried out with a total of 20 models with/without augmentation. ? Outputs of 10 different models with augmentation were evaluated from five different aspects in the development of ensemble models which are: AP, AR, single stage, two stage and LRP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>With the new ensemble model developed within the scope of this study, an approximately 10% increase was achieved compared to the best AP result of the currently available detection models in the literature. ?</p><p>A clinical dataset (non-public) obtained from Gazi University Hospital was used in the study, which improves the applicability of the study in hospitals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The augmentation method used in this study and/or an ensemble model with a logic similar to the ensemble model developed within the scope of this study can be used for similar detection studies that may be conducted in the future on X-ray images, particularly in the field of biomedical research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The literature includes fracture detection studies based on artificial intelligence on both open source and clinical bone image datasets collected from various medical devices. The average precision (AP) score obtained from the fracture detection performed by Guan et al., using a dilated convolutional feature pyramid network (DCFPN) on 3842 thigh fracture X-ray images is 82.1%. <ref type="bibr" target="#b13">[3]</ref>. The highest result of fracture detection performed by Guan et al., on approximately 4000 arm fracture X-ray images in a musculoskeletal radiograph (MURA) dataset, 62.04% AP, was obtained using proposed two-stage regionbased convolutional neural networks (R-CNN) method <ref type="bibr" target="#b14">[4]</ref>. The AP50 score achieved by Wang et al., was 87.8% with the ParallelNet method developed for fracture detection in a dataset of 3842 thigh fracture X-ray images, using a TripleNet backbone network <ref type="bibr" target="#b15">[5]</ref>. Using a part of the dataset of 1052 bone images in total, Ma and Luo carried out fracture detection with Faster R-CNN, followed by fracture/non-fracture classification with the proposed CrackNet model using the entire dataset, achieving an accuracy of 90.11% <ref type="bibr" target="#b16">[6]</ref>. The AP score achieved as the result of the fracture detection performed by Wu et al., on a dataset consisting of 9040 hand, wrist, pelvic, knee, ankle, foot, shoulder and ankle radiographs in total by Feature Ambiguity Mitigate Operator (FAMO) model used with ResNeXt101 and feature pyramid network (FPN) was 77.4% <ref type="bibr" target="#b17">[7]</ref>. Qi et al., carried out fracture detection procedures, achieving a mean AP (mAP) score of 68.8%, using the anchor-based Faster R-CNN model with multi-resolution FPN and ResNet50 backbone network for a total of 2333 femoral fracture X-ray images with nine different types of fracture <ref type="bibr" target="#b18">[8]</ref>. Thian et al., used Inception-ResNet Faster R-CNN model for fracture detection on 7356 wrist radiographic images <ref type="bibr" target="#b19">[9]</ref>. Sha et al., achieved a mAP score of 75.3% using the You Only Look Once (YOLOv2)-based model developed for fracture detection in a dataset of 5134 spinal fracture CT images <ref type="bibr" target="#b20">[10]</ref>. With the fracture detection performed using another model based on Faster R-CNN developed by Sha et al., for the same dataset, the mAP achieved was 73.3% <ref type="bibr" target="#b21">[11]</ref>. The sensitivity achieved as the result of segmentation and detection performed with the proposed U-Net-based FracNet in a total of 7473 rib  <ref type="bibr" target="#b22">[12]</ref>. The AP score with the proposed guided anchoring method Faster R-CNN model for fracture detection in 3067 hand fracture X-ray images performed by Xue et al., was 70.7% <ref type="bibr" target="#b23">[13]</ref>.</p><p>In addition to studies on fracture detection, there are a number of studies on deeplearning-based classification in the literature in which the class of fracture is identified. Uysal et al., performed 26 different deep-learning-based classification procedures to determine the class of fracture in shoulder bone X-ray images in the musculoskeletal radiograph (MURA) dataset, and then developed two different ensemble learning models to further improve the results of the classification <ref type="bibr" target="#b24">[14]</ref>. Raghavendra et al., achieved a classification accuracy of 99.1% with the proposed CNN model in the thoracolumbar CT dataset containing 420 normal images and 700 fracture images <ref type="bibr" target="#b25">[15]</ref>. Within the scope of fracture classification performed by Beyaz et al., on 1341 femoral neck X-ray images using the proposed CNN model and genetic algorithm (GA), the accuracy achieved was 79.3% <ref type="bibr" target="#b26">[16]</ref>. Tobler et al., carried out fracture classification with the ResNet18 model in the dataset consisting of 15,775 frontal and lateral radiographs, resulting in the highest accuracy of 94% <ref type="bibr" target="#b27">[17]</ref>. As the result of classification performed to identify the class of fracture in 1389 wrist radiographs using the InceptionV3 model, Kim et al., achieved an area under the receiver operator characteristic curve (AUC) score of 0.954 <ref type="bibr" target="#b28">[18]</ref>. Chen et al., achieved an accuracy of 73.59% for the vertebral fracture class in a dataset of 1306 plain frontal radiographs using the ResNeXt model <ref type="bibr" target="#b29">[19]</ref>. Within the scope of the fracture classification performed by Tanzi et al., in 2453 proximal femur X-ray images using InceptionV3, VGG16 and ResNet50 models, the highest accuracies achieved for structures of grade three and grade five were 87% and 78%, respectively <ref type="bibr" target="#b30">[20]</ref>. ?ks?z et al., proposed a segmentation network in which training is carried out that optimizes three different tasks in cardiac MR images: image artefact detection, artefact correction and image segmentation <ref type="bibr" target="#b32">[21]</ref>. A distance for structural similarity metric and Fuzzy C-Means algorithm were developed for image segmentation by Tang et al. <ref type="bibr" target="#b33">[22]</ref>.</p><p>It is concluded based on the studies available in the literature that the classification models such as ResNet and VGG used to identify the class of fracture are used as backbone networks in deep-learning-based object detection models. Moreover, it is observed upon examination of the studies in the literature on detection of bone fractures that mostly Faster R-CNN or YOLO-based detection including backbone networks, such as ResNet or ResNeXt, is performed on CT and X-ray images. In this study, fracture detection procedures are performed on wrist X-ray images collected from Gazi University Hospital. For detection, Deformable Convolutional Networks (DCN), Dynamic R-CNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), Libra R-CNN, Probabilistic Anchor Assignment (PAA), RetinaNet, RegNet and Side-Aware Boundary Localization (SABL) models with various backbone networks were used in the first place. Based on the results of bone detection obtained therein, ensemble models were developed for fracture detection in wrist X-ray images for better detection results, providing a contribution to the literature.</p><p>The third part of the study explains the deep-learning-based object detection models and the proposed ensemble object detection models used for fracture detection. The fourth part of the study, titled "Experiments," includes the dataset collected from Gazi University Hospital used within the scope of the study and labelling of the site of fracture, as well as data preprocessing and data augmentation, AP, AR, LRP scores obtained using the fracture detection models and predicted bounding boxes and precision-recall curves. The fifth and final part of the study, titled "Conclusion and Future works," explains the contribution of the ensemble model developed within the scope of the study to the literature, as well as potential future developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Within the scope of the study, deep-learning-based object detection models consisting of various backbone networks were used and developed for the detection of fracture areas in wrist X-ray images. Firstly, deep-learning-based DCN (Faster R-CNN), Dynamic R-CNN, Faster R-CNN, FSAF, Libra R-CNN (RetinaNet), PAA, RetinaNet, RegNet (RetinaNet) and SABL (Faster R-CNN and RetinaNet) models available in the literature, as well as RegNet RegNetX-3.2GF, as backbones for these models, and ResNet50 was used for object detection for the models. Therefore, a total of 20 different procedures of fracture detection were performed with and without data augmentation with these models.</p><p>The transfer learning method was implemented in all deep-learning-based models used for fracture detection. With the use of the transfer learning method, the current weight of each detection model pre-trained with the COCO dataset was used. COCO is a dataset containing a significant number of images and various object categories, in which object detection and segmentation can be performed <ref type="bibr" target="#b34">[23]</ref>. In the deep learning models used for detection in <ref type="figure" target="#fig_2">Figure 2</ref>, the number of object categories was reduced from 80 to 1. The reason for this is while there are 80 object categories in the models trained with the COCO dataset, there is only 1 object category in this study as the fracture detection is performed in the wrist X-ray images. Based on an examination of the results obtained using the detection procedures in <ref type="figure" target="#fig_2">Figure 2</ref>, new ensemble models were developed to further improve the results of the fracture detection. The details of deep-learning-based built object detection models used for fracture detection in wrist X-ray images and the ensemble models developed are provided in sub-headings as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fracture Detection Models Based on DL for Wrist X-ray Images</head><p>Firstly, deep-learning-based built object detection models were used for fracture detection in wrist X-ray images. The models used at this stage have different structures, but are mainly single-stage and two-stage. The general structure of the structures used within the scope of the study is provided in <ref type="figure" target="#fig_3">Figures 3 and 4</ref> below.  When the general structure is shown in <ref type="figure" target="#fig_3">Figure 3</ref> regarding single-stage object detectors is examined, it is seen that the object location and object class can be obtained at the output by first passing the input images through the backbone and then through the neck and DenseHead operations, respectively. When looking at the two-stage object detectors structure specified in <ref type="figure" target="#fig_4">Figure 4</ref>, although there are similar structures such as backbone and neck in single-stage object detectors, the most basic difference between the two is that they also contain RoIHead in addition to the backbone, neck and DenseHead in two-stage detectors. In addition, when examined in terms of training times, in general, it is understood that single-stage detectors can detect faster than two-stage detectors. When examined in terms of accuracy, although the training time of two-stage detectors is slower, they generally have higher accuracy than single-stage detectors. However, this may vary depending on the dataset.</p><p>In both models' stages, it is seen that FPN is used in the neck parts and ResNet50 is used in the backbone. Additionally, RegNetX-3.2GF can be used in single-stage object detectors. FPN is a feature extractor that is used in object detection operations and is independent of backbone convolutional architectures. In FPNs, they generate proportionally sized feature maps at multiple levels <ref type="bibr" target="#b35">[24]</ref>. RegNetX is used as a backbone network in RegNet, one of the object detection models. RegNetX is a convolutional network design space where there is a linear parameterization of block widths <ref type="bibr" target="#b36">[25]</ref>. In this study, RegNetX-3.2GF was used in the RegNet model used for fracture detection. ResNet, on the other hand, is a deep learning model that contains more than one residual block <ref type="bibr" target="#b37">[26]</ref>. The number of these blocks may vary depending on the number of layers. In the object detection models used in this study, ResNet50 with 50 layers was used in all models where ResNet was used as the backbone network.</p><p>Regarding the sample outputs of both types of object detection models, the bbox numbers produced by the single-stage PAA and two-stage Dynamic R-CNN models at certain threshold values in the [0.1, 0.9] range are given in <ref type="table" target="#tab_3">Table 1</ref>. In addition, more detailed bbox numbers for these two models are available in <ref type="figure" target="#fig_5">Figure 5</ref>. When the results are examined in detail, PAA produces a low probability value but many bboxes, while the Dynamic RCNN model produces a high probability value but few outputs. This is clearly understood when looking at the test results from 0.1 to 0.9 of the threshold value.  FSAF, PAA, RetinaNet and RetinaNet-based models (SABL, Libra, RegNet) were used as single-stage models, and Dynamic R-CNN, Faster R-CNN and Faster R-CNNbased models (SABL, DCN) were used as two-stage models. The subheadings below provide details of the models used within the scope of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">RetinaNet</head><p>RetinaNet is a single-stage deep-learning-based object detection model that can achieve higher detection results compared to many different two-stage object detection models. RetinaNet models mainly consist of ResNet in the backbone, Feature Pyramid Network (FPN) in the neck and RetinaHead in the DenseHead, that is, the classification subnet and box regression subnet <ref type="bibr" target="#b38">[27]</ref>. In this study, a RetinaNet object detection model pretrained with the COCO dataset with a ResNet50 backbone was used for fracture detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Feature Selective Anchor-Free (FSAF)</head><p>Feature Selective Anchor-Free (FSAF) is an anchor-free model for single-shot object detectors, consisting of a simple and effective building block. It is concluded upon examination of FSAF models that they are RetinaNet based. FSAF uses ResNet as backbone, FPN in neck and FSAFHead in the bounding box head <ref type="bibr" target="#b39">[28]</ref>. Different from RetinaNet, this study uses FSAFHead as DenseHead in FSAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Dynamic R-CNN</head><p>Dynamic R-CNN consists of two main components, the Dynamic Label Assignment (DLA) process and the Dynamic SmoothL1 Loss (DSL). The IOU threshold is increased automatically in a dynamic manner to improve the quality of proposals in the DLA. In DSL, the shape of the regression loss function is adjusted to ensure such improvement <ref type="bibr" target="#b40">[29]</ref>. In the Dynamic R-CNN model used within the scope of this study, fracture detection was performed by pretraining with the COCO dataset with a ResNet50 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Libra R-CNN</head><p>Libra R-CNN consists of three main components to reduce any imbalance at sample, feature and objective levels, and to further improve detection performances. These components are IoU-balanced sampling, a balanced feature pyramid and balanced L1 loss. This model appears to improve detection results when applied in both Faster R-CNN and RetinaNet models <ref type="bibr" target="#b41">[30]</ref>. The hardware used within the scope of this study allowed the use of the Libra RetinaNet model for fracture detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Faster R-CNN</head><p>R-CNN and Fast R-CNN use a protracted selective search affecting the network performance to find region proposals. In Faster R-CNN, a Region Proposal Network (RPN) is proposed to find the region proposals. This enables the development of a two stage detector that can perform faster object detection compared to Fast R-CNN <ref type="bibr" target="#b42">[31]</ref>. A Faster R-CNN two-stage detector with a ResNet50 backbone was used for fracture detection in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6.">Side-Aware Boundary Localization (SABL)</head><p>SABL is an approach that can be applied to both single-stage and two-stage detectors, which can localize each side of the bounding box with a dedicated network branch, respectively. This approach can be applied to Faster R-CNN, RetinaNet and Cascade R-CNN <ref type="bibr" target="#b43">[32]</ref>. For detection procedures in this study, SABL was applied to both Faster R-CNN and RetinaNet with a ResNet50 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7.">Deformable Convolutional Networks (DCN)</head><p>DCN mainly consists of two modules, which are deformable convolution and deformable RoI pooling. Using these modules, the transformation modeling capacity of CNNs is improved. The procedures performed in object detection and instance segmentation suggest that this model has a positive contribution to the results <ref type="bibr" target="#b44">[33]</ref>. DCNv2, that is, the 2nd version of this model, with Faster R-CNN and a ResNet50 backbone, was used for fracture detection within the scope of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8.">Probabilistic Anchor Assignment (PAA)</head><p>With PAA, a new anchor assignment that can separate anchors into positive and negative samples with adaptable anchors, was proposed and applied. In this model, anchor scores are calculated in the first place, and the probability distributions of these scores are fitted. Subsequently, the model is trained with anchors that are separated into positive and negative samples based on their probabilities. Basically, in the PAA model, only a single convolutional layer is added to the RetinaNet model <ref type="bibr" target="#b45">[34]</ref>. Within the scope of this study, fracture detection was performed using the version of the PAA model with a ResNet50 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.9.">RegNet</head><p>RegNet proposes and uses a network design paradigm where stage widths and depths are determined by the quantized linear function. With this paradigm, design spaces that combine the advantages of manual design and neural architecture search and parameterize network populations are designed. The RegNet model can be used with Faster R-CNN, Mask R-CNN and RetinaNet <ref type="bibr" target="#b36">[25]</ref>. The hardware used in this study only allowed the use of the RegNet RetinaNet model with a RegNetX-3.2GF backbone as a RegNet model for fracture detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Fracture Detection Models Based on DL for Wrist X-ray Images</head><p>Based on the results of the detection of deep-learning-based models performed on wrist X-ray images and used for fracture detection within the scope of the study, ensemble models were used in order to further improve the results of fracture detection. While determining the submodels to be used for the ensemble model, 5 different cases were taken into consideration, consisting of optimal threshold values of AP50, AR and LRP, as well as the single-stage and two-stage structures of the models. Details of the ensemble model used and proposed specifically for the study are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrist Fracture Detection (WFD) Ensemble Model Based on Weighted Boxes Fusion</head><p>Firstly, five different Weighted Boxes Fusion-based WFD ensemble models were used. While the ensemble models developed and proposed using the submodels determined based on the single-stage and two-stage structures examined are WFD-1 and WFD-2, respectively, the ensemble models developed and proposed using the submodels determined based on the AP50, AR and LRP optimal threshold values are WFD-3, WFD-4 and WFD-5, respectively. While the WFD-3 ensemble model was developed using the detection models with the 5 best AP50 scores in total, consisting of Dynamic R-CNN, FSAF, Libra RetinaNet, PAA and RetinaNet, the WFD-4 ensemble model was developed with the models with the 5 best AR scores, consisting of FSAF, Libra RetinaNet, PAA, RetinaNet and SABL RetinaNet, using the most compatible models and weight coefficients determined out of 4715 different combinations with varying weight coefficients between 1-5. WFD-1 and WFD-2 ensemble models were developed, respectively, based on the most compatible models and weight coefficients determined out of 40,593 different combinations using weight coefficients between 1-6 based on 6 single-stage models and 388 different combinations using weight coefficients between 1-4 based on 4 two-stage models. WFD-5 ensemble models was developed based on the most compatible models and weight coefficients determined out of 4715 different combinations using weight coefficients between 1-5 using DCN, Faster R-CNN, FSAF, Libra RetinaNet and RetinaNet with the best LRP optimal threshold values. All WFD ensemble models developed are based on weight boxes fusion.</p><p>For the ensemble models named WFD-1,2,3,4,5, different trials were carried out in each WFD to determine the most performance model combinations. The steps followed while performing these operations is basically as follows:</p><p>Step 1: First of all, the object detectors associated with the relevant WFD model and the number of them are determined.</p><p>Step 2: Since the number of models required for the formation of the WFD model is at least two, different combinations are tried, from two to the maximum number of models.</p><p>Step 3: The weights used during these combinations are for a maximum of n object detectors; this changes as an integer in the range of 1-n. Therefore, while determining the weight coefficients, it starts from 1 and increases consecutively by the maximum number of object detector models. For example, detectors associated with WFD-1 are single-stage models with RegNet, FSAF, RetinaNet, SABL RetinaNet, PAA and Libra RetinaNet, and their number is 6. Therefore, while the number of models is minimally 2 and maximally 6 in combinations made with WFD-1, the weight coefficients vary in the range of 1-6. Similar steps have been applied to other WFDs.</p><p>The weight boxes fusion ensemble structure is an ensemble model initially developed by Solovyev et al., which uses the confidence scores of all bounding boxes proposed to develop the averaged boxes <ref type="bibr" target="#b46">[35]</ref>. The basic structure of the weight boxes fusion ensemble method is shown in <ref type="figure" target="#fig_6">Figure 6</ref> below. The scheme of the main WFD ensemble models used within the scope of the study is displayed in <ref type="figure" target="#fig_7">Figure 7</ref> below.  With the WFD ensemble model based on weight boxes fusion in <ref type="figure" target="#fig_7">Figure 7</ref> above, firstly, JSON outputs for fracture detection are obtained, using the MMDetection Toolbox. Subsequently, JSON files are converted into CSV format. In these files, ensemble results are achieved by performing weight boxes fusion with different weight coefficients varying from 1 to n, taking into account the number of models (n) used for detection.</p><p>The scheme of the wrist fracture detection-combo (WFD-C) ensemble model developed in this study for fracture detection in wrist X-ray images is presented in <ref type="figure" target="#fig_8">Figure  8</ref> below. In the WFD-C model, another weight boxes fusion-based ensemble procedure was performed using the previously achieved WFD-1, 2, 3, 4 and 5 models. The submodels were used in 6 different ensemble models in total, and the corresponding weight coefficients thereof are displayed in <ref type="table" target="#tab_7">Table 2</ref> below.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Deep-learning-based object detection models were used in this study, in which bone fracture detection was performed on wrist X-ray images. Using a dataset collected from Gazi University Hospital, the first procedures performed were the labeling of bone fractures, data preprocessing and data augmentation. Subsequently, 20 different detection procedures were performed using a number of different object detection models with various backbones, which are based on deep-learning. Taking into account the results achieved herein, ensemble models were developed to further improve the detection results. The bone fracture detection models used and proposed for wrist X-ray images are presented in <ref type="figure" target="#fig_9">Figure 9</ref> below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Labeling of Wrist X-ray Images</head><p>The wrist X-ray images collected from Gazi University Hospital were used within the scope of the study. The official document (date and number: 27.04.2020-E.51150) confirming that collection and use of this dataset does not constitute any ethical inconvenience was provided by the Measurement and Evaluation Ethics Working Sub-Group of Gazi University. The physicians Dr. Murat ?i?eklida?, Assoc. Prof. Dr. Tolga Tolunay and Prof. Dr. Nil Tokg?z, who work at Gazi University Hospital, provided assistance in data collection and labeling of the fracture area. The number of wrist X-ray images collected from the Radiology Department of the hospital is 542, and the image format is Digital Imaging and Communications in Medicine (DICOM). This dataset belongs to patients between the years 2010-2020. All of these patients are patients who presented to the emergency department. All images in the dataset were obtained from the same X-ray machine. The X-ray machine model used is the Samsung GC70. There is a heterogeneous distribution of both right-wrist and left-wrist images in the dataset. No distinction was made in the treatment and use process of these. According to the results obtained using the Python pydicom library, there was a total of 275 patients. Of these, 134 females and 141 males were available. The average age was 44.99. There were a total of 21 individuals aged 12 and under. A total of 92.37% of all patients were adults and 7.63% were under 12 years old. In order to use the collected data in CNN-based object detection models, the format was converted from DICOM format to 3-channel png format. Pydicom library was used to read and extract information from images taken in DICOM format. Images in DICOM format were converted to grayscale PNG format using the library. After certain operations were made on the images in PNG format, normalization was made within the framework in the object detection network, and training and testing were carried out by converting them to RGB format in order to perform operations such as coloring on the image in the future studies. Moreover, the graphical image annotation tool titled LabelImg <ref type="bibr" target="#b47">[36]</ref> was used to label the areas of fracture in the images.</p><p>An X-ray device only was used for imaging the wrists of the patients. The X-ray images obtained from this device were also examined by three physicians who are experts in their fields. For this reason, there was no need for a different imaging technique such as CT or MRI. The labeling of the fractures was jointly examined by one radiologist and two orthopedists.</p><p>Each of the images in the dataset was examined by one radiologist (Nil Tokg?z) working in the Radiology Department at Gazi University Hospital and two orthopedist (Murat ?i?ekda?, Tolga Tolunay) working in the Orthopedics and Traumatology Department (Murat ?i?ekda?, Tolga Tolunay) and the fractures were detected. Subsequently, the use of the labeling library was explained to the physicians in detail and the labeling was provided by the physicians. The fracture areas examined and labeled by the physicians in the bone X-ray images used within the scope of the study belong only to the radius and ulna bones.</p><p>All of the wrist images taken from Gazi University Hospital consist of fracture (abnormal, unhealthy, positive) images. Therefore, there is at least one fracture in each of the images in the datasets. There are a total of 569 fractures in 542 wrist images used within the scope of the study. The distribution of the 569 fractures in the dataset is as follows: there are 459 labels in 434 training data, 55 labels in 54 validation data and 56 labels in 54 test data. <ref type="figure" target="#fig_0">Figure 10</ref> below shows the distribution of the wrist X-ray dataset, which initially had different resolutions in terms of quantity and percentage, as a training, validation and test dataset. The images in the training, validation and test dataset belong to different patients. Therefore, if the images of a patient are more than one, they are included in the same group dataset. The number of patients with both hands fractures was 10 in the train dataset, 1 in the test dataset, and none in the validation dataset. In addition, out of 434 images in the training dataset, 28 were pairs (right and left hands), 187 belong to the right hand, and 219 belong to the left hand. In the validation dataset, there are 3 pairs and 24 right and 27 left hand images out of 54 images In the test dataset, there are 7 pairs and 22 right and 25 left hand images out of 54 images. Of the total images, 7% are pair, 43% are right-handed and 50% are left-handed images. All of the wrist X-ray images used in the study and taken from Gazi University Hospital are abnormal (positive, unhealthy, fracture) images. No normal (negative) images were obtained from the hospital. The distribution of our wrist X-ray abnormal (fracture) image dataset, consisting of 542 images, is 80% training, 10% validation, 10% test. All details of the wrist fracture dataset are given in <ref type="table" target="#tab_10">Table 3</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Preprocessing of Wrist X-ray Images</head><p>When the wrist X-ray image dataset used within the scope of the study and collected from Gazi University Hospital was examined after the format was converted from DICOM to 3-channel png format, a difference in their background was observed.</p><p>When the wrist X-ray images used within the scope of the study were examined, especially after they were converted from DICOM format to png format, it was understood that there was no standard in image sizes, but that they were different sizes. To the extent that the object detection models used and local PC hardware supported it, the wrist images in the images were first manually cropped to include the hand and wrist parts. Afterward, all input image sizes were rescaled to 800 ? 800 ? 3, so as not to affect the fracture detection process to the extent supported by local PC hardware.</p><p>The dominant color displayed by the background of the image was identified by using the binary K-means algorithm to recover the difference in the backgrounds of the images. Following this procedure, the images were inverted (white if black, black if white) if the dominant color is white, and the background was converted into black and the foreground into white. This procedure ensured that all images used in the dataset were in a certain color format. After adjustment of the color format, contrast-limited adaptive histogram equalization (CLAHE) with 11 ? 11 grid size and 7.0 clip limit parameters was applied on the images. <ref type="figure" target="#fig_0">Figure 11</ref> below shows the sample images of the wrist X-ray image dataset achieved as the result of the data preprocessing steps. <ref type="figure" target="#fig_0">Figure 11</ref>. The wrist X-ray images after data preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation of Wrist X-ray Images</head><p>The amount of data is essential for network training with deep-learning-based object detection models. Therefore, data augmentation was performed within the scope of this study in order to make the best of network training and to achieve high scores for fracture detection. As for the initial dataset, various augmentations were tried using the Albumentations <ref type="bibr" target="#b48">[37]</ref> library. Albumentations is a flexible and fast image augmentation python library that can be used in different computer vision tasks that include object detection, classification and segmentation, which are particularly deep-learning-based open source projects <ref type="bibr" target="#b48">[37]</ref>. In augmentations performed using this library, random brightness contrast, sharpness, noise, gamma, gaussian blur and median blur were used. Experiments of augmentation were conducted using these six different methods used in these procedures, either individually or together. Upon analysis of the detection procedures performed with augmentation, methods that have a negative impact on the result of detection, methods with no impact or the methods inclined to augmentation were identified. Based on the experiments, it was concluded that random brightness contrast augmentation made the greatest contribution for detection of fractures in the Albumentations library. This type of augmentation was used in addition to the random flip ratio 0.5 augmentation available in the training stage of the models in the MMdetection tool used for detection. Samples of the images obtained as the result of the augmentation steps are presented in <ref type="figure" target="#fig_0">Figure 12</ref> below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fracture Detection Results</head><p>Offline local PCs and Google Colab were used as hardware in fracture detection procedures performed using deep-learning-based object detection models in wrist X-ray images. The graphics cards of the hardware used for fracture detection performed in these systems are as follows: 4 GB nVIDIA GTX1650 and GTX1050 for the offline local PCs, and nVIDIA Tesla T4 16 GB GDDR6 for Google Colab. In addition to these, Albumentations, LRP error <ref type="bibr" target="#b49">[38]</ref>, review object detection metrics <ref type="bibr" target="#b50">[39]</ref>, weight boxes fusion <ref type="bibr" target="#b46">[35]</ref> packages were used. MMDetection <ref type="bibr" target="#b51">[40]</ref> toolbox was used for object detection procedures.</p><p>MMDetection is an open source object detection toolbox based on PyTorch containing many different deep-learning-based object detection models, including single-stage, twostage and multi-stage, which is used in object detection and instance segmentation procedures in particular <ref type="bibr" target="#b51">[40]</ref>. The program codes written for this study using PyTorch- Loss, IoU Loss, Balanced L1 Loss or L1 Loss as bounding box loss function. In order to improve the success of network learning used in all these models, the learning rate value was not kept constant. During the 40 epochs of training, the learning rate decreased 10 times in the 5th, 10th, 15th, 25th and 35th epochs, respectively. The parameters used in the ensemble models developed specifically for the study are skip box threshold 0.3, intersection over union threshold 0.5 and limit boxes 6000. Although not defaulted in the training parameters, it has been chosen on a per-study basis to create an overall structure similar to the suggested configurations. Since the number of epochs depends on the complexity of the model, it has been kept high and has been chosen as 40 in order to appeal to each model. Accordingly, choosing a higher epoch number has allowed the selection of a lower and generally accepted learning rate. In addition, augmentation steps and other optimization settings are the same in all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Evaluation Metrics</head><p>In order to evaluate the results achieved in object detection problems in the most efficient way, average precision (AP), average recall (AR), precision-recall curve, ground truth-prediction comparison on image and localization recall precision (LRP) error <ref type="bibr" target="#b49">[38]</ref> scores must be obtained. Precision and Recall scores depend on the threshold value of Intersection Over Union (IOU), as well as the True Positive (TP), False Positive (FP) and False Negative (FN) values. IOU suggests the ratio (part) of the overlapping area to union area of the predicted bounding box and the ground-truth bounding box. In order for classification of the detection result to be true or false, a threshold value must be specified. If the IOU score is greater than or equal to the threshold, the detection result is considered to be true, in other cases (if the IOU is less than the threshold), the detection result is considered to be false. TP refers to the correct detection of ground-truth bounding. FP is the misplaced detection of an existing object or the false detection of a non-existing object. FN stands for failure of detection of ground-truth bounding. Although it is used in classification, there is no value True Negative (TN) in object detection problems. This is because the TN value cannot be used as there are an infinite number of bounding boxes that should not be detected in any image. Precision (P) refers to the ratio of TP to all detections (TP + FP). Recall (R) is the ratio of TP to all ground-truths (TP + FN). AP is the average precision over all unique recall levels and refers to the area under the precisionrecall curve. AR is the average recall over the whole IOU in the [0.5, 1.0] interval and is calculated as twice the area under the recall-IOU curve. LRP error is a metric proposed by <ref type="bibr" target="#b52">Oksuz et al., in 2018</ref> that is applicable to all object detection tasks <ref type="bibr" target="#b52">[41]</ref>. This error metric is also an alternative to AP for key point detection, instance segmentation and object detection <ref type="bibr" target="#b49">[38]</ref>. <ref type="table" target="#tab_12">Table 4</ref> below explains the meanings of IOU, TP, TN, FP and FN values and how the P, R, AP, AR and optimal LRP are calculated. For fracture detection in wrist X-ray images, a total of 20 fracture detection procedures were performed, with and without augmentation, in 10 different deeplearning-based models. <ref type="figure" target="#fig_0">Figures 13 and 14</ref> and <ref type="table" target="#tab_13">Tables 5-7</ref>      It can be observed upon examination of <ref type="table" target="#tab_13">Table 5</ref> that the number of epochs with the highest validation accuracy varies between 6-21 in models without augmentation and 5-12 in models with augmentation, and that the lowest loss values achieved from different training times in models both with and without augmentation is achieved in SABL Faster R-CNN for train bbox loss and in DCN Faster R-CNN model for train loss. The analysis of the values in <ref type="table" target="#tab_14">Table 6</ref> and the graph in <ref type="figure" target="#fig_0">Figure 13</ref> suggest that the best AP50 scores in validation were obtained in Dynamic R-CNN models with/without augmentation among the models used for detection. The <ref type="figure" target="#fig_0">Figure 14</ref> and <ref type="table">Table 7</ref> show that the lowest LRP optimal threshold values for validation are available in FSAF in all models.</p><p>The results of the tests performed with the test data following the training with and without augmentation performed with each model for fracture detection are presented in detail in <ref type="figure" target="#fig_0">Figures 15 and 16</ref> and <ref type="table" target="#tab_15">Tables 8 and 9</ref> below.    It is observed upon examination of <ref type="table" target="#tab_15">Table 8</ref> and <ref type="figure" target="#fig_0">Figure 15</ref> that among all the models used for fracture detection in wrist X-ray images, the highest AP50 score obtained on the test data was 0.754 in PAA model with augmentation. <ref type="figure" target="#fig_0">Figure 16</ref> and <ref type="table">Table 9</ref> indicate that the lowest values of LRP optimal threshold, oLRPLoc, oLRPFP, oLRPFN and oLRP in models with augmentation were obtained in the FSAF, SABL Faster R-CNN, RegNet RetinaNet, PAA/RetinaNet and PAA models, respectively.</p><p>The bounding box outputs achieved from the fracture detection performed with the PAA model with the best AP50 score are displayed in <ref type="figure" target="#fig_0">Figure 17</ref> for right/left hand as a sample in the dataset, and the precision-recall graph is provided in <ref type="figure" target="#fig_0">Figure 18</ref>.  In addition to the 20 procedures of fracture detection performed on wrist X-ray images based on deep learning provided in this section and thereof, the outputs of the ensemble models proposed to improve the detection results further are explained in the next section. Based on the results of 20 models based on deep learning in which fracture detection was performed in wrist X-ray images as explained in the previous section, ensemble models were developed, thus leading to an improvement in the detection results. AP50, AR and LRP scores achieved from six different WFD-based ensemble models are presented in <ref type="figure" target="#fig_0">Figure 19</ref> and <ref type="table" target="#tab_3">Table 10</ref> below.  <ref type="table" target="#tab_3">Table 10</ref>. Results (AP50, AR, LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN) of Ensemble Model (WFD-1, WFD-2, WFD-3, WFD-4, WFD-5, WFD-C). The outputs in the <ref type="table" target="#tab_3">Table 10</ref> and <ref type="figure" target="#fig_0">Figure 19</ref> obtained regarding the ensemble models developed within the scope of the study indicate that the highest AP50 score was 0.8639 in the WFD-C ensemble model. It is possible to state upon comparison of AP50 score of 0.754 in the PAA model, which was the highest result in the detection before the ensemble with the WFD-C model, that an increase of over 10% in AP50 score was achieved with the best ensemble model developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble</head><p>The bounding box outputs achieved from fracture detection performed with ensemble models in wrist X-ray images are provided in <ref type="figure" target="#fig_0">Figures 20 and 21</ref> below as a sample for the right/left hand in the dataset. The predicted bounding boxes achieved fracture detection suggest that, as shown in the images in <ref type="figure" target="#fig_0">Figures 20 and 21</ref>, the model that provides the most accurate results that are closest to the ground-truth bounding boxes is the WFD-C ensemble model developed in this study. Moreover, the examination of the number of predicted bounding boxes reveal that the model with the lowest number on the test data is also WFD-C. For the fracture detection in wrist X-ray images, <ref type="figure" target="#fig_2">Figure 22</ref> below shows the total number of predicted bounding boxes obtained for each model on the test dataset as the result of the fracture detection performed with a total of 26 deep learning models, six of which are ensemble models. For the results of detection carried out with the ensemble models, the precision-recall curve of the WFD-C ensemble mode with the highest AP score is shown in <ref type="figure" target="#fig_2">Figure 23</ref> below.  When the results given in the table are examined and analyzed in terms of AP50, while the PAA model result was 0.754 in the Gazi University Hospital test dataset, the WFD-C model result was 0.8639, and an increase of 0.1099 was observed. In addition, when the test amount is doubled in the Gazi University Hospital dataset, it is seen that the developed WFD-C model achieves better AP results than the PAA model. When <ref type="table" target="#tab_3">Table  11</ref> is examined, it is understood that WFD-C has better detection results than the YOLO [42] model on a different number of datasets. When all these results are examined, it is understood that an increase in AP50 values was observed on a different number of datasets of the developed ensemble model and a contribution to the literature was obtained by obtaining good results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>Within the scope of this study, the aim was to develop the most compatible model for performing fracture detection in wrist X-ray images. The clinical dataset collected from Gazi University Hospital was used as the dataset in the study. Following the data preprocessing on the data, fracture detection was performed using 10 different object detection models based on deep learning. Subsequently, following experiments on various data augmentation methods and by performing augmentation on the training dataset using the method with the greatest contribution, new detection procedures were carried out with these 10 models. After 20 different procedures of fracture detection performed with the deep-learning-based models available in the literature, the results achieved from these procedures were examined from different perspectives, and six different ensemble models were developed to further improve the results of detection. As the result of the procedures of fracture detection performed using 26 different models in total, the highest AP score was obtained using the WFD-C ensemble model developed within the scope of this study. The contributions of the study to the literature are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Because a clinical dataset was used as a dataset in the study, the new ensemble model developed within the scope of the study has the potential to be used in hospitals in the future, thus contributing to the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The augmentation method used in this study determined and used specifically for this study as the result of the data augmentation methods following the data preprocessing performed will be a good reference for those who will work on similar types of (medical X-ray) images in the future, constituting another contribution of the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>For the evaluation of the results of fracture detection, in addition to AP and AR scores, which are among the parameters currently available in the literature, LRP parameters were calculated on medical data for the first time in the literature within the scope of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>To further improve the best result achieved in 20 different procedures of detection performed with deep-learning-based models available in the literature, the results were examined from five different perspectives to develop ensemble models. An approximately 10% increase in AP score was achieved with the best ensemble model developed based on weight box fusion to further improve the fracture detection results. With the new ensemble model developed with this approach, a unique detection model that will contribute to the literature was created.</p><p>Within the scope of this study, in which fracture detection was performed in wrist Xray images, the aim is to provide assistance to physicians who are not specialized in their fields and/or especially to those working in emergency services in diagnosing fractures on X-ray images to allow them to apply the required treatments. Further in the study, an application can be developed to assist physicians, which can be used on portable devices such as mobile phones, tablets and laptops by operating in real-time, by studying other types of bone fractures that are frequently encountered in emergency services in addition to wrist images. In addition, if there is a portable X-ray in medical vehicles sent to help people in major disasters, epidemics or countries with underdeveloped health systems, evaluation can be made without the need for a radiologist. Regarding fracture detection in wrist bone X-ray images, in future works, in addition to fracture detection, an application can be developed that can perform classification, fracture detection and segmentation processes in normal and abnormal (fracture) image datasets, and that physicians can use on a portable device. For this purpose in the future, a hybrid system can be developed by using deep learning and various machine learning methods, especially for fracture detection processes. When the study is considered especially in terms of the dataset, information on inclusion/exclusion criteria is given in <ref type="table" target="#tab_3">Table 12</ref>. <ref type="table" target="#tab_3">Table 12</ref>. Inclusion/exclusion criteria for dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inclusion Criteria Exclusion Criteria</head><p>Fracture labeling: Only fractures in the radius and ulna bones are labeled in Wrist.</p><p>Fracture labeling: other small bone (trapezoid, trapezium, scaphoid, capitate, hamate, triquetrum, pisiform, lunate) fractures in Wrist were not studied and ignored. Image size: Images were rescaled to 800 ? 800 ? 3, and deep learning models supporting this size were used.</p><p>Image size: YOLO and other deep learning models that do not support 800 ? 800 ? 3 size were not used. Data collection process: X-ray images of patients from the last 10 years (between 2010 and 2020) were used in 2020.</p><p>Data collection process: X-ray images of patients after 2020 were not used. Number of fractures: there are 570 fractures in 542 images. Thus, there are multiple fracture ones in an image.</p><p>Number of fractures: images with not just one fracture, but one and/or more than one fracture were used. The number of patients with fractures in both hands is 11. The distribution of these patients is 10 in Train and 1 in Test.</p><p>The patient with a fracture in both hands was not included in the validation.</p><p>The number of patients under the age of 12 and adults are 21 and 254, respectively. There is heterogeneity in these patient numbers.</p><p>There is no homogeneity, that is, an equal distribution, in the number of patients under the age of 12 and adults.</p><p>One radiologist and 2 orthopedists were jointly involved in the labeling of the fractures.</p><p>No more than 3 physicians were used in the labeling of fractures. There is no difference of opinion in the labeling of physicians.</p><p>In the Dataset distribution, the number of fractures per image is highest in the train dataset.</p><p>In the distribution of the dataset, the number of fractures per image was not considered to be equal in the train, validation and test dataset. In the dataset, the number of females is 134 and the number of males is 141.</p><p>Equality in the number of males and females was not considered in the dataset. The images in the dataset are 7% pair (right, left), 43% right hand, 50% left hand.</p><p>Equality was not observed in the number of pairs, righthanded and left-handed images in the dataset. Since the graphics card in the local PC hardware used in the study is 4 GB, object detection models that support this are used in MMDetection.</p><p>In the study, models that support a graphics card higher than 4 GB from the object detection model-hands in MMDetection could not be used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The anatomy of the wrist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>Lunate: A crescent-shaped bone beside the scaphoid. ? Trapezium: A rounded, square-shaped bone above the scaphoid and under the thumb. ? Trapezoid: The bone beside the trapezium shaped like a wedge. ? Capitate: An oval bone in the middle of the wrist. ? Hamate: The bone under the pinky finger side of the hand. ? Triquetrum: The pyramid-shaped bone under the hamate. ? Pisiform: A small, round bone that sits on top of the triquetrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Deep-learning based detection models using transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Single-stage object detectors (RetinaNet, FSAF, etc.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Two-stage object detectors (Dynamic R-CNN, Faster R-CNN, etc.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>PAA and Dynamic R-CNN bbox numbers according to different threshold values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Weight boxes fusion ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>WFD ensemble models based on weight boxes fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Proposed WFD-C ensemble models based on weight boxes fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>The proposed models for fracture detection of wrist X-ray images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>The wrist X-ray dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>The wrist X-ray images after data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>/github.com/fatihuysal88/wrist-d (accessed on 30 December 2021). The following were used in all fracture detection procedures performed using deep-learning-based object detection models in Figure 2: learning rate 0.001, epoch number 40, optimizer SGD, momentum 0.9, Cross Entropy Loss or Focal Loss as class loss function and Smooth L1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>present the following results for fracture detection procedures performed with each model: train bbox loss (TB_Loss), train loss (T_Loss) and training time (TT) values for the training phase, the epoch where the highest validation accuracy is achieved for AP50, AR, oLRP, oLRPLoc, oLRPFP and oLRPFN values for the validation phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Validation AP50 and AR results of detection models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 . 9 *DCN</head><label>149</label><figDesc>Validation LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN results of detection models. (* with Aug.). LRP-opt, *oLRP Loc *oLRP FP *oLRP FN *oLRP LRP-opt, oLRP Loc oLRP FP oLRP FN oLRP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Test AP50 and AR results of detection models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 .</head><label>16</label><figDesc>Test LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN results of detection models. (* with Aug.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>9 *DCN</head><label>9</label><figDesc>LRP-opt, *oLRP Loc *oLRP FP *oLRP FN *oLRP LRP-opt, oLRP Loc oLRP FP oLRP FN oLRP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>PAAFigure 17 .</head><label>17</label><figDesc>Sample of left/right wrist fracture results [ground-truth bounding box (green), predicted bounding box (red)] for PAA (Best score of 20 models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 18 .</head><label>18</label><figDesc>Precision-recall curve of PAA (best score of 20 models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 19 .</head><label>19</label><figDesc>Results (AP50, AR, LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN) of Ensemble Model (WFD-1, WFD-2, WFD-3, WFD-4, WFD-5, WFD-C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 20 .Figure 21 .</head><label>2021</label><figDesc>Sample of right wrist fracture results [ground-truth bounding box (green), predicted bounding box (red)]. Sample of left wrist fracture results [ground-truth bounding box (green), predicted bounding box (red)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 .</head><label>22</label><figDesc>Count of predicted bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 23 .</head><label>23</label><figDesc>Precision-recall curve of WFD-C (best score of ensemble models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>CT images collected from 900 patients by Jin et al., was 92.9%</figDesc><table><row><cell>fracture</cell></row></table><note>Accepted at Sensors, MDPI, 2022, 22, 1285. Hardala?, F.; Uysal, F.; Peker, O.; ?i?eklida?, M.; Tolunay, T.; Tokg?z, N.; Kutbay, U.; Demirciler, B.; Mert, F. Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. for citation and final version please click here: https://www.mdpi.com/1424-8220/22/3/1285</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>PAA and Dynamic R-CNN bbox numbers according to different threshold values.</figDesc><table><row><cell>Threshold Value</cell><cell>PAA Bbox</cell><cell>Dynamic R-CNN Bbox</cell></row><row><cell>0.1</cell><cell>4317</cell><cell>291</cell></row><row><cell>0.2</cell><cell>1098</cell><cell>176</cell></row><row><cell>0.3</cell><cell>415</cell><cell>124</cell></row><row><cell>0.4</cell><cell>159</cell><cell>97</cell></row><row><cell>0.5</cell><cell>75</cell><cell>79</cell></row><row><cell>0.6</cell><cell>36</cell><cell>71</cell></row><row><cell>0.7</cell><cell>4</cell><cell>65</cell></row><row><cell>0.8</cell><cell>0</cell><cell>57</cell></row><row><cell>0.9</cell><cell>0</cell><cell>48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Submodels and weight coefficients of WFD ensemble models.</figDesc><table><row><cell>Ensemble Models</cell><cell>Model-1</cell><cell>Model-2</cell><cell>Model-3</cell><cell>Model-4</cell><cell>Model-5</cell><cell>Weight Coefficients</cell></row><row><cell cols="2">WFD-1 (single stage) RegNet</cell><cell>FSAF</cell><cell cols="2">RetinaNet SABL Rt.Net</cell><cell>PAA</cell><cell>(1, 5, 5, 5, 5)</cell></row><row><cell>WFD-2 (two stage)</cell><cell>DCN</cell><cell>SABL Fs. R-CNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>(2, 2)</cell></row><row><cell>WFD-3 (AP50)</cell><cell>RegNet</cell><cell>PAA</cell><cell>FSAF</cell><cell>Libra Rt.Net</cell><cell>-</cell><cell>(3, 3, 3, 4)</cell></row><row><cell>WFD-4 (AR)</cell><cell>RegNet</cell><cell>PAA</cell><cell>FSAF</cell><cell>SABL Rt.Net</cell><cell>-</cell><cell>(1, 3, 3, 1)</cell></row><row><cell>WFD-5 (LRP-opt.)</cell><cell>FSAF</cell><cell>Fs. R-CNN</cell><cell>DCN</cell><cell cols="2">Libra Rt.Net RetinaNet</cell><cell>(2, 1, 4, 2, 3)</cell></row><row><cell>WFD-C (combo)</cell><cell>WFD-1</cell><cell>WFD-3</cell><cell>WFD-4</cell><cell>WFD-5</cell><cell>WFD-2</cell><cell>(4, 4, 3, 5, 5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>Details of wrist fracture dataset.</figDesc><table><row><cell>X-ray Device</cell><cell>Samsung GC70</cell><cell>Time Period of Collection</cell><cell>2010-2020</cell></row><row><cell>X-ray images and total fractures</cell><cell>542, 569</cell><cell>Specialist physicians</cell><cell>1 radiologist, 2 orthopedists</cell></row><row><cell>Dataset (train, validation, test)</cell><cell>%80, %10, %10</cell><cell>Fracture types</cell><cell>radius and ulna</cell></row><row><cell>Training data and fractures</cell><cell>434, 459</cell><cell>Patients (female, male, total)</cell><cell>134, 141, 275</cell></row><row><cell>Validation data and fractures</cell><cell>54, 55</cell><cell>Average age, pediatrics, adult</cell><cell>45, 21, 254</cell></row><row><cell>Test data and fractures</cell><cell>54, 56</cell><cell>Patients with fractures in both wrists (train, validation, test, total)</cell><cell>10, 0, 1, 11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 .</head><label>4</label><figDesc>Evaluation metrics definition and calculations. /1285 4.4.2. Fracture Detection Results of 20 DL-Based Models</figDesc><table><row><cell>Evaluation Metrics</cell><cell>Definition and Calculations</cell></row><row><cell>Intersection over Union (IOU)</cell><cell>area (BBoxp ? Bboxg)/area (BBoxp U Bboxg)</cell></row><row><cell>True Positive (TP)</cell><cell>IUO ? 0.5</cell></row><row><cell>False Positive (FP)</cell><cell>IUO &lt; 0.5</cell></row><row><cell>False Negative (FN)</cell><cell>failing to detect Bboxg</cell></row><row><cell>True Negative (TN)</cell><cell>can't be used (infinite)</cell></row><row><cell>Precision (P)</cell><cell>TP/(TP + FP) = TP/all detections</cell></row><row><cell>Recall (R)</cell><cell>TP/(TP + FN) = TP/all ground truths</cell></row><row><cell>Average Precision (AP)</cell><cell>under area of P-R curve</cell></row><row><cell>Average Recall (AR)</cell><cell>twice the under area of R-IOU curve</cell></row><row><cell cols="2">Optimal Localization Recall Precision (oLRP) minimum achievable average matching error over the confidence scores</cell></row></table><note>Accepted at Sensors, MDPI, 2022, 22, 1285. Hardala?, F.; Uysal, F.; Peker, O.; ?i?eklida?, M.; Tolunay, T.; Tokg?z, N.; Kutbay, U.; Demirciler, B.; Mert, F. Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. for citation and final version please click here: https://www.mdpi.com/1424-8220/22/3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 .</head><label>5</label><figDesc>Training loss and epoch results for the highest validation AP50 in detection models.</figDesc><table><row><cell>Models</cell><cell cols="4">Without Augmentation TB_Loss T_Loss TT Best Epoch</cell><cell>TB_Loss</cell><cell cols="3">With Augmentation T_Loss TT Best Epoch</cell></row><row><cell>DCN Faster R-CNN</cell><cell>0.0938</cell><cell cols="2">0.1821 174</cell><cell>6</cell><cell>0.0969</cell><cell cols="2">0.1839 174</cell><cell>5</cell></row><row><cell>Dynamic R-CNN</cell><cell>0.2978</cell><cell cols="2">0.5033 139</cell><cell>8</cell><cell>0.2664</cell><cell cols="2">0.4605 156</cell><cell>10</cell></row><row><cell>Faster R-CNN</cell><cell>0.0873</cell><cell cols="2">0.1573 126</cell><cell>10</cell><cell>0.0864</cell><cell cols="2">0.1634 137</cell><cell>12</cell></row><row><cell>FSAF</cell><cell>0.3142</cell><cell cols="2">0.5419 130</cell><cell>7</cell><cell>0.2605</cell><cell cols="2">0.4718 128</cell><cell>8</cell></row><row><cell>RetinaNet</cell><cell>0.3103</cell><cell cols="2">0.4987 120</cell><cell>16</cell><cell>0.348</cell><cell>0.576</cell><cell>120</cell><cell>8</cell></row><row><cell>Libra RetinaNet</cell><cell>0.5033</cell><cell cols="2">0.7278 131</cell><cell>8</cell><cell>0.5374</cell><cell cols="2">0.7813 130</cell><cell>6</cell></row><row><cell>PAA</cell><cell>0.3256</cell><cell cols="2">0.8387 127</cell><cell>6</cell><cell>0.33</cell><cell>0.834</cell><cell>104</cell><cell>7</cell></row><row><cell>RegNet RetinaNet</cell><cell>0.315</cell><cell>0.543</cell><cell>256</cell><cell>8</cell><cell>0.313</cell><cell>0.539</cell><cell>225</cell><cell>7</cell></row><row><cell>SABL Faster R-CNN</cell><cell>0.0536</cell><cell cols="2">0.2244 243</cell><cell>8</cell><cell>0.0501</cell><cell cols="2">0.2140 212</cell><cell>12</cell></row><row><cell>SABL RetinaNet</cell><cell>0.1034</cell><cell cols="2">0.3817 132</cell><cell>21</cell><cell>0.1857</cell><cell cols="2">0.6559 123</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 .Models Without Augmentation With Augmentation LRPt oLRPL oLRPFP oLRPFN oLRP LRPt oLRPL oLRPFP oLRPFN oLRP DCN</head><label>6</label><figDesc>Validation AP50 and AR results of detection models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Models</cell><cell></cell><cell cols="3">Without Augmentation AP50 AR</cell><cell cols="3">With Augmentation AP50 AR</cell></row><row><cell></cell><cell></cell><cell cols="3">DCN Faster R-CNN</cell><cell>0.599</cell><cell></cell><cell>0.365</cell><cell>0.619</cell><cell></cell><cell>0.393</cell></row><row><cell></cell><cell></cell><cell cols="2">Dynamic R-CNN</cell><cell></cell><cell>0.77</cell><cell></cell><cell>0.416</cell><cell>0.777</cell><cell></cell><cell>0.375</cell></row><row><cell></cell><cell></cell><cell cols="2">Faster R-CNN</cell><cell></cell><cell>0.61</cell><cell></cell><cell>0.362</cell><cell>0.598</cell><cell></cell><cell>0.404</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FSAF</cell><cell></cell><cell>0.579</cell><cell></cell><cell>0.415</cell><cell>0.684</cell><cell></cell><cell>0.46</cell></row><row><cell></cell><cell></cell><cell cols="2">RetinaNet</cell><cell></cell><cell>0.634</cell><cell></cell><cell>0.436</cell><cell>0.668</cell><cell></cell><cell>0.44</cell></row><row><cell></cell><cell></cell><cell cols="2">Libra RetinaNet</cell><cell></cell><cell>0.691</cell><cell></cell><cell>0.456</cell><cell>0.679</cell><cell></cell><cell>0.496</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PAA</cell><cell></cell><cell>0.617</cell><cell></cell><cell>0.498</cell><cell>0.629</cell><cell></cell><cell>0.547</cell></row><row><cell></cell><cell></cell><cell cols="2">RegNet RetinaNet</cell><cell></cell><cell>0.609</cell><cell></cell><cell>0.458</cell><cell>0.685</cell><cell></cell><cell>0.471</cell></row><row><cell></cell><cell></cell><cell cols="3">SABL Faster R-CNN</cell><cell>0.632</cell><cell></cell><cell>0.429</cell><cell>0.658</cell><cell></cell><cell>0.386</cell></row><row><cell></cell><cell></cell><cell cols="2">SABL RetinaNet</cell><cell></cell><cell>0.67</cell><cell></cell><cell>0.436</cell><cell>0.67</cell><cell></cell><cell>0.456</cell></row><row><cell></cell><cell cols="10">Table 7. Validation LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN results of</cell></row><row><cell></cell><cell cols="3">detection models.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN</cell><cell>0.681</cell><cell>0.309</cell><cell>0.216</cell><cell>0.473</cell><cell>0.824</cell><cell>0.506</cell><cell>0.318</cell><cell>0.261</cell><cell>0.382</cell><cell>0.815</cell></row><row><cell>Dynamic R-CNN</cell><cell>0.88</cell><cell>0.321</cell><cell>0.2</cell><cell>0.345</cell><cell>0.798</cell><cell>0.775</cell><cell>0.318</cell><cell>0.208</cell><cell>0.236</cell><cell>0.769</cell></row><row><cell>Faster R-CNN</cell><cell>0.553</cell><cell>0.329</cell><cell>0.404</cell><cell>0.382</cell><cell>0.851</cell><cell>0.641</cell><cell>0.311</cell><cell>0.319</cell><cell>0.418</cell><cell>0.827</cell></row><row><cell>FSAF</cell><cell>0.322</cell><cell>0.304</cell><cell>0.397</cell><cell>0.364</cell><cell>0.824</cell><cell>0.404</cell><cell>0.3</cell><cell>0.271</cell><cell>0.364</cell><cell>0.794</cell></row><row><cell>RetinaNet</cell><cell>0.441</cell><cell>0.317</cell><cell>0.2</cell><cell>0.418</cell><cell>0.814</cell><cell>0.439</cell><cell>0.304</cell><cell>0.25</cell><cell>0.4</cell><cell>0.804</cell></row><row><cell>Libra RetinaNet</cell><cell>0.465</cell><cell>0.295</cell><cell>0.159</cell><cell>0.327</cell><cell>0.756</cell><cell>0.437</cell><cell>0.273</cell><cell>0.234</cell><cell>0.345</cell><cell>0.752</cell></row><row><cell>PAA</cell><cell>0.503</cell><cell>0.31</cell><cell>0.443</cell><cell>0.382</cell><cell>0.842</cell><cell>0.552</cell><cell>0.28</cell><cell>0.314</cell><cell>0.364</cell><cell>0.783</cell></row><row><cell>RegNet RetinaNet</cell><cell>0.421</cell><cell>0.294</cell><cell>0.292</cell><cell>0.382</cell><cell>0.797</cell><cell>0.499</cell><cell>0.298</cell><cell>0.205</cell><cell>0.364</cell><cell>0.779</cell></row><row><cell>SABL Faster R-CNN</cell><cell>0.46</cell><cell>0.331</cell><cell>0.4</cell><cell>0.345</cell><cell>0.846</cell><cell>0.436</cell><cell>0.345</cell><cell>0.403</cell><cell>0.339</cell><cell>0.859</cell></row><row><cell>SABL RetinaNet</cell><cell>0.591</cell><cell>0.19</cell><cell>0.382</cell><cell>0.382</cell><cell>0.763</cell><cell>0.534</cell><cell>0.28</cell><cell>0.238</cell><cell>0.418</cell><cell>0.783</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 .</head><label>8</label><figDesc>Test AP50 and AR results of detection models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Models</cell><cell></cell><cell cols="3">Without Augmentation AP50 AR</cell><cell cols="3">With Augmentation AP50 AR</cell></row><row><cell></cell><cell></cell><cell cols="3">DCN Faster R-CNN</cell><cell>0.547</cell><cell></cell><cell>0.391</cell><cell>0.577</cell><cell></cell><cell>0.323</cell></row><row><cell></cell><cell></cell><cell cols="2">Dynamic R-CNN</cell><cell></cell><cell>0.63</cell><cell></cell><cell>0.341</cell><cell>0.654</cell><cell></cell><cell>0.323</cell></row><row><cell></cell><cell></cell><cell cols="2">Faster R-CNN</cell><cell></cell><cell>0.617</cell><cell></cell><cell>0.343</cell><cell>0.624</cell><cell></cell><cell>0.395</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FSAF</cell><cell></cell><cell>0.739</cell><cell></cell><cell>0.398</cell><cell>0.746</cell><cell></cell><cell>0.412</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell></cell><cell>0.621</cell><cell></cell><cell>0.377</cell><cell>0.652</cell><cell></cell><cell>0.338</cell></row><row><cell></cell><cell></cell><cell cols="2">Libra RetinaNet</cell><cell></cell><cell>0.695</cell><cell></cell><cell>0.432</cell><cell>0.715</cell><cell></cell><cell>0.445</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PAA</cell><cell></cell><cell>0.666</cell><cell></cell><cell>0.491</cell><cell>0.754</cell><cell></cell><cell>0.496</cell></row><row><cell></cell><cell></cell><cell cols="3">RegNet RetinaNet</cell><cell>0.713</cell><cell></cell><cell>0.452</cell><cell>0.74</cell><cell></cell><cell>0.468</cell></row><row><cell></cell><cell></cell><cell cols="3">SABL Faster R-CNN</cell><cell>0.622</cell><cell></cell><cell>0.427</cell><cell>0.595</cell><cell></cell><cell>0.402</cell></row><row><cell></cell><cell></cell><cell cols="2">SABL RetinaNet</cell><cell></cell><cell>0.613</cell><cell></cell><cell>0.386</cell><cell>0.65</cell><cell></cell><cell>0.404</cell></row><row><cell></cell><cell cols="10">Table 9. Test LRP-optimal threshold (LRPt), oLRPLoc (oLRPL), oLRPFP and oLRPFN results of</cell></row><row><cell></cell><cell cols="3">detection models.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="5">Without Augmentation LRPt oLRPL oLRPFP oLRPFN oLRP</cell><cell>LRPt</cell><cell cols="4">With Augmentation oLRPL oLRPFP oLRPFN oLRP</cell></row><row><cell>DCN Faster R-CNN</cell><cell>0.69</cell><cell>0.343</cell><cell>0.306</cell><cell>0.554</cell><cell>0.883</cell><cell>0.47</cell><cell>0.335</cell><cell>0.364</cell><cell>0.375</cell><cell>0.848</cell></row><row><cell>Dynamic R-CNN</cell><cell>0.825</cell><cell>0.349</cell><cell>0.302</cell><cell>0.339</cell><cell>0.845</cell><cell>0.784</cell><cell>0.352</cell><cell>0.362</cell><cell>0.339</cell><cell>0.858</cell></row><row><cell>Faster R-CNN</cell><cell>0.644</cell><cell>0.339</cell><cell>0.308</cell><cell>0.357</cell><cell>0.839</cell><cell>0.54</cell><cell>0.35</cell><cell>0.419</cell><cell>0.357</cell><cell>0.869</cell></row><row><cell>FSAF</cell><cell>0.354</cell><cell>0.338</cell><cell>0.236</cell><cell>0.25</cell><cell>0.803</cell><cell>0.371</cell><cell>0.322</cell><cell>0.241</cell><cell>0.214</cell><cell>0.777</cell></row><row><cell>RetinaNet</cell><cell>0.343</cell><cell>0.357</cell><cell>0.359</cell><cell>0.268</cell><cell>0.851</cell><cell>0.376</cell><cell>0.376</cell><cell>0.286</cell><cell>0.286</cell><cell>0.862</cell></row><row><cell>Libra RetinaNet</cell><cell>0.6</cell><cell>0.321</cell><cell>0.094</cell><cell>0.482</cell><cell>0.824</cell><cell>0.443</cell><cell>0.333</cell><cell>0.196</cell><cell>0.339</cell><cell>0.81</cell></row><row><cell>PAA</cell><cell>0.516</cell><cell>0.309</cell><cell>0.296</cell><cell>0.321</cell><cell>0.798</cell><cell>0.56</cell><cell>0.310</cell><cell>0.184</cell><cell>0.286</cell><cell>0.766</cell></row><row><cell>RegNet RetinaNet</cell><cell>0.466</cell><cell>0.311</cell><cell>0.235</cell><cell>0.304</cell><cell>0.783</cell><cell>0.552</cell><cell>0.317</cell><cell>0.163</cell><cell>0.357</cell><cell>0.791</cell></row><row><cell cols="2">SABL Faster R-CNN 0.378</cell><cell>0.303</cell><cell>0.494</cell><cell>0.286</cell><cell>0.834</cell><cell>0.592</cell><cell>0.292</cell><cell>0.27</cell><cell>0.509</cell><cell>0.827</cell></row><row><cell>SABL RetinaNet</cell><cell>0.561</cell><cell>0.309</cell><cell>0.32</cell><cell>0.393</cell><cell>0.819</cell><cell>0.514</cell><cell>0.31</cell><cell>0.233</cell><cell>0.411</cell><cell>0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Accepted at Sensors, MDPI, 2022, 22, 1285. Hardala?, F.; Uysal, F.; Peker, O.; ?i?eklida?, M.; Tolunay, T.; Tokg?z, N.; Kutbay, U.; Demirciler, B.; Mert, F. Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. for citation and final version please click here: https://www.mdpi.com/1424-8220/22/3/1285 4.4.3. Fracture Detection Results of Proposed Models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 .</head><label>11</label><figDesc>Comparison with various amounts of wrist test datasets.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell>Dataset</cell><cell>Amount</cell><cell>AP50</cell><cell>AR</cell><cell>LRPt</cell><cell>oLRPL</cell></row><row><cell>PAA</cell><cell>800 ? 800 ? 3</cell><cell>Gazi</cell><cell>54 test</cell><cell>0.754</cell><cell>0.496</cell><cell>0.56</cell><cell>0.310</cell></row><row><cell>YOLOv3</cell><cell>750 ? 750 ? 3</cell><cell>Gazi</cell><cell>54 test</cell><cell>0.531</cell><cell>0.298</cell><cell>0.164</cell><cell>0.378</cell></row><row><cell>Proposed WFD-C</cell><cell>800 ? 800 ? 3</cell><cell>Gazi</cell><cell>54 test</cell><cell>0.8639</cell><cell>0.33</cell><cell>0.357</cell><cell>0.349</cell></row><row><cell>PAA</cell><cell>800 ? 800 ? 3</cell><cell>Gazi</cell><cell>54 test + 54 valid</cell><cell>0.629</cell><cell>0.499</cell><cell>0.552</cell><cell>0.304</cell></row><row><cell>YOLOv3</cell><cell>750 ? 750 ? 3</cell><cell>Gazi</cell><cell>54 test + 54 valid</cell><cell>0.516</cell><cell>0.286</cell><cell>0.164</cell><cell>0.364</cell></row><row><cell>Proposed WFD-C</cell><cell>800 ? 800 ? 3</cell><cell>Gazi</cell><cell>54 test + 54 valid</cell><cell>0.709</cell><cell>0.344</cell><cell>0.454</cell><cell>0.315</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Accepted at Sensors, MDPI, 2022, 22, 1285. Hardala?, F.; Uysal, F.; Peker, O.; ?i?eklida?, M.; Tolunay, T.; Tokg?z, N.; Kutbay, U.; Demirciler, B.; Mert, F. Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. for citation and final version please click here: https://www.mdpi.com/1424-8220/22/3/1285</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This research received no external funding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Institutional Review Board Statement:</head><p>The study was conducted according to the guidelines of the Declaration of Helsinki, and approved by the Ethics Committee of Gazi University (27.04.2020-E.51150).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Informed Consent Statement: Not applicable.</head><p>Data Availability Statement: Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">References 1. Fractures, Health, Hopkins Medicine</title>
		<ptr target="https://www.hopkinsmedicine.org/health/conditions-and-diseases/fractures" />
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="https://www.healthline.com/health/wrist-bones" />
		<title level="m">Important Joints: Hand and Wrist Bones, Healthline. Available online</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Thigh fracture detection using deep learning method based on new dilated convolutional feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="521" to="526" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arm fracture detection in X-rays based on improved deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">106530</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple backbone network for detection tasks on thigh bone fracture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parallelnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1091" to="1100" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bone fracture detection through the two-stage system of Crack-Sensitive Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Med. Unlocked</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100452</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Feature Ambiguity Mitigate Operator model helps improve bone fracture detection on X-ray radiograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="page">1589</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ground Truth Annotated Femoral X-Ray Image Dataset and Object Detection Based Method for Fracture Types Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="189436" to="189444" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Automated Fracture Detection and Localization on Wrist Radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Thian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jagmohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detection of Spinal Fracture Lesions based on Improved Yolov2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)</title>
		<meeting>the IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)<address><addrLine>Dalian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detection of Spinal Fracture Lesions Based on Improved Faster-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS)</title>
		<meeting>the IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS)<address><addrLine>Dalian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-03" />
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep-learning-assisted detection and segmentation of rib fractures from CT scans: Development and validation of FracNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">103106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detection and localization of hand fractures based on GA_Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chaikovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alex. Eng. J</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4555" to="4562" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification of Shoulder X-ray Images with Deep Learning Ensemble Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">2723</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated system for the detection of thoracolumbar fractures using a CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gudigar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="184" to="189" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Femoral neck fracture detection in X-ray images using deep learning and genetic algorithm approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>A??c?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?mer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jt. Dis. Relat. Surg</title>
		<imprint>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AI-based detection and classification of distal radius fractures using low-effort data labeling: Evaluation of applicability and effect of training set size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cyriac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sexauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paciolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stieltjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hirschmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6816" to="6824" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Artificial intelligence in fracture detection: Transfer learning from deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Radiol</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="439" to="445" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Application of deep learning algorithm to detect and visualize vertebral fractures on plain frontal radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">252454</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical fracture classification of proximal femur X-Ray images using a multistage Deep Learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vezzetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aprato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Audisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mass?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">109373</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardala?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?i?eklida?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kutbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mert</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/22/3/1285" />
		<imprint>
			<date type="published" when="1285" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Learning-Based Detection and Correction of Cardiac MR Motion Artefacts During Reconstruction for High-Quality Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ruijsink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="4001" to="4010" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedrycz, W. Fuzzy C-means clustering through SSIM and patch for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">105928</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Common Objects in Context. arXiv 2015</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Designing Network Design Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature Selective Anchor-Free Module for Single-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards High Quality Object Detection via Dynamic Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynamic R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards Balanced Learning for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Libra R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faster R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Side-Aware Boundary Localization for More Precise Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deformable ConvNets V2: More Deformable, Better Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weighted boxes fusion: Ensembling boxes from different object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gabruseva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Labelimg</surname></persName>
		</author>
		<ptr target="https://github.com/tzutalin/labelImg" />
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Albumentations: Fast and Flexible Image Augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3130188</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3130188" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">IEEE</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L B</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A B</forename><surname>Da Silva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">279</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155v1</idno>
		<title level="m">Open MMLab Detection Toolbox and Benchmark. arXiv 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Localization Recall Precision (LRP): A New Performance Metric for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolov3</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<ptr target="https://fatihuysal88.github.io" />
	</analytic>
	<monogr>
		<title level="j">An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Code is</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
