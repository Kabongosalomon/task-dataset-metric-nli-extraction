<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DATA SPLITS AND METRICS FOR METHOD BENCHMARKING ON SURGICAL ACTION TRIPLET DATASETS A BENCHMARK STUDY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-12">April 12, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<email>nwoye@unistra.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<email>npadoy@unistra.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ICube Laboratory University of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DATA SPLITS AND METRICS FOR METHOD BENCHMARKING ON SURGICAL ACTION TRIPLET DATASETS A BENCHMARK STUDY</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-12">April 12, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Surgical activity recognition ? Tool-tissue interaction ? Action triplet ? CholecT40 ? CholecT45 ? CholecT50</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In addition to generating data and annotations, devising sensible data splitting strategies and evaluation metrics is essential for the creation of a benchmark dataset. This practice ensures consensus on the usage of the data, homogeneous assessment, and uniform comparison of research methods on the dataset. This study focuses on CholecT50, which is a 50 video surgical dataset that formalizes surgical activities as triplets of ?instrument, verb, target?. In this paper, we introduce the standard splits for the CholecT50 and CholecT45 datasets and show how they compare with existing use of the dataset. CholecT45 is the first public release of 45 videos of CholecT50 dataset. We also develop a metrics library, ivtmetrics, for model evaluation on surgical triplets. Furthermore, we conduct a benchmark study by reproducing baseline methods in the most predominantly used deep learning frameworks (PyTorch and TensorFlow) to evaluate them using the proposed data splits and metrics and release them publicly to support future research. The proposed data splits and evaluation metrics will enable global tracking of research progress on the dataset and facilitate optimal model selection for further deployment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An illustration of the dataset splits. Top: CholecT50 split as used in the Rendezvous <ref type="bibr" target="#b0">[1]</ref>. Middle: CholecT50 split as used in the CholecTriplet challenge <ref type="bibr" target="#b1">[2]</ref>. Bottom: the official CholecT45 and CholecT50 cross-validation splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of Artificial Intelligence (AI) techniques is increasingly driving research and development across many disciplines. Yet, there has been a delay in introducing large-scale data science to interventional medicine, partly due to the unavailability of large annotated datasets <ref type="bibr" target="#b2">[3]</ref>. While huge efforts have been made in creating small to medium scale datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, little or no effort has been made to standardize the data usage for tracking the global research progress. For instance, in laparoscopic and cataract surgeries, many published methods on the most prominent tasks of surgical phase <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and tool detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> are reported on varying data splits of the same dataset, e.g: Cholec80 <ref type="bibr" target="#b4">[5]</ref>, Cataract <ref type="bibr" target="#b6">[7]</ref>, etc. Without a consensus data split, tracking research progress on these experimental datasets is not straightforward. Oftentimes, it complicates results comparison, making model selection for further clinical translation more challenging.</p><p>In this paper, we present standard data splits for the recently introduced CholecT50 dataset <ref type="bibr" target="#b0">[1]</ref>. The label formalism in the dataset provides comprehensive and fine-grained details on every tool-tissue interaction in any given surgical scene. A subset of the dataset, named CholecT45, was released after the CholecTriplet2021 challenge <ref type="bibr" target="#b1">[2]</ref> while withholding 5 test set videos from public access. The remaining part of the dataset is planned to be released after the CholecTriplet2022 challenge.</p><p>The data split patterns, illustrated in <ref type="figure">Fig. 1</ref>, are fashioned on three criteria:</p><p>1. Reproducibility: to maintain consistent splits with the earlier published experiments that introduced the dataset, 2. Accessibility: owing that only a part of the dataset is publicly released to date, we consider a representative setup for its utilization and fair comparison, <ref type="bibr" target="#b2">3</ref>. Thoroughness: to counter the effect of class-imbalance predominant in a single test set by using a rigorous and exhaustive k-fold cross-validation approach, which enables alternating evaluation on the entire dataset.</p><p>Furthermore, we define and standardize the evaluation metrics for assessing the quality of triplet detection and recognition on the dataset. These metrics build on the evaluation setup used in earlier research <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> on the dataset. In this work, we describe the evaluation algorithm and develop a standard metrics library, named ivtmetrics, for both triplet recognition and detection/localization evaluation. The metrics library is available online and can be installed via pip or conda package installers for method development and validation. The metrics library is usable in all python-based deep learning frameworks.</p><p>Finally, we re-implement our previously proposed deep learning methods for surgical action triplet recognition in two widely used deep learning frameworks: PyTorch and TensorFlow. The reproduced models are evaluated on the proposed data splits for CholecT45 and CholecT50 using the developed ivtmetrics library thus providing baselines for future comparison. By conducting this benchmark experiment on the newly introduced dataset, this study provides a definition of standard practice for the official data splits and an evaluation protocol to guide future research.</p><p>The CholecT45 dataset is released on http://camma.u-strasbg.fr/datasets. The evaluation metrics library is installable using python package and environment managers (pip and conda). The code and weights for the reproduced models are available on the CAMMA public GitHub https://github.com/CAMMA-public.</p><p>In the following sections, we present the proposed data splits and their constituting videos, followed by the evaluation protocols and the developed metrics library. Afterwards, we present the reproduced models, their performance across the proposed splits, and comprehensive per-category performance analysis on the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Splits</head><p>CholecT50 <ref type="bibr" target="#b0">[1]</ref> is a surgical dataset for action triplet recognition. It is an extension of CholecT40 dataset <ref type="bibr" target="#b8">[9]</ref> with additional 10 videos. It contains 50 videos of laparoscopic cholecystectomy surgery annotated with 100 action triplet classes. Action triplet is a formalism to represent fine-grained activity in the form of ?instrument, verb, target?. In this dataset, they are composed from 6 instruments, 10 verbs, and 15 target classes resulting in over 161K triplet instances at 1 fps video frames.</p><p>In the CholecTriplet2021 [2] / CholecTriplet2022 challenges, the participants are given access to a subset of the CholecT50 dataset, also known as CholecT45. This subset is the first public release of the CholecT50 dataset available on http://camma.u-strasbg.fr/datasets. The videos of CholecT45 are part of the Cholec80 <ref type="bibr" target="#b4">[5]</ref> dataset. The video indexes correspond between the two datasets with the prefix "video" in Cholec80 changed to "VID" in CholecT45. The statistics of the CholecT45 dataset is presented in <ref type="table" target="#tab_1">Table 1</ref>   </p><formula xml:id="formula_0">VID01 VID15 VID26 VID40 VID52 VID65 VID79 VID08 VID06 VID51 VID02 VID18 VID27 VID43 VID56 VID66 VID92 VID12 VID10 VID73 VID04 VID22 VID31 VID47 VID57 VID68 VID96 VID29 VID14 VID74 VID05 VID23 VID35 VID48 VID60 VID70 VID103 VID50 VID32 VID80 VID13 VID25 VID36 VID49 VID62 VID75 VID110 VID78 VID42 VID111</formula><p>To describe the official usage of CholecT45 and CholecT50 datasets, we present the different splits of the datasets in the following sections. We first present the data split of CholecT50 as used (a) in the Rendezvous paper <ref type="bibr" target="#b0">[1]</ref> that introduces the dataset and (b) in the CholecTriplet challenges <ref type="bibr" target="#b1">[2]</ref> for reproducibility. Afterward, we present the official cross-validation splits of the CholecT45 and CholecT50 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rendezvous (RDV) Split of CholecT50 dataset</head><p>This split is used in the original paper <ref type="bibr" target="#b0">[1]</ref> that introduces the dataset. It is presented for reproducibility of the earlier published methods on this task. In this setup, the dataset is split into three: (1) training, (2) validation, and (3) testing as presented in <ref type="table" target="#tab_2">Table 2</ref>. The videos in each dataset split are distributed in the same ratio to include annotations from each of the (surgeon) annotators. This helps to minimize the effect of annotation bias on the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CholecTriplet Split of CholecT50 dataset</head><p>This split is introduced by the organizers of the CholecTriplet2021 and CholecTriplet2022 challenges for surgical action triplet recognition and detection. Here, it is selected for consistency with the methods presented at the MICCAI 2021 EndoVis challenge. The dataset is split into two: (1) trainval, and (2) testing set, as presented in <ref type="table" target="#tab_3">Table 3</ref>. During the challenge and for model hyper-parameter tuning, participants are allowed to further split the trainval split into training and validation subsets at their own discretion. All the videos in the trainval are drawn from the publicly available Cholec80 <ref type="bibr" target="#b4">[5]</ref> dataset. Nevertheless, the testing set containing 5 videos are not in the public domain. The rationale for this data split is to ensure that the participants do not have access to the testing set of the challenge dataset for fairness in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Official Cross-Validation (CV) Splits for CholecT45 and CholecT50 datasets</head><p>K-fold cross-validation is known for its robustness in result analysis. As some of the triplet classes can be unrepresented in any testing set sampling, cross-validation is a more robust, and stable way of assessing the quality of model predictions on all observed triplet classes. This enables a result analysis that covers all the 100 class labels of the triplet datasets. In this setup, the dataset is split into 5 equal subsets called folds. Different copies of a model are trained on different combinations of 4 out of 5 folds, each time leaving out one alternating fold for testing. The final result is averaged over the 5 hold-out testing splits.</p><p>To ensure that all folds have similar levels of complexity, the 50 videos of CholecT50 are sorted by their difficulty, determined by procedure duration. The sorted videos are divided into 10 clusters with each containing 5 videos of the same/similar complexity or duration. The videos in each cluster are randomly distributed to all the 5 folds split.</p><p>The CholecT50 CV split contains the full 50 video dataset divided into 5 folds with each fold containing 10 videos as presented in <ref type="table" target="#tab_4">Table 4</ref>) (rows 1-10).</p><p>The CholecT45 CV split, on the other hand, contains 45 videos of the dataset divided into 5 folds with each fold containing 9 videos each as shown in <ref type="table" target="#tab_4">Table 4</ref>) (rows 1-9). The CholecT45 excludes the test videos (row 10) of the CholecTriplet challenge. Hence, this split equally supports exhaustive cross-validation but only on the publicly released subset of the entire dataset.  <ref type="table" target="#tab_1">VID01  VID10  VID22  VID29  VID42  VID50  VID60  VID73  VID05  VID92  VID02  VID12  VID23  VID31  VID43  VID51  VID62  VID75  VID18  VID96  VID04  VID13  VID25  VID32  VID47  VID52  VID66  VID78  VID36  VID103  VID06  VID14  VID26  VID35  VID48  VID56  VID68  VID79  VID65  VID110  VID08  VID15  VID27  VID40  VID49  VID57  VID70  VID80</ref> VID74 VID111 </p><formula xml:id="formula_1">VID79 VID80 VID31 VID42 VID78 2 VID02 VID32 VID57 VID29 VID43 3 VID51 VID05 VID36 VID60 VID62 4 VID06 VID15 VID18 VID27 VID35 5 VID25 VID40 VID52 VID65 VID74 6 VID14 VID47 VID68 VID75 VID01 7 VID66 VID26 VID10 VID22 VID56 8 VID23 VID48 VID08 VID49 VID04 9 VID50 VID70 VID73 VID12 VID13 10 VID111 VID96 VID103 VID110 VID92</formula><p>3 Metrics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recognition Average Precision</head><p>Triplet recognition performance is evaluated using the Average Precision (AP) metric measured as the area under the precision-recall (p-r) curve per class:</p><formula xml:id="formula_2">AP = 1 0 p(r)dr.<label>(1)</label></formula><p>AP summarizes a p-r curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight. Video-specific AP score for evaluating surgical action triplet recognition is computed as follows:</p><p>i. per-category AP is computed across all frames in a given video.</p><p>ii. category AP is obtained by averaging per-category APs across all videos.</p><p>iii. mean AP is obtained by averaging N category AP, serving as the final score:</p><formula xml:id="formula_3">mAP = 1 N N i=1 AP i .<label>(2)</label></formula><p>The same process is followed when computing mAP for the individual components of the triplets. The predictive capacity of a model at recognizing correctly a triplet and its components is evaluated in two ways:</p><p>1. Component average precision: This includes three APs assessing the correct recognition of the instrument (AP I ), verb (AP V ), and target (AP T ) components of the triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Triplet average precision:</head><p>This includes three APs assessing the correct recognition of tool-tissue interactions by observing different sets of triplet components, which includes: APs for instrument-verb (AP IV ), instrumenttarget (AP IT ), and instrument-verb-target (AP IV T ), which is the main metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Disentangling Action Triplet Prediction</head><p>This is introduced to support the evaluation of all triplet component prediction for models that produce only the final triplet (Y IV T ) probabilities. If IV T represents all the triplets in a single image, and D = {I, V, T, IV, IT } is a set of the triplets' components and their possible combinations, then, ?d ? D, the indirect component's output vector Y d of class size C d (e.g. for d = I, C d = 6 as there are 6 instrument classes) can be filtered from Y IV T following Equation 3:</p><formula xml:id="formula_4">Y d = max d k ?{i,v,t} Y ivt ? ivt ? IV T, k : 0 ? k &lt; C d .<label>(3)</label></formula><p>This filtering algorithm <ref type="bibr" target="#b0">[1]</ref> directly translates to obtaining the probability of a given component class as the maximum probability value among all triplet labels having the same component class label in a video frame. Likewise, the groundtruths of the component labels can be obtained using the filtering setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detection Average Precision</head><p>When evaluating the localization of the triplets as in CholecTriplet2022 challenge, AP metrics consider the overlap or Intersection of Union (IoU) of the predicted bounding boxes with the ground truth. The detection AP is evaluated in two ways:</p><p>1. Instrument Localization AP: In this metric, a detection is assigned a true positive (TP) if the degree of overlap between a predicted bounding box and the ground truth exceeds a certain threshold (usually 0.5) and the instrument ID is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Triplet Detection AP:</head><p>This metric assess the correctness of every associated action triplet to every localized instrument. Here, a prediction is considered a TP if the predicted triplet ID is correct, assigned to the right instrument involved in the tool-tissue interaction, which must also be localized at a minimum IoU threshold with the ground-truth bounding box.</p><p>The missed predictions are marked as false negatives (FN) whereas false alarms are marked as false positives (FP). Following this, their corresponding precision (p) and recalls (r) are calculated as follows:</p><formula xml:id="formula_5">p = T P T P + F P , r = T P T P + F N ,<label>(4)</label></formula><p>and using the computed p, r, the AP is calculated following Equation 1, averaged across videos.</p><p>In future, when target localization AP is considered, the triplet detection AP will take into account a satisfied bounding box IoU for both instruments and targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Metrics Library</head><p>To standardize the use of these evaluation metrics, we develop ivtmetrics library, which can be used in both training and inference mode. The library can be imported in a python-based script using import ivtmetrics with a prerequisite installation step: pip install ivtmetrics or conda install -c nwoye ivtmetrics. gives the mean AP across all frames in all seen videos. The component ? ("i", "v", "t", "iv", "it", "ivt") is a string argument that describes the respective sub-task's (instrument, verb, target, instrument-verb, instrument-target, instrument-verb-target) performance to be computed for. A top K performance is obtained by AP.topk(k: int) while top predicted class IDs are given by AP.topClass(k: int). More details and usage examples of the ivtmetrics can be found on GitHub https://github.com/CAMMA-public/ivtmetrics .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmark Study Design</head><p>To provide a benchmark study on CholecT45 and CholecT50 datasets using our proposed data splits and metrics, we re-implement and reproduce three proposed methods in PyTorch and TensorFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>We summarize the reproduced methods as follows:</p><p>1. Tripnet: As shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, Tripnet [9] is a multi-task learning method that uses activation maps resulting from the instrument branch to enhance verb and target feature encoding in a new module known as class activation guide (CAG). It is followed by a 3D interaction space where relationships between instruments-verbs-targets components are resolved to triplets. Code and weights are publicly released on GitHub https://github.com/CAMMA-public/tripnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attention Tripnet:</head><p>This is an upgrade of Tripnet with an attention mechanism. The main difference is the use of class activation guided attention mechanism (CAGAM) <ref type="bibr" target="#b0">[1]</ref> over CAG where the verb and target feature discovery are obtained by channel and position attention processes respectively. Code and weights are publicly released on GitHub https://github.com/CAMMA-public/attention-tripnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rendezvous (RDV):</head><p>In this model <ref type="bibr" target="#b0">[1]</ref>, the network encoder uses a weakly supervised approach to localize the instruments and the CAGAM module to detect the verb and target components of the triplets as shown in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>. The association part is achieved by both self and cross attention mechanisms in a new module known as multi-head of mixed attention (MHMA), and terminated by a simple classifier after 8 successive layers of association decoding. Code and weights are publicly released on GitHub https://github.com/CAMMA-public/rendezvous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We made few changes in the original implementations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> as follows:</p><p>1. Output resolution: The original implementation lowered the strides of the last two blocks of the ResNet by one pixel to provide higher resolution (32 ? 56) output. However, using original strides of size 2, we implement a faster version (size: <ref type="figure">8 ? 14)</ref>, trading-off precise localization to speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attention normalization:</head><p>The huge parameter layer-norms in the AddNorm layers of the attention module are replaced with batch-norms without affecting model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Loss function:</head><p>We integrates warmup parameters within the auxilliary task' cross-entropies without requiring additional uncertainty loss balancing <ref type="bibr" target="#b18">[19]</ref> as in <ref type="bibr" target="#b0">[1]</ref>. This removes the excessive parameters introduced by the uncertainty loss. All the 100 triplet classes are evaluated in this setup. The benchmark results on the RDV split is presented in <ref type="table" target="#tab_5">Table  5</ref>. The results follow the same trend as in the original paper <ref type="bibr" target="#b0">[1]</ref> as it is observed that the Attention Tripnet leverages CAGAM to improve the verb and target detections while Rendezvous utilizes its transformer-inspired MHMA to improve the triplet association performance. We show the performance across deep learning frameworks in <ref type="table" target="#tab_5">Table 5</ref>. The PyTorch models approximates the performance of their TensorFlow counterparts (version 1). We observe that these results are comparable in some of the sub tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Benchmark Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Results on CholecT50 using CholecTriplet Split</head><p>The challenge rule excludes the 6 null triplet classes (IDs: 94-99) from evaluation. The results are presented in <ref type="table" target="#tab_6">Table  6</ref>. It is observed that AP IV T is higher than in the RDV split for each model, likely due to the reduced number of triplet classes (94 v 100). Also, the direct outputs (Y D ) for the individual components (i.e.:</p><formula xml:id="formula_6">Y I , Y V , Y T , Y IV or Y IT )</formula><p>of the triplets (Y IV T ) are not provided by the challenge approaches, instead they are filtered from the main triplet predictions (Y IV T ) following the filtering formula in Section 3.2. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the AP performances on the filtered prediction are generally lower compared to AP on directly predicted probabilities of those components when provided, however, they are more informative and a better representation of how a model understands the triplet's composition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Results on CholecT50 using Cross-Validation Split</head><p>The benchmarking results on the CholecT50 cross-validation split are presented in <ref type="table" target="#tab_7">Table 7</ref> along with the standard deviation (std) over the folds. All the 100 triplet classes are evaluated in this setup. This presents a less biased and less optimistic estimate of the models with confidence intervals. Their standard deviation (std) spread shows the extent of performances approximation of the three models positioning Tripnet as the least and Rendezvous as the best in terms of performance. Similarly, the benchmarking results on the CholecT45 cross-validation split, presented in <ref type="table" target="#tab_8">Table 8</ref>, justifies the use of attention mechanisms for surgical action triplet recognition. The analysis shows that the results obtained on the CholecT45 CV approximates the ones of the CholecT50 CV in all the sub-tasks, justifying its use/sufficiency in the absence of the complete CholecT50 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Class-wise Performances of the Benchmark Models</head><p>We present the per-class performance for the triplet components <ref type="table" target="#tab_1">(Tables 9 -11</ref>) and their association <ref type="table" target="#tab_1">(Table 12</ref>) recognition using the cross-validation dataset splitting strategy on both CholecT45 and CholecT50. We observe similar performance pattern across the two datasets in all classes of each sub task showing the reliability of cross-validation split approach in model evaluation.</p><p>On instrument presence detection, as shown in <ref type="table" target="#tab_9">Table 9</ref>, grasper and hook are the most detected owing to their highest occurrence frequencies in the dataset. However, hook is a little better detected than grasper owing to its uniqueness unlike the grasper which sometimes share some similarities with clipper, bipolar and scissor. The scissors and irrigator, on the other hand, are the least detected likely due to their low occurrence distributions in the datasets. For the verb recognition, grasp, retract, dissect, coagulate, clip, cut, and aspirate are better detect above 50% at all time as shown in <ref type="table" target="#tab_1">Table 10</ref>. This is due to their strong affinities with unique instrument classes. Pack and irrigate are very challenging to discriminate from the dominant verbs of their instruments namely retract and aspirate. Null action, being a compendium of unconsidered actions, is the least recognized verb.  A presentation of class-wise results of the complete 100 triplet classes is only possible using the cross-validation approach. As shown in <ref type="table" target="#tab_1">Table 12</ref>, the models recognizes the most important triplets such as grasper retracting gallbladder or grasping specimen-bag, hook dissecting either gallbladder or omentum, bipolar coagulating liver, clipper clipping cystic-artery and -duct with scissors cutting the same, and irrigator aspirating fluid or irrigating the cystic-duct. The good performance on these specific triplets are expected from the models' high performance on their specific triplet component classes. Surprising, the irrigator rare use in dissecting cystic-pedicle is well detected by RDV model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>With the public release of the CholecT45 dataset to support research on surgical action triplet recognition, we present in this paper a standard practice for splitting the dataset to enable a uniform comparison of research methods. These splits will remain relevant in the planned future release of the entire CholecT50 dataset. We also design, implement, and publicly release a python packaged metrics library, ivtmetrics for the evaluation of surgical action triplet recognition and localization on the dataset. For the benchmark study, we re-implement the current state-of-the-art models in two predominant deep learning frameworks, PyTorch and TensorFlow, and then train and evaluate them on the proposed data splits. Owing to the well-articulated rationale for dataset splits and exhaustive cross-validation, results obtained reflect better the generalization capability of the models. This study sets a rich foundation for fair comparison of methods researched on the CholecT45 and CholecT50 datasets using the same data splits and metrics. Future work will extend the metrics library to include more statistical evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The library provides metrics classes for triplet recognition: AP = ivtmetrics.Recognition(N : int) and triplet detection AP = ivtmetrics.Detection(N: int), as well as an internally implemented triplet component filtering AP = ivtmetrics.Disentangle(N: int), where N = number of triplet classes. Invoking the metrics class initializes the metrics accumulators by a AP.reset(). This reset function is to be called at the beginning of every training epoch. Other reset options include AP.reset_video() to reset scores accumulated for all seen videos, and AP.reset_global() to reset every accumulator. The metrics update function takes in the predicted and target labels over each iteration by calling AP.update(targets: array, predictions: array). If a video-specific AP is needed, AP.video_end() must be called at the end of each video. The AP scores from a current time up to the last reset() call are obtained via AP.compute_AP(component : str). The mean AP average across videos is obtained by calling AP.compute_video_AP(component : str) while AP.compute_global_AP(component : str)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Reproduced models for surgical action triplet recognition: (a) Tripnet<ref type="bibr" target="#b8">[9]</ref>, (b) Rendezvous<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of CholecT45 Dataset</figDesc><table><row><cell>Videos</cell><cell>Frames</cell><cell>Instances</cell><cell>Instruments</cell><cell>Verbs</cell><cell>Targets</cell><cell>Triplets</cell></row><row><cell>45</cell><cell>90489</cell><cell>127385</cell><cell>6</cell><cell>10</cell><cell>15</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: CholecT50 dataset split as used in Rendezvous publication [1].</cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell>Validation</cell><cell>Testing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CholecT50 dataset split as used in CholecTriplet2021 [2] &amp; CholecTriplet2022 challenges.</figDesc><table /><note>Trainval (= CholecT45) Testing</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Official cross-validation data splits of CholecT45 and CholecT50 datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Fold 1</cell><cell>Fold 2</cell><cell>Fold 3</cell><cell>Fold 4</cell><cell>Fold 5</cell></row><row><cell>1</cell><cell>CholecT50 Cross-Val. splits</cell><cell>CholecT45 Cross-Val.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Benchmark triplet recognition AP (%) on CholecT50 dataset for different frameworks using RDV split.</figDesc><table><row><cell cols="2">Framework Method</cell><cell cols="3">Component detection</cell><cell cols="3">Triplet association</cell></row><row><cell></cell><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IV T</cell></row><row><cell></cell><cell>Tripnet [9]</cell><cell>92.1</cell><cell>54.5</cell><cell>33.2</cell><cell>29.7</cell><cell>26.4</cell><cell>20.0</cell></row><row><cell>TensorFlow</cell><cell>Attention Tripnet [1]</cell><cell>92.0</cell><cell>60.2</cell><cell>38.5</cell><cell>31.1</cell><cell>29.8</cell><cell>23.4</cell></row><row><cell></cell><cell>Rendezvous (RDV) [1]</cell><cell>92.0</cell><cell>60.7</cell><cell>38.3</cell><cell>39.4</cell><cell>36.9</cell><cell>29.9</cell></row><row><cell></cell><cell>Tripnet [9]</cell><cell>88.7</cell><cell>59.2</cell><cell>39.3</cell><cell>31.9</cell><cell>27.9</cell><cell>21.6</cell></row><row><cell>PyTorch</cell><cell>Attention Tripnet [1]</cell><cell>87.9</cell><cell>59.7</cell><cell>40.6</cell><cell>34.2</cell><cell>29.0</cell><cell>23.2</cell></row><row><cell></cell><cell>Rendezvous [1]</cell><cell>89.1</cell><cell>62.3</cell><cell>43.8</cell><cell>40.0</cell><cell>35.8</cell><cell>29.5</cell></row></table><note>5.1 Quantitative Results on CholecT50 using Rendezvous Split</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Benchmark triplet recognition AP (%) on CholecT50 dataset using CholecTriplet split.</figDesc><table><row><cell cols="2">Framework Method</cell><cell cols="3">Component detection</cell><cell cols="3">Triplet association</cell></row><row><cell></cell><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IV T</cell></row><row><cell></cell><cell>Tripnet [9]</cell><cell>74.6</cell><cell>42.9</cell><cell>32.2</cell><cell>27.0</cell><cell>28.0</cell><cell>23.4</cell></row><row><cell>TensorFlow</cell><cell>Attention Tripnet [1]</cell><cell>77.1</cell><cell>43.4</cell><cell>30.0</cell><cell>32.3</cell><cell>29.7</cell><cell>25.5</cell></row><row><cell></cell><cell>Rendezvous [1]</cell><cell>77.5</cell><cell>47.5</cell><cell>37.7</cell><cell>34.4</cell><cell>38.2</cell><cell>32.7</cell></row><row><cell></cell><cell>Tripnet [9]</cell><cell>73.9</cell><cell>41.6</cell><cell>32.1</cell><cell>28.8</cell><cell>29.2</cell><cell>27.4</cell></row><row><cell>PyTorch</cell><cell>Attention Tripnet [1]</cell><cell>73.7</cell><cell>43.7</cell><cell>35.3</cell><cell>31.6</cell><cell>31.9</cell><cell>27.7</cell></row><row><cell></cell><cell>Rendezvous [1]</cell><cell>75.9</cell><cell>46.0</cell><cell>38.7</cell><cell>35.9</cell><cell>37.1</cell><cell>32.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Benchmark triplet recognition AP (%) on CholecT50 dataset using the official cross-validation split.</figDesc><table><row><cell>Method (in PyTorch)</cell><cell cols="3">Component detection</cell><cell cols="3">Triplet association</cell></row><row><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IV T</cell></row><row><cell>Tripnet [9]</cell><cell cols="3">89.1?1.7 58.8?3.1 38.4?1.3</cell><cell cols="3">32.7?2.4 29.0?0.8 25.3?2.4</cell></row><row><cell>Attention Tripnet [1]</cell><cell cols="3">88.7?1.3 61.1?2.0 40.7?3.2</cell><cell cols="3">33.1?2.7 30.3?1.6 27.2?2.9</cell></row><row><cell>Rendezvous [1]</cell><cell cols="3">89.4?2.0 60.4?2.8 40.3?2.2</cell><cell cols="3">34.5?2.8 31.8?1.0 29.4?2.5</cell></row><row><cell cols="4">5.4 Quantitative Results on CholecT45 using Cross-Validation Split</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Benchmark triplet recognition AP (%) on CholecT45 dataset using the official cross-validation split.</figDesc><table><row><cell>Method (in PyTorch)</cell><cell cols="3">Component detection</cell><cell cols="3">Triplet association</cell></row><row><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IV T</cell></row><row><cell>Tripnet [9]</cell><cell cols="3">89.9?1.0 59.9?0.9 37.4?1.5</cell><cell cols="3">31.8?4.1 27.1?2.8 24.4?4.7</cell></row><row><cell>Attention Tripnet [1]</cell><cell cols="3">89.1?2.1 61.2?0.6 40.3?1.2</cell><cell cols="3">33.0?2.9 29.4?1.2 27.2?2.7</cell></row><row><cell>Rendezvous [1]</cell><cell cols="3">89.3?2.1 62.0?1.3 40.0?1.4</cell><cell cols="3">34.0?3.3 30.8?2.1 29.4?2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Per-class instrument presence detection AP (%) on cross-validation splits (Method in PyTorch)</figDesc><table><row><cell>Classes</cell><cell></cell><cell>CholecT45</cell><cell></cell><cell></cell><cell>CholecT50</cell><cell></cell></row><row><cell></cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell></row><row><cell>grasper</cell><cell>96.5?0.4</cell><cell>96.4?0.7</cell><cell>96.6?0.6</cell><cell>96.4?0.7</cell><cell>96.5?0.6</cell><cell>96.6?0.6</cell></row><row><cell>bipolar</cell><cell>88.4?4.2</cell><cell>86.0?4.2</cell><cell>87.4?4.7</cell><cell>89.0?3.4</cell><cell>88.2?4.2</cell><cell>88.5?4.1</cell></row><row><cell>hook</cell><cell>97.5?1.6</cell><cell>97.1?1.3</cell><cell>97.4?1.5</cell><cell>97.5?1.3</cell><cell>97.3?1.3</cell><cell>97.6?1.3</cell></row><row><cell>scissors</cell><cell>80.3?6.0</cell><cell>79.6?8.4</cell><cell>78.4?5.4</cell><cell>82.8?4.9</cell><cell>81.3?5.8</cell><cell>82.6?6.5</cell></row><row><cell>clipper</cell><cell>91.2?3.9</cell><cell>90.1?3.9</cell><cell>90.9?3.8</cell><cell>89.6?5.6</cell><cell>89.8?5.8</cell><cell>89.9?5.0</cell></row><row><cell>irrigator</cell><cell>86.0?4.1</cell><cell>85.3?2.8</cell><cell>84.5?6.8</cell><cell>79.6?6.5</cell><cell>79.1?5.7</cell><cell>81.3?4.9</cell></row><row><cell>Mean</cell><cell>89.9?1.0</cell><cell>89.1?2.1</cell><cell>89.3?2.1</cell><cell>89.1?1.7</cell><cell>88.7?1.3</cell><cell>89.4?2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Per-class verb recognition AP (%) on cross-validation splits (Method in PyTorch)The per-class target detection reveals the most interesting areas of improvement. The predominant targets such as gallbladder, liver, and specimen-bag are well detected above 70% as shown inTable 11. The main challenge comes in detecting tiny anatomical structures such as cystic-artery, peritoneum, cystic-plate, cytic-pedicle, etc. as against conspicuous structures such as liver, omentum, cystic-duct, fluid, etc. Some anatomies with no clear boundaries such as abdominal wall, cystic-artery, etc. are fairly detected.</figDesc><table><row><cell>Classes</cell><cell></cell><cell>CholecT45</cell><cell></cell><cell></cell><cell>CholecT50</cell><cell></cell></row><row><cell></cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell></row><row><cell>grasp</cell><cell>70.5?5.8</cell><cell>60.5?9.9</cell><cell>69.8?3.7</cell><cell>67.1?3.4</cell><cell>66.1?5.4</cell><cell>68.3?3.0</cell></row><row><cell>retract</cell><cell>90.5?5.4</cell><cell>84.0?9.8</cell><cell>89.7?7.2</cell><cell>86.7?5.1</cell><cell>85.8?5.4</cell><cell>86.7?5.8</cell></row><row><cell>dissect</cell><cell>93.0?2.8</cell><cell>86.5?9.9</cell><cell>93.2?3.9</cell><cell>90.9?2.4</cell><cell>90.6?2.4</cell><cell>91.0?3.3</cell></row><row><cell>coagulate</cell><cell>67.2?6.1</cell><cell>56.5?9.9</cell><cell>68.7?5.5</cell><cell>67.9?5.0</cell><cell>68.5?6.2</cell><cell>69.7?6.1</cell></row><row><cell>clip</cell><cell>85.4?6.4</cell><cell>67.8?9.8</cell><cell>85.5?3.7</cell><cell>85.5?6.3</cell><cell>86.1?5.4</cell><cell>86.5?5.5</cell></row><row><cell>cut</cell><cell>70.5?9.1</cell><cell>57.7?9.9</cell><cell>72.0?4.8</cell><cell>74.9?3.4</cell><cell>72.3?6.1</cell><cell>74.9?7.6</cell></row><row><cell>aspirate</cell><cell>60.7?9.2</cell><cell>47.1?9.9</cell><cell>57.8?9.9</cell><cell>57.4?4.9</cell><cell>57.1?7.3</cell><cell>56.7?5.5</cell></row><row><cell>irrigate</cell><cell>29.6?8.2</cell><cell>17.4?9.7</cell><cell>25.7?5.8</cell><cell>27.6?9.4</cell><cell>25.4?7.9</cell><cell>25.1?9.2</cell></row><row><cell>pack</cell><cell>32.1?9.9</cell><cell>25.8?9.9</cell><cell>31.2?9.9</cell><cell>26.8?9.9</cell><cell>33.2?9.9</cell><cell>20.0?9.9</cell></row><row><cell>null-verb</cell><cell>23.0?2.4</cell><cell>21.1?5.0</cell><cell>24.0?4.1</cell><cell>24.5?1.8</cell><cell>25.5?4.0</cell><cell>24.9?2.6</cell></row><row><cell>Mean</cell><cell>59.9?0.9</cell><cell>61.2?0.6</cell><cell>62.0?1.3</cell><cell>58.8?3.1</cell><cell>61.1?2.0</cell><cell>60.4?2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Per-class target recognition AP (%) on cross-validation splits (Method in PyTorch) 7?8.6 44.4?9.9 48.0?9.9 45.8?8.0 45.8?9.9 46.2?9.9 peritoneum 17.7?5.5 24.1?9.9 26.6?4.5 19.5?9.9 28.8?9.9 25.7?8.1 gut 10.7?7.7 09.6?6.9 09.5?6.9 13.6?8.3 14.4?6.6 15.5?7.4 specimen-bag 85.8?2.5 70.1?9.9 84.4?1.2 85.5?1.8 84.1?1.3 84.6?1.2 null-target 22.8?2.3 21.1?5.4 23.5?4.1 24.5?2.1 25.5?3.9 25.2?2.6 Mean 37.4?1.4 40.3?1.2 40.0?1.4 38.4?1.3 40.7?3.2 40.3?2.2</figDesc><table><row><cell>Classes</cell><cell></cell><cell>CholecT45</cell><cell></cell><cell></cell><cell>CholecT50</cell></row><row><cell></cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell></row><row><cell>gallbladder</cell><cell cols="3">93.6?1.1 91.2?6.5 93.7?1.2</cell><cell cols="3">93.6?1.3 93.8?1.0 93.6?1.6</cell></row><row><cell>cystic-plate</cell><cell cols="3">11.6?3.9 10.1?2.0 11.0?3.5</cell><cell cols="3">11.1?2.2 11.2?2.7 09.9?3.3</cell></row><row><cell>cystic-duct</cell><cell cols="3">47.2?5.8 41.9?9.9 47.1?2.8</cell><cell cols="3">47.6?5.6 48.1?3.1 47.2?3.9</cell></row><row><cell>cystic-artery</cell><cell cols="3">31.9?3.7 29.6?9.7 31.2?2.2</cell><cell cols="3">35.0?4.7 34.6?2.8 35.6?4.6</cell></row><row><cell>cystic-pedicle</cell><cell cols="3">04.0?2.4 08.7?6.0 13.4?7.8</cell><cell cols="3">10.3?5.0 06.9?8.2 10.4?6.5</cell></row><row><cell>blood-vessel</cell><cell cols="3">08.4?5.6 15.6?9.9 06.7?6.2</cell><cell cols="3">12.7?9.9 23.5?9.9 18.7?9.9</cell></row><row><cell>fluid</cell><cell cols="3">58.4?9.2 48.9?9.9 58.0?9.9</cell><cell cols="3">56.3?5.1 54.5?6.8 57.0?5.1</cell></row><row><cell>abdominal-wall/cavity</cell><cell cols="3">30.0?4.6 20.4?9.9 25.9?7.2</cell><cell cols="3">25.7?3.4 28.5?5.0 31.3?9.9</cell></row><row><cell>liver</cell><cell cols="3">71.8?5.5 65.3?9.9 72.9?2.5</cell><cell cols="3">72.7?6.5 73.5?4.4 74.8?4.2</cell></row><row><cell>adhesion</cell><cell cols="3">04.2?0.3 13.9?3.3 07.2?0.5</cell><cell cols="3">05.2?0.0 33.3?9.9 11.3?2.8</cell></row><row><cell>omentum</cell><cell>46.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Per-class triplet recognition AP (%) on cross-validation splits (Method in PyTorch) 4?04.7 27.2?02.7 29.4?02.8 25.3?02.4 27.2?02.9 29.4?02.5</figDesc><table><row><cell>Classes</cell><cell>CholecT45</cell><cell></cell><cell></cell><cell>CholecT50</cell><cell>classes</cell><cell></cell><cell>CholecT45</cell><cell></cell><cell></cell><cell>CholecT50</cell><cell></cell></row><row><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell><cell>Tripnet</cell><cell>Attention Tripnet</cell><cell>RDV</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">CholecT45 &amp; CholecT50 Dataset Splits and MetricsA BENCHMARK STUDY</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>This work was supported by French state funds managed within the Investissements d'Avenir program by BPI France (project CONDOR) and by the ANR under references ANR-11-LABX-0004 (Labex CAMI), ANR-16-CE33-0009 (DeepSurg), ANR-10-IAHU-02 (IHU Strasbourg) and ANR-20-CHIA-0029-01 (National AI Chair AI4ORSafety). It was granted access to the HPC resources of Unistra Mesocentre and GENCI-IDRIS (Grant 2021-AD011011638R1). The authors also thank the IHU and IRCAD research teams for their help with the initial data annotation during the CONDOR project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seeliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841522000846" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102433</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cholectriplet2021: a benchmark challenge for surgical action triplet recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alapatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surgical data science: Enabling next-generation surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven visual tracking in retinal microsurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jopling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Azagury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cataracts: Challenge on automatic tool annotation for cataract surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mar?alkait?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Dedmari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prellberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cadis: Cataract dataset for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grammatikopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11586</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition of instrumenttissue interactions in endoscopic videos via action triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robust medical instrument segmentation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Full</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Filimon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The saras endoscopic surgeon action detection (esad) dataset: Challenges and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Bawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kapinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Skarga-Bandurova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oleari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leporini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Landolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Trans-svnet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09712</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal coherence-based self-supervised learning for laparoscopic workflow analysis,&quot; in OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Image-Based Procedures, and Skin Image Analysis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toolnet: holistically-nested real-time segmentation of robotic surgical tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gruijthuijsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devreker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Attilakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vander Poorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5717" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning for tool localization in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep learning methods for the detection and recognition of surgical tools and activities in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<ptr target="http://icube-publis.unistra.fr/8-Nwoy21" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>Universit? de Strasbourg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
