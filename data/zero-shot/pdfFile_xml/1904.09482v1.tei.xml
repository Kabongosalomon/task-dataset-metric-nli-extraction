<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN)  for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to distill knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7% (1.5% absolute improvement 1 ). The code and pre-trained models will be made publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ensemble learning is an effective approach to improve model generalization, and has been used to achieve new state-of-the-art results in a wide range of natural language understanding (NLU) tasks, including question answering and machine reading comprehension <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b10">Huang et al., 2017;</ref><ref type="bibr" target="#b8">Hancock et al., 2019)</ref>. A recent survey is included in . However, these ensemble models typically consist of tens or hundreds of different deep neural network (DNN) models and are prohibitively expensive to deploy due to the computational cost 1 Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of <ref type="bibr">April 1, 2019.</ref> of runtime inference. Recently, large-scale pretrained models, such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> and GPT <ref type="bibr" target="#b16">(Radford et al., 2018)</ref>, have been used effectively as the base models for building taskspecific NLU models via fine-tuning. The pretrained models by themselves are already expensive to serve at runtime (e.g. BERT contains 24 transformer layers with 344 million parameters, and GPT-2 contains 48 transformer layers with 1.5 billion parameters), the ensemble versions of these models multiplying the extreme for online deployment.</p><p>Knowledge distillation is a process of distilling or transferring the knowledge from a (set of) large, cumbersome model(s) to a lighter, easierto-deploy single model, without significant loss in performance <ref type="bibr" target="#b2">(Bucilu et al., 2006;</ref><ref type="bibr" target="#b9">Hinton et al., 2015;</ref><ref type="bibr" target="#b1">Balan et al., 2015;</ref><ref type="bibr">Ba et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b17">Tan et al., 2019)</ref>.</p><p>In this paper, we explore the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN)  for learning text representations across multiple NLU tasks. Since MT-DNN incorporates a pre-trained BERT model, its ensemble is expensive to serve at runtime.</p><p>We extend the knowledge distillation method <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> to the multi-task learning setting <ref type="bibr" target="#b3">(Caruana, 1997;</ref><ref type="bibr" target="#b20">Xu et al., 2018;</ref><ref type="bibr" target="#b5">Collobert et al., 2011;</ref><ref type="bibr" target="#b21">Zhang and Yang, 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2015)</ref>. In the training process, we first pick a few tasks, each with an available task-specific training dataset which is stored in the form of (x, y) pairs, where x is an input and y is its correct target. For each task, we train an ensemble of MT-DNN models (teacher) that outperform the best single model. Although the ensemble model is not feasible for online deployment, it can be utilized, in an offline manner, to produce a set of soft targets for each x in the training dataset , which, for example, in a classification task are the class probabilities averaged over the ensemble of different models. Then, we train a single MT-DNN (student) via multi-task learning with the help of the teachers by using both the soft targets and correct targets across different tasks. We show in our experiments that knowledge distillation effectively transfers the generalization ability of the teachers to the student. As a result, the distilled MT-DNN outperforms the vanilla MT-DNN that is trained in a normal way, as described in , on the same training data as was used to train the teachers.</p><p>We validate the effectiveness of our approach on the General Language Understanding Evaluation (GLUE) dataset <ref type="bibr" target="#b19">(Wang et al., 2019)</ref> which consists of 9 NLU tasks. We find that the distilled MT-DNN outperforms the vanilla MT-DNN on 7 tasks, including the tasks where we do not have teachers. This distilled model improves the GLUE benchmark (single model) to 83.7%, amounting to 3.2% absolute improvement over BERT and 1.5% absolute improvement over the previous state of the art model based on the GLUE leaderboard 2 as of April 1, 2019.</p><p>In the rest of the paper, Section 2 describes the MT-DNN of  which is the baseline and vanilla model for this study. Section 3 describes in detail knowledge distillation for multitask learning. Section 4 presents our experiments on GLUE. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MT-DNN</head><p>The architecture of the MT-DNN model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The lower layers are shared across all tasks, while the top layers represent task-specific outputs. The input X, which is a word sequence (either a sentence or a set of sentences packed together) is first represented as a sequence of embedding vectors, one for each word, in l 1 . Then the transformer encoder captures the contextual information for each word via self-attention, and generates a sequence of contextual embeddings in l 2 . This is the shared semantic representation that is trained by our multi-task objectives.</p><p>Lexicon Encoder (l 1 ): The input X = {x 1 , ..., x m } is a sequence of tokens of length m. Following <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, the first token x 1 is always the [CLS] token. If X is packed by a set of sentences (X 1 , X 2 ), we separate the these 2 https://gluebenchmark.com sentences with special tokens <ref type="bibr">[SEP]</ref>. The lexicon encoder maps X into a sequence of input embedding vectors, one for each token, constructed by summing the corresponding word, segment, and positional embeddings.</p><p>Transformer Encoder (l 2 ): We use a multilayer bidirectional Transformer encoder <ref type="bibr" target="#b18">(Vaswani et al., 2017)</ref> to map the input representation vectors (l 1 ) into a sequence of contextual embedding vectors C ? R d?m . This is the shared representation across different tasks.</p><p>Task-Specific Output Layers: We can incorporate arbitrary natural language tasks, each with its task-specific output layers. For example, we implement the output layers as a neural decoder for text generation, a neural ranker for relevance ranking, a logistic regression for text classification, and so on. Below, we elaborate the implementation detail using text classification as an example.</p><p>Suppose that x is the contextual embedding (l 2 ) of the token <ref type="bibr">[CLS]</ref>, which can be viewed as the semantic representation of input sentence X. The probability that X is labeled as class c (i.e., the sentiment is postive or negative) is predicted by a logistic regression with softmax:</p><formula xml:id="formula_0">P r (c|X) = softmax(W t ? x),<label>(1)</label></formula><p>where W t is the task-specific parameter matrix for task t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Training Procedure</head><p>The training procedure of MT-DNN consists of two stages: pre-training and multi-task learning (MTL). In the pre-training stage,  used a publicly available pre-trained BERT model to initialize the parameters of the shared layers (i.e., the lexicon encoder and the transformer encoder).</p><p>In the MTL stage, mini-batch based stochastic gradient descent (SGD) is used to learn the model parameters (i.e., the parameters of all the shared layers and the task-specific layers), as shown in Algorithm 1. First, the training samples from multiple tasks (e.g., 9 GLUE tasks) are packed into mini-batches. We denote a mini-batch by b t , indicating that it contains only the samples from task t. In each epoch, a mini-batch b t is selected, and the model is updated according to the task-specific objective for task t, denoted by L t (?). This approximately optimizes the sum of all multi-task objectives.  . The lower layers are shared across all tasks while the top layers are task-specific. The input X (either a sentence or a set of sentences) is first represented as a sequence of embedding vectors, one for each word, in l 1 . Then the Transformer encoder captures the contextual information for each word and generates the shared contextual embedding vectors in l 2 . Finally, for each task, additional task-specific layers generate task-specific representations, followed by operations necessary for classification, similarity scoring, or relevance ranking. Take text classification as an example. We use the cross-entropy loss as the objective in Line 3 of Algorithm 1:</p><formula xml:id="formula_1">? c 1(X, c) log(P r (c|X)),<label>(2)</label></formula><p>where 1(X, c) is the binary indicator (0 or 1) if class label c is the correct classification for X, and P r (.) is defined by Equation 1.</p><p>Then, in Line 5, the parameters of the shared layers and the output layers corresponding to task t are updated using the gradient computed in Line 4.</p><p>After MT-DNN is trained via MTL, it can be fine-tuned (or adapted) using task-specific labeled training data to perform prediction on any individual task, which can be a task used in the MTL stage or a new task that is related to the ones used in MTL.  showed that the shared layers of MT-DNN produce more universal text representations than that of BERT. As a result, MT-DNN allows fine-tuning or adaptation with substantially fewer task-specific labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge Distillation</head><p>The process of knowledge distillation for MTL is illustrated in <ref type="figure">Figure 2</ref>. First, we pick a few tasks </p><formula xml:id="formula_2">D = D 1 ? D 2 ... ? D T 2. Shuffle D for b t in D do //b t is a mini-batch of task t. 3. Compute task-specific loss : L t (?) 4. Compute gradient: ?(?) 5. Update model: ? = ? ? ?(?) end end</formula><p>where there are task-specific labeled training data. Then, for each task, we train an ensemble of different neural nets as a teacher. Each neural net is Figure 2: Process of knowledge distillation for multi-task learning. A set of tasks where there is task-specific labeled training data are picked. Then, for each task, an ensemble of different neural nets (teacher) is trained. The teacher is used to generate for each task-specific training sample a set of soft targets. Given the soft targets of the training datasets across multiple tasks, a single MT-DNN (student) is trained using multi-task learning and back propagation as described in Algorithm 1, except that if task t has a teacher, the task-specific loss in Line 3 is the average of two objective functions, one for the correct targets and the other for the soft targets assigned by the teacher.</p><p>an instance of MT-DNN described in Section 2, and is fine-tuned using task-specific training data while the parameters of its shared layers are initialized using the MT-DNN model pre-trained on the GLUE dataset via MTL, as in Algorithm 1, and the parameters of its task-specific output layers are randomly initialized.</p><p>For each task, a teacher generates a set of soft targets for each task-specific training sample. Take text classification as an example. A neural network model typically produces class probabilities using a softmax layer as in Equation 1. Let Q k be the class probabilities produced by the k-th single network of the ensemble. The teacher produces the soft targets by averaging the class probabilities across networks:</p><formula xml:id="formula_3">Q = avg([Q 1 , Q 2 , ..., Q K ]).<label>(3)</label></formula><p>We want to approximate the teacher using a student neural network model, which also has a softmax output for the same task P r (c|X), as in Equation 1. Hence, we use the standard cross entropy loss: ? c Q(c|X) log(P r (c|X)).</p><p>Note that the above loss function differs from the cross entropy loss in Equation 2 in that the former uses the soft targets Q(c|X) while the latter uses the hard correct target via the indicator 1(X, c).</p><p>As pointed out by <ref type="bibr" target="#b9">Hinton et al. (2015)</ref>, the use of soft targets produced by the teacher is the key to successfully transferring the generalization ability of the teacher to the student. The relative probabilities of the teacher labels contain information about how the teacher generalizes. For example, the sentiment of the sentence "I really enjoyed the conversation with Tom" has a small chance of being classified as negative. But the sentence "Tom and I had an interesting conversation" can be either positive or negative, depending on its context if available, leading to a high entropy of the soft targets assigned by the teacher. In these cases, the soft targets provide more information per training sample than the hard targets and less variance in the gradient between training samples. By optimizing the student for the soft targets produced by the teacher, we expect the student to learn to generalize in the same way as the teacher. In our case, each task-specific teacher is the average of a set of different neural networks, and thus generalizes well. The single MT-DNN (student) trained to generalize in the same way as the teachers is expected to do much better on test data than the vanilla MT-DNN that is trained in the normal way on the same training dataset. We will demonstrate in our experiments that this is indeed the case.</p><p>We also find that when the correct targets are known, the model performance can be significantly improved by training the distilled model on a combination of soft and hard targets. We do so by defining a loss function for each task that take a weighted average between the cross entropy loss with the correct targets as Equation 2 and the cross entropy with the soft targets as Equation 4. <ref type="bibr" target="#b9">Hinton et al. (2015)</ref> suggested using a considerably lower weight on the first loss term. But in our experiments we do not observe any significant difference by using different weights for the two loss terms, respectively.</p><p>Finally, given the soft targets of the training datasets across multiple tasks, the student MT-DNN can be trained using MTL as described in Algorithm 1, except that if task t has a teacher, the task-specific loss in Line 3 is the average of two objective functions, one for the correct targets and the other for the soft targets assigned by the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the MT-DNN trained using Knowledge Distillation, termed as MT-DNN KD in this section, on the General Language Understanding Evaluation (GLUE) benchmark. GLUE is a collection of nine NLU tasks as in <ref type="table" target="#tab_0">Table 1</ref>, including question answering, sentiment analysis, text similarity and textual entailment. We refer readers to <ref type="bibr" target="#b19">Wang et al. (2019)</ref> for a detailed description of GLUE. We compare MT-DNN KD with existing state-of-the-art models including BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, STILT <ref type="bibr" target="#b15">(Phang et al., 2018)</ref>, Snorkel MeTal <ref type="bibr" target="#b8">(Hancock et al., 2019)</ref>, and MT-DNN . Furthermore, we investigate the relative contribution of using knowledge distillation for MTL with an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Our implementation is based on the PyTorch implementations of MT-DNN 3 and BERT 4 . We used Adamax <ref type="bibr" target="#b11">(Kingma and Ba, 2014)</ref> as our optimizer with a learning rate of 5e-5 and a batch size of 32. The maximum number of epochs was set to 5. A linear learning rate decay schedule with warm-up over 0.1 was used, unless stated otherwise. We also set the dropout rate of all the task-specific layers as 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion issue, we clipped the gradient norm within 1. All the texts were tokenized using wordpieces, and were chopped to spans no longer than 512 tokens.</p><p>To obtain a set of diverse single models to form ensemble models (teachers), we first trained 6 single MT-DNNs, initialized using Cased/Uncased BERT models as <ref type="bibr" target="#b8">(Hancock et al., 2019)</ref> with a different dropout rate, ranged in {0.1, 0.2, 0.3}, on the shared layers, while keeping other training hyperparameters the same as aforementioned. Then, we selected top 3 best models according to the results on the MNLI and RTE development datasets. Finally, we fine-tuned the 3 models on each of the MNLI, QQP, RTE and QNLI tasks to form four task-specific ensembles (teachers), each consisting of 3 single MT-DNNs fine-tuned for the task. The teachers are used to generate soft targets for the four tasks as Equation 3, described in Section 3. We only pick four out of nine GLUE tasks to train teachers to investigate the generalization ability of MT-DNN KD , i.e., its performance on the tasks with and without teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GLUE Main Results</head><p>We compare MT-DNN KD with a list of state-ofthe-art models that have been submitted to the GLUE leaderboard.</p><p>BERT LARGE This is the large BERT model released by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, which we used as a baseline. We used single-task fine-tuning to produce the best result for each GLUE task according to the development set.</p><p>MT-DNN This is the model described in Section 2 and . We used the pre-trained BERT LARGE model to initialize its shared layers, refined the shared layers via MTL on all GLUE tasks, and then perform a fine-tune for each GLUE task using the task-specific data.</p><p>MT-DNN KD This is the MT-DNN model trained using knowledge distillation as described in Section 3. MT-DNN KD uses the same model architecture as that of MT-DNN. But the former is trained with the help from four task-specific ensembles (teachers). MT-DNN KD is optimized for the multitask objectives that are based on the hard correct targets, as well as the soft targets produced by the teachers if available. After knowledge distillation based MTL, MT-DNN KD is further fine-tuned for each task using task-specific data to produce the final predictions for each GLUE task on blind test data for evaluation.    <ref type="bibr" target="#b19">(Wang et al., 2019)</ref> ; 2 : <ref type="bibr" target="#b16">(Radford et al., 2018)</ref>; 3 : <ref type="bibr" target="#b15">(Phang et al., 2018)</ref>; 4 : <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>; 5 : ; 6 : <ref type="bibr" target="#b8">(Hancock et al., 2019)</ref>.</p><p>The main results on the official test datasets of GLUE are reported in <ref type="table" target="#tab_3">Table 2</ref>. Compared to other recent submissions on the GLUE leaderboard, MT-DNN KD is the best performer, creating a new state-of-the-art result of 83.7%. The margin between MT-DNN KD and the second-best model ALICE is 0.5%, larger than the margin of 0.1% between the second and the third (and the fourth) places. It is worth noting that MT-DNN KD is a single model while Snorkel MetaL <ref type="bibr" target="#b8">(Hancock et al., 2019)</ref> is an ensemble model. The description of ALICE is not disclosed yet. <ref type="table" target="#tab_3">Table 2</ref> also shows that MT-DNN KD significantly outperforms MT-DNN not only in overall score but on 7 out of 9 GLUE tasks, including the tasks without a teacher. Since MT-DNN KD and MT-DNN use the same network architecture, and are trained with the same initialization and on the same datasets, the improvement of MT-DNN KD is solely attributed to the use of knowledge distillation in MTL.</p><p>We note that the most significant per-task improvements are from CoLA (65.4% vs. 61.5%) and RTE (85.1% vs. 75.5%). Both tasks have relatively small amounts of in-domain data. Similarly, for the same type of tasks, the improvements of MT-DNN KD over MT-DNN are much more substantial for the tasks with less in-domain training  data e.g., for the two NLI tasks, the improvement in RTE is much larger than that in MNLI; for the two paraphrase tasks, the improvement in MRPC is larger than that in QQP. These results suggest that knowledge distillation based MTL is effective at improving model performance for not only tasks with teachers but also ones without teachers, and more so for tasks with fewer in-domain labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We perform an ablation study to investigate how effective it can distill knowledge from the ensemble models (teachers) to a single MT-DNN (student). To this end, we compare the performance of the ensemble models with the corresponding student model. The results on dev sets are shown in <ref type="table" target="#tab_5">Table  3</ref>, where MT-DNN-ensemble are the task-specific ensemble models trained using the process described in Section 3. We only use four ensemble models (i.e., the models for MNLI, QQP, RTE, QNLI) as teachers. The results of the other ensemble models (i.e., MRPC, CoLa, SST-2, STS-B) are reported to show the effectiveness of the knowledge distillation based MTL at improving the performance on tasks without a teacher.</p><p>We can draw several conclusions from the results in <ref type="table" target="#tab_5">Table 3</ref>. First, MT-DNN KD significantly outperforms MT-DNN and BERT LARGE across multiple GLUE tasks on the dev sets, which is consistent with what we observe on test sets in <ref type="table" target="#tab_3">Table 2</ref>. Second, comparing MT-DNN KD with MT-DNN-ensemble, we see that the MT-DNN KD successfully distills knowledge from the teachers. Although the distilled model is simpler than the teachers, it retains nearly all of the improvement that is achieved by the ensemble models. More interestingly, we find that incorporating knowledge distillation into MTL improves the model perfor-mance on the tasks where no teacher is used. On MRPC, CoLA, and STS-B, the performance of MT-DNN KD is much better than MT-DNN and is close to the ensemble models although the latter are not used as teachers in MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have extended knowledge distillation to MTL in training a MT-DNN for natural language understanding. We have shown that distillation works very well for transferring knowledge from a set of ensemble models (teachers) into a single, distilled MT-DNN (student). On the GLUE datasets, the distilled MT-DNN creates new state of the art result on 7 out of 9 NLU tasks, including the tasks where there is no teacher, pushing the GLUE benchmark (single model) to 83.7%.</p><p>We show that the distilled MT-DNN retains nearly all of the improvements achieved by ensemble models, while keeping the model size the same as the vanilla MT-DNN model.</p><p>There are several research areas for future exploration. First, we will seek better ways of combining the soft targets and hard correct targets for multi-task learning. Second, the teachers might be used to produce the soft targets for large amounts of unlabeled data, which in turn can be used to train a better student model in a way conceptually similar to semi-supervised learning. Third, instead of compressing a complicated model to a simpler one, knowledge distillation can also be used to improve the model performance regardless of model complexity, in machine learning scenarios such as self-learning in which both the student and teacher are the same model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the MT-DNN model for representation learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Training a MT-DNN model. Initialize model parameters ? randomly. Initialize the shared layers (i.e., the lexicon encoder and the transformer encoder) using a pre-trained BERT model. Set the max number of epoch: epoch max . //Prepare the data for T tasks. for t in 1, 2, ..., T do Pack the dataset t into mini-batch: D t . end for epoch in 1, 2, ..., epoch max do 1. Merge all the datasets:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of the GLUE benchmark.</figDesc><table><row><cell>Model</cell><cell cols="3">CoLA SST-2 MRPC STS-B</cell><cell cols="3">QQP MNLI-m/mm QNLI RTE WNLI AX Score</cell></row><row><cell></cell><cell>8.5k 67k</cell><cell>3.7k</cell><cell>7k</cell><cell>364k</cell><cell>393k</cell><cell>108k 2.5k 634</cell></row><row><cell cols="6">BiLSTM+ELMo+Attn 1 36.0 90.4 84.9/77.9 75.1/73.3 64.8/84.7 76.4/76.1</cell><cell>79.8 56.8 65.1 26.5 70.0</cell></row><row><cell>Singletask Pretrain Transformer 2</cell><cell cols="5">45.4 91.3 82.3/75.7 82.0/80.0 70.3/88.5 82.1/81.4</cell><cell>87.4 56.0 53.4 29.8 72.8</cell></row><row><cell>GPT on STILTs 3</cell><cell cols="5">47.2 93.1 87.7/83.7 85.3/84.8 70.1/88.1 80.8/80.6</cell><cell>-</cell><cell>69.1 65.1 29.4 76.9</cell></row><row><cell>BERTLARGE 4</cell><cell cols="5">60.5 94.9 89.3/85.4 87.6/86.5 72.1/89.3 86.7/85.9</cell><cell>92.7 70.1 65.1 39.6 80.5</cell></row><row><cell>MT-DNN 5</cell><cell cols="5">61.5 95.6 90.0/86.7 88.3/87.7 72.4/89.6 86.7/86.0</cell><cell>-</cell><cell>75.5 65.1 40.3 82.2</cell></row><row><cell>Snorkel MeTaL 6</cell><cell cols="5">63.8 96.2 91.5/88.5 90.1/89.7 73.1/89.9 87.6/87.2</cell><cell>93.9 80.9 65.1 39.9 83.2</cell></row><row><cell>ALICE  *</cell><cell cols="5">63.5 95.2 91.8/89.0 89.8/88.8 74.0/90.4 87.9/87.4</cell><cell>95.7 80.9 65.1 40.7 83.3</cell></row><row><cell>MT-DNNKD</cell><cell cols="5">65.4 95.6 91.1/88.2 89.6/89.0 72.7/89.6 87.5/86.7</cell><cell>96.0 85.1 65.1 42.8 83.7</cell></row><row><cell>Human Performance</cell><cell cols="5">66.4 97.8 86.3/80.8 92.7/92.6 59.5/80.4 92.0/92.8</cell><cell>91.2 93.6 95.9</cell><cell>-</cell><cell>87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>GLUE test set results scored using the GLUE evaluation server. The number below each task denotes the number of training examples. The state-of-the-art results are in bold. MT-DNN KD uses BERT LARGE to initialize its shared layers. All the results are obtained from https://gluebenchmark.com/leaderboard on April 1, 2019. Note that Snorkel MeTaL is an ensemble model. -denotes the missed result of the latest GLUE version. * denotes the unpublished work, thus not knowing whether it is a single model or an ensemble model. For QNLI, we treat it as two tasks, pair-wise ranking and classification task on v1 and v2 training datasets, respectively, and then merge results on the test set. Model references: 1 :</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>GLUE dev set results. The best result on each task produced by a single model is in bold. MT-DNN uses BERT LARGE as their initial shared layers. MT-DNN KD is the MT-DNN trained using the proposed knowledge distillation based MTL. MT-DNN-ensemble denotes the results of the ensemble models described in Section 4.1. The ensemble models on MNLI, QQP, RTE and QNLI are used as teachers in the knowledge distillation based MTL, while the other ensemble modes, whose results are in blue and italic, are not used as teachers.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/namisan/mt-dnn 4 https://github.com/huggingface/pytorch-pretrained-BERT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Asli Celikyilmaz, Xuedong Huang, Moontae Lee, Chunyuan Li, Xiujun Li, and Michael Patterson for helpful discussions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Anoop Korattikara Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05641</idno>
		<title level="m">Net2net: Accelerating learning via knowledge transfer</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural approaches to conversational ai. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="127" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Massive multi-task learning with snorkel metal: Bringing more supervision to bear</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Mccreery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paroma</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fusionnet: Fusing via fullyaware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic answer networks for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-task learning for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06963</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<title level="m">A survey on multitask learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
