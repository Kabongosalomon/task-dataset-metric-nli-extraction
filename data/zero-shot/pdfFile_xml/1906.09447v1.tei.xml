<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Period Embedding for Representing Oriented Objects in Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
							<email>jundu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Period Embedding for Representing Oriented Objects in Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Oriented object detection</term>
					<term>aerial images</term>
					<term>IoU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method for representing oriented objects in aerial images named Adaptive Period Embedding (APE). While traditional object detection methods represent object with horizontal bounding boxes, the objects in aerial images are oritented. Calculating the angle of object is an yet challenging task. While almost all previous object detectors for aerial images directly regress the angle of objects, they use complex rules to calculate the angle, and their performance is limited by the rule design. In contrast, our method is based on the angular periodicity of oriented objects. The angle is represented by two two-dimensional periodic vectors whose periods are different, the vector is continuous as shape changes. The label generation rule is more simple and reasonable compared with previous methods. The proposed method is general and can be applied to other oriented detector. Besides, we propose a novel IoU calculation method for long objects named length independent IoU (LIIoU). We intercept part of the long side of the target box to get the maximum IoU between the proposed box and the intercepted target box. Thereby, some long boxes will have corresponding positive samples. Our method reaches the 1 st place of DOAI2019 competition task1 (oriented object) held in workshop on Detecting Objects in Aerial Images in conjunction with IEEE CVPR 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Traditional object detections mainly detect objects with horizontal bounding boxes. However, objects in aerial images are oriented and cannot be effectively represented by horizontal bounding boxes. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, detecting oriented objects with horizontal bounding boxes will contain more background and cannot accurately locate the objects. Besides, overlap calculation based on horizontal bounding box is not accurate for oriented objects, as the overlap between horizontal bounding boxes of two oriented objects may be too large, so NMS based on horizontal bounding boxes is not suitable for oriented objects. Thus, representing oriented objects with oriented bounding box is necessary for object detection in aerial images. Regressing oriented bounding box is more challenging than regressing horizontal bounding box. Four variables can represent a horizontal bounding box, such as x,y coordinates of top left corner and bottom right corner. However, oriented bounding box representation needs an extra variable ? to represent its angle. It is hard to directly regress ? because the angle is periodic.</p><p>Most of previous oriented detectors <ref type="bibr">[</ref>  box and the label is calculated by complicated rules, which is hard for the network to learn. Some methods try to design a simple label calculation rule for oriented objects. For example, <ref type="bibr" target="#b5">[6]</ref> adopts Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> for detecting oriented text lines. <ref type="bibr" target="#b7">[8]</ref> regresses the outline of objects with multiple points on sliding lines. But these methods introduce additional parameters and cannot be adopted by RPN (region proposal networks).</p><p>In this study, we propose a novel method for representing oriented objects. Oriented bounding box can be represented by (x, y, w, h, ?), where x, y are the coordinates of the center of the bounding box, and w, h are the lengths of the long and short sides, respectively. We do not directly regress ?. Instead, ? is represented by two two-dimensional periodic vector. The proposed method is different from <ref type="bibr" target="#b8">[9]</ref>, as in our method, the two vectors' periods are 90 ? and 180 ? respectively. Finally, we calculate the angle with these vectors. Our method is versatile and can be applied in other detectors. Besides, we design a novel cascade R-CNN method for long objects such as harbors. Generally, a two-stage model firstly proposes bounding boxes with RPN, and the output bounding boxes of the second step (R-CNN) are limited by RPN's results. Due to the limited size of the receptive field, some long objects cannot be covered by RPN. With this in mind, we adopt a two-stage cascade R-CNN model with length independent IoU (LIIoU) to detect 2 long objects. In the first stage, some bounding boxes which only cover part of the objects are also set to positive samples. In this way, the first R-CNN can propose longer bounding boxes. The main contributions of our work are summarized as follows:</p><p>1. We present a novel method for representing oriented bounding boxes in aerial images. We do not directly regress ? of oriented bounding boxes, but instead embed ? with vectors whose periods are different. In this way, we do not need complex rules to label the angle which avoids ambiguity.</p><p>2. We present a novel IoU calculation method named length independent IoU (LIIoU), which is designed for long objects. The presented method makes the detector more robust to long objects.</p><p>3. The presented method achieves state-of-the-art on DOTA and wins the first place of Challenge-2019 on Object Detection in Aerial Images task1 (oriented task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Horizontal objects detection</head><p>Labels of traditional object detection tasks are horizontal bounding boxes. <ref type="bibr" target="#b9">[10]</ref> presents a real-time object detection method based on region proposal networks (RPN) which shares feature maps of RPN and R-CNN and use anchors with different sizes and aspect ratios in RPN stage. Though Faster R-CNN shares feature maps, it still requires much computation in R-CNN's fully connected layer. Region-based fully convolutional networks (R-FCN) <ref type="bibr" target="#b10">[11]</ref> presents Positionsensitive score maps and Position-sensitive RoI pooling for saving computation in R-CNN stage. Scale variation is always a very challenging issue in object detection; to help solve this problem, <ref type="bibr" target="#b11">[12]</ref> presents Feature Pyramid Network (FPN). FPN generates feature maps of different scales on different layers, and detects large objects on higher layers but detects small objects on lower layers; the parameters of RPN is shared over layers. Based on FPN, Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> presents RoIAlign which calculates values in RoI via bilinear interpolation instead of maximum to avoid quantization errors, and add several convolution layers on mask-head to generate instance segmentation maps. <ref type="bibr">[13]</ref> improves Mask R-CNN by adding Bottom-up Path Augmentation and feature fusion.</p><p>Two-stage methods require more computation than onestage methods, so one-stage methods are more suitable for real-time object detection tasks. Single shot multibox detector (SSD) <ref type="bibr" target="#b13">[14]</ref> generates multiple layers, and then detect objects with different sizes on different layers. Deconvolutional single shot detector (DSSD) <ref type="bibr" target="#b14">[15]</ref> upsamples feature maps and detects small objects on lower layers which improves SSD's performance for small objects. <ref type="bibr" target="#b15">[16]</ref> presents focal loss to handle the imbalance between positive and negative samples. Although anchors are widely used in object detection, many models adopt anchor-free method. <ref type="bibr" target="#b16">[17]</ref> does not use anchors in RPN, but uses a shrunk segmentation map as the label. <ref type="bibr" target="#b17">[18]</ref> also uses segmentation maps as ground truth. GA-RPN <ref type="bibr" target="#b18">[19]</ref> combines anchor-free and anchor-based ideas: the label for the first step is generated by a shrunk segmentation map, and the label for the second step is calculated based on the output anchor of the first step.</p><p>Traditional object detection in aerial images only focuses on horizontal bounding box. <ref type="bibr" target="#b19">[20]</ref> presents local-contextual feature fusion network which is designed for remote sensing images. The RPN includes multiangle, multiscale and multiaspect-ratio anchors which can deal with the oriented objects, but the final output bounding box is still horizontal. <ref type="bibr" target="#b20">[21]</ref> presents rotationinvariant matrix (RIM) which can get both the angular spatial information and the radial spatial information. <ref type="bibr" target="#b21">[22]</ref> presents an automatic and accurate localization method for detecting objects in high resolution remote sensing images based on Faster R-CNN. <ref type="bibr" target="#b22">[23]</ref> presents a method to detect seals in aerial remote sensing images based on convolutional network. <ref type="bibr" target="#b23">[24]</ref> presents a hybrid DNN (HDNN), where the last convolutional and max-pooling layer of DNN is divided into multiple blocks, so HDNN can generate multi-scale features which improves the detector's performance for small objects. Unlike the images used for general object detection, the aerial image has a large resolution. However, large models cannot be implemented due to limited memory. So <ref type="bibr" target="#b24">[25]</ref> proposes a self-reinforced network named remote sensing region-based convolutional neural network (R2-CNN) including Tiny-Net and intermediate global attention blocks. It adopts a lightweight residual structure, so the network can feedward huge resolution sensing images at high speeds. <ref type="bibr" target="#b25">[26]</ref> proposes a novel method for ship detection in synthetic aperture radar (SAR) images. It redesign the network structure, does not pre-train on ImageNet, and specifically design the system for small objects such as ships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Oriented objects detection</head><p>Oriented object detection is firstly presented in text detection field. Textboxes <ref type="bibr" target="#b26">[27]</ref> presents a novel SSD-based text detection method, which adapts the size and aspect ratio of anchor and uses 1 ? 5 convolutional filters for long text lines. Textboxes++ <ref type="bibr" target="#b1">[2]</ref> is based on textboxes but directly regresses the 8 vertices of the oriented bounding box. <ref type="bibr" target="#b27">[28]</ref> designs rules for calculating the order of the vertices of the oriented bounding box, and proposes parallel IoU computation for timesaver. <ref type="bibr" target="#b2">[3]</ref> presents rotation region proposal networks (RRPN) that proposes oriented bounding boxes in RPN stage and uses Rotation Region-of-Interest (RRoI) pooling layer in R-CNN stage. The aspect ratio of text lines varies greatly, and limited anchors cannot cover the size or aspect ratio of all objects; thus, many methods are anchor-free. Both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b0">[1]</ref> generate labels with shrunk segmentation maps, and regress the vertices or angles of the bounding box on positive pixels. <ref type="bibr" target="#b28">[29]</ref> generates a corner map and a position-sensitive segmentation map, calculates oriented bounding boxes based on the corner map, and calculates the score for each bounding box using the position-sensitive segmentation map. <ref type="bibr" target="#b29">[30]</ref> presents anchor-free region proposal network (AF-RPN) based on Faster R-CNN with the same design as FPN <ref type="bibr" target="#b11">[12]</ref>, and the label is calculated from the shrunk segmentation map instead of the anchors.</p><p>Horizontal bounding boxes cannot closely surround objects in aerial images, so the academic community begins to pay attention to oriented bounding box detection in aerial images. <ref type="bibr" target="#b30">[31]</ref> labels a large-scale dataset which contains 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPN with APE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIIoU=1</head><p>IoU=0.33 categories and 188282 instances, each labeled with an arbitrary quadrilateral (8 vertices). A novel detector which directly regresses 8 vertices based on Faster R-CNN is also presented. ICPR ODAI <ref type="bibr" target="#b31">[32]</ref> and CVPR DOTA <ref type="bibr" target="#b32">[33]</ref> competitions are organized based on this dataset. <ref type="bibr" target="#b4">[5]</ref> presents a two-stage R-CNN method with RoI Transformer, which, in the first step, proposes horizontal bounding boxes. The first R-CNN outputs oriented bounding boxes, and the inputting of the second R-CNN are oriented bounding boxes. <ref type="bibr" target="#b33">[34]</ref> proposes a novel method named rotatable region-based residual network (R3-Net) which can detect multi-oriented vehicles in aerial images and videos. The rotatable region proposal network (R-RPN) is adopted to generate rotatable region of interests (R-RoIs) which crops rotated rectangle areas from feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Our method's pipeline is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Recently, anchorfree methods <ref type="bibr" target="#b16">[17]</ref> [18] <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b29">[30]</ref> are widely used in object detection. In this study, we also use anchor-free RPN. In particular, the label of RPN is not calculated based on the overlap between the anchor and the ground truth; instead, the label is generated from the shrunk segmentation map of the oriented bounding box. Unlike traditional object detection tasks, the output bounding box of RPN is oriented, so a novel angle embedding method is adopted to better represent oriented bounding boxes. Segmentation maps with 8 channels (x, y, w, h, angle embedding) are generated in RPN stage. Then our model proposes oriented bounding boxs with Rotated RoIAlign in R-CNN stage, where Cascade R-CNN is used. In the first R-CNN, a novel IoU calculation method named length independent IoU (LIIoU) is adopted. To make IoU independent of the length of target box, we intercept part of the long side of the target box to obtain the maximum IoU between the proposed box and the intercepted target box. In this way, some long boxes will also have corresponding positive samples. Traditional IoU calculation method is used in the second R-CNN. The backbone of our network is based on FPN <ref type="bibr" target="#b11">[12]</ref>, and we augment the network in the same way as PANet <ref type="bibr">[13]</ref> by adopting bottom-up Path Augmentation and feature fusion. Next, we will introduce each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Design</head><p>Inspired by recent object detection works <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b6">[7]</ref> [13], we use Feature Pyramid Networks (FPN) as our backbone. FPN generates multiple feature maps with different sizes, and detect objects of different sizes on different layers. FPN is robust to <ref type="bibr" target="#b3">4</ref> scale variation expecially for small objects, which is suitable for this task. Besides scale variation, aspect ratio variation is another challenging problem. Most traditional object detection methods use anchors of different sizes and aspect ratios to calculate labels in RPN stage. Thus, we have to manually set the hyperparameter of the anchors which is too troublesome, and when the aspect ratio of objects varies greatly, limited anchors cannot cover all the objects. The performance of these detectors highly relies on anchor design. Recently, many methods <ref type="bibr" target="#b16">[17]</ref> [18] <ref type="bibr" target="#b18">[19]</ref> [30] <ref type="bibr" target="#b3">[4]</ref> [1] adopt the anchor-free strategy. In this study, we also adopt anchor-free method and generate the label of RPN from the shrunk segmentation map. Different layers extract different features, and the detector can achieve better performance by combining these features <ref type="bibr">[13]</ref>. In R-CNN stage, we fuse features of different layers after the first fully connected layer with max pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Anchor-free label generation</head><p>Region proposals network (RPN) is adopted to propose candidate bounding boxes. Most of the previous methods are based on anchors in this stage. Considering the huge difference in aspect ratio, we use anchor-free RPN. The shrunk segmentation label is shown in <ref type="figure">Fig. 4</ref>. The shrinking method is the same as EAST <ref type="bibr" target="#b3">[4]</ref>. In particular, r 1 is set to 0.1 and r 2 is set to 0.25. We shrink the oriented bounding box with r 2 ratio and set the pixels in the shrunk bounding box to positive samples (blue area). Next, we shrink the oriented bounding box with r 1 ratio, set the pixels in the shrunk bounding box but not set to positive samples (blue area) to "do not care" (purple area), and set the loss weight of these pixels to 0. FPN outputs multi-scale feature maps, and we detect objects of different scales on different layers. We assign a target object whose shorter side is h to the level p k , and k is calculated as follows:</p><formula xml:id="formula_0">k = k 0 + log 2 (h/128)<label>(1)</label></formula><p>where k is the layer that objects should be assigned to; k 0 is the target layer when the height h of the object is greater than 128 and less than 256, which we set to 4. As the objects of different scales share the regression and classification parameters of RPN, the regression targets should be normalized. An oriented bounding box is labeled as:</p><formula xml:id="formula_1">(x c , y c , w, h, ?)<label>(2)</label></formula><p>where (x c , y c ) is the coordinates of the center point, w, h are the lengths of the long side and the short side respectively, and ? is the angle of the long side. The pixel on k layer is labeled as x k , y k . First, we normalize the target bounding box with the stride of k layer:</p><formula xml:id="formula_2">x c = x c s k , y c = y c s k , w = w s k , h = h s k , ? = ?<label>(3)</label></formula><p>where s k is the stride of k layer calculated as:</p><formula xml:id="formula_3">s k = 2 ? 2 k<label>(4)</label></formula><p>The regression targets are calcualted as follows:</p><p>RPN with APE C 1 2 <ref type="figure">Fig. 4</ref>: The shrunk segmentation label of anchor-free method.</p><p>Purple area is ignored area which is shrunk with r 1 ratio, blue area is positive area which is shrunk with r 2 ratio.</p><formula xml:id="formula_4">t xc = x c ? x k N , t yc = y c ? y k N t w = log w N , t h = log h N<label>(5)</label></formula><p>where N is a constant and is set to 6 as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adaptive Period Embedding</head><p>A horizontal bounding box can be easily represented by 4 variables (x, y, w, h). But we need an extra variable ? to represent an oriented bounding box. The primary challenge of oriented bounding box detection is to regress the angle of objects. The property of ? is different from other variables such as x, y, w, h, as ? is a periodic variable. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, if the length and width of the rectangle are equal, the rectangle is a square, and the peroid of ? is 90 ? . Otherwise, the peroid of ? is 180 ? . In neural networks, the periodic property cannot be represented by one variable. Even though <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b34">[35]</ref> [36] all use two-dimensional periodic vector (cos ?, sin ?) for representing angle, they do not adapt vector's period. The proposed Adaptive Period Embedding (APE) uses two twodimensional vectors to represent the angle. The the first vector has a period of 90 ? and can be formulated as:</p><formula xml:id="formula_5">u 1 = (cos 4?, sin 4?)<label>(6)</label></formula><p>where ? is the angle of rectangle's long side. The period of the second vector is 180 ? which represents the angle of rectangle's long side. It is calculated as follows: v = (cos 2?, sin 2?)</p><formula xml:id="formula_6">u 2 = v ? min( (w ? h) ?h , 1)<label>(7)</label></formula><p>where ? is set to 0.5, w is rectangle's long side, h is short side. Each component of u 1 , u 2 is in [?1, 1], so we use sigmoid as activation, and then multiply them by 2 and subtract 1. Smooth L1 loss <ref type="bibr" target="#b36">[37]</ref> is used in all regression tasks of this study which can be formulated as:</p><formula xml:id="formula_8">smooth L1 (z, z * ) = 0.5(z ? z * ) 2 if |z ? z * | &lt; 1 |z ? z * | ? 0.5 otherwise (9)</formula><p>The final outputs of the neural network are (x, y, w, h, u 1 , u 2 ). Next, we calculate the angle of the rectangle's long side based on (u 1 , u 2 ). Firstly, ? 90 ? whose peroid is 90 ? can be calculated as:</p><formula xml:id="formula_9">? 90 ? = atan2(u 1 ) 4<label>(10)</label></formula><p>where atan2 function calculates one unique arctangent value from a two-dimensional vector. The ? of rectangle's long side may be ? 90 ? or ? 90 ? + 90 ? . The ? 180 ? whose peroid is 180 ? can be calculated as:</p><formula xml:id="formula_10">? 180 ? = atan2(u 2 ) 2<label>(11)</label></formula><p>Then we calculate the distance between ? 90 ? and ? 180 ?</p><formula xml:id="formula_11">dis = |(2? 90 ? ? 2? 180 ? + 180 ? ) mod 360 ? ? 180 ? | (12)</formula><p>So the final ? is calculated as:</p><formula xml:id="formula_12">? = ? 90 ? dis &lt; 90 ? ? 90 ? + 90 ? otherwise (13)</formula><p>? 180 ? is the angle of the rectangle's long side. When the length and width of the rectangle are equal, the norm of u 2 is nearly zero, so the angle calculated by u 2 is not accurate. ? 90 ? is accurate but it may be the angle of the long side or the short side. So we find the angle closer to ? 180 ? from ? 90 ? and ? 90 ? + 90 ? , which is the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. length independent IoU</head><p>IoU is the evaluation protocol of object detection, the more accurate the regression, the better the performance. But the receptive field of a neural network is limited and thus cannot cover some long objects. The detector proposes candidate bounding boxes in RPN, and then classifies and regresses these boxes again. The result of R-CNN highly relies on the output bounding boxes of RPN. In R-CNN stage, only the proposed bounding boxes whose IoU is higher than 0.5 is set to positive samples. Some target objects that are not well regressed in RPN cannot be detected in R-CNN. One idea is multiple regression <ref type="bibr" target="#b37">[38]</ref> in R-CNN stage, but if there are no positive proposed bounding boxes in the first R-CNN, the improvement is limited. Considering this, we propose a novel IoU calculation method named length independent IoU (LIIoU). We intercept part of the target box along its long side, and make the length of the intercepted box the same as the proposed box. The presented method is inspired by Seglink <ref type="bibr" target="#b38">[39]</ref>, but in our method, the aspect ratio of the proposed bounding box is arbitrary. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the traditional IoU is only 0.3, but our proposed LIIoU is nearly 1. The details of LIIoU calculation is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, where AB is the center line of the target box, and point C is the center of the proposed box. First, we find the perpendicular of AB through point C and label the intersection of the perpendicular and AB as point D. Next, we intercept a rectangle from the target bounding box as follows: if the length of the target box is smaller than the proposed box, we do not intercept; otherwise, the center of the intercepted rectangle is point D and the length is the same with proposed box (green box). Finally, we calculate IoU between the intercepted target box and the proposed box. The procedure is summarized in Algorithm 1. In this way, more bounding boxes will regress targets in R-CNN which can improve the overall quality of the bounding boxes Algorithm 1 LIIoU calculation Input: pbbox(x p , y p , w p , h p , ? p ), gbbox(x g , y g , w g , h g , ? g ) pbbox -proposed bounding box gbbox -ground truth bounding box Output: LIIoU 1: if w p &gt;= w g then 2:</p><p>x g = x g ; y g = y g ; w g = w g ; h g = h g ; ? g = ? g 3: else 4:</p><formula xml:id="formula_13">A x = x g ? cos(? g ) ? w g 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>A y = y g ? sin(? g ) ? w g 2 6: w g = w 2 ? w 1 ; h g = h g ; ? g = ? g 18: end if <ref type="bibr">19:</ref> calulate overlaps between (x p , y p , w p , h p , ? p ) and (x g , y g , w g , h g , ? g )</p><formula xml:id="formula_14">B x = x g + cos(? g ) ? w g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Cascade R-CNN</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, two R-CNNs are used after RPN. In the first R-CNN, we only refine the center, height and width of the oriented bounding box without regressing the vertices of the target box. This is because the output of the first R-CNN is the input of the second R-CNN, and Rotated RoIAlign can only handle oriented rectangle but not quadrangle. In the second R-CNN, we regress the vertices of the target box.</p><p>Rotated RoIAlign is adopted, so the ground truth is calculated in a rotated coordinate system. If the center of a Rotated RoIAlign is (x p c , y p c ) and the angle is ? p , the affine 6 Intercept target box IoU: 0.  </p><formula xml:id="formula_15">? ? = ? ? cos ? p sin ? p (1 ? cos ? p )x p c ? y p c * sin ? p ? sin ? p cos ? p (1 ? cos ? p )y p c + x p c * sin ? p 0 0 1 ? ? (14) ? ? x y 1 ? ? = M M M ? ? x y 1 ? ?<label>(15)</label></formula><p>We set the coordinate system to rotated coordinate system with Eq. 14 and Eq. 15. The final ground truth in the rotated coordinate system is (x, y), and (x , y ) is the coordinates in the original coordinate system. In the first R-CNN, the regression targets are (t rcnn1 xc , t rcnn1</p><formula xml:id="formula_16">yc , t rcnn1 w , t rcnn1 h</formula><p>) which can be formulated as:</p><formula xml:id="formula_17">t rcnn1 xc = x c ? x p c w p ; t rcnn1 yc = y c ? y p c h p (16) t rcnn1 w = log( w w p ); t rcnn1 h = log( h h p )<label>(17)</label></formula><p>In the second R-CNN, the regression targets are (t rcnn2 xi , t rcnn2 yi ), i = 1, 2, 3, 4 which can be formulated as:</p><formula xml:id="formula_18">t rcnn2 xi = x i ? x p c w p ; t rcnn2 yi = y i ? y p c h p , i = 1, 2, 3, 4<label>(18)</label></formula><p>where (x c , y c , w, h) are the center, width and height of the ground truth, (x i , y i ) is the vertex of the ground truth bounding box, and (x p c , y p c , w p , h p ) are the center, width and height of the proposed bounding box.</p><p>G. Experiment 1) Datasets: DOTA [31] is a large dataset which contains 2806 aerial images from different sensors and platforms. The size of the image varies greatly, ranging from about 800 ? 800 to 4000 ? 4000 pixels, so it is necessary to crop the image and detect the objects in the cropped images. As the instances in arial images are oriented such as car, ship and bridge, each instance is labeled by an arbitrary <ref type="bibr">(8 d</ref>.o.f.) quadrilateral. For the oriented task, the output bounding boxes are quadrilatera; to evaluate the performance of our detector on quadrilateral, we use the evaluation system provided along with this dataset. There are two versions of DOTA dataset, DOTA-v1.0 and DOTA-v1.5; DOTA-v1.5 fixes some errors and is provided for DOAI2019 competition <ref type="bibr" target="#b32">[33]</ref>. We use DOTA-v1.5 for this competition, but in the following experiments, we use DOTA-v1.0 for fair comparison.</p><p>2) Implementation Details: The backbone of our detector is ResNet-50 <ref type="bibr" target="#b39">[40]</ref> pre-trained on ImageNet <ref type="bibr" target="#b40">[41]</ref>. The number of FPN channels is set to 256. In R-CNN stage, two fully connected (FC) layers are used, the channel of which is set to 1024. Feature fusion is applied after the first FC layer along with maxpooling. Batchnorm is not used in this study. Our network is trained with SGD, where the batchsize of images is 1 and the initial learning rate is set to 0.00125, which is then divided by 10 at 2 3 and 8 9 of the entire training. Due to the limited memory, we crop images to 1024 ? 1024 with the stride of 256 for training and testing. The model is trained and tested at a single scale. Data augmentation is used for better performance; in particular, we randomly rotate images with angle among 0, ?/2, ?, 3?/2, and class balance resampling is adopted to solve class imbalance problem. In default, we train our model with training set and evaluate it on validation set and testing set.</p><p>3) Ablation Study: In order to evaluate the effect of each component, we conduct abalation experiments on validation set of DOTA. The model is not modified except the component being tested.</p><p>The effect of adaptive period embedding: We need to propose oriented bounding boxes in RPN stage, but it is challenging to effectively represent a oriented bounding box. Most of previous methods <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b2">[3]</ref> which directly regress the angle do not notice the periodicity of the angle. When the angle is too diverse, the performance of the system will drop significantly. To evaluate whether the proposed APE can well handle the diversity of the angles, we conduct abalation experiments: one model directly regress the angle of the long side of the target box, while the other regresses adaptive period    <ref type="table" target="#tab_2">Table I</ref>, RPN achieves much better performance with APE. We show the comparison in <ref type="figure">Fig. 6</ref>, where we can see that RPN outputs more accurate angle with APE compared with directly regressing the angle. LIIoU vs. IoU: To evaluate the efficiency of LIIoU, we conduct control experiment. Faster R-CNN means there is only one R-CNN. When Cascade R-CNN is adopted, two R-CNNs are used. In the first model, we calculate the overlap between oriented bounding boxes with traditional IoU in both two R-CNNs. In the second model, the overlap between oriented bounding boxes is calculated with LIIoU in the first R-CNN and with traditional IoU in the last R-CNN, and the threshold is set to 0.5. Results are shown in <ref type="table" target="#tab_2">Table II</ref>, where we can see that Cascade R-CNN gains much better performance with LIIoU. We show their comparison in <ref type="figure" target="#fig_6">Fig. 7</ref>, where we can find that LIIoU can improve the quality of the proposed bounding boxes and the recall rate. Regardless of the aspect ratio and  <ref type="figure">Fig. 8</ref>. The angle, size, aspect ratio of objects in aerial images vary greatly, but our proposed method can well handle these challenging conditions. 8   <ref type="bibr" target="#b32">[33]</ref> is held in workshop on Detecting Objects in Aerial Images in conjunction with IEEE CVPR 2019. The competition is more difficult and requires detecting all objects including samples labeled as difficult. Based on our proposed methods including APE and LIIoU, we adopt class balance resampling, image rotation, multi-scale training and testing and model assembling for better performance. Three models are used whose backbone is ResNeXt-101(32x4) <ref type="bibr" target="#b41">[42]</ref>. Finally, we combine the training set with the validation set for training. The results of the competition are shown in <ref type="table" target="#tab_2">Table IV</ref>. Our method wins the first place on oriented task, with a gain of about 1.7% over the most competing competitor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) DOAI2019 competition: DOAI2019 competition</head><p>6) Conclusion and Future Work: Detecting oriented objects in arial images is a challenging task. In this study, we make full use of the periodicity of the angle. A novel method named adaptive period embedding (APE) is proposed which can well regress oriented bounding boxes in arial images. The vector with adaptive period can learn the periodicity of the angle, which can not be implemented with one-dimensional vector. The proposed method can be applied to both onestage methods such as RPN and two-stage methods, and we believe other detectors can also directly adopt APE module.</p><p>Besides, we propose a novel length independent IoU (LIIoU). LIIoU set more proposed bounding boxes to positive samples expecially for long objects which can improve the quality of R-CNN regression. Our ablation study proves that each proposed module is effective. Our method achieves state-ofthe-art on DOTA. Based on our method, we win the first place on oriented task of DOAI2019. In the future, we will explore more efficient and accurate detector for detecting oriented objects in arial images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Left: horizonal bounding boxes, right: oriented bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of our proposed architecture. From left to right: Anchor-free RPN, Cascade R-CNN. Yellow bounding box is groud truth, red bounding box is proposed box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The peroid of oriented bounding box. Top: rectangle whose peroid is 180 ? , down: squre whose peroid is 90 ? . (bounding box's sides are in different colors for better visualization)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 7 : 8 :w 1 = z ? w p 2 ; w 2 = z + w p 2 11 :else if w 2 &gt;= w g then 14 :w 2 = 2 ;</head><label>2781221121422</label><figDesc>B y = y g + sin(? g ) ? w g 2 C x = x p ; C y = y p 9: z = (C?A)?(B?A) ||(B?A)||10:if w 1 &lt;= 0 then12: w 1 = 0; w 2 = w p 13: w g ; w 1 = w g ? w p 15: end if 16:x g = A x +cos(?)? w2+w1 y g = A y +sin(?)? w2+w1 2 17:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The details of LIIoU calculation, green bounding box is proposed bounding box, orange bounding box is target box. transformation can be represented by an affine matrix: sin ? p 0 ? sin ? p cos ? p 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 Fig. 6 :</head><label>76</label><figDesc>The comparison of RPN with APE and without APE. top: without APE, down: with APE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>The comparison of LIIoU and IoU. From Left to right: calculate overlaps with IoU in the first R-CNN, calculate overlaps with LIIoU in the first R-CNN (overlaps are both calculated with IoU in the last R-CNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>The experiment of APE on DOTA validation set in RPN stage. (in %)</figDesc><table><row><cell cols="2">Methods w/o APE</cell><cell>w/ APE</cell></row><row><cell>AP</cell><cell>70.16</cell><cell>72.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>The ablation experiments of LIIoU and IoU on DOTA validation set (in %).</figDesc><table><row><cell>Methods</cell><cell cols="3">Faster R-CNN Cascade R-CNN Cascade R-CNN+LIIoU</cell></row><row><cell>mAP</cell><cell>71.40</cell><cell>72.76</cell><cell>73.88</cell></row><row><cell cols="4">embedding (APE) vectors. We evaluate the quality of proposed</cell></row><row><cell cols="4">oriented bounding box in RPN stage. Network only classifies</cell></row><row><cell cols="4">objects into 2 classes (positive sample and negative sample) in</cell></row><row><cell cols="2">RPN stage. As shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Results on DOTA testing set (in %). * indicates validation set is also used for training, otherwise only training set is used for training. We compare our method with other state-of-the-art methods. The results are shown in Table III. Our model is trained and tested with the single-scale setting. When our model is only trained with training set ex validation set, our method significantly outperforms other methods, if validation set is also used for training, it achieves better performance. The detection results are shown in</figDesc><table><row><cell>Method</cell><cell>Ours</cell><cell>Ours *</cell><cell cols="2">FR-O [31] RoI Transformer * [5]</cell></row><row><cell>Plane</cell><cell>89.67</cell><cell>89.96</cell><cell>79.09</cell><cell>88.64</cell></row><row><cell>BD</cell><cell>76.77</cell><cell>83.62</cell><cell>69.12</cell><cell>78.52</cell></row><row><cell>Bridge</cell><cell>51.28</cell><cell>53.42</cell><cell>17.17</cell><cell>43.44</cell></row><row><cell>GTF</cell><cell>71.65</cell><cell>76.03</cell><cell>63.49</cell><cell>75.92</cell></row><row><cell>SV</cell><cell>73.11</cell><cell>74.01</cell><cell>34.2</cell><cell>68.81</cell></row><row><cell>LV</cell><cell>77.18</cell><cell>77.16</cell><cell>37.16</cell><cell>73.68</cell></row><row><cell>Ship</cell><cell>79.54</cell><cell>79.45</cell><cell>36.2</cell><cell>83.59</cell></row><row><cell>TC</cell><cell>90.79</cell><cell>90.83</cell><cell>89.19</cell><cell>90.74</cell></row><row><cell>BC</cell><cell>79.01</cell><cell>87.15</cell><cell>69.6</cell><cell>77.27</cell></row><row><cell>ST</cell><cell>84.54</cell><cell>84.51</cell><cell>58.96</cell><cell>81.46</cell></row><row><cell>SBF</cell><cell>66.51</cell><cell>67.72</cell><cell>49.4</cell><cell>58.39</cell></row><row><cell>RA</cell><cell>64.71</cell><cell>60.33</cell><cell>52.52</cell><cell>53.54</cell></row><row><cell>Harbor</cell><cell>73.97</cell><cell>74.61</cell><cell>57.79</cell><cell>62.83</cell></row><row><cell>SP</cell><cell>67.73</cell><cell>71.84</cell><cell>44.8</cell><cell>58.93</cell></row><row><cell>HC</cell><cell>58.40</cell><cell>65.55</cell><cell>46.3</cell><cell>47.67</cell></row><row><cell>mAP</cell><cell>73.66</cell><cell>75.75</cell><cell>52.93</cell><cell>69.56</cell></row><row><cell cols="5">size, nearly every object has positive samples with LIIoU, so</cell></row><row><cell cols="5">the detector can handle objects with large aspect ratios and</cell></row><row><cell cols="2">lengths well.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">4) Comparing with other state-of-the-art methods:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Task1 -Oriented Leaderboard on DOAI2019 (in %).</figDesc><table><row><cell cols="2">Team Name USTC-NELSLIP</cell><cell>pca lab</cell><cell>czh</cell><cell>AICyber</cell><cell>CSULQQ</cell><cell>peijin</cell></row><row><cell>Plane</cell><cell>89.2</cell><cell>88.2</cell><cell>89.0</cell><cell>88.4</cell><cell>87.8</cell><cell>80.9</cell></row><row><cell>BD</cell><cell>85.3</cell><cell>86.4</cell><cell>83.2</cell><cell>85.4</cell><cell>83.6</cell><cell>83.6</cell></row><row><cell>Bridge</cell><cell>57.3</cell><cell>59.4</cell><cell>54.5</cell><cell>56.7</cell><cell>56.7</cell><cell>55.1</cell></row><row><cell>GTF</cell><cell>80.9</cell><cell>80.0</cell><cell>73.8</cell><cell>74.4</cell><cell>74.4</cell><cell>70.7</cell></row><row><cell>SV</cell><cell>73.9</cell><cell>68.1</cell><cell>72.6</cell><cell>63.9</cell><cell>63.2</cell><cell>59.9</cell></row><row><cell>LV</cell><cell>81.3</cell><cell>75.6</cell><cell>80.3</cell><cell>72.7</cell><cell>71.0</cell><cell>76.4</cell></row><row><cell>Ship</cell><cell>89.5</cell><cell>87.2</cell><cell>89.3</cell><cell>87.9</cell><cell>87.8</cell><cell>88.3</cell></row><row><cell>TC</cell><cell>90.8</cell><cell>90.9</cell><cell>90.8</cell><cell>90.9</cell><cell>90.8</cell><cell>90.9</cell></row><row><cell>BC</cell><cell>85.9</cell><cell>85.3</cell><cell>84.4</cell><cell>86.3</cell><cell>84.6</cell><cell>79.2</cell></row><row><cell>ST</cell><cell>85.6</cell><cell>84.1</cell><cell>85.0</cell><cell>85.0</cell><cell>84.0</cell><cell>78.3</cell></row><row><cell>SBF</cell><cell>69.5</cell><cell>73.8</cell><cell>68.7</cell><cell>68.9</cell><cell>67.8</cell><cell>59.1</cell></row><row><cell>RA</cell><cell>76.7</cell><cell>77.5</cell><cell>75.3</cell><cell>76.0</cell><cell>75.5</cell><cell>74.8</cell></row><row><cell>Harbor</cell><cell>76.3</cell><cell>76.4</cell><cell>74.2</cell><cell>74.1</cell><cell>67.4</cell><cell>74.1</cell></row><row><cell>SP</cell><cell>76.0</cell><cell>73.7</cell><cell>74.4</cell><cell>72.9</cell><cell>71.2</cell><cell>74.9</cell></row><row><cell>HC</cell><cell>77.8</cell><cell>69.5</cell><cell>73.4</cell><cell>73.4</cell><cell>68.8</cell><cell>59.8</cell></row><row><cell>CC</cell><cell>57.3</cell><cell>49.6</cell><cell>42.1</cell><cell>37.9</cell><cell>22.5</cell><cell>39.5</cell></row><row><cell>mAP</cell><cell>78.3</cell><cell>76.6</cell><cell>75.7</cell><cell>74.7</cell><cell>72.3</cell><cell>71.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Fig. 8: Some results of our method on DOTA. The image's size is 1024 ? 1024.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning roi transformer for detecting oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fused text segmentation networks for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03272</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sliding line point regression for shape robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09969</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dssd: Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03278</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rotation-insensitive and contextaugmented object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature extraction by rotationinvariant matrix representation for object detection in aerial image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="851" to="855" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate object localization in remote sensing images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2486" to="2498" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detection of seals in remote sensing images using features extracted from deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1893" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vehicle detection in satellite images by hybrid deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and remote sensing letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1797" to="1801" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">R-cnn: Fast tiny object detection in large-scale remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep ship detector in sar images from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An anchor-free region proposal network for faster r-cnn based text detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09003</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Icpr-odai</title>
		<ptr target="https://captain-whu.github.io/ODAI/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cvpr-dota</title>
		<ptr target="https://captain-whu.github.io/DOAI2019/challenge.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">R-net: A deep network for multioriented vehicle detection in aerial images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Textmountain: Accurate scene text detection via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12786</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00726</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
