<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">J</forename><surname>Bester</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">D</forename><surname>James</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">D</forename><surname>Konidaris</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Brown University</orgName>
								<address>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method-multi-pass deep Qnetworks, or MP-DQN-to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) and deep RL in particular have demonstrated remarkable success in solving tasks that require either discrete actions, such as Atari <ref type="bibr" target="#b0">[Mnih et al., 2015]</ref>, or continuous actions, such as robot control <ref type="bibr" target="#b0">[Schulman et al., 2015;</ref><ref type="bibr" target="#b0">Lillicrap et al., 2016]</ref>. Reinforcement learning with parameterised actions <ref type="bibr" target="#b0">[Masson et al., 2016]</ref> that combine discrete actions with continuous action-parameters has recently emerged as an additional setting of interest, allowing agents to learn flexible behavior in tasks such as 2D robot soccer <ref type="bibr" target="#b0">[Hausknecht and Stone, 2016a;</ref><ref type="bibr" target="#b0">Hussein et al., 2018]</ref>, simulated human-robot interaction <ref type="bibr" target="#b0">[Khamassi et al., 2017]</ref>, and terrain-adaptive bipedal and quadrupedal locomotion <ref type="bibr">[Peng et al., 2016]</ref>.</p><p>There are two main approaches to learning with parameterised actions: alternate between optimising the discrete actions and continuous action-parameters separately <ref type="bibr" target="#b0">[Masson et al., 2016;</ref><ref type="bibr" target="#b0">Khamassi et al., 2017]</ref>, or collapse the parameterised action space into a continuous one <ref type="bibr" target="#b0">[Hausknecht and Stone, 2016a]</ref>. Both of these approaches fail to fully exploit the structure present in parameterised action problems. The former does not share information between the action and action-parameter policies, while the latter does not take into account which action-parameter is associated with which action, or even which discrete action is executed by the agent. More recently, Xiong et al. <ref type="bibr">[2018]</ref> introduced P-DQN, a method for learning behaviours directly in the parameterised action space. This leverages the distinct nature of the action space and is the current state-of-the-art algorithm on 2D robot soccer and King of Glory, a multiplayer online battle arena game. However, the formulation of the approach is flawed due to the dependence of the discrete action values on all action-parameters, not only those associated with each action. In this paper, we show how the above issue leads to suboptimal decision-making. We then introduce a novel multi-pass method to separate action-parameters, and demonstrate that the resulting algorithm-MP-DQN-outperforms existing methods on the Platform, Robot Soccer Goal, and Half Field Offense domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Parameterised action spaces <ref type="bibr" target="#b0">[Masson et al., 2016]</ref> consist of a set of discrete actions, A d = [K] = {k 1 , k 2 , ..., k K }, where each k has a corresponding continuous action-parameter x k ? X k ? R m k with dimensionality m k . This can be written as</p><formula xml:id="formula_0">A = k?[K] {a k = (k, x k )|x k ? X k }.<label>(1)</label></formula><p>We consider environments modelled as a Parameterised Action Markov Decision Process (PAMDP) <ref type="bibr" target="#b0">[Masson et al., 2016]</ref>. For a PAMDP M = (S, A, P, R, ?): S is the set of all states, A is the parameterised action space, P (s |s, k, x k ) is the Markov state transition probability function, R(s, k, x k , s ) is the reward function, and ? ? [0, 1) is the future reward discount factor. An action policy ? : S ? A maps states to actions, typically with the aim of maximising Q-values Q(s, a), which give the expected discounted return of executing action a in state s and following the current policy thereafter. The Q-PAMDP algorithm <ref type="bibr" target="#b0">[Masson et al., 2016]</ref> alternates between learning a discrete action policy with fixed actionparameters using Sarsa(?) <ref type="bibr" target="#b0">[Sutton and Barto, 1998</ref>] with the Fourier basis <ref type="bibr" target="#b0">[Konidaris et al., 2011]</ref> and optimising the continuous action-parameters using episodic Natural Actor Critic (eNAC) <ref type="bibr">[Peters and Schaal, 2008]</ref>   Note that the joint action-parameter vector x is fed into the Qnetwork.</p><p>Gradients (DDPG) algorithm <ref type="bibr" target="#b0">[Lillicrap et al., 2016]</ref> to parameterised action spaces by treating both the discrete actions and their action-parameters as a joint continuous action vector. This can be seen as relaxing the parameterised action space (Equation 1) into a continuous one:</p><formula xml:id="formula_1">A = {(f 1:K , x 1:K )|f k ? R, x k ? X k ?k ? [K]},<label>(2)</label></formula><p>where f 1 , f 2 , . . . , f K are continuous values in [?1, 1]. An -greedy or softmax policy is then used to select discrete actions. However, not only does this fail to exploit the disjoint nature of different parameterised actions, but optimising over the joint action and action-parameter space can result in premature convergence to suboptimal policies, as occurred in experiments by <ref type="bibr" target="#b0">Masson et al. [2016]</ref>. We henceforth refer to the algorithm used by Hausknecht and Stone as PA-DDPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parameterised Deep Q-Networks</head><p>Unlike previous approaches, Xiong et al. <ref type="bibr">[2018]</ref> introduce a method that operates in the parameterised action space directly by combining DQN and DDPG. Their P-DQN algorithm achieves state-of-the-art performance using a Qnetwork to approximate Q-values used for discrete action selection, in addition to providing critic gradients for an actor network that determines the continuous action-parameter values for all actions. By framing the problem as a PAMDP directly, rather than alternating between discrete and continuous action MDPs as with Q-PAMDP, or using a joint continuous action MDP as with PA-DDPG, P-DQN necessitates a change to the Bellman equation to incorporate continuous action-parameters:</p><formula xml:id="formula_2">Q(s, k, x k ) = E r,s r+? max k sup x k ?X k Q(s , k , x k ) s, k, x k .</formula><p>(3) To avoid the computationally intractable calculation of the supremum over X k , Xiong et al. <ref type="bibr">[2018]</ref> state that when the Q function is fixed, one can view argsup x k ?X k Q(s, k, x k ) as a function x Q k : S ? X k for any state s ? S and k ? [K]. This allows the Bellman equation to be rewritten as:</p><formula xml:id="formula_3">Q(s, k, x k ) = E r,s r + ? max k Q(s , k , x Q k (s )) s, k, x k .</formula><p>(4) P-DQN uses a deep neural network with parameters ? Q to represent Q(s, k, x k ; ? Q ), and a second deterministic actor network with parameters ? x to represent the action-parameter policy x k (s; ? x ) : S ? X k , an approximation of x Q k (s). With this formulation it is easy to apply the standard DQN approach of minimising the mean-squared Bellman error to update the Q-network using minibatches sampled from replay memory D <ref type="bibr" target="#b0">[Mnih et al., 2015]</ref>, replacing a with (k, x k ):</p><formula xml:id="formula_4">L Q (? Q ) = E (s,k,x k ,r,s )?D 1 2 y ? Q(s, k, x k ; ? Q ) 2 , (5) where y = r + ? max k ?[K] Q(s , k , x k (s ; ? x ); ? Q )</formula><p>is the update target derived from Equation (4). Then, the loss for the actor network in P-DQN is given by the negative sum of Q-values:  <ref type="formula">4</ref>) since each Q-value is a function of the joint action-parameter vector x = (x 1 , . . . , x K ), rather than only the action-parameter x k corresponding to the associated action:</p><formula xml:id="formula_5">L x (? x ) = E s?D ? K k=1 Q s, k, x k (s; ? x ); ? Q .<label>(</label></formula><formula xml:id="formula_6">Q(s, k, x) = E r,s r + ? max k Q(s , k , x Q (s )) s, k, x . (7)</formula><p>This in turn affects both the updates to the Q-values and the action-parameters. Firstly, we consider the effect on the action-parameter loss, specifically that each Q-value produces gradients for all action-parameters. Consider for demonstration purposes the action-parameter loss (Equation 6) over a single sample with state s:</p><formula xml:id="formula_7">L x (? x ) = ? K k=1 Q s, k, x(s; ? x ); ? Q .<label>(8)</label></formula><p>The policy gradient is then given by:  In a particular state (a), the optimal action is to run forward to be able to traverse a gap, while choosing to leap would cause the agent to fall and die. The Q-value of the leap action should change with its action-parameter, but (b) shows that varying the leap action-parameter while the others are kept fixed changes the Q-values predicted by P-DQN for all actions. Near the start of training, this can alter the discrete policy such that a suboptimal action is chosen. After 80 000 episodes, P-DQN correctly learns to choose the optimal action regardless of the unrelated leap action-parameter, although the other Q-values still vary.</p><formula xml:id="formula_8">? ?x x(s; ? x ) = ? K k=1 ? x Q s, k, x(s; ? x ); ? Q ? ?x x(s; ? x ).<label>(9)</label></formula><p>Expanding the gradients with respect to the action-parameters gives</p><formula xml:id="formula_9">? x Q = ?Q1 ?x1 + ?Q2 ?x1 + ? ? ? + ?QK ?x1 , ? ? ? , ?Q1 ?xK + ? ? ? + ?QK ?xK , (10) where Q k = Q(s, k, x(s; ? x ); ? Q ). Theoretically, if each Q- value were a function of just x k as the P-DQN formulation in- tended, then ?Q k /?xj = 0 ?k, j ? [K], j = k and ? x Q sim- plifies to: ? x Q = ?Q1 ?x1 , ?Q2 ?x2 , ? ? ? , ?Q K ?x K .<label>(11)</label></formula><p>However this is not the case in P-DQN, so the gradients with respect to other action-parameters ?Q k /?xj are not zero in general. This is a problem because each Q-value is updated only when its corresponding action is sampled, as per Equation <ref type="formula">5</ref>, and thus has no information on what effect other action-parameters x j , j = k have on transitions or how they should be updated to maximise the expected return. They therefore produce what we term false gradients. This effect may be mitigated by the summation over all Q-values in the action-parameter loss, since the gradients from each Q-value are summed and averaged over a minibatch. The dependence of Q-values on all action-parameters also negatively affects the discrete action policy. Specifically, updating the continuous action-parameter policy of any action perturbs the Q-values of all actions, not just the one associated with that action-parameter. This can lead to the relative ordering of Q-values changing, which in turn can result in suboptimal greedy action selection. We demonstrate a situation where this occurs on the Platform domain in <ref type="figure" target="#fig_3">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Pass Q-Networks</head><p>The na?ve solution to the problem of joint action-parameter inputs in P-DQN would be to split the Q-network into separate networks for each discrete action. Then, one can input only the state and relevant action-parameter x k to the network corresponding to Q k . However, this drastically increases the computational and space complexity of the algorithm due to the duplication of network parameters for each action. Furthermore, the loss of the shared feature representation between Q-values may be detrimental.</p><p>We therefore consider an alternative approach that does not involve architectural changes to the network structure of P-DQN. While separating the action-parameters in a single forward pass of a single Q-network with fully connected layers is impossible, we can do so with multiple passes. We perform a forward pass once per action k with the state s and action-parameter vector xe k as input, where e k is the standard basis vector for dimension k. Thus xe k = (0, . . . , 0, x k , 0, . . . , 0) is the joint action-parameter vector where each x j , j = k is set to zero. This causes all false gradients to be zero, ?Q k /?xj = 0, and completely negates the impact of the network weights for unassociated actionparameters x j from the input layer, making Q k only depend on x k . That is, Q (s, k, xe k ) Q (s, k, x k ) .</p><p>(12) Both problems are therefore addressed without introducing any additional neural network parameters. We refer to this as the multi-pass Q-network method, or MP-DQN.</p><p>A total of K forward passes are required to predict all Qvalues instead of one. However, we can make use of the parallel minibatch processing capabilities of artificial neural networks, provided by libraries such as PyTorch and Tensorflow, to perform this in a single parallel pass, or multi-pass. A multi-pass with K actions is processed in the same manner as a minibatch of size K:</p><formula xml:id="formula_10">? ? ? Q (s, ? , xe 1 ; ? Q ) . . . Q (s, ? , xe K ; ? Q ) ? ? ? = ? ? ? Q 11 Q 12 ? ? ? Q 1K . . . . . . . . . . . . Q K1 Q K2 ? ? ? Q KK ? ? ? ,<label>(13)</label></formula><p>where Q ij is the Q-value for action j generated on the i th pass where x i is non-zero. Only the diagonal elements Q ii are valid and used in the final output Q i ? Q ii . This process is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Compared to separate Q-networks, our multi-pass technique introduces a relatively minor amount of overhead during forward passes. Although minibatches for updates are similarly duplicated K times, backward passes to accumulate gradients are not duplicated since only the diagonal elements Q ii are used in the loss function. The computational complexity of this overhead scales linearly with the number of actions and minibatch size during updates. Unlike separate Q-networks (and even when a larger Q-network with more hidden layers and neurons is used) if the number of actions does not change, then the overhead of multi-passes would be the same as with a smaller Q-network, provided the minibatch is of a reasonable size and can be processed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare the original P-DQN algorithm with a single Q-network against our proposed multi-pass Q-network (MP-DQN), as well as against separate Q-networks (SP-DQN). We also compare against Q-PAMDP and PA-DDPG, the former state-of-the-art approaches on their respective domains. We are unable to use King of Glory as a benchmark domain as it is closed-source and proprietary.</p><p>Similar to Mnih et al. <ref type="bibr">[2015]</ref> and Hausknecht and Stone [2016a], we add target networks to P-DQN to compute the update targets y for stability. Soft updates (Polyak averaging) are used for the target networks. Adam [Kingma and Ba, 2014] with ? 1 = 0.9, ? 2 = 0.999 is used to optimise the neural network parameters for P-DQN and PA-DDPG. Layer weights are initialised following the strategy of He et al. <ref type="bibr">[2015]</ref> with rectified linear unit (ReLU) activation functions. We employ the inverting gradients approach to bound action-parameters for both algorithms, as Hausknecht and Stone [2016a] claim PA-DDPG is unable to learn without it on Half Field Offense. Action-parameters are scaled to [?1, 1], as we found this increased performance for all algorithms.</p><p>We perform a hyperparameter grid search for Platform and Robot Soccer Goal over: the network learning rates ? Q , ? x ? {10 ?1 , 10 ?2 , 10 ?3 , 10 ?4 , 10 ?5 } s.t. ? x ? ? Q ; Polyak averaging factors ? Q , ? x ? {0.1, 0.01, 0.001} s.t. ? x ? ? Q ; minibatch size B ? {32, 64, 128}; and number of hidden layers and neurons in {(256, 128), (128, 64), (256), (128)}. The hidden layers are kept symmetric between the actor and critic networks as in previous works. Each combination is tested over 5 random runs for P-DQN and PA-DDPG separately on each domain. The same hyperparameters are used for P-DQN, SP-DQN and MP-DQN.</p><p>To keep the comparison with PA-DDPG fair, we do not use dueling networks <ref type="bibr">[Wang et al., 2016]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Platform</head><p>The Platform domain <ref type="bibr" target="#b0">[Masson et al., 2016]</ref> has three actions-run, hop, and leap-each with a continuous actionparameter to control horizontal displacement. The agent has to hop over enemies and leap across gaps between platforms to reach the goal state. The agent dies if it touches an enemy or falls into a gap. A 9-dimensional state space gives the position and velocity of the agent and local enemy along with features of the current platform such as length.</p><p>We train agents on this domain for 80 000 episodes, using the same hyperparameters for Q-PAMDP as <ref type="bibr" target="#b0">Masson et al. [2016]</ref>, except we reduce the learning rate for eNAC (? eNAC ) to 0.1, and exploration noise variance (?) to 0.0001, to account for the scaled action-parameters. For P-DQN, shallow networks with one hidden layer (128) were found to perform best with ? Q = 10 ?3 , ? x = 10 ?4 , ? Q = 0.1, ? x = 0.001, and B = 128. PA-DDPG uses two hidden layers (256, 128) with ? Q = 10 ?3 , ? ? = 10 ?4 , ? Q = 0.01, ? ? = 0.01, and B = 32. A replay memory size of 10 000 samples is used for both algorithms, update gradients are clipped at 10, and ? = 0.9.</p><p>We introduce a passthrough layer to the actor networks of P-DQN and PA-DDPG to initialise their action-parameter policies to the same linear combination of state variables that <ref type="bibr" target="#b0">Masson et al. [2016]</ref> use to initialise the Q-PAMDP policy. The weights of the passthrough layer are kept fixed to avoid instability; this does not reduce the range of actionparameters available as the output of the actor network compensates before inverting gradients are applied. We use an -greedy discrete action policy with additive Ornstein-Uhlenbeck noise for action-parameter exploration, similar to Lillicrap et al. <ref type="bibr">[2016]</ref>, which we found gives slightly better performance than Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robot Soccer Goal</head><p>The Robot Soccer Goal domain <ref type="bibr" target="#b0">[Masson et al., 2016]</ref> is a simplification of RoboCup 2D <ref type="bibr" target="#b0">[Kitano et al., 1997]</ref> in which an agent has to score a goal past a keeper that tries to intercept the ball. The three parameterised actions-kick-to, shoot-goal-left, and shoot-goal-right-are all related to kicking the ball, which the agent automatically approaches between actions until close enough to kick again. The state space consists of 14 continuous features describing the position, velocity, and orientation of the agent and keeper, and the ball's position and distance to the keeper and goal.</p><p>Training consisted of 100 000 episodes, using the same hyperparameters for Q-PAMDP as Masson et al.</p><p>[2016] except we set ? eNAC = 0.06 and ? = 0.0001. P-DQN uses a single hidden layer (256), with ? Q = 10 ?3 , ? x = 10 ?5 , ? Q = 0.1, ? x = 0.001, and B = 128. Two hidden layers (128, 64) are used for PA-DDPG, with ? Q = 10 ?4 , ? ? = 10 ?5 , ? Q = 0.01, ? ? = 0.01, and B = 64. Both algorithms use a replay memory size of 20 000, ? = 0.95, gradients clipping at 1, and the same action-parameter policy initialisation as Q-PAMDP with additive Ornstein-Uhlenbeck noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Half Field Offense</head><p>The third and final domain, Half Field Offense (HFO) <ref type="bibr" target="#b0">[Hausknecht and Stone, 2016a]</ref>, is also the most complex. It has 58 state features and three parameterised actions available: dash, turn, and kick. Unlike Robot Soccer Goal, the agent must first learn to approach the ball and then kick it into the goals, although there is no keeper in this task.</p><p>We use 30 000 episodes for training on HFO. This is more than the 20 000 episodes (or roughly 3 million transitions) used by Hausknecht and Stone [2016a] and Xiong et al. <ref type="bibr">[2018]</ref> so that ample opportunity is given for the algorithms to converge in order to fairly evaluate the final policy performance. We use the same network structure as previous works with hidden layers of (256, 128, 64) neurons for P- <ref type="bibr">DQN and (1024, 512, 256, 128)</ref> neurons for PA-DDPG. The leaky ReLU activation function with negative slope 0.01 is used on HFO because of these deeper networks. Xiong et al. <ref type="bibr">[2018]</ref> use 24 asynchronous parallel workers for nstep returns on HFO. For fair comparison and due to the lack of sufficient hardware, we instead use mixed n-step return targets [Hausknecht and Stone, 2016b] with a mixing ratio of ? = 0.25 for both P-DQN and PA-DDPG, as this technique does not require multiple workers. The ? value was selected after a search over ? ? {0, 0.25, 0.5, 0.75, 1}. We otherwise use the same hyperparameters as Hausknecht and Stone [2016b] apart from the network learning rates: ? Q = 10 ?3 , ? x = 10 ?5 for P-DQN and ? Q = 10 ?3 , ? ? = 10 ?3 for PA-DDPG. In the absence of an initial actionparameter policy, we use the same -greedy with uniform random action-parameter exploration strategy as the original authors. In general we kept as many factors consistent between the two algorithms as possible for a fair comparison.</p><p>We select 10 of the most relevant state features for Q-PAMDP to avoid intractable Fourier basis calculations. These features include: player orientation, stamina, proximity to ball, ball angle, ball-kickable, goal centre position, and goal centre proximity. Even with this reduced selection, we  respectively-and asynchronous P-DQN uses 24 parallel workers to implement n-step returns rather than the mixing strategy we use.</p><p>found at most a Fourier basis of order 2 could be used. We use an adaptive step-size [Dabney and <ref type="bibr" target="#b0">Barto, 2012]</ref> for Sarsa(?) with an eNAC learning rate of 0.2. The Q-PAMDP agent initially learns with Sarsa(?) for a period of 1000 episodes before alternating between ? = 50 eNAC updates of 25 rollouts each, and 1000 episodes of discrete action re-exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The resulting learning curves of MP-DQN, SP-DQN, P-DQN, PA-DDPG, and Q-PAMDP on the three parameterised action benchmark domains are shown in <ref type="figure" target="#fig_5">Figure 4</ref>, with mean evaluation scores detailed in <ref type="table">Table 1</ref>.</p><p>Our results show that MP-DQN learns significantly faster than baseline P-DQN with joint action-parameter inputs and achieves the highest mean evaluation scores across all three domains. SP-DQN similarly shows better performance than P-DQN on Platform and Robot Soccer Goal but to a slightly lesser extent than MP-DQN. Notably, SP-DQN exhibits fast initial learning on HFO but plateaus at a lower performance level than P-DQN. This is likely due to the aforementioned lack of a shared feature representation between the separate Q-networks and the duplicate network parameters which require more updates to optimise.</p><p>In general, we observe that P-DQN and its variants outperform Q-PAMDP on Platform and Robot Soccer Goal, while PA-DDPG consistently converges prematurely to suboptimal policies. Wei et al. <ref type="bibr">[2018]</ref> observe similar behaviour for PA-DDPG on Platform. This highlights the problem with updating the action and action-parameter policies simultaneously and was also observed when using eNAC for direct policy search on Platform <ref type="bibr" target="#b0">[Masson et al., 2016]</ref>. On HFO, Q-PAMDP fails to learn to score any goals-likely due to its reduced feature space and use of linear function approximation rather than neural networks. Unexpectedly, baseline P-DQN appears to learn slower than PA-DDPG on HFO. This suggests that the dueling networks and asynchronous parallel workers used by Xiong et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Many recent deep RL approaches follow the strategy of collapsing the parameterised action space into a continuous one. Hussein et al.</p><p>[2018] present a deep imitation learning approach for scoring goals on HFO using long-short-termmemory networks with a joint action and action-parameter policy. <ref type="bibr" target="#b0">Agarwal [2018]</ref> introduces skills for multi-goal parameterised action space environments to achieve multiple related goals; they demonstrate success on robotic manipulation tasks by combining PA-DDPG with hindsight experience replay and their skill library.</p><p>One can alternatively view parameterised actions as a 2level hierarchy: Klimek et al. <ref type="bibr">[2017]</ref> use this approach to learn a reach-and-grip task using a single network to represent a distribution over macro (discrete) actions and their lower-level action-parameters. The work most relevant to this paper is by Wei et al. <ref type="bibr">[2018]</ref>, who introduce a parameterised action version of TRPO (PATRPO). They also take a hierarchical approach but instead condition the action-parameter policy on the discrete action chosen to avoid predicting all action-parameters at once. While their preliminary results show the method achieves good performance on Platform, we omit comparison with PATRPO as it fails to learn to score goals on HFO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We identified a significant problem with the P-DQN algorithm for parametrised action spaces: the dependence of its Q-values on all action-parameters causes false gradients and can lead to suboptimal action selection. We introduced a new algorithm, MP-DQN, with separate action-parameter inputs which demonstrated superior performance over P-DQN and former state-of-the-art techniques Q-PAMDP and PA-DDPG. We also found that PA-DDPG was unstable and converged to suboptimal policies on some domains. Our results suggest that future approaches should leverage the disjoint nature of parameterised action spaces and avoid simultaneous optimisation of the policies for discrete actions and continuous action-parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The P-DQN network architecture[Xiong et al., 2018].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>6)Although this choice of loss function was not motivated byXiong et al. [2018], it resembles the deterministic policy gradient loss used by PA-DDPG where a scalar critic value is used over all action-parameters<ref type="bibr" target="#b0">[Hausknecht and Stone, 2016a]</ref>. During updates, the estimated Q-values are backpropagated through the critic to the actor, producing gradients indicating how the action-parameters should be updated to increase the Q-values.3 Problems with Joint Action-ParametersThe P-DQN architecture inputs the joint action-parameter vector over all actions to the Q-network, as illustrated inFigure 1. This was pointed out by Xiong et al.[2018] but they did not discuss it further. While this may seem like an inconsequential implementation detail, it changes the formulation of the Bellman equation used for parameterised actions (Equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Predicted Q-values versus leap action-parameter. Vertical lines indicate the leap value actually chosen. Ideally, the run and hop Q-values should remain constant since their actionparameters are fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Example of dependence on unrelated action-parameters affecting discrete action selection on the Platform domain. Three parameterised actions are available: run, hop, and leap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the multi-pass Q-network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves on Platform (a), Robot Soccer Goal (b), and Half Field Offense (c). The running average scores-episodic return for Platform and HFO, and goal scoring probability for Robot Soccer Goal-are smoothed over 5000 episodes and include random exploration, higher is better. Shaded areas represent standard error of the running averages over the different agents. The oscillating behaviour of Q-PAMDP on Platform is a result of re-exploration between the alternating optimisation steps. MP-DQN clearly performs best overall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[2018]  were major factors improving P-DQN in their comparisons.2 Average over 7 runs [Hausknecht and Stone, 2016a]. 3 Average over 9 runs with 24 workers [Xiong et al., 2018].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>nor asynchronous parallel workers as Xiong et al. [2018] used for P-DQN. For each algorithm and domain, we train 30 agents with unique random seeds and evaluate them without exploration for another 1000 episodes. Our experiments are implemented in Python using PyTorch [Paszke et al., 2017] and OpenAI Gym [Brockman et al., 2016], and run on the following hardware: Intel Core i7-7700, 16GB DRAM, NVidia GTX 1060 GPU. Complete source code is available online. 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mean evaluation scores over 30 random runs for each algorithm, averaged over 1000 episodes after training with no random exploration. We include previously published results from Hausknecht and Stone [2016a] and Xiong et al. [2018] on HFO, although they are not directly comparable with ours as we use a longer training period and have a much larger sample size of agents-30 versus 7 and 9</figDesc><table><row><cell></cell><cell>Platform</cell><cell>Robot Soccer Goal</cell><cell cols="2">Half Field Offense</cell></row><row><cell></cell><cell>Return</cell><cell>P(Goal)</cell><cell>P(Goal)</cell><cell>Avg. Steps to Goal</cell></row><row><cell>Q-PAMDP</cell><cell>0.789 ? 0.188</cell><cell>0.452 ? 0.093</cell><cell>0 ? 0</cell><cell>n/a</cell></row><row><cell>PA-DDPG</cell><cell>0.284 ? 0.061</cell><cell>0.006 ? 0.020</cell><cell>0.875 ? 0.182</cell><cell>95 ? 7</cell></row><row><cell>P-DQN</cell><cell>0.964 ? 0.068</cell><cell>0.701 ? 0.078</cell><cell>0.883 ? 0.085</cell><cell>111 ? 11</cell></row><row><cell>SP-DQN</cell><cell>0.941 ? 0.164</cell><cell>0.752 ? 0.131</cell><cell>0.718 ? 0.131</cell><cell>99 ? 7</cell></row><row><cell>MP-DQN</cell><cell>0.987 ? 0.039</cell><cell>0.789 ? 0.070</cell><cell>0.913 ? 0.070</cell><cell>99 ? 12</cell></row><row><cell>PA-DDPG 2</cell><cell>-</cell><cell>-</cell><cell>0.923 ? 0.073</cell><cell>112 ? 5</cell></row><row><cell>Async. P-DQN 3</cell><cell>-</cell><cell>-</cell><cell>0.989 ? 0.006</cell><cell>81 ? 3</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/cycraig/MP-DQN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is based on the research supported in part by the National Research Foundation of South Africa (Grant Number: 113737).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<idno>arXiv:1810.06394</idno>
	</analytic>
	<monogr>
		<title level="m">Deep reinforcement learning with skill library: Exploring with temporal abstractions and coarse approximate dynamics models. Master&apos;s thesis</title>
		<editor>Xiong, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu, and Han Liu</editor>
		<meeting><address><addrLine>Pittsburgh, PA; Ba; Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra; Cambridge, MA, USA; Tom Schaul</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University ; Sutton and Barto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI Spring Symposium Series. Xiong et al.. Parametrized deep Q-networks learning: Reinforcement learning with discrete-continuous hybrid action space</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
