<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Knowledge Distillation with Progressive Refinement of Targets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungyul</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LG CNS AI Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongmoon</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LG CNS AI Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyoung</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LG CNS AI Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangheum</forename><surname>Hwang</surname></persName>
							<email>shwang@seoultech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University of Science and Technology</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Knowledge Distillation with Progressive Refinement of Targets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model's own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent progress made in deep neural networks (DNNs) has significantly improved performance in various tasks related to computer vision as well as natural language processing, e.g., image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">38]</ref>, object detection / segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">33]</ref>, machine translation <ref type="bibr" target="#b42">[42]</ref> and language modeling <ref type="bibr" target="#b20">[21]</ref>. Scaling up of DNN is widely * Corresponding author adopted as a promising strategy to achieve higher performances <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">40]</ref>. However, deeper networks require a large number of model parameters that need to be learned, which could make models more prone to overfitting. Thus, DNNs typically produce overconfident predictions even for incorrect predictions, and this is because the predictions are highly miscalibrated <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>To improve generalization performance and training efficiency of DNNs, a number of regularization methods have been proposed. The widely employed methods in practice include: L 1 -and L 2 -weight decay <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> to restrict the function space, dropout <ref type="bibr" target="#b37">[37]</ref> to inject randomness during training, batch normalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">34]</ref> to accelerate training speed by normalizing internal activations in every layer. There also have been several methods that are specifically designed for a particular task. For example, advanced data augmentation techniques that are specific to computer vision tasks such as Cutout <ref type="bibr" target="#b5">[6]</ref>, Mixup <ref type="bibr" target="#b52">[52]</ref>, AugMix <ref type="bibr" target="#b16">[17]</ref> and CutMix <ref type="bibr" target="#b50">[50]</ref> have shown to boost classification accuracy and also improve robustness and uncertainty of a model. Another effective regularization method is to adjust the targets when they are given in the form of one-hot coded vectors (i.e., hard targets), including label smoothing (LS) <ref type="bibr" target="#b39">[39]</ref>, label perturbation <ref type="bibr" target="#b44">[44]</ref>, etc.</p><p>Among those methods about adjusting targets, LS <ref type="bibr" target="#b39">[39]</ref> has been widely applied to many applications <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref> and has shown to improve generalization performance as well as the quality of confidence estimates (in terms of calibration) on image classification and machine translation tasks <ref type="bibr" target="#b25">[26]</ref>. LS softens a hard target as a smoothed distribution by assigning a small amount of probability mass to non-target classes. However, it is also empirically confirmed that it is not complementary to current advanced regularization techniques. For example, if we utilize LS and CutMix simultaneously for image classification, the performance on both classification and confidence estimation is substantially degraded <ref type="bibr" target="#b2">[3]</ref>.</p><p>One natural question raised on LS could be: is there a more effective strategy to soften hard targets so as to obtain more informative labels? To answer this question, we propose a simple regularization technique named progressive self-knowledge distillation (PS-KD) that distills the knowledge in a model itself and uses it for training the model. It means that a student model becomes a teacher model itself, which gradually utilizes its own knowledge for softening hard targets to be more informative during training. Specifically, the model is trained with the soft targets which are computed as a linear combination of the hard targets and the past predictions at a certain epoch, which are adjusted adaptively as training proceeds.</p><p>To justify our proposed method, we have shown that PS-KD gives more weights to hard-to-learn examples by a gradient rescaling scheme during training, which clearly reveals that a student model can be enhanced even if trained with a teacher worse than the student (e.g., past predictions). The proposed method is easy to implement, can be applied to any supervised learning tasks where the hard targets are given as the ground-truth labels. Moreover, it can be easily combined with current advanced regularization techniques, thereby enhancing further their generalization performance. With this simple method, the generalization ability of DNNs can be greatly improved regarding the target metrics (e.g., accuracy) as well as the quality of confidence estimates in terms of both calibration and ordinal ranking (i.e., misclassification detection) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To rigorously evaluate the advantages of PS-KD, we conduct extensive experiments on diverse tasks with popular benchmark datasets: image classification on CIFAR-100 and ImageNet, object detection on PASCAL VOC, and machine translation on IWSLT15 and Multi30k. The experimental results demonstrate that training with PS-KD further enhances the generalization performance of the stateof-the-art baselines. For image classification on CIFAR-100 and ImageNet, our results show that the model trained with PS-KD provides not only better predictive performance, but also high quality of confidence estimates. We further confirm that the advanced image augmentation techniques such as Cutout and CutMix also benefit from PS-KD. From the evaluation on object detection with PASCAL VOC, it is observed that PS-KD makes a model learn better representa-tions compared to the existing approaches. To show the wide applicability of PS-KD, we also conduct the experiments on machine translation, which improve BLEU scores of baselines significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional Knowledge Distillation (KD) <ref type="bibr" target="#b17">[18]</ref> methods use knowledge from a larger and better performing teacher model to generate soft targets for a student network. Recently, several works have tried to use the student network itself as a teacher, named self-knowledge distillation (self-KD) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>One approach to self-KD is to reduce the feature distance between similar inputs from a single network. Xu and Liu <ref type="bibr" target="#b46">[46]</ref> propose a mechanism based on image distortion. Given an image, it generates two separate distorted images by using random mirroring and cropping. Then, model is trained to decrease the distance between features extracted from those two images. Yun et al. <ref type="bibr" target="#b51">[51]</ref> propose a method called class-wise self-KD (CS-KD) which focuses on distilling knowledge between samples in the same class. For an input x, another data x ? with the same label is randomly sampled, and the KL divergence between predictive distributions from those is minimized during training. However, this approach may cause overfitting since it forces all samples in a specific class to have a high density in the learned representation space.</p><p>Another approach is to directly use outputs from a teacher whose architecture is exactly the same as a student. This approach is closer to the original notion of KD than the aforementioned approach. Born-Again Networks (BANs) <ref type="bibr" target="#b8">[9]</ref> first train a network and use this pretrained network as a teacher for next generation. Then, it repeats this process, and thereby, performs multiple generations of KD where the k-th generation model is trained with knowledge distilled from the (k ? 1)-th model. Similar to BANs, Yuan et al. <ref type="bibr" target="#b49">[49]</ref> empirically examine the common belief on KD and suggest the teacher-free KD (TF-KD) method, which uses a pretrained student as a teacher for a single generation. Zhang et al. <ref type="bibr" target="#b53">[53]</ref> propose a method which di-vides a network into several blocks and attaches auxiliary classifiers to them independently. These auxiliary classifiers are trained using outputs of the final classifier as a teacher's knowledge. Yang et al. <ref type="bibr" target="#b48">[48]</ref> introduce utilizing a snapshot model at a certain epoch as a teacher: the whole training process is split into multiple mini-generations and the last snapshot model of each mini-generation is used as a teacher for the next mini-generation. These methods require augmenting a network's architecture <ref type="bibr" target="#b53">[53]</ref> or several hyperparameters to be carefully tuned, e.g., a learning rate policy and the number of mini-generation <ref type="bibr" target="#b48">[48]</ref>.</p><p>Along with previous studies, our proposed PS-KD method uses a model's own predictions as a teacher's knowledge to enhance the generalization performance of DNNs. However, our method provides distinct advantages over them from a practical viewpoint: it does not require a pretraining phase unlike BANs <ref type="bibr" target="#b8">[9]</ref> and TF-KD <ref type="bibr" target="#b49">[49]</ref>, and can be easily applied to any supervised learning tasks due to its generality and simplicity compared to <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b48">48]</ref>. More importantly, it is shown that we can enjoy the advantages of KD even if a student is taught by a poor teacher (i.e., lower predictive performance than a student), which is distinguishable from previous works relying on a wellperforming teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Knowledge Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Knowledge Distillation as Softening Targets</head><p>KD is a technique to transfer knowledge from one model (i.e., a teacher) to another (i.e., a student), usually from a larger model to a smaller one. The student learns from more informative sources, the predictive probabilities from the teacher, besides one-hot labels. Hence, it can attain a similar performance compared to the teacher although it is usually a much smaller model, and show comparable or even better performance when the student has the same capacity as the teacher <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">49]</ref>.</p><p>For an input x and a K-dimensional one-hot target y, a model produces the logit vector z(x) = [z 1 (x), ? ? ? , z K (x)], and then outputs the predicted probabilities P (x) = [p 1 (x), ? ? ? , p K (x)] by a softmax function. Hinton et al. <ref type="bibr" target="#b17">[18]</ref> suggest to utilize temperature scaling to soften these probabilities for better distillation:</p><formula xml:id="formula_0">p i (x; ? ) = exp(z i (x)/? ) j exp(z j (x)/? )<label>(1)</label></formula><p>where ? denotes a temperature parameter. By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:</p><formula xml:id="formula_1">L KD (x, y) =(1 ? ?)H y, P S (x) + ?? 2 H P T (x; ? ), P S (x; ? )<label>(2)</label></formula><p>where H is a cross-entropy loss and ? is a hyperparameter. Note that when the temperature ? is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):</p><formula xml:id="formula_2">L KD (x, y) = H (1 ? ?)y + ?P T (x), P S (x) . (3)</formula><p>Therefore, the existing methods that use the soft targets for regularization can be interpreted within the framework of knowledge distillation. For example, LS <ref type="bibr" target="#b25">[26]</ref> is equivalent to distilling the knowledge from the teacher which produces uniformly distributed probabilities on any inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distilling Knowledge from the Past Predictions</head><p>We propose a new way of KD, called progressive selfknowledge distillation (PS-KD), which distills the knowledge of itself to enhance the generalization capability. In other words, the student becomes the teacher itself, and utilizes its past predictions to have more informative supervisions during training as can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. Let P S t (x) be the prediction about x from the student at t-th epoch. Then, our objective at t-th epoch can be written as:</p><formula xml:id="formula_3">L KD,t (x, y) = H (1 ? ?)y + ?P S i&lt;t (x), P S t (x) .</formula><p>(4) Note that using the predictions from t-th epoch as the teacher's knowledge is trivial since it will not incur any loss.</p><p>The main difference from the conventional KD is that the teacher is not a static model, but dynamically evolves as training proceeds. Among all past models that are candidates for the teacher, we use the model at (t?1)-th epoch as the teacher since it can provide most valuable information among the candidates. Concretely, in t-th epoch of training, the target for the input x is softened as (1??)y+?P S t?1 (x). It is empirically observed that this approach utilizing the past model as a teacher regularizes the model effectively.</p><p>One more thing we have to consider is how to determine ? in Eq. (4). The ? controls how much we are going to trust the knowledge from the teacher. In the conventional KD, the teacher remains unchanged so the ? is usually set to a fixed value during training. However, in PS-KD, the reliability of the teacher should be considered since the model generally does not have enough knowledge about data at the early stage of training. To this end, we increase the value of ? gradually. Like the learning rate scheduling, there are several strategies to increase the ? as a function of the epoch, e.g., step-wise, exponential, linear growth, etc. To minimize the number of hyperparameters involved in the scheduling, we apply the linear growth approach. Therefore, the ? at t-th epoch is computed as follows:</p><formula xml:id="formula_4">? t = ? T ? t T ,<label>(5)</label></formula><p>where T is the total epoch for training and ? T is the ? at the last epoch, which is a single hyperparameter to be determined via validation process. Surprisingly, this simple strategy combined with past predictions improves the generalization performance significantly across a wide range of tasks. To summarize, our objective function at t-th epoch can be written as:</p><formula xml:id="formula_5">L KD,t (x, y) = H (1 ? ? t )y + ? t P S t?1 (x), P S t (x) .<label>(6)</label></formula><p>Theoretical support. We show that PS-KD pays more attention to hard examples during training when a hyperparameter ? t is properly set, and the ? t value should be gradually increased to preserve such an effect of hard example mining. This effect is realized by example re-weighting, motivated by Proposition 2 in Tang et al. <ref type="bibr" target="#b41">[41]</ref>. The gradient of L KD,t in Eq. 6 with respect to a logit value z i , i = 1, ..., K for a fixed ? 1 is given by</p><formula xml:id="formula_6">?L KD,t ?z i = ? KD,t i = (1 ? ?)(p t,i ? y i ) + ?(p t,i ? p t?1,i ).</formula><p>(7) Therefore, for z GT where GT denotes the target class,</p><formula xml:id="formula_7">? KD,t GT = (1 ? ?)(p t,GT ? 1) + ?(p t,GT ? p t?1,GT ) = (p t,GT ? 1) ? ?(p t?1,GT ? 1),<label>(8)</label></formula><p>and</p><formula xml:id="formula_8">for z i , i ? = GT , ? KD,t i = (1 ? ?)p t,i + ?(p t,i ? p t?1,i ) = p t,i ? ?p t?1,i (9) If ? is set to be p t,i ? ?p t?1,i ? 0 for all i ? = GT , i.e., ? ? min i (p t,i /p t?1,i ), then (p t,GT ? 1) ? ?(p t?1,GT ? 1) &lt; 0 and i? =GT |p t,i ??p t?1,i | = (1?p t,GT )??(1?p t?1,GT ) holds. Therefore, L 1 norm i |? KD,t i</formula><p>| of the gradient can be written as:</p><formula xml:id="formula_9">i |? KD,t i | = 2(1 ? p t,GT ) ? 2?(1 ? p t?1,GT ). (10)</formula><p>Let us consider the ratio of L 1 norms i |? KD,t i |/ i |? i |, which represents the gradient rescaling factor induced by PS-KD. By combining Eq. 10 and the fact that i |? i | = 2(1 ? p t,GT ), the rescaling factor is given by:</p><formula xml:id="formula_10">i |? KD,t i | i |? i | = 1 ? ? 1 ? p t?1,GT 1 ? p t,GT ? 1 ? ? ? t?1 ? t .</formula><p>(11) Note that ? represents the probability of being incorrect. Without loss of generality, it can be assumed that p t,GT ? p t?1,GT and p t,i ? p t?1,i for all i ? = GT since P S t provides better prediction than P S t?1 on training data. Therefore, ? t?1 ? ? t always holds. A large ?t?1 ?t means that <ref type="bibr" target="#b0">1</ref> For notational simplicity, we ignore t in ?t. predictions on that example are greatly improved during iterations (i.e., easy-to-learn). Conversely, hard-to-learn examples will have a small value. Consequently, the gradient rescaling factors for hard-to-learn examples are greater than those for easy-to-learn examples, which implies that PS-KD gives more weights on hard-to-learn examples during training, and it is empirically confirmed as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> shows that other methods are more overconfident on incorrect predictions than PS-KD, and clearly demonstrates that PS-KD focuses on hard examples implicitly by the gradient rescaling scheme. To expect such hard example mining effects, ? should satisfy the condition described above for all examples. From this, ? should be set to a sufficiently small value during an early training phase. As training proceeds, the differences of ?t?1 ?t 's for all examples become smaller. Therefore, ? should be gradually increased to preserve the effect.</p><p>Implementation. For PS-KD, the predictions from the model at (t ? 1)-th epoch are necessary for training at t-th epoch. There are two ways to obtain these past predictions. One is to load the model at (t ? 1)-th epoch on memory when t-th epoch is started so that the past predictions for softening targets are also computed in forward passes. The other is to save the past predictions on disk in advance during (t ? 1)-th epoch, and read this information to compute the soft targets at t-th epoch. These two approaches have pros and cons. The former way may need more GPU memory. On the other hand, the latter way does not need additional GPU memory but requires more space to store past predictions.</p><p>The choice of how to obtain the past predictions depends on the task we are dealing with. For example, on machine translation with a large-scale corpus, it is nearly impossible to store the predicted probabilities over all tokens. For this, we can choose the former strategy. Note that softening targets via a moving average <ref type="bibr" target="#b0">[1]</ref> or distilling knowledge with task-specific operations <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b51">51]</ref> is not applicable to this task. In our experiments, we employ an efficient way according to the task, e.g., the past predictions from the model on GPU memory is utilized for the tasks on Ima-geNet classification and IWSLT15 machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we show the effectiveness of PS-KD across a variety of tasks including image classification, object detection, and machine translation. More details on datasets, evaluation metrics and experimental settings are available in the supplementary material. All experiments were performed on NVIDIA DGX-1 system with Py-Torch <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR-100 Classification</head><p>On CIFAR-100 classification, we consider four CNN models: ResNet <ref type="bibr" target="#b14">[15]</ref>, ResNeXt <ref type="bibr" target="#b45">[45]</ref>, DenseNet <ref type="bibr" target="#b18">[19]</ref>, and PyramidNet <ref type="bibr" target="#b12">[13]</ref>. First, we compare PS-KD with LS and recent self-KD methods, CS-KD <ref type="bibr" target="#b51">[51]</ref> and TF-KD <ref type="bibr" target="#b49">[49]</ref>, on the architectures we consider. Then, we demonstrate that PS-KD is complementary to the existing regularization methods including data augmentation (e.g. Cutout <ref type="bibr" target="#b5">[6]</ref>, Cut-Mix <ref type="bibr" target="#b50">[50]</ref>, etc.), and ensembles.</p><p>Experimental settings. The detailed architectures we consider are PreAct ResNet-18 <ref type="bibr" target="#b15">[16]</ref>, ResNet-101 <ref type="bibr" target="#b14">[15]</ref>, ResNeXt-29 (cardinality=8, width=64) <ref type="bibr" target="#b45">[45]</ref>, DenseNet-121 (growth rate=32) <ref type="bibr" target="#b18">[19]</ref> and PyramidNet-200 (widening fac-tor=240) <ref type="bibr" target="#b12">[13]</ref>. We follow standard data augmentation schemes: 32?32 random crop after padding with 4 pixels and random horizontal flip. All CNNs are trained using SGD with a momentum of 0.9 for 300 epochs, and the learning rate is decayed by a factor of 10 at 150 and 225 epochs. For ResNet, ResNeXt, and DenseNet, we set the mini-batch size, a weight decay, and an initial learning rate to 128, 0.0005, and 0.1, respectively. For PyramidNet, the minibatch size, a weight decay, and an initial learning rate are set to 64, 0.0001, and 0.25, respectively, following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>To compare the performance of PS-KD with LS, CS-KD 2 and TF-KD 3 , the hyperparameters are set according to those reported in the corresponding studies. Our PS-KD has only a single hyperparameter ? T . To determine the optimal ? T , we use randomly sampled 10% of training data as a validation dataset. In this experiment, we set the ? T to 0.8 which shows the optimal validation performance in terms of accuracy and calibration on ResNet-18. <ref type="bibr" target="#b3">4</ref> With this parameter value, we then train a model on the entire training dataset for a fair comparison. Note that the value tuned for ResNet-18 is also used for other all architectures since we expect that PS-KD is fairly robust to the hyperparameter ? T , and it is confirmed from the experimental results.</p><p>2 https://github.com/alinlab/cs-kd 3 https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation <ref type="bibr" target="#b3">4</ref> The hyperparameters of each method and the ablation study on ? T are provided in the supplementary material A.4. For existing regularization methods to be combined with PS-KD, we also follow the hyperparameter values reported in the literature, for example, the hole size in Cutout is set to 8 and the parameter ? of Beta distribution in CutMix is set to 1. Cutout and CutMix produce randomly synthesized images from two inputs at every iteration. In this case, applying PS-KD with them at the same time is not straightforward. Therefore, for the experiments where PS-KD is combined with Cutout or CutMix, each data selects the regularization method with a probability of 0.5. Simply, PS-KD is applied to the half of the data in a randomly shuffled mini-batch, and Cutout or CutMix is performed on another half of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model + Method</head><p>Evaluation metrics. We use top-1 and top-5 error as standard performance measures for multi-class classification. We also employ the negative log-likelihood (NLL), expected calibration error (ECE) <ref type="bibr" target="#b29">[30]</ref> and the area under the risk-coverage curve (AURC) <ref type="bibr" target="#b9">[10]</ref> to evaluate the quality of predictive probabilities in terms of confidence estimation. ECE is a widely used metric to determine whether a model's predictions are well-calibrated, approximating the difference in expectation between classification accuracy and confidence estimates. AURC measures the area under the curve from plotting the risk (i.e., error rate) according to coverage. A low AURC implies that correct and incorrect predictions can be well-separable based on confidence estimates. In these experiments, the maximum class probability is used as a confidence estimator.</p><p>Result. The comparison results are summarized in Table 1. Note that we report average values over three runs. First, we observe that training with PS-KD performs better than baseline and LS in terms of classification accuracy across all architectures, e.g., an improvement of 1.37% and 0.32% from baseline and LS on ResNeXt, respectively. Compared with CS-KD and TF-KD, PS-KD also shows better accuracy while significantly improving the performance on confidence estimation, for example, it improves accuracy by 1.74% and 1.15% from CS-KD and TF-KD on DenseNet-121, respectively, and it reduces ECE by 12.87% and 8.65% from CS-KD and TF-KD on PyramidNet, respectively. The performance improvement on confidence estimation is consistently observed across all metrics (i.e., NLL, ECE, and AURC) except for the two cases, ECE on ResNet-101 and ResNeXt. <ref type="bibr" target="#b4">5</ref> Nevertheless, PS-KD provides robust calibration performance on all architectures, e.g., less than 10% in ECE, as can be seen from the calibration plots in <ref type="figure" target="#fig_3">Fig. 3</ref>. Interestingly, ResNeXt provides well-calibrated predictions in terms of ECE, and it would be worth investigating which architectural factors of CNNs affect calibration performance.</p><p>From the results, it is confirmed that PS-KD is the only method which shows consistent and robust performance improvement across all metrics. Importantly, CS-KD performs worse than LS or even baseline in some cases from our experiments. We suspect that these results are caused by the implicit property of CS-KD, which pulls all samples in the same class each other, and it may accelerate overfitting when a model has high capacity or hyperparameters are not properly tuned.</p><p>To show that PS-KD can be used in conjunction with other advanced regularization methods, we present the detailed experimental results on PyramidNet in <ref type="table" target="#tab_1">Table 2</ref>. Compared with Cutout and CutMix on PyramidNet, PS-KD shows slightly higher accuracy while significantly improving performances on confidence estimation. We observe the top-1 error of 14.82% when Cutout is combined with PS-KD, which is 1.23% improvement of Cutout. When PS-KD, CutMix, and SD are utilized simultaneously, the top-1 error from the combination of CutMix and SD is reduced by 0.48%. In this setting, it is confirmed again that PS-KD provides a positive effect on confidence estimation: all metrics, NLL, ECE, and AURC, are improved by PS-KD. More experimental results on other self-KD methods <ref type="bibr" target="#b4">5</ref> When ? T is tuned on ResNeXt-29, we observed that most metrics are further improved: for ? T = 0.7, Top-1=17.06%, Top-5=3.68%, NLL=0.69, ECE=6.3%, and AURC=38.64.  are presented in the supplementary material A.4. These results demonstrate that current state-of-the-art regularization methods benefit from PS-KD in terms of not only classification accuracy, but also confidence estimation. From the previous study <ref type="bibr" target="#b2">[3]</ref>, it is known that LS might be harmful to generalization performance when applied concurrently with the advanced methods. Our empirical findings reveal that how to soften the hard targets is important and the distilled knowledge from a model itself can be a good source to create more informative targets. We also examine the ensemble effects of PS-KD. For ensembling, we utilize the three trained models from the previous experiment. <ref type="table" target="#tab_2">Table 3</ref> shows the performances of ensembles from baseline, CS-KD, TF-KD, and PS-KD models. From the results, it is shown that ensembles can also benefit from PS-KD, which implies that PS-KD does not degrade the diversity of independently trained models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model + Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ImageNet Classification</head><p>In the case of a large-scale dataset like ImageNet <ref type="bibr" target="#b4">[5]</ref>, the knowledge (i.e., predictions) from the previous snapshot model at (t ? 1)-th epoch might be too outdated since the model at t-th epoch further learns from a large number of samples during a single epoch. Nevertheless, we observe that the model benefits from PS-KD even for such a largescale dataset.</p><p>Experimental settings. As a baseline, we train PS-KD using ResNet (depth=152) with standard data augmentation schemes including random resize cropping, random horizontal flip, color jittering, and lighting, following <ref type="bibr" target="#b50">[50]</ref>. We train ResNet-152 for 90 epochs with a weight decay of 0.0001 and an initial learning rate of 0.1, followed by decaying the learning rate by a factor of 10 at 30 and 60 epochs. We employ SGD with a momentum of 0.9 as an optimizer and set the mini-batch size to 256. For LS, we set the hyperparameter ? as 0.1. Since the optimal hyperparameters of CS-KD and TF-KD on ImageNet are not reported in the literature, we conduct a random search over five runs to compare in a fair setting. 6 <ref type="bibr" target="#b5">6</ref> All results can be found in the supplementary material A.5. Result. <ref type="table" target="#tab_3">Table 4</ref> shows performances evaluated by the metrics used in the previous section. Our method shows better accuracy than LS and other self-KD methods, achieving a top-1 error of 21.41%. Also, PS-KD achieves better performance on confidence estimation, i.e., it reduces ECE by 3.28% for CS-KD and 2.19% from TF-KD, respectively. From our validation results provided in the supplementary material A.5, we observe that CS-KD is sensitive to the hyperparameters while PD-KD is much more robust to the hyperparameter ? T . Additionally, PS-KD is further improved on all metrics when combined with CutMix, especially in terms of ECE. It is consistent with the results on CIFAR-100, which demonstrates that PS-KD provides additional benefits to the existing regularization methods. On the other hand, other self-KD methods in conjunction with CutMix are even worse than the vanilla CutMix. We expect that the performance improvement can be greater if the knowledge from the recent past model is utilized, for example, the predictions from the model at (t ? 0.5)-th epoch.</p><p>Examples of how our PS-KD improves the quality of predicted probability are shown in <ref type="figure" target="#fig_4">Fig. 4</ref> (see the supple-mentary material for more examples). For the top image whose label is "bulletproof vest", both baseline and PS-KD produce an incorrect prediction. However, PS-KD outputs the class probabilities distributed over the classes that have similar visual characteristics while the baseline outputs overconfident prediction on non-target class. The bottom image is labeled as "stove" while containing multiple objects including "coffee pot" and "stove". Both baseline and PS-KD correctly classify this image, however, PS-KD also produces a high probability on "Dutch oven" that is visually similar to the objects in the image. These quantitative and qualitative results support the advantage of PS-KD which acts as an effective and strong regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection</head><p>We also examine that other visual recognition tasks can benefit from PS-KD. For this, we perform the experiment on the task of object detection using PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> dataset. We use the 5k VOC 2007 trainval and 15k VOC 2012 trainval as training sets, and use the PASCAL VOC 2007 test as a test set, following <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b50">50]</ref>. As a baseline, Faster R-CNN <ref type="bibr" target="#b33">[33]</ref> is considered, and the improvement of detection performance is examined by replacing the original VGG-16 <ref type="bibr" target="#b35">[35]</ref> backbone network with a ResNet-152 trained on ImageNet. We utilize six different backbones trained for the previous section: ResNet-152, ResNet-152 with LS, PS-KD, CS-KD, TF-KD, and PS-KD+CutMix. We then fine-tune Faster R-CNN with each backbone network for 10 epochs with a mini-batch size of 1, an initial learning rate of 0.001 decayed by a factor of 10 at 5 epochs. As shown in <ref type="table">Table 5</ref>, ResNet-152 with PS-KD significantly improves the detection performance by 1.06%, 1.17%, and 1.22% of the mean average precision (mAP) compared to ResNet-152 with LS, CS-KD, and TF-KD, respectively. <ref type="bibr" target="#b6">7</ref> Furthermore, PS-KD shows better mAP when it is combined with CutMix. Note that this improvement is achieved by just replacing the backbone network. From this result, it is verified that training with PS-KD provides a strong backbone network, which provides generic representations that can be transferred to other visual recognition <ref type="bibr" target="#b6">7</ref> APs over all classes are presented in the supplementary material. tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Machine Translation</head><p>To verify the effectiveness of PS-KD on other tasks rather than multi-class classification, a machine translation task where classification is performed on a token-level, not an input-level is considered.</p><p>We use two benchmark datasets including IWSLT15 English to German (EN-DE) and German to English (DE-EN) <ref type="bibr" target="#b1">[2]</ref>, and Multi30k <ref type="bibr" target="#b6">[7]</ref> from WMT16 <ref type="bibr" target="#b36">[36]</ref>. The original purpose of Multi30k is for multimodal learning, consisting of images and descriptions associated with them. For the experiment, we extract only image descriptions written in English and German translations by professional translators. This dataset consists of 29K train data, 1K validation data, and 10K test data with 9,521 vocabularies.</p><p>We consider Transformer <ref type="bibr" target="#b42">[42]</ref> as our baseline model. <ref type="bibr" target="#b7">8</ref> All hyperparameters involved in the architecture and training are set to those reported in <ref type="bibr" target="#b43">[43]</ref>. In specific, we use the architecture with N = 6, d model = 512, h = 4, d k = 64, d f f = 1024. We train the model for 150 epochs with a maximum of 4,096 tokens per a mini-batch, and employ Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with ? 1 = 0.9, ? 2 = 0.98. As a metric, BLEU, commonly used to evaluate the performance on machine translation, is used. The hyperparameter ? T = 0.7 is determined through validation. The results are summarized in <ref type="table" target="#tab_5">Table 6</ref>. Our PS-KD achieves the best BLEU scores on all datasets. Consistent with the results from image classification and object detection, PS-KD shows better performance than the baseline Transformer and that with LS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a simple way to improve the generalization performance of DNNs, which distills the knowledge of a model itself to generate more informative targets for training. The targets are softened by using past predictions about data from the model at the previous epoch. We also provide theoretical justification, which shows that our method performs hard example mining implicitly during training. From the experimental results conducted across diverse tasks, we observe that the proposed method is effective to improve the generalization capability of DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Classification</head><p>A.1. Evaluation Metrics ECE. Expected calibration error (ECE) <ref type="bibr" target="#b29">[30]</ref> is a widely used metric for evaluating confidence calibration performance. To estimate the expected gap between accuracy and confidence, it partitions samples into total M bins, B m for m = 1, ..., M , by confidence. Then, each bin B m contains samples with confidence within [ m?1 M , m M ]. With this binning, ECE is defined as follows,</p><formula xml:id="formula_11">ECE = 1 n M m=1 |B m | ? |Acc(B m ) ? Conf(B m )|</formula><p>where n is number of samples, Acc(B m ) represents accuracy of samples in B m , and Conf(B m ) represents average confidence of samples in B m . The lower value of ECE indicates that a model is well-calibrated. The reliability diagram <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> and calibration plot are visualization tools to show how well confidence of a model is calibrated by plotting accuracy against confidence values.</p><p>AURC. Area under risk-coverage curve (AURC) <ref type="bibr" target="#b9">[10]</ref> measures how well predictions are ordered by confidence values. Given a classifier, we can define a selective classifier with a threshold which covers only samples with higher confidence than the threshold. Then, coverage can be defined as the proportion of covered samples (i.e., not rejected samples by the selective classifier) to the entire dataset. Risk is defined as an error rate computed by using the covered samples. Therefore, as coverage increases from 0 to 1, risk approaches to the top-1 error on the entire data. AURC is defined as the area under the risk-coverage curve. If a model has a low AURC value, it means that correct and incorrect predictions from the model are well-separable by confidence values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Methods</head><p>Label smoothing. Szegedy et al. <ref type="bibr" target="#b39">[39]</ref> proposes a method named label smoothing which improves the performance of deep learning models by adjusting one-hot targets to be soft targets. Soft targets y LS are computed as a weighted sum of the hard targets y and the uniform distribution over classes, i.e.,</p><formula xml:id="formula_12">y LS = (1 ? ?)y + ? K</formula><p>where ? is a smoothing parameter and K is the number of classes.</p><p>Cutout. Cutout <ref type="bibr" target="#b5">[6]</ref> is a simple regularization method designed for image classification. Motivated by dropout and image augmentation, Cutout generates a partially occluded version of input samples, which can be interpreted as an augmented data by applying the structured dropout to an input space. In detail, a square-shaped region with the predefined size is randomly selected on an input image, and that region is zeroed-out during training.</p><p>CutMix. Yun et al. <ref type="bibr" target="#b50">[50]</ref> suggests a method inspired by Cutout <ref type="bibr" target="#b5">[6]</ref> and Mixup <ref type="bibr" target="#b52">[52]</ref>. This method generates a new training sample (x,?) from two samples (x a , y a ) and (x b , y b ). From x a , a rectangular region with bounding box coordinates (r x , r y , r w , r h ) will be sampled as a patch. Then, the region of the same coordinates in x b will be replaced by the patch to generatex. For the generated samplex, its target? is defined as</p><formula xml:id="formula_13">y = ?y a + (1 ? ?)y b</formula><p>where ? denotes the combination ratio sampled from the uniform distribution (0, 1).</p><p>ShakeDrop. ShakeDrop <ref type="bibr" target="#b47">[47]</ref> is a regularization technique designed for ResNet and its variants. This method gives regularization effect by replacing residual blocks to ShakeDrop blocks. Let an input be x and an output of residual block be F (x), then the output of l-th ShakeDrop block G(x) is defined as,</p><formula xml:id="formula_14">G(x) = ? ? ? ? ? x + (b l + ? ? b l ?)F (x)</formula><p>, for the train-forward phase</p><formula xml:id="formula_15">x + (b l + ? ? b l ?)F (x), for the train-backward phase x + E[b l + ? ? b l ?]F (x), for test phase</formula><p>where ?, ? are independent uniform random variables and b l is a Bernoulli random variable with probability P (b l = 1) = p l , which is a parameter with a linear decay according to the block index l:</p><formula xml:id="formula_16">p l = 1 ? l L (1 ? P L )</formula><p>where L is the total number of building blocks and P L is an initial parameter. In our experiments, we use P L = 0.5 as suggested in <ref type="bibr" target="#b47">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Datasets</head><p>CIFAR-100 is a dataset for multi-class image classification. It consists of 50K training images and 10K test images of 32?32 resolutions with 100 classes, and has the same number of images per class. The ImageNet is a large-scale dataset. It consists of 1.2M training images and 50K validation images of various resolutions with 1K classes. It contains some images that have multiple objects. In training, we use an input image that is resized to 256?256, and it is randomly cropped to have a size of 224?224. For inference, we resize an image as 256?256 and perform the center crop to have a 224?224 sized input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Experimental Results on CIFAR-100</head><p>Hyperparameters. For LS, we use the smoothing parameter ? of 0.1. For CS-KD 9 , we set the temperature ? to 4, and the weight ? cls to 1 <ref type="bibr" target="#b51">[51]</ref>. For TF-KD 10 , we use TF-KD self method presented in <ref type="bibr" target="#b49">[49]</ref>. The hyperparameters, the temperature ? and weight ?, for ResNet-18, DenseNet-121 and ResNeXt-29 are set to the values reported in <ref type="bibr" target="#b49">[49]</ref>. For ResNet-101 and PyramidNet, we use the temperature ? = 20 and weight ? = 0.95, which are most widely used settings in the paper.</p><p>Ablation study on the hyperparameter ? T of PS-KD. To investigate the effect of our hyperparameter ? T , we provide the validation performances in terms of top-1 error and ECE on CIFAR-100 with ResNet-18. The results are given in <ref type="figure">Fig. S5</ref>. Considering both top-1 error and ECE metrics, we determine the optimal ? T as 0.8. For ? T &gt; 0.8, we observe that ECE suffers from PS-KD while top-1 accuracy still improves, implying that PS-KD with a large value of ? T &gt; 0.8 tends to produce underconfident predictions as can be seen in <ref type="figure">Fig. S6. Fig. S6</ref> shows the reliability diagrams on the validation dataset with PS-KD. PS-KD with ? T = 0.8 shows best calibration performance. Additionally, to examine the effect of using past predictions to soften hard targets, we conduct experiments with a fixed value of ? t ? {0.1, 0.2, 0.4, 0.6, 0.8} so that the effect of adjusting ? t is excluded. From the curves of NLL and top-1 error in <ref type="figure">Fig. S7</ref>, we observe that PS-KD with a fixed ? t = 0.1 shows lower NLL and top-1 error than LS with ? = 0.1 (refer to the shaded area on the curves), and the performances are improved as a fixed ? t increases. Therefore, it can be concluded that softening hard targets with predictions from the model itself is much better than just using a static softening operation like LS. To further investigate the effect of adjusting ? t , the curves from the linear growth strategy toward ? T = 0.8 are also depicted. Compared to the curves from the fixed ? t = 0.8, we conclude that the simplest approach, the linear growth, works surprisingly well for regularizing the model. Additional calibration plots <ref type="figure">Fig. S8</ref> shows the calibration plots of existing regularization methods on CIFAR-100. From this figure, we can observe that the advanced regularization methods such as Cutout, CutMix, CutMix+SD benefit from PS-KD in terms of calibration. Extension results for self-KD methods combined with advanced data augmentations As summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Experimental Results on ImageNet</head><p>Random search results of the hyperparameters. To find out the optimal hyperparmeter of CS-KD, TF-KD and PS-KD, we perform a random search of hyperparameters over five trials with ResNet-152 for a fair comparison. We set the minibatch size to 512, and the other training setting is set to the same as ImageNet experiments in the main manuscript. For CS-KD, we consider the range of the hyperparameters as follow: ? ? {1, 2, ? ? ? , 20} and ? cls ? {0.1, 0.5, 1, ? ? ? , 4}. For TF-KD, we use TF-KD reg method which shows better performance on ImageNet in the original paper <ref type="bibr" target="#b49">[49]</ref>. We consider the hyperparamters, the temperature ? ? {20, 30, 40}, weight ? ? {0.1, 0.2, ? ? ? , 0.5} and probability for the ground-truth class a ? {0.90, 0.91, ? ? ? , 0.99}. For PS-KD, the range of ? T ? {0.1, 0.2, ? ? ? , 1} is used. The results are presented in <ref type="table" target="#tab_7">Table S8</ref>. Additional calibration plots <ref type="figure">Fig. S9</ref> shows the calibration plots of comparison targets and CutMix. From this figure, we observe that PS-KD is better calibrated than other methods as well as improves calibration performance of the existing advanced regularization method, CutMix. Additional samples from ImageNet validation dataset. In <ref type="figure" target="#fig_0">Fig. S10</ref>, additional samples from ImageNet validation dataset and their predicted probabilities are presented. From these samples, we observe that PS-KD provides better outputs in the sense of human interpretation. <ref type="figure" target="#fig_0">Figure S10</ref>. Predicted probabilities for sample images from the baseline and PS-KD. From the top left, the ground-truth labels of these images are "king snake", "water snake", "cabbage butterfly", "buckle", "desk", "measuring cup", "sliding door" and "orange", respectively. <ref type="table">Table S9</ref> shows the values of average precision (AP) over all classes. PS-KD shows higher AP values than the baseline and other methods (i.e., LS, CS-KD and TF-KD) for 10 classes out of 20 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Precision mAP  <ref type="table">Table S9</ref>. APs over all classes on PASCAL VOC 2007 testset. The best result for each class is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Machine Translation</head><p>C.1. Evaluation Metrics BLEU. BLEU (Bilingual Evaluation Understudy) is an algorithm for numerically measuring the quality of machine translation from one natural language to another one. By using human translation as a reference, BLEU evaluates the quality of machine translation via two aspects. One is how many n-grams in the translated output of a model appears in the reference. If the more n-grams appear in both machine translation and human translation, the quality of machine translation is considered as better. We set n to 4, which is generally used for the evaluation. Another aspect of BLEU is the length of machine translated sentence. If we evaluate the performance by using only n-grams, very short sentence with only few words in the reference will have nearly a perfect score. To prevent this, an additional term comparing the length of machine translation and human translation is considered in the calculation of BLEU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A schematic of PS-KD. At epoch t, a student at epoch (t ? 1) becomes a teacher and a model at epoch t is trained with the soft targets computed as a linear combination of hard targets and the predictions from the teacher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Target and (b) maximum probabilities for 100 hardto-learn samples (correctly classified less than 50 times during 300 epochs) on CIFAR-100. PS-KD keeps learning from hard examples by giving more weights to them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Calibration plots of all architectures on CIFAR-100. A diagonal dashed line represents perfect calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Predicted probabilities for samples in the validation dataset from baseline and PS-KD. The ground-truth labels of these images are "bulletproof vest" (top), and "stove" (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S5 .Figure S6 .</head><label>S5S6</label><figDesc>Validation top-1 error and ECE according to ?T from three repeated experiments on CIFAR-100 for ResNet-18. ?T = 0.8 is chosen as the best one and used for all other experiments.<ref type="bibr" target="#b8">9</ref> CS-KD implementation:https://github.com/alinlab/cs-kd 10 TF-KD implementation: https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation 0Reliability diagrams on the validation dataset of CIFAR100 with ResNet-18+PS-KD. The number on the top of each bin represents the number of samples belonging to that bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>: t = 0.1 (fixed) PS-KD: t = 0.2 (fixed) PS-KD: t = 0.4 (fixed) PS-KD: t = 0.6 (fixed) PS-KD: t = 0.8 (fixed) PS-KD: T = 0.8 (linear growth) Figure S7. NLL (left) and top-1 error (right) curves on CIFAR-100 with different ?t values for DenseNet-121. Linear growth with ?T = 0.8 achieves the lowest NLL and top-1 error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>PyramidNet with CutMix + ShakeDrop Figure S8. Calibration plots of advanced regularization methods on CIFAR-100 with PyramidNet. PS-KD provides additional benefits to existing methods in terms of calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>ResNet-152 with PS-KD + CutMix Figure S9. Calibration plots on ImageNet with ResNet-152. (a) PS-KD shows slightly better performance compared to LS, CS-KD, and TF-KD. (b) PS-KD provides additional benefits to CutMix in terms of calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results on CIFAR-100 compared to other methods with popular architectures, averaged over three runs. The best result is shown in boldface.</figDesc><table><row><cell></cell><cell>Top-1 Err (%)</cell><cell>Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>ResNet-18</cell><cell>24.18</cell><cell>6.90</cell><cell cols="2">1.10 11.84</cell><cell>67.65</cell></row><row><cell>+ LS</cell><cell>20.94</cell><cell>6.02</cell><cell cols="2">0.98 10.79</cell><cell>57.74</cell></row><row><cell>+ CS-KD</cell><cell>21.30</cell><cell>5.70</cell><cell>0.88</cell><cell>6.24</cell><cell>56.56</cell></row><row><cell>+ TF-KD</cell><cell>22.88</cell><cell>6.01</cell><cell cols="2">1.05 11.96</cell><cell>61.77</cell></row><row><cell>+ PS-KD</cell><cell>20.82</cell><cell>5.10</cell><cell>0.76</cell><cell>1.77</cell><cell>52.10</cell></row><row><cell>ResNet-101</cell><cell>20.75</cell><cell>5.28</cell><cell cols="2">0.89 10.02</cell><cell>55.45</cell></row><row><cell>+ LS</cell><cell>19.84</cell><cell>5.07</cell><cell>0.93</cell><cell>3.43</cell><cell>95.76</cell></row><row><cell>+ CS-KD</cell><cell>20.76</cell><cell>5.62</cell><cell cols="2">1.02 12.18</cell><cell>64.44</cell></row><row><cell>+ TF-KD</cell><cell>20.10</cell><cell>5.10</cell><cell>0.84</cell><cell>6.14</cell><cell>58.8</cell></row><row><cell>+ PS-KD</cell><cell>19.43</cell><cell>4.30</cell><cell>0.74</cell><cell>6.92</cell><cell>49.01</cell></row><row><cell>DenseNet-121</cell><cell>20.05</cell><cell>4.99</cell><cell>0.82</cell><cell>7.34</cell><cell>52.21</cell></row><row><cell>+ LS</cell><cell>19.80</cell><cell>5.46</cell><cell>0.92</cell><cell>3.76</cell><cell>91.06</cell></row><row><cell>+ CS-KD</cell><cell>20.47</cell><cell>6.21</cell><cell cols="2">1.07 13.80</cell><cell>73.37</cell></row><row><cell>+ TF-KD</cell><cell>19.88</cell><cell>5.10</cell><cell>0.85</cell><cell>7.33</cell><cell>69.23</cell></row><row><cell>+ PS-KD</cell><cell>18.73</cell><cell>3.90</cell><cell>0.69</cell><cell>3.71</cell><cell>45.55</cell></row><row><cell>ResNeXt-29</cell><cell>18.65</cell><cell>4.47</cell><cell>0.74</cell><cell>4.17</cell><cell>44.27</cell></row><row><cell>+ LS</cell><cell>17.60</cell><cell>4.23</cell><cell cols="2">1.05 22.14</cell><cell>41.92</cell></row><row><cell>+ CS-KD</cell><cell>18.26</cell><cell>4.37</cell><cell>0.80</cell><cell>5.95</cell><cell>42.11</cell></row><row><cell>+ TF-KD</cell><cell>17.33</cell><cell>3.87</cell><cell>0.74</cell><cell>6.73</cell><cell>40.34</cell></row><row><cell>+ PS-KD</cell><cell>17.28</cell><cell>3.60</cell><cell>0.71</cell><cell>9.15</cell><cell>39.78</cell></row><row><cell>PyramidNet</cell><cell>16.80</cell><cell>3.69</cell><cell>0.73</cell><cell>8.04</cell><cell>36.95</cell></row><row><cell>+ LS</cell><cell>17.82</cell><cell>4.72</cell><cell>0.89</cell><cell>3.46</cell><cell>105.02</cell></row><row><cell>+ CS-KD</cell><cell>18.31</cell><cell>5.70</cell><cell cols="2">1.17 14.70</cell><cell>70.05</cell></row><row><cell>+ TF-KD</cell><cell>16.48</cell><cell>3.37</cell><cell cols="2">0.79 10.48</cell><cell>37.04</cell></row><row><cell>+ PS-KD</cell><cell>15.49</cell><cell>3.08</cell><cell>0.56</cell><cell>1.83</cell><cell>32.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance enhancement of data augmentation methods, Cutout and CutMix, by PS-KD, averaged over three runs. The best result is shown in boldface.</figDesc><table><row><cell></cell><cell cols="3">Top-1 Err (%)</cell><cell cols="2">Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>PyramidNet</cell><cell></cell><cell cols="2">16.80</cell><cell cols="2">3.69</cell><cell>0.73</cell><cell>8.04</cell><cell>36.95</cell></row><row><cell>+ PS-KD</cell><cell></cell><cell cols="2">15.49</cell><cell cols="2">3.08</cell><cell>0.56</cell><cell>1.83</cell><cell>32.14</cell></row><row><cell>+ Cutout [6]</cell><cell></cell><cell cols="2">16.05</cell><cell cols="2">3.42</cell><cell>0.67</cell><cell>7.15</cell><cell>33.20</cell></row><row><cell>+ Cutout + PS-KD</cell><cell></cell><cell cols="2">14.82</cell><cell cols="2">2.86</cell><cell>0.54</cell><cell>3.69</cell><cell>29.77</cell></row><row><cell>+ CutMix [50]</cell><cell></cell><cell cols="2">15.62</cell><cell cols="2">3.38</cell><cell>0.68</cell><cell>8.16</cell><cell>34.60</cell></row><row><cell>+ CutMix + PS-KD</cell><cell></cell><cell cols="2">15.03</cell><cell cols="2">2.91</cell><cell>0.58</cell><cell>5.81</cell><cell>30.22</cell></row><row><cell>+ CutMix + SD [47]</cell><cell></cell><cell cols="2">14.07</cell><cell cols="2">2.38</cell><cell>0.51</cell><cell>3.96</cell><cell>28.65</cell></row><row><cell cols="2">+ CutMix + SD + PS-KD</cell><cell cols="2">13.59</cell><cell cols="2">2.18</cell><cell>0.49</cell><cell>3.46</cell><cell>25.98</cell></row><row><cell>Model + Method</cell><cell cols="2">Top-1 Err (%)</cell><cell cols="2">Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>ResNet-18</cell><cell>21.36</cell><cell></cell><cell cols="2">5.05</cell><cell>0.89</cell><cell>4.29</cell><cell>54.61</cell></row><row><cell>+ CS-KD</cell><cell>18.39</cell><cell></cell><cell cols="2">4.21</cell><cell>0.74</cell><cell>3.07</cell><cell>43.33</cell></row><row><cell>+ TF-KD</cell><cell>21.07</cell><cell></cell><cell cols="2">4.80</cell><cell>0.90</cell><cell>6.48</cell><cell>52.70</cell></row><row><cell>+ PS-KD</cell><cell>18.79</cell><cell></cell><cell cols="2">4.17</cell><cell>0.68</cell><cell>5.12</cell><cell>44.29</cell></row><row><cell>ResNet-101</cell><cell>18.27</cell><cell></cell><cell cols="2">3.99</cell><cell>0.74</cell><cell>4.01</cell><cell>44.46</cell></row><row><cell>+ CS-KD</cell><cell>17.97</cell><cell></cell><cell cols="2">4.00</cell><cell>0.78</cell><cell>4.61</cell><cell>43.83</cell></row><row><cell>+ TF-KD</cell><cell>18.16</cell><cell></cell><cell cols="2">3.97</cell><cell>0.71</cell><cell>1.52</cell><cell>43.66</cell></row><row><cell>+ PS-KD</cell><cell>17.26</cell><cell></cell><cell cols="2">3.42</cell><cell>0.63</cell><cell>2.15</cell><cell>37.85</cell></row><row><cell>DenseNet-121</cell><cell>17.09</cell><cell></cell><cell cols="2">3.51</cell><cell>0.65</cell><cell>1.96</cell><cell>39.01</cell></row><row><cell>+ CS-KD</cell><cell>17.04</cell><cell></cell><cell cols="2">3.85</cell><cell>0.73</cell><cell>4.08</cell><cell>42.40</cell></row><row><cell>+ TF-KD</cell><cell>16.96</cell><cell></cell><cell cols="2">3.32</cell><cell>0.71</cell><cell>5.36</cell><cell>39.31</cell></row><row><cell>+ PS-KD</cell><cell>16.25</cell><cell></cell><cell cols="2">2.85</cell><cell>0.57</cell><cell>1.92</cell><cell>34.64</cell></row><row><cell>ResNeXt-29</cell><cell>16.72</cell><cell></cell><cell cols="2">3.44</cell><cell>0.65</cell><cell>3.78</cell><cell>36.98</cell></row><row><cell>+ CS-KD</cell><cell>16.86</cell><cell></cell><cell cols="2">3.51</cell><cell>0.75</cell><cell>8.58</cell><cell>37.06</cell></row><row><cell>+ TF-KD</cell><cell>16.03</cell><cell></cell><cell cols="2">3.33</cell><cell>0.70</cell><cell>8.89</cell><cell>35.14</cell></row><row><cell>+ PS-KD</cell><cell>15.99</cell><cell></cell><cell cols="2">3.10</cell><cell>0.68</cell><cell>11.1</cell><cell>21.79</cell></row><row><cell>PyramidNet</cell><cell>14.58</cell><cell></cell><cell cols="2">2.85</cell><cell>0.60</cell><cell>2.63</cell><cell>30.04</cell></row><row><cell>+ CS-KD</cell><cell>15.29</cell><cell></cell><cell cols="2">3.93</cell><cell>0.76</cell><cell>4.72</cell><cell>36.53</cell></row><row><cell>+ TF-KD</cell><cell>14.77</cell><cell></cell><cell cols="2">2.77</cell><cell>0.65</cell><cell>5.60</cell><cell>30.55</cell></row><row><cell>+ PS-KD</cell><cell>14.11</cell><cell></cell><cell cols="2">2.58</cell><cell>0.50</cell><cell>2.79</cell><cell>27.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Performance improvement of ensembles by PS-KD. For ensembling, three trained models in Table 1 are used. The best result is shown in boldface.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model + Method</cell><cell>Top-1 Err (%)</cell><cell>Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>DenseNet-264* [19]</cell><cell>22.15</cell><cell>6.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNeXt-101* [45]</cell><cell>21.20</cell><cell>5.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-152</cell><cell>22.19</cell><cell>6.19</cell><cell>0.88</cell><cell>3.84</cell><cell>61.79</cell></row><row><cell>+ LS</cell><cell>21.73</cell><cell>5.85</cell><cell>0.92</cell><cell>3.91</cell><cell>68.24</cell></row><row><cell>+ CS-KD</cell><cell>21.61</cell><cell>5.92</cell><cell>0.90</cell><cell>5.79</cell><cell>62.12</cell></row><row><cell>+ TF-KD</cell><cell>22.76</cell><cell>6.43</cell><cell>0.91</cell><cell>4.70</cell><cell>65.28</cell></row><row><cell>+ PS-KD</cell><cell>21.41</cell><cell>5.85</cell><cell>0.84</cell><cell>2.51</cell><cell>61.01</cell></row><row><cell>+ CutMix</cell><cell>21.04</cell><cell>5.56</cell><cell>0.81</cell><cell>2.19</cell><cell>58.43</cell></row><row><cell>+ CutMix + LS</cell><cell>20.77</cell><cell>5.36</cell><cell>0.85</cell><cell>1.90</cell><cell>63.45</cell></row><row><cell>+ CutMix + CS-KD</cell><cell>21.08</cell><cell>5.53</cell><cell>0.83</cell><cell>1.56</cell><cell>59.02</cell></row><row><cell>+ CutMix + TF-KD</cell><cell>22.00</cell><cell>5.93</cell><cell>0.85</cell><cell>2.18</cell><cell>62.57</cell></row><row><cell>+ CutMix + PS-KD</cell><cell>20.76</cell><cell>5.34</cell><cell>0.80</cell><cell>0.54</cell><cell>58.25</cell></row></table><note>. Top-1/top-5 error, NLL, ECE and AURC results on Ima- geNet validation dataset.'*' denotes results reported in the original papers. The best result is in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">IWSLT15</cell><cell>Multi30k</cell></row><row><cell>+ Method</cell><cell cols="2">EN-DE DE-EN</cell><cell>DE-EN</cell></row><row><cell>Transformer</cell><cell>28.5</cell><cell>34.6</cell><cell>29.0</cell></row><row><cell>+ LS</cell><cell>29.3</cell><cell>35.6</cell><cell>29.3</cell></row><row><cell>+ PS-KD</cell><cell>30.0</cell><cell>36.2</cell><cell>32.3</cell></row></table><note>BLEU scores on Transformer with LS or PS-KD</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S7</head><label>S7</label><figDesc>, we provide additional experimental results: [Cutout, CutMix, CutMix+SD] + LS, CS-KD, and TF-KD on CIFAR 100 with PyramidNet. The results show that PS-KD can be effectively combined with advanced regularization techniques.Table S7. Performance evaluation of self-KD methods with advanced data augmentation techniques. The values averaged over three runs are reported. The best result is shown in boldface.</figDesc><table><row><cell>Model + Method</cell><cell>Top-1 Err (%)</cell><cell>Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>PyramidNet</cell><cell>16.80</cell><cell>3.69</cell><cell>0.73</cell><cell>8.04</cell><cell>36.95</cell></row><row><cell>+ LS</cell><cell>17.82</cell><cell>4.72</cell><cell>0.89</cell><cell>3.46</cell><cell>105.02</cell></row><row><cell>+ CS-KD</cell><cell>18.31</cell><cell>5.70</cell><cell cols="2">1.17 14.70</cell><cell>70.05</cell></row><row><cell>+ TF-KD</cell><cell>16.48</cell><cell>3.37</cell><cell cols="2">0.79 10.48</cell><cell>37.04</cell></row><row><cell>+ PS-KD</cell><cell>15.49</cell><cell>3.08</cell><cell>0.56</cell><cell>1.83</cell><cell>32.14</cell></row><row><cell>+ Cutout</cell><cell>16.05</cell><cell>3.42</cell><cell>0.67</cell><cell>7.15</cell><cell>33.20</cell></row><row><cell>+ Cutout + LS</cell><cell>17.15</cell><cell>4.38</cell><cell>0.82</cell><cell>4.65</cell><cell>82.61</cell></row><row><cell>+ Cutout + CS-KD</cell><cell>18.20</cell><cell>5.25</cell><cell cols="2">1.06 13.78</cell><cell>66.69</cell></row><row><cell>+ Cutout + TF-KD</cell><cell>16.29</cell><cell>3.18</cell><cell>0.74</cell><cell>9.77</cell><cell>35.78</cell></row><row><cell>+ Cutout + PS-KD</cell><cell>14.82</cell><cell>2.86</cell><cell>0.54</cell><cell>3.69</cell><cell>29.77</cell></row><row><cell>+ CutMix</cell><cell>15.62</cell><cell>3.38</cell><cell>0.68</cell><cell>8.16</cell><cell>34.60</cell></row><row><cell>+ CutMix + LS</cell><cell>15.68</cell><cell>3.66</cell><cell>0.70</cell><cell>4.60</cell><cell>37.71</cell></row><row><cell>+ CutMix + CS-KD</cell><cell>15.89</cell><cell>3.60</cell><cell>0.73</cell><cell>9.28</cell><cell>35.47</cell></row><row><cell>+ CutMix + TF-KD</cell><cell>16.61</cell><cell>3.29</cell><cell>0.66</cell><cell>7.47</cell><cell>36.57</cell></row><row><cell>+ CutMix + PS-KD</cell><cell>15.03</cell><cell>2.91</cell><cell>0.58</cell><cell>5.81</cell><cell>30.22</cell></row><row><cell>+ CutMix + SD</cell><cell>14.07</cell><cell>2.38</cell><cell>0.51</cell><cell>3.96</cell><cell>28.65</cell></row><row><cell>+ CutMix + SD + LS</cell><cell>14.05</cell><cell>2.37</cell><cell>0.54</cell><cell>2.54</cell><cell>33.09</cell></row><row><cell>+ CutMix + SD + CS-KD</cell><cell>14.99</cell><cell>2.56</cell><cell>0.56</cell><cell>3.27</cell><cell>34.40</cell></row><row><cell>+ CutMix + SD + TF-KD</cell><cell>15.34</cell><cell>2.58</cell><cell>0.53</cell><cell>3.31</cell><cell>31.41</cell></row><row><cell>+ CutMix + SD + PS-KD</cell><cell>13.59</cell><cell>2.18</cell><cell>0.49</cell><cell>3.46</cell><cell>25.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S8 .</head><label>S8</label><figDesc>Results over five trials of random search with ResNet-152. The best result for each method is shown in boldface.</figDesc><table><row><cell>Model + Method</cell><cell>Top-1 Err (%)</cell><cell>Top-5 Err (%)</cell><cell>NLL</cell><cell>ECE (%)</cell><cell>AURC (?10 3 )</cell></row><row><cell>ResNet-152</cell><cell>21.95</cell><cell>6.16</cell><cell>0.89</cell><cell>5.08</cell><cell>61.64</cell></row><row><cell>+ LS</cell><cell>21.80</cell><cell>6.03</cell><cell>0.94</cell><cell>3.42</cell><cell>70.83</cell></row><row><cell>+ CS-KD (? = 10, ? = 4)</cell><cell>23.28</cell><cell>7.02</cell><cell>1.04</cell><cell>4.31</cell><cell>69.68</cell></row><row><cell>+ CS-KD (? = 20, ? = 2)</cell><cell>22.30</cell><cell>6.46</cell><cell>0.95</cell><cell>4.92</cell><cell>54.13</cell></row><row><cell>+ CS-KD (? = 1, ? = 0.1)</cell><cell>21.68</cell><cell>6.04</cell><cell>0.85</cell><cell>1.46</cell><cell>61.09</cell></row><row><cell>+ CS-KD (? = 4, ? = 0.5)</cell><cell>21.67</cell><cell>6.01</cell><cell>0.88</cell><cell>3.79</cell><cell>61.39</cell></row><row><cell>+ CS-KD (? = 10, ? = 3)</cell><cell>22.43</cell><cell>6.55</cell><cell>0.98</cell><cell>5.45</cell><cell>65.99</cell></row><row><cell>+ TF-KD (? = 0.3, ? = 20, a = 0.91)</cell><cell>22.72</cell><cell>6.49</cell><cell>0.92</cell><cell>4.69</cell><cell>65.30</cell></row><row><cell>+ TF-KD (? = 0.1, ? = 40, a = 0.95)</cell><cell>22.66</cell><cell>6.46</cell><cell>0.91</cell><cell>4.61</cell><cell>64.29</cell></row><row><cell>+ TF-KD (? = 0.2, ? = 40, a = 0.97)</cell><cell>22.99</cell><cell>6.66</cell><cell>0.93</cell><cell>5.13</cell><cell>65.69</cell></row><row><cell>+ TF-KD (? = 0.3, ? = 40, a = 0.92)</cell><cell>22.82</cell><cell>6.61</cell><cell>0.92</cell><cell>4.72</cell><cell>64.79</cell></row><row><cell>+ TF-KD (? = 0.1, ? = 30, a = 0.92)</cell><cell>22.74</cell><cell>6.52</cell><cell>0.92</cell><cell>5.25</cell><cell>64.76</cell></row><row><cell>+ PS-KD (?T = 0.9)</cell><cell>22.69</cell><cell>6.44</cell><cell>1.06</cell><cell>17.1</cell><cell>69.75</cell></row><row><cell>+ PS-KD (?T = 0.5)</cell><cell>21.67</cell><cell>5.92</cell><cell>0.88</cell><cell>7.33</cell><cell>63.19</cell></row><row><cell>+ PS-KD (?T = 0.1)</cell><cell>21.89</cell><cell>6.00</cell><cell>0.86</cell><cell>2.96</cell><cell>60.88</cell></row><row><cell>+ PS-KD (?T = 0.3)</cell><cell>21.51</cell><cell>5.86</cell><cell>0.84</cell><cell>1.85</cell><cell>60.61</cell></row><row><cell>+ PS-KD (?T = 0.8)</cell><cell>22.40</cell><cell>6.40</cell><cell cols="2">1.00 13.65</cell><cell>68.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Experiments were conducted using Fairseq (https://github.com/ pytorch/fairseq) toolkit<ref type="bibr" target="#b28">[29]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The dataset can be downloaded from https://https://wit3.fbk.eu/2015-01.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Dataset</head><p>Dataset. We use IWSLT15 English to German (EN-DE) and German to English (DE-EN) dataset. It consists of 191K training sentence pairs <ref type="bibr" target="#b10">11</ref> , and 8,300 pairs of the training data are used for validation. We concatenate dev2010, dev2012, tst2010, tst2011, tst2012, tst2013 datasets for a test set.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Softtarget regularization: An effective technique to reduce over-fitting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Cybernetics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The iwslt 2015 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical evaluation on robustness and uncertainty of regularization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Uncertainty and Robustness in Deep Leaning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The comparison and evaluation of forecasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Statistician</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi30k: Multilingual english-german image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Vision and Language</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biasreduced uncertainty estimation for deep neural classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-knowledge distillation in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangchul</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01851</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Augmix: A simple method to improve robustness and uncertainty under data shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence-aware learning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangheum</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Mai Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Zeming</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anima Singh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03532,2021.4</idno>
	</analytic>
	<monogr>
		<title level="m">and Sagar Jain. Understanding and improving knowledge distillation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Disturblabel: Regularizing CNN on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno>abs/1605.00055</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data-distortion guided self-distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shakedrop regularization for deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongjin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
