<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Underwater Image Enhancement for Improved Visual Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Jahidul</forename><surname>Islam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Minnesota Robotics Institute</orgName>
								<orgName type="laboratory">Interactive Robotics and Vision Laboratory</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<addrLine>Twin Cities</addrLine>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youya</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Minnesota Robotics Institute</orgName>
								<orgName type="laboratory">Interactive Robotics and Vision Laboratory</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<addrLine>Twin Cities</addrLine>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Minnesota Robotics Institute</orgName>
								<orgName type="laboratory">Interactive Robotics and Vision Laboratory</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<addrLine>Twin Cities</addrLine>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Underwater Image Enhancement for Improved Visual Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a largescale dataset of a paired and an unpaired collection of underwater images (of 'poor' and 'good' quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/ funie-gan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visually-guided AUVs (Autonomous Underwater Vehicles) and ROVs (Remotely Operated Vehicles) are widely used in important applications such as the monitoring of marine species migration and coral reefs <ref type="bibr" target="#b38">[39]</ref>, inspection of submarine cables and wreckage <ref type="bibr" target="#b4">[5]</ref>, underwater scene analysis, seabed mapping, human-robot collaboration <ref type="bibr" target="#b22">[24]</ref>, and more. One major operational challenge for these underwater robots is that despite using high-end cameras, visual sensing is often greatly affected by poor visibility, light refraction, absorption, and scattering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">24]</ref>. These optical artifacts trigger non-linear distortions in the captured images, which severely affect the performance of visionbased tasks such as tracking, detection and classification, Input Generated (a) Perceptual enhancement of underwater images.  <ref type="bibr" target="#b21">[23]</ref>, human body-pose estimation <ref type="bibr" target="#b8">[9]</ref>, and saliency prediction <ref type="bibr" target="#b39">[40]</ref>. segmentation, and visual servoing. Fast and accurate image enhancement techniques can alleviate these problems by restoring the perceptual and statistical qualities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref> of the distorted images in real-time.</p><p>As light propagation differs underwater (than in the atmosphere), a unique set of non-linear image distortions occur which are propelled by a variety of factors. For instance, underwater images tend to have a dominating green or blue hue <ref type="bibr" target="#b14">[15]</ref> because red wavelengths get absorbed in deep water (as light travels further). Such wavelength dependant attenuation <ref type="bibr" target="#b1">[2]</ref>, scattering, and other optical properties of the waterbodies cause irregular non-linear distortions <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b44">45]</ref> which result in low-contrast, often blurred, and color-degraded images. Some of these aspects can be modeled and well estimated by physics-based solutions, particularly for dehazing and color correction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4]</ref>. How-ever, information such as the scene depth and optical waterquality measures are not always available in many robotic applications. Besides, these models are often computationally too demanding for real-time deployments.</p><p>A practical alternative is to approximate the underlying solution by learning-based methods, which demonstrated remarkable success in recent years. Several models based on deep Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) provide state-of-theart performance <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">47]</ref> in learning to enhance perceptual image quality from a large collection of paired or unpaired data. For underwater imagery, in particular, a number of GAN-based models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref> and CNN-based residual models <ref type="bibr" target="#b28">[29]</ref> report inspiring progress for automatic color enhancement, dehazing, and contrast adjustment. However, there is significant room for improvement as learning perceptual enhancement for underwater imagery is a more challenging ill-posed problem (than terrestrial imagery). Additionally, due to the high costs and difficulties associated with acquiring large-scale underwater data, most learning-based models use small-scale and often only synthetically generated images that fail to capture a wide range of natural variability. Moreover, designing robust yet efficient image enhancement models and investigating their applicability for improving real-time underwater visual perception have not been explored in the literature in depth.</p><p>We attempt to address these challenges by designing a fast underwater image enhancement model and analyzing its feasibility for real-time applications. We formulate the problem as an image-to-image translation problem by assuming there exists a non-linear mapping between the distorted (input) and enhanced (output) images. Then, we design a conditional GAN-based model to learn this mapping by adversarial training on a large-scale dataset named EUVP (Enhancement of Underwater Visual Perception). From the perspective of its design, implementation, and experimental validation, we make the following contributions in this paper:</p><p>(a) We present a fully-convolutional conditional GANbased model for real-time underwater image enhancement, which we refer to as FUnIE-GAN. We formulate a multi-modal objective function to train the model by evaluating the perceptual quality of an image based on its global content, color, local texture, and style information. (b) Additionally, we present the EUVP dataset, a paired and an unpaired collection of 20K underwater images (of poor and good quality) that can be used for oneway and two-way adversarial training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref>. The dataset is available at http://irvlab.cs.umn. edu/resources/euvp-dataset. (c) Furthermore, we present qualitative and quantitative performance evaluations compared to state-of-the-art models. The results suggest that FUnIE-GAN can learn to enhance perceptual image quality from both paired and unpaired training. More importantly, the enhanced images significantly boost the performance of several underwater visual perception tasks such as object detection, human pose estimation, and saliency prediction; a few sample demonstrations are highlighted in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>In addition to presenting the conceptual model of FUnIE-GAN, we analyze important design choices and relevant practicalities for its efficient implementation. We also conduct a user study and a thorough feasibility analysis to validate its effectiveness for improving the real-time perception performance of visually-guided underwater robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Automatic Image Enhancement</head><p>Automatic image enhancement is a well-studied problem in the domains of computer vision, robotics, and signal processing. Classical approaches use hand-crafted filters to enforce local color constancy and improve contrast/lightness rendition <ref type="bibr" target="#b36">[37]</ref>. Additionally, prior knowledge or statistical assumptions about a scene (e.g., haze-lines, dark channel prior <ref type="bibr" target="#b3">[4]</ref>, etc.) are often utilized for global enhancements such as image deblurring, dehazing <ref type="bibr" target="#b17">[19]</ref>, etc. Over the last decade, single image enhancement has made remarkable progress due to the advent of deep learning and the availability of large-scale datasets. The contemporary deep CNN-based models provide state-of-the-art performance for problems such as image colorization <ref type="bibr" target="#b43">[44]</ref>, color/contrast adjustment <ref type="bibr" target="#b10">[11]</ref>, dehazing <ref type="bibr" target="#b7">[8]</ref>, etc. These models learn a sequence of non-linear filters from paired training data, which provide much better performance compared to using handcrafted filters.</p><p>Moreover, the GAN-based models <ref type="bibr" target="#b15">[16]</ref> have shown great success for style-transfer and image-to-image translation problems <ref type="bibr" target="#b23">[25]</ref>. They employ a two-player min-max game where the 'generator' tries to fool the 'discriminator' by generating fake images that appear to be sampled from the real distribution. Simultaneously, the discriminator tries to get better at discarding fake images and eventually (in equilibrium) the generator learns to model the underlying distribution. Although such adversarial training can be unstable, several tricks and choices of loss functions are proposed in the literature to mitigate that. For instance, Wasserstein GAN <ref type="bibr" target="#b2">[3]</ref> improves the training stability by using the earth-mover distance to measure the distance between the data distribution and the model distribution. Energy-based GANs <ref type="bibr" target="#b45">[46]</ref> also improve training stability by modeling the discriminator as an energy function, whereas the Least-Squared GAN <ref type="bibr" target="#b32">[33]</ref> addresses the vanishing gradients problem by adopting a least-square loss function for the discrim-inator. On the other hand, conditional GANs <ref type="bibr" target="#b33">[34]</ref> allow constraining the generator to produce samples that follow a pattern or belong to a specific class, which is particularly useful to learn a pixel-to-pixel (Pix2Pix) mapping <ref type="bibr" target="#b23">[25]</ref> between an arbitrary input domain (e.g., distorted images) and the desired output domain (e.g., enhanced images).</p><p>A major limitation of the above-mentioned models is that they require paired training data, which may not be available or can be difficult to acquire for many practical applications. The two-way GANs (e.g., CycleGAN <ref type="bibr" target="#b46">[47]</ref>, Du-alGAN <ref type="bibr" target="#b41">[42]</ref>, etc.) solve this problem by using a 'cycleconsistency loss' that allows learning the mutual mappings between two domains from unpaired data. Such models have been effectively used for unpaired learning of perceptual image enhancement <ref type="bibr" target="#b9">[10]</ref> as well. Furthermore, Ignatov et al. <ref type="bibr" target="#b19">[21]</ref> showed that additional loss-terms for preserving the high-level feature-based content improve the quality of image enhancement using GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Improving Underwater Visual Perception</head><p>Traditional physics-based methods use the atmospheric dehazing model to estimate the transmission and ambient light in a scene to recover true pixel intensities <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>. Another class of methods design a series of bilateral and trilateral filters to reduce noise and improve global contrast <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>. In recent work, Akkaynaket al. <ref type="bibr" target="#b1">[2]</ref> proposed a revised imaging model that accounts for the unique distortions pertaining to underwater light propagation; this contributes to a more accurate color reconstruction and overall a better approximation to the ill-posed underwater image enhancement problem. Nevertheless, these methods require scene depth (or multiple images) and optical waterbody measurements as prior.</p><p>On the other hand, several single image enhancement models based on deep adversarial <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28]</ref> and residual learning <ref type="bibr" target="#b28">[29]</ref> have reported inspiring results of late. However, they typically use only synthetically distorted images for paired training, which often limit their generalization performance. The extent of large-scale unpaired training on naturally distorted underwater images have not been explored in the literature. Moreover, most existing models fail to ensure fast inference on single-board robotic platforms, which limits their applicability for improving real-time visual perception. We attempt to address these aspects in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model and Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FUnIE-GAN Architecture</head><p>Given a source domain X (of distorted images) and desired domain Y (of enhanced images), our goal is to learn a mapping G : X ? Y in order to perform automatic image enhancement. We adopt a conditional GAN-based model where the generator tries to learn this mapping by evolving with an adversarial discriminator through an iterative min-max game. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, we design a generator network by following the principles of U-Net <ref type="bibr" target="#b37">[38]</ref>. It is an encoder-decoder network (e 1 -e 5 ,d 1 -d 5 ) with connections between the mirrored layers, i.e., between (e 1 , d 5 ), (e 2 , d 4 ), (e 3 , d 2 ), and (e 4 , d 4 ). Specifically, the outputs of each encoders are concatenated to the respective mirrored decoders. This idea of skip-connections in the generator network is shown to be very effective <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref> for imageto-image translation and image quality enhancement problems. In FUnIE-GAN, however, we employ a much simpler model with fewer parameters in order to achieve fast inference. The input to the network is set to 256 ? 256 ? 3 and the encoder (e 1 -e 5 ) learns only 256 feature-maps of size 8 ? 8. The decoder (d 1 -d 5 ) utilizes these feature-maps and inputs from the skip-connections to learn to generate a 256 ? 256 ? 3 (enhanced) image as output. The network is fully-convolutional as no fully-connected layers are used. Additionally, 2D convolutions with 4 ? 4 filters are applied at each layer, which is then followed by a Leaky-ReLU non-linearity <ref type="bibr" target="#b31">[32]</ref> and Batch Normalization (BN) <ref type="bibr" target="#b20">[22]</ref>. The feature-map sizes in each layer and other model parameters are annotated in <ref type="figure" target="#fig_2">Fig. 2a</ref>.</p><p>For the discriminator, we employ a Markovian Patch-GAN <ref type="bibr" target="#b23">[25]</ref> architecture that assumes the independence of pixels beyond the patch-size, i.e., only discriminates based on the patch-level information. This assumption is important to effectively capture high-frequency features such as local texture and style <ref type="bibr" target="#b41">[42]</ref>. In addition, this configuration is computationally more efficient as it requires fewer parameters compared to discriminating globally at the image level. As shown in <ref type="figure" target="#fig_2">Fig. 2b</ref>, four convolutional layers are used to transform a 256 ? 256 ? 6 input (real and generated image) to a 16 ? 16 ? 1 output that represents the averaged validity responses of the discriminator. At each layer, 3 ? 3 convolutional filters are used with a stride of 2; then the non-linearity and BN are applied the same way as the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Objective Function Formulation</head><p>A standard conditional GAN-based model learns a mapping G : {X, Z} ? Y , where X (Y ) represents the source (desired) domain, and Z denotes random noise. The conditional adversarial loss function <ref type="bibr" target="#b33">[34]</ref> is expressed as:</p><formula xml:id="formula_0">L cGAN (G, D) = E X,Y log D(Y ) + E X,Y log(1 ? D(X, G(X, Z)))<label>(1)</label></formula><p>Here, the generator G tries to minimize L cGAN while the discriminator D tries to maximize it. In FUnIE-GAN, we associate three additional aspects, i.e., global similarity, image content, and local texture and style information in the objective to quantify perceptual image quality.  ? Global similarity: existing methods have shown that adding an L 1 (L 2 ) loss to the objective function enables G to learn to sample from a globally similar space in an L 1 (L 2 ) sense <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b42">43]</ref>. Since the L 1 loss is less prone to introduce blurring, we add the following loss term in the objective:</p><formula xml:id="formula_1">L 1 (G) = E X,Y,Z Y ? G(X, Z) 1<label>(2)</label></formula><p>? Image content: we add a content loss term in the objective in order to encourage G to generate enhanced image that has similar content (i.e., feature representation) as the target (i.e., real) image. Being inspired by <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b19">21]</ref>, we define the image content function ?(?) as the high-level features extracted by the block5 conv2 layer of a pre-trained VGG-19 network. Then, we formulate the content loss as follows:</p><formula xml:id="formula_2">L con (G) = E X,Y,Z ?(Y ) ? ?(G(X, Z)) 2 (3)</formula><p>? Local texture and style: as mentioned, Markovian PatchGANs are effective in capturing highfrequency information pertaining to the local texture and style <ref type="bibr" target="#b23">[25]</ref>. Hence, we rely on D to enforce the local texture and style consistency in adversarial fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Paired Training</head><p>For paired training, we formulate an objective function that guides G to learn to improve the perceptual image quality so that the generated image is close to the respective ground truth in terms of its global appearance and high-level feature representation. On the other hand, D will discard a generated image that has locally inconsistent texture and style. Specifically, we use the following objective function for paired training:</p><formula xml:id="formula_3">G * = arg min G max D L cGAN (G, D)+? 1 L 1 (G)+? c L con (G)</formula><p>Here, ? 1 = 0.7 and ? c = 0.3 are scaling factors that we empirically tuned as hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Unpaired Training</head><p>For unpaired training, we do not enforce the global similarity and content loss constraints as the pairwise ground truth is not available. Instead, the objective is to learn both the forward mapping G F : {X, Z} ? Y and the reconstruction G R : {Y, Z} ? X simultaneously by maintaining cycle-consistency. As suggested by Zhu et al. <ref type="bibr" target="#b46">[47]</ref>, we formulate the cycle-consistency loss as follows:</p><formula xml:id="formula_4">L cyc (G F , G R ) = E X,Y,Z X ? G R (G F (X, Z)) 1 + E X,Y,Z Y ? G F (G R (Y, Z)) 1<label>(4)</label></formula><p>Therefore, our objective for the unpaired training is:</p><formula xml:id="formula_5">G * F , G * R = arg min G F ,G R max D Y ,D X L cGAN (G F , D Y ) + L cGAN (G R , D X ) + ? cyc L cyc (G F , G R )</formula><p>Here, D Y (D X ) is the discriminator associated with the generator G F (G R ), and the scaling factor ? cyc = 0.1 is an empirically tuned hyper-parameter. We do not enforce additional global similarity loss-term because the L cyc computes analogous reconstruction loss for each domain in L 1 space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">EUVP Dataset</head><p>The EUVP dataset contains a large collection of paired and unpaired underwater images of poor and good perceptual quality. We used seven different cameras, which include multiple GoPros [17], Aqua AUV's uEye cameras <ref type="bibr" target="#b13">[14]</ref>, low-light USB cameras <ref type="bibr" target="#b5">[6]</ref>, and Trident ROV's HD camera <ref type="bibr" target="#b34">[35]</ref>, to capture images for the dataset. The data was collected during oceanic explorations and human-robot cooperative experiments in different locations under various visibility conditions. Additionally, images extracted from a few publicly available YouTube TM videos are included in the dataset. The images are carefully selected to accommodate a wide range of natural variability (e.g., scenes, waterbody types, lighting conditions, etc.) in the data.</p><p>The unpaired data is prepared, i.e., good and poor quality images are separated based on visual inspection by six  human participants. They inspected several image properties (e.g., color, contrast, and sharpness) and considered whether the scene is visually interpretable, i.e., foreground/objects are identifiable. Hence, the unpaired training endorses the modeling of human perceptual preferences of underwater image quality. On the other hand, the paired data is prepared by following a procedure suggested in <ref type="bibr" target="#b14">[15]</ref>. Specifically, a CycleGAN [47]-based model is trained on our unpaired data to learn the domain transformation between the good and poor quality images. Subsequently, the good quality images are distorted by the learned model to generate respective pairs; we also augment a set of underwater images from the ImageNet dataset <ref type="bibr" target="#b12">[13]</ref> and from Flickr TM .</p><p>There are over 12K paired and 8K unpaired instances in the EUVP dataset; a few samples are provided in <ref type="figure" target="#fig_4">Fig. 3</ref>. It is to be noted that our focus is to facilitate perceptual image enhancement for boosting robotic scene understanding, not to model the underwater optical degradation process for image restoration, which requires scene depth and waterbody properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We use TensorFlow libraries <ref type="bibr" target="#b0">[1]</ref> to implement the FUnIE-GAN model. It is trained separately on 11K paired and 7.5K unpaired instances; the rest are used for respective validation and testing. Four NVIDIA TM GeForce GTX 1080 graphics cards are used for training; both models are trained for 60K-70K iterations with a batch-size of 8. We now present the experimental evaluations based on a qualitative analysis, standard quantitative metrics, and a user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Evaluations</head><p>We first qualitatively analyze the enhanced color and sharpness of the FUnIE-GAN-generated images compared to their respective ground truths. As <ref type="figure" target="#fig_6">Fig. 4a</ref> shows, the true color, and sharpness is mostly recovered in the enhanced images. Additionally, as shown in <ref type="figure" target="#fig_6">Fig. 4b</ref>, the greenish hue in underwater images are rectified and the global contrast is enhanced. These are the primary characteristics of an effective underwater image enhancer. We further demonstrate the contributions of each loss-terms of FUnIE-GAN: global similarity loss (L 1 ), and image content loss (L con ), for learning the enhancement. We observe that the L 1 term helps to generate sharper images, while the L con term contributes to furnishing finer texture details (see <ref type="figure" target="#fig_6">Fig. 4c</ref>). Moreover, we found slightly better numeric stability for L con with the block5 conv2 layer of VGG-19 compared to its last feature extraction layer (block5 conv4).</p><p>Next, we conduct a qualitative comparison of perceptual image enhancement by FUnIE-GAN with several state-ofthe-art models. We consider five learning-based models: (i) underwater GAN with gradient penalty (UGAN-P <ref type="bibr" target="#b14">[15]</ref>), (ii) Pix2Pix <ref type="bibr" target="#b23">[25]</ref>, (iii) least-squared GAN (LS-GAN <ref type="bibr" target="#b32">[33]</ref>), (iv) GAN with residual blocks <ref type="bibr" target="#b26">[27]</ref> in the generator (Res-GAN), and (v) Wasserstein GAN <ref type="bibr" target="#b2">[3]</ref> with residual blocks in the generator (Res-WGAN). These models are implemented with 8 encoder-decoder pairs (or 16 residual blocks) in the generator network and 5 convolutional layers in the discriminator. They are trained on the paired EUVP dataset using the same setup as the FUnIE-GAN. Additionally, we consider CycleGAN <ref type="bibr" target="#b46">[47]</ref> as a baseline for comparing the performance of FUnIE-GAN with unpaired training (i.e., FUnIE-GAN-UP). We also include two physics-based models in the comparison: Multi-band fusion-based enhancement (Mbad-EN <ref type="bibr" target="#b11">[12]</ref>), and haze-line-aware color restoration (Uw-HL <ref type="bibr" target="#b3">[4]</ref>). A common test set with 1K images (of 256?256 resolution) are used for the qualitative evaluation; it also includes 72 images with known waterbody types <ref type="bibr" target="#b3">[4]</ref>. A few sample comparisons are illustrated in <ref type="figure">Fig. 5</ref>.</p><p>As demonstrated in <ref type="figure">Fig. 5</ref>, Res-GAN, Res-WGAN, and Mbad-EN often suffer from over-saturation, while LS-GAN generally fails to rectify the greenish hue in images. UGAN-P, Pix2Pix, and Uw-HL perform reasonably well and their enhanced images are comparable to that of FUnIE-GAN; however, UGAN-P often over-saturates bright objects in the scene while Pix2Pix fails to enhance global brightness in some cases. On the other hand, we observe that achieving color consistency and hue rectification are relatively more challenging through unpaired learning. This  is mostly because of the lack of reference color or texture information in the loss function. Nevertheless, FUnIE-GAN-UP still outperforms CycleGAN in general. Overall, FUnIE-GAN performs as well and often better without using scene depth or prior waterbody information as the physics-based models, and despite having a much simpler network architecture compared to the existing learningbased models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Evaluation</head><p>We consider two standard metrics <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">20]</ref> named Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) in order to quantitatively compare FUnIE-GANenhanced images with their respective ground truths. The PSNR approximates the reconstruction quality of a gener-ated image x compared to its ground truth y based on their Mean Squared Error (MSE) as follows: P SN R(x, y) = 10 log 10 255 2 /M SE(x, y)</p><p>On the other hand, the SSIM <ref type="bibr" target="#b40">[41]</ref> compares the image patches based on three properties: luminance, contrast, and structure. It is defined as follows:</p><formula xml:id="formula_7">SSIM (x, y) = 2?x?y + c 1 ? 2 x + ? 2 y + c 1 2?xy + c 2 ? 2 x + ? 2 y + c 2<label>(6)</label></formula><p>In Eq. 6, ? x (? y ) denotes the mean, and ? 2 x (? 2 y ) is the variance of x (y); whereas ? xy denotes the crosscorrelation between x and y. Additionally, c 1 = (255 ? 0.01) 2 and c 2 = (255 ? 0.03) 2 are constants that ensure numeric stability.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we provide the averaged PSNR and SSIM values over 1K test images for FUnIE-GAN and compare the results with the same models used in the qualitative evaluation. The results indicate that FUnIE-GAN performs best on both PSNR and SSIM metrics. We conduct a similar analysis for Underwater Image Quality Measure (UIQM) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>, which quantifies underwater image colorfulness, sharpness, and contrast. We present the results in <ref type="table" target="#tab_2">Table 2</ref>, which indicates that although FUnIE-GAN-UP performs better than CycleGAN, its UIQM values on the the paired dataset are relatively poor. Interestingly, the models trained on paired data, particularly FUnIE-GAN, UGAN-P, and Pix2Pix, produce better results. We postulate that  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUnIE-GAN LS-GAN Res-GAN Pix2Pix</head><p>Res-WGAN FUnIE-GAN-UP* CycleGAN* UGAN-P Mband-En Uw-HL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning-based models</head><p>Physics-based models <ref type="figure">Figure 5</ref>: Qualitative performance comparison of FUnIE-GAN and FUnIE-GAN-UP with learning-based methods: Cycle-GAN <ref type="bibr" target="#b46">[47]</ref>, UGAN-P <ref type="bibr" target="#b14">[15]</ref>, Pix2Pix <ref type="bibr" target="#b23">[25]</ref>, LS-GAN <ref type="bibr" target="#b32">[33]</ref>, Res-GAN <ref type="bibr" target="#b26">[27]</ref>, and Res-WGAN <ref type="bibr" target="#b2">[3]</ref>; the super-scripted asterisk ( * ) denotes unpaired training. Two physics-based models: Mband-EN <ref type="bibr" target="#b11">[12]</ref> and Uw-HL <ref type="bibr" target="#b3">[4]</ref>, are also included in the comparison.</p><p>(Best viewed at 400% zoom) the global similarity loss in FUnIE-GAN and Pix2Pix, or the gradient-penalty term in UGAN-P contribute to this enhancement, as they all add L 1 terms in the adversarial objective. Our ablation experiments of FUnIE-GAN (see <ref type="figure" target="#fig_6">Fig. 4c</ref>) reveal that the L 1 loss-term contributes to 4.58% improvements in UIQM, while L con contributes 1.07%. Moreover, without both L 1 and L con loss-terms, the average UIQM values drop by 17.6%; we observe similar statistics for PSNR and SSIM as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">User Study</head><p>We also conduct a user study to add human preferences to our qualitative performance analysis. The participants are shown different sets of 9 images (one for each learningbased models) and asked to rank top 3 best quality images. A total of 78 individuals participated in the study and a total of 312 responses are recorded. <ref type="table" target="#tab_3">Table 3</ref> compares the average rank-1, rank-2, and rank-3 accuracy of the top 4 categories. The average rank-3 accuracy of the original images is recorded to be 6.67, which suggests that the users clearly preferred enhanced images over the original ones. Moreover, the results indicate that the users prefer the images enhanced by FUnIE-GAN, UGAN-P, and Pix2Pix compared to the other models; these statistics are consistent with our qualitative and quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Improved Visual Perception</head><p>As demonstrated in <ref type="figure" target="#fig_9">Fig. 6a</ref>, we conduct further experiments to quantitatively interpret the effectiveness of FUnIE-GAN-enhanced images for underwater visual perception over a variety of test cases. We analyze the performance of standard deep visual models for underwater object detection <ref type="bibr" target="#b21">[23]</ref>, 2D human body-pose estimation <ref type="bibr" target="#b8">[9]</ref>, and visual attention-based saliency prediction <ref type="bibr" target="#b39">[40]</ref>; although results vary depending on the image qualities of a particular test set, on an average, we observe 11-14%, 22-28%, and 26-28% improvements, respectively. We also evaluate other state-of-the-art models on the same test sets; as <ref type="figure" target="#fig_9">Fig. 6b</ref> suggests, images enhanced by UGAN-P, Res-GAN, Res-WGAN, Uw-HL, and Pix2Pix also achieve consider-   able performance improvements. However, these models offer significantly slower inference-rates than FUnIE-GAN, most of which are not suitable for real-time deployment in robotic platforms. FUnIE-GAN's memory requirement is 17 MB and it operates at a rate of 25.4 FPS (frames per second) on a singleboard computer (NVIDIA TM Jetson TX2), 148.5 FPS on a graphics card (NVIDIA TM GTX 1080), and 7.9 FPS on a robot CPU (Intel TM Core-i3 6100U). These computational aspects are ideal for it to be used as an image processing pipeline by visually-guided underwater robots in real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitations and Failure Cases</head><p>We observe a couple of challenging cases for FUnIE-GAN, which are depicted by a few examples in <ref type="figure">Fig. 7</ref>. First, FUnIE-GAN is not very effective for enhancing severely degraded and texture-less images. The generated images in such cases are often over-saturated by noise amplification. Although the hue rectification is generally correct, the color and texture recovery remains poor. Secondly, FUnIE-GAN-UP is prone to training instability. Our investigations suggest that the discriminator often becomes too good too early, causing a diminishing gradient effect that halts the generator's learning. As shown in <ref type="figure">Fig. 7 (right)</ref>, the gen-Original Generated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Issues with extremely low contrast and lack of textures</head><p>Inconsistent coloring by unstable training <ref type="figure">Figure 7</ref>: Extremely low-contrast and texture-less images are generally challenging for FUnIE-GAN, whereas FUnIE-GAN-UP often suffers from inconsistent coloring due to training instability.</p><p>erated images in such cases lack color consistency and accurate texture details. This is a fairly common issue in unpaired training of GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b24">26]</ref>, and requires meticulous hyper-parameter tuning.</p><p>FUnIE-GAN balances a trade-off between robustness and efficiency which limits its performance to a certain degree. More powerful deep models (i.e., denser architectures with more parameters) can be adopted for non-real-time applications; moreover, the input/output layers can be modified with additional bottleneck layers for learning enhancement at higher resolution than 256?256. On the other hand, FUnIE-GAN does not guarantee the recovery of true pixel intensities as it is designed for perceptual image quality enhancement. If scene depth and optical waterbody properties are available, underwater light propagation and image formation characteristics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref> can be incorporated into the optimization for more accurate image restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a simple yet efficient conditional GAN-based model for underwater image enhancement. The proposed model formulates a perceptual loss function by evaluating image quality based on its global color, content, local texture, and style information. We also present a large-scale dataset containing a paired and an unpaired collection of underwater images for supervised training. We perform extensive qualitative and quantitative evaluations, and conduct a user study which show that the proposed model performs as well and often better compared to the state-of-the-art models, in addition to ensuring much faster inference time. Moreover, we demonstrate its effectiveness in improving underwater object detection, saliency prediction, and human body-pose estimation performances. In the future, we plan to investigate its feasibility in other underwater humanrobot cooperative applications, marine trash identification, etc. We seek to improve its color consistency and stability for unpaired training as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c t d e te c ti o n a n d h u m a n p o se e st im a ti o n S a li en cy p re d ic ti o n O b je ct d et ec ti o n a n d h u m a n p o se es ti m a ti o n Sa li en cy p re d ic ti o n (b) Improved performance for underwater object detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Demonstration of underwater image enhancement using our proposed model and its practical feasibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of the proposed model: FUnIE-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Paired instances: ground truth images and their respective distorted pairs are shown on the top and bottom row, respectively. Good Poor (b) Unpaired instances: good and poor quality images are shown on the top and bottom row (in no particular order), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>A few sample images from the EUVP dataset are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>True color and sharpness is restored in the enhanced image.Input Generated (b) The greenish hue is rectified and global contrast is enhanced. Input w/o L1, Lcon w/o L1 w/o Lcon FUnIE-GAN (c) Ablation experiment: learning enhancement without (w/o) L1 and Lcon, w/o L1, and w/o Lcon loss-terms in FUnIE-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Demonstration of improved image attributes by FUnIE-GAN in terms of color, sharpness, and contrast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>A few snapshots showing qualitative improvement on FUnIE-GAN-generated images; a detailed demonstration can be found at: https://youtu.be/1ewcXQ-jgB4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Improvement versus inference-time comparison with the stateof-the-art models; FUnIE-GAN offers over 10 FPS speed (on common platform: Intel TM Core-i5 3.6GHz CPU); note that the run-times are evaluated on 256 ? 256 image patches for all the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Improved performance for object detection, saliency prediction, and human body-pose estimation on enhanced images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Generator: five encoder-decoder pairs with mirrored skip-connections (inspired by the success of U-Net<ref type="bibr" target="#b37">[38]</ref>; however, it is a much simpler model).</figDesc><table><row><cell>Input: 256 x 256 x 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output: 256 x 256 x 3</cell><cell cols="2">Input: 256 x 256 x 6</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">CONV, Leaky-ReLU, BN DECONV, Drop-out, BN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CONV, Leaky-ReLU, BN</cell><cell></cell></row><row><cell>128 x 128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>128 x 128</cell><cell>128 x 128</cell><cell></cell><cell></cell><cell></cell></row><row><cell>64 x 64 32 x 32</cell><cell>16 x 16</cell><cell>8 x 8</cell><cell>16 x 16</cell><cell>32 x 32</cell><cell>64 x 64</cell><cell>64 x 64</cell><cell>32 x 32</cell><cell>16 x 16</cell><cell>Output: 16 x 16</cell></row><row><cell cols="10">32 128 (a) 32 256 256 256 512 512 256 64 32 (b) Discriminator: a Markovian PatchGAN [25] 64 128 256 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">with four layers and a patch-size of 16?16.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison for average PSNR and SSIM values on 1K paired test images of EUVP dataset.</figDesc><table><row><cell>Model</cell><cell>P SN R G(x), y</cell><cell>SSIM G(x), y</cell></row><row><cell></cell><cell>Input: 17.27 ? 2.88</cell><cell>Input: 0.62 ? 0.075</cell></row><row><cell>Uw-HL</cell><cell>18.85 ? 1.76</cell><cell>0.7722 ? 0.066</cell></row><row><cell>Mband-EN</cell><cell>12.11 ? 2.55</cell><cell>0.4565 ? 0.097</cell></row><row><cell>Res-WGAN</cell><cell>16.46 ? 1.80</cell><cell>0.5762 ? 0.014</cell></row><row><cell>Res-GAN</cell><cell>14.75 ? 2.22</cell><cell>0.4685 ? 0.122</cell></row><row><cell>LS-GAN</cell><cell>17.83 ? 2.88</cell><cell>0.6725 ? 0.062</cell></row><row><cell>Pix2Pix</cell><cell>20.27 ? 2.66</cell><cell>0.7081 ? 0.069</cell></row><row><cell>UGAN-P</cell><cell>19.59 ? 2.54</cell><cell>0.6685 ? 0.075</cell></row><row><cell>CycleGAN</cell><cell>17.14 ? 2.65</cell><cell>0.6400 ? 0.080</cell></row><row><cell>FUnIE-GAN-UP</cell><cell>21.36 ? 2.17</cell><cell>0.8164 ? 0.046</cell></row><row><cell>FUnIE-GAN</cell><cell>21.92 ? 1.07</cell><cell>0.8876 ? 0.068</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison for average UIQM values on 1K paired and 2K unpaired test images of EUVP dataset.</figDesc><table><row><cell></cell><cell>Paired data</cell><cell>Unpaired data</cell></row><row><cell>Model</cell><cell>Input: 2.20 ? 0.69</cell><cell>Input: 2.29 ? 0.62</cell></row><row><cell></cell><cell>G. Truth: 2.91 ? 0.65</cell><cell>G. Truth: N/A</cell></row><row><cell>Uw-HL</cell><cell>2.62 ? 0.35</cell><cell>2.75 ? 0.32</cell></row><row><cell>Mband-EN</cell><cell>2.28 ? 0.87</cell><cell>2.34 ? 0.45</cell></row><row><cell>Res-WGAN</cell><cell>2.55 ? 0.64</cell><cell>2.46 ? 0.67</cell></row><row><cell>Res-GAN</cell><cell>2.62 ? 0.89</cell><cell>2.28 ? 0.34</cell></row><row><cell>LS-GAN</cell><cell>2.37 ? 0.78</cell><cell>2.59 ? 0.52</cell></row><row><cell>Pix2Pix</cell><cell>2.65 ? 0.55</cell><cell>2.76 ? 0.39</cell></row><row><cell>UGAN-P</cell><cell>2.72 ? 0.75</cell><cell>2.77 ? 0.34</cell></row><row><cell>CycleGAN</cell><cell>2.44 ? 0.71</cell><cell>2.62 ? 0.67</cell></row><row><cell>FUnIE-GAN-UP</cell><cell>2.56 ? 0.63</cell><cell>2.81 ? 0.65</cell></row><row><cell>FUnIE-GAN</cell><cell>2.78 ? 0.43</cell><cell>2.98 ? 0.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Rank-n accuracy (n = 1, 2, 3) for the top four models based on 312 responses provided by 78 individuals.</figDesc><table><row><cell>Model</cell><cell>Rank-1 (%)</cell><cell>Rank-2 (%)</cell><cell>Rank-3 (%)</cell></row><row><cell>FUnIE-GAN</cell><cell>24.50</cell><cell>68.50</cell><cell>88.60</cell></row><row><cell>FUnIE-GAN-UP</cell><cell>18.67</cell><cell>48.25</cell><cell>76.18</cell></row><row><cell>UGAN-P</cell><cell>21.25</cell><cell>65.75</cell><cell>80.50</cell></row><row><cell>Pix2Pix</cell><cell>11.88</cell><cell>45.15</cell><cell>72.45</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Revised Underwater Image Formation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6723" to="6732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01343</idno>
		<title level="m">Underwater Single Image Color Restoration using Haze-Lines and a New Quantitative Dataset</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robotic Tools for Deep Water Archaeology: Surveying an Ancient Shipwreck with an Autonomous Underwater Vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Camilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Delaporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics (JFR)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bluerobotics</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Low-Light</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usb Camera</surname></persName>
		</author>
		<ptr target="https://www.bluerobotics.com/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">True Color Correction of Autonomous Underwater Vehicle Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics (JFR)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DehazeNet: An End-to-end System for Single Image Haze Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime Multiperson 2d Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6306" to="6314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-assisted Multiband Fusion for Single Image Enhancement and Applications to Robot Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giguere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Prahacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saunderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Torres-Mendez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An Amphibious Autonomous Robot. Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing Underwater Imagery using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7159" to="7165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Underwater Image Enhancement Using a Multiscale Dense Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Oceanic Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single Image Haze Removal using Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Quality Metrics: PSNR vs. SSIM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DSLR-quality Photos on Mobile Devices with Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3277" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward a Generic Diver-Following Algorithm: Balancing Robustness and Efficiency in Deep Visual Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding Human Motion and Gestures for Underwater Human-Robot Collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Field Robotics (JFR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image-toimage Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual Losses for Real-time Style Transfer and Super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual Generative Adversarial Networks for Small Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="387" to="394" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Underwater Image Enhancement With a Deep Residual Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05320</idno>
		<title level="m">Real-world Underwater Enhancement: Challenging, Benchmark and Efficient Solutions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Underwater Image Enhancement using Guided Trigonometric Bilateral Filter and Fast Automatic Color Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Serikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trident</surname></persName>
		</author>
		<ptr target="https://www.openrov.com" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human-visual-systeminspired Underwater Image Quality Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Oceanic Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Retinex Processing for Automatic Image Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="111" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-domain Monitoring of Marine Environments using a Heterogeneous Robot Team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shkurti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meghjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C G</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giguere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1747" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salient Object Detection Driven by Fixation Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: from Error Visibility to Structural Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DualGAN: Unsupervised Dual Learning for Image-to-image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Underwater-GAN: Underwater Image Restoration via Conditional Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Underwater Image Enhancement via Extended Multi-scale Retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Energy-based Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired Image-to-image Translation using Cycle-consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
