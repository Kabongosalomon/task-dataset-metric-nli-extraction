<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Visual Geo-localization for Large-Scale Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Berton</surname></persName>
							<email>gabriele.berton@polito.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Masone</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CINI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Politecnico di Torino</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Visual Geo-localization for Large-Scale Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual Geo-localization (VG) is the task of estimating the position where a given photo was taken by comparing it with a large database of images of known locations. To investigate how existing techniques would perform on a real-world city-wide VG application, we build San Francisco eXtra Large, a new dataset covering a whole city and providing a wide range of challenging cases, with a size 30x bigger than the previous largest dataset for visual geo-localization. We find that current methods fail to scale to such large datasets, therefore we design a new highly scalable training technique, called CosPlace, which casts the training as a classification problem avoiding the expensive mining needed by the commonly used contrastive learning. We achieve state-of-the-art performance on a wide range of datasets and find that CosPlace is robust to heavy domain changes. Moreover, we show that, compared to the previous state-of-the-art, CosPlace requires roughly 80% less GPU memory at train time, and it achieves better results with 8x smaller descriptors, paving the way for city-wide real-world visual geo-localization. Dataset, code and trained models are available for research purposes at https://github.com/gmberton/CosPlace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual geo-localization (VG), also known as visual place recognition <ref type="bibr" target="#b0">[1]</ref> or image localization <ref type="bibr" target="#b31">[32]</ref>, is a staple of computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b53">54]</ref> and robotics research <ref type="bibr">[9-11, 17, 22, 26]</ref> and it is defined as the task of coarsely recognizing the geographical location where a photo was taken, usually with a tolerance of few meters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>. This task is commonly approached as an image retrieval problem where the query to be localized is compared to a database of geo-tagged images: the most similar images retrieved from the database, together with their metadata, represent the hypotheses of the query's geographical location. In particular, all recent VG methods are learningbased and use a neural network to project the images into an embedding space that well represents the similarity of their <ref type="bibr">Figure 1</ref>. Map of various datasets on the city of San Francisco. Previous datasets only cover a sector of the city (green points) or are sparse (red points). SF-XL densely covers the whole city and provides a realistic case-study for large-scale applications. locations, and that can be used for the retrieval.</p><p>So far, research on VG has focused on recognizing the location of images in moderately sized geographical areas (e.g., a neighborhood). However, real-world applications of this technology, such as autonomous driving <ref type="bibr" target="#b14">[15]</ref> and assistive devices <ref type="bibr" target="#b11">[12]</ref>, are posed to operate at a much larger scale (e.g., cities or metropolitan areas), thus requiring massive databases of geo-tagged images to execute the retrieval. Having access to such massive databases, it would be advisable to use them also to train the model rather than just for the execution of the retrieval (inference). This idea requires us to rethink VG, addressing the two following limitations.</p><p>Non-representative datasets. The current datasets for VG are not representative of realistic large-scale applications, because they are either too small in the geographical coverage <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref> or too sparse <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref> (see <ref type="figure">Fig. 1</ref> for an example of these limitations). Moreover, current datasets follow the common practice of splitting the collected images into geographically disjoint sets for training and inference. However, this practice does not find a correspondence in the real world where one would likely opt to use images from the target geographical area to train the model. Considering also the cost of collecting the images, it would be advisable to use the whole database also for training. Scalability of training. Having access to a massive amount of data raises the question of how to use it effectively for training. All the recent state-of-the-art methods in VG use contrastive learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref> (mostly relying on a triplet loss), which heavily depends on mining of negative examples across the training database <ref type="bibr" target="#b0">[1]</ref>. This operation is expensive, and it becomes prohibitive when the database is very large. Lightweight mining strategies that explore only a small pool of samples can reduce the duration of the mining phase <ref type="bibr" target="#b49">[50]</ref>, but they still result in a slow convergence and possibly less effective use of the data. Contributions. In this paper, we address these two limitations with the following contributions:</p><p>? A new large-scale and dense dataset, called San Francisco eXtra Large (SF-XL), that is roughly 30x bigger than what is currently available (see <ref type="figure">Fig. 1</ref>). The dataset includes crowd-sourced (i.e., multi-domain) queries that make for a challenging problem.</p><p>? A procedure that uses a classification task as a proxy to train the model that is used at inference to extract discriminative descriptors for the retrieval. We call this method CosPlace. CosPlace is remarkably simple, it does not necessitate to mine negative examples, and it can effectively learn from massive collections of data.</p><p>Through extensive experimental validation, we demonstrate that not only CosPlace requires roughly 80% less GPU memory at train time than current SOTA, but also that a simple model trained with CosPlace on SF-XL surpasses the SOTA while using 8x smaller embeddings. Additionally, we show that this model generalizes far better to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Visual geo-localization as image retrieval. Visual geolocalization is commonly approached as an image retrieval problem, with a retrieved image deemed correct if it is within a predefined range (usually 25 meters) from the query's ground truth position <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>. All recent VG methods perform the retrieval using learned embeddings that are produced by a feature extraction backbone equipped with a head that implements some form of aggregation or pooling, the most notable being NetVLAD <ref type="bibr" target="#b0">[1]</ref>. These architectures are trained via contrastive learning, typically using a triplet loss <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref> and leveraging the geo-tags of the database images as a form of weak supervision to mine negative examples <ref type="bibr" target="#b0">[1]</ref>. There are various alternatives to this scheme, such as the Stochastic Attraction-Repulsion Embedding (SARE) loss <ref type="bibr" target="#b31">[32]</ref>, which allows to efficiently use multiple negatives at once, while keeping the training methodology from <ref type="bibr" target="#b0">[1]</ref>. Another solution is presented in <ref type="bibr" target="#b17">[18]</ref>, and it achieves new state-of-theart results by computing the loss not only over the full images but also over cleverly mined crops. Despite the variations, all these methods suffer from poor train-time scalability caused by expensive mining techniques. The problem of scalability also arises at test time, and it relates to the size of the descriptors, which impacts the required memory and the retrieval time. Although the dimensionality of NetVLAD descriptors can be reduced using PCA, this leads to a degradation of results <ref type="bibr" target="#b0">[1]</ref>, hence works like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> prefer to keep a high dimensionality of 4096. Alternatively, other works drop NetVLAD and instead use pooling <ref type="bibr" target="#b39">[40]</ref> to build smaller embeddings.</p><p>Visual geo-localization as classification. An alternative approach to visual geo-localization is to consider it a classification problem <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52]</ref>. These works build on the idea that two images coming from the same geographical region, although representing different scenes, are likely to share similar semantics, such as architectural styles, types of vehicles, vegetation, etc. In practice, these methods divide the geographical area of interest in cells and group the database of images in classes according to their cell. This formulation allows to scale the problem to the whole globe, but at the cost of reduced accuracy in the estimates because each class can span many kilometers. Therefore, these methods are not used to perform geo-localization when the estimates require tolerance of a few meters.</p><p>Relation to prior works. In this work, we propose a new approach to VG that combines the advantages of both retrieval (tolerance of a few meters) and classification (high scalability). Our method uses a classification task as a proxy to train the model without requiring any mining. This makes it possible to train with massive datasets, unlike the solutions that use contrastive learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>. At test time, the trained model is used to extract image descriptors and perform a classic retrieval. While CosPlace might appear similar to previous classificationbased works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52]</ref>, given that they also partition a map into classes, there are substantial differences. These prior works tackle the task of global classification and group images within very large cells (up to hundreds of kilometers wide), building on the idea that nearer scenes have similar semantics (e.g. if two images are from China, they might both depict Chinese ideograms). On the other hand, our partitioning strategy is designed to leverage the availability of dense data and ensure that if two images are from the same class, they visualize the same scene. Moreover, unlike <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52]</ref>, once trained our method can be used to perform geo-localization through image retrieval on any given geographical area.  <ref type="bibr" target="#b12">[13]</ref> 48k St Lucia <ref type="bibr" target="#b33">[34]</ref> 33k NCLT <ref type="bibr" target="#b6">[7]</ref> 3.8M Oxford RobotCar <ref type="bibr" target="#b32">[33]</ref> 27k CMU <ref type="bibr" target="#b1">[2]</ref> 128k Pittsburgh250k <ref type="bibr" target="#b0">[1]</ref> 278k TokyoTM/247 <ref type="bibr" target="#b45">[46]</ref> 189k MSLS <ref type="bibr" target="#b49">[50]</ref> 1.7M San Francisco Landmark <ref type="bibr" target="#b7">[8]</ref> 1.1M Aachen <ref type="bibr" target="#b40">[41]</ref> 4k SF-XL (Ours) 41.2M <ref type="table">Table 1</ref>. Comparison of various VG datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The San Francisco XL dataset</head><p>There are numerous datasets for VG (see Tab. 1), but none of them reflects the scenario in which the geolocalization must be performed in a large environment and with few meters of tolerance: some datasets are limited to a small geographical area <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref> whereas others do not densely cover the area <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. The San Francisco Landmark Dataset <ref type="bibr" target="#b7">[8]</ref> partially overcomes these limitations, but it does not cover the whole city, nor has long-term temporal variations, which are essential for robustly training a neural network <ref type="bibr" target="#b0">[1]</ref>. Here we propose the first city-wide, dense, and temporally variable dataset: San Francisco eXtra Large (SF-XL). Database. Like other datasets used in this task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, SF-XL's database is created from Google StreetView imagery. We collected 3.43M equirectangular panoramas (360?images) and split them horizontally in 12 crops, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref>. This results in a total of 41.2M images (some examples are presented in <ref type="figure">Fig. 9</ref>). Each crop is labeled with 6 DoF information (which includes GPS and heading). The images were taken between 2009 and 2021, thus providing an abundance of long-term temporal variations.</p><p>Besides scale and density, SF-XL differs from previous datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> in that the database is not split in geographically non-overlapping subsets for training, validation and testing. We argue that such division of the database does not reflect the reality of applications that may use VG. In fact, when tasked with building a VG application for a large geographical area, one would likely opt to train a neural network on the imagery of such area rather than additionally collecting images from a disjointed one (perhaps adjacent).</p><p>To this end, we use the 41.2M images as a training set, which therefore covers the whole area of San Francisco. While at test time, we could use the whole database of 41.2M images for the retrieval, this would be prohibitive for research purposes because extracting the descriptors for all the images on a single GPU requires days. Therefore, we use as test time database only the 2.8M images from the  year 2013, which still cover the whole SF-XL geographical area, We found this choice to be a good solution because the set of images is small enough to be a feasible research option for testing but large enough to simulate a real-world scenario and prevent the bad practice of validating or tuning hyperparameters on the test set (as it would take too long to validate on it every epoch).</p><p>Finally, for validation, we use a small set of images scattered through the whole city, made of 8k database images and 8k queries.</p><p>Queries. While previous methods require the train set to be split into database and queries, CosPlace does not need this distinction. For this reason, we release the training set as a whole, and we believe that methods relying on database/queries splits for training (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref>) should choose the split that best suits their needs, given that this choice can heavily influence the results (see in Tab. 3 the difference in results between the third and fourth row as an example of this phenomenon).</p><p>Concerning the test queries, we believe that they should not be from the same domain as the database, because in most real-world scenarios test time queries could come from unseen domains. This is also in agreement with what is done in the San Francisco Landmark Dataset <ref type="bibr" target="#b7">[8]</ref> and Tokyo 24/7 <ref type="bibr" target="#b45">[46]</ref>. Hence, in SF-XL we include two different sets of test queries:</p><p>? test set v1: a set of 1000 images collected from Flickr (similarly to Oxford5k <ref type="bibr" target="#b37">[38]</ref>, Paris6k <ref type="bibr" target="#b38">[39]</ref>, Sfm120k <ref type="bibr" target="#b39">[40]</ref>). Given the inaccurate GPS coordinates of Flickr imagery, all the images in this set were hand-picked, and their location has been manually verified. We also made sure to blur out faces and plate numbers to anonymize the pictures. These images are very diverse and have a wide range of viewpoint and illumination (day/night) changes (see <ref type="figure" target="#fig_0">Fig. 2</ref>);</p><p>? test set v2: a set of 598 images from the queries of the San Francisco Landmark Dataset <ref type="bibr" target="#b7">[8]</ref>, for which the 6 DoF coordinates have been generated by <ref type="bibr" target="#b47">[48]</ref>. Given that it provides 6 DoF labels, this set can also be used for large-scale pose estimation.</p><p>Tab. 2 presents a summary of SF-XL, while further information about it are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Given that a city-wide dataset requires millions of images (Sec. 3) and that visual geo-localization is by nature a large-scale task, we believe that a proper method for VG should be highly scalable, both at train and at test time. We find that current (and previous) state of the art <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> lacks these important qualities:</p><p>? at train time, they require to periodically compute the features of all database images and keep them in a cache: this results in a space and time complexity of O(n), which is suitable only for small datasets;</p><p>? these methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> rely on a NetVLAD layer <ref type="bibr" target="#b0">[1]</ref> or its variants <ref type="bibr" target="#b26">[27]</ref>, which produce high dimensional vectors and require large amounts of memory for inference (given that database descriptors should be kept in memory for efficient retrieval). As an example, a VGG-16 with NetVLAD produces vectors of dimension 32k, which for a 10M database would require 32k ? 10M ? 4B = 1220GB of memory. Smaller embeddings are usually obtained with reduction techniques such as PCA, although using dimensions lower than 4096 leads to a rapid decline of results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>To reduce train time complexity we take inspiration from the domain of face recognition, where cosFace <ref type="bibr" target="#b48">[49]</ref> and arcFace <ref type="bibr" target="#b13">[14]</ref> are key in achieving state-of-the-art results <ref type="bibr" target="#b43">[44]</ref>. These losses require the training set to be divided into classes; however, in VG the label space is continuous (GPS coordinates and optionally heading/orientation information), making it not straightforward to divide it into discrete classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Splitting the dataset into classes</head><p>A naive approach to divide the database into classes would be to split it into square geographical cells (see <ref type="figure" target="#fig_1">Fig. 3</ref>), using UTM coordinates {east, north} 1 , and further slice each cell into a set of classes according to each image's orientation/heading {heading}. Formally, the set of images assigned to the class C ei,nj ,h k would be</p><formula xml:id="formula_0">x : east M = e i , north M = n j , heading ? = h k<label>(1)</label></formula><p>where M and ? are two parameters (respectively in meters and degrees) that determine the extent of each class in Within any given cell there are images with very different headings/orientations (see the two images on the right), which should belong to different classes, hence the split of each cell into multiple classes according to their heading. We also see that the two images at the bottom, although representing the same scene/building, belong to different cells (and therefore to different classes), and this would be confusing for a naive classification-based algorithm.</p><p>position and heading. While this solution creates a set of classes from a VG dataset, it has a big limitation: nearly identical images (e.g., taken with the same orientation but a few centimeters apart) may be assigned to different classes due to quantization errors (see <ref type="figure" target="#fig_1">Fig. 3</ref>), which would confuse a classification-based training algorithm.</p><p>To overcome this limitation, we propose not to train the model using all the classes at once, but just groups of nonadjacent classes (two classes are adjacent if an infinitesimal difference in position or heading can bring an image from one class to the other). Intuitively, these groups, which we call CosPlace Groups, are akin to separate datasets and our proposed training procedure (explained later in Sec. 4.2) iterates over them, one at a time. We generate groups by fixing the minimum spatial separation that two classes of the same group should have, either in terms of translation or orientation. For this purpose, we introduce two hyperparameters: N controls the minimum number of cells between two classes of the same group, and L is the equivalent for the orientation (see <ref type="figure" target="#fig_2">Fig. 4</ref> for a visual description). Formally, we define the CosPlace Group G uvw as the set of classes defined as follows</p><formula xml:id="formula_1">G uvw = C einj h k : (e i mod N = u) ? ? (n j mod N = v) ? (h k mod L = w)<label>(2)</label></formula><p>By construction, CosPlace Groups are disjoint sets of The total number of groups with this configuration is therefore 3 ? 3 ? 2 = 18, and each cell contains 6 classes from 2 groups. classes from Eq. (1) with the following properties: Property 1: each class belongs to exactly one group. Property 2: within a given group, if two images belong to different classes they are at least M ? (N ? 1) meters apart or ? ? (L ? 1) degrees apart (see <ref type="figure" target="#fig_2">Fig. 4</ref>); Property 3: the total number of CosPlace Groups is N ? N ? L. Property 4: no two adjacent classes can belong to the same group (unless N = 1 or L = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training the network</head><p>We will now describe how the CosPlace Groups are used to train the model. We take inspiration from the Large Margin Cosine Loss (LCML) <ref type="bibr" target="#b48">[49]</ref> also known as cosFace <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, which has shown remarkable results in face recognition <ref type="bibr" target="#b43">[44]</ref> and landmark retrieval <ref type="bibr" target="#b50">[51]</ref>. Nonetheless, vanilla LCML cannot be applied directly to any VG dataset, given that images are not split into a finite number of classes. However, with the proposed partitioning of the dataset, we can perform LCML sequentially over each CosPlace group (where each group can be considered as a separate dataset) and iterate over the many groups. We call this training procedure CosPlace. As the LCML relies on a fully connected layer with output dimensionality equal to the number of classes, CosPlace requires one fully connected layer per group: these layers are then discarded for validation/test. Note that not all groups must necessarily be used for training, as one could simply use a single group and ignore the others; however, the full ablation at <ref type="figure" target="#fig_7">Fig. 10</ref> shows that using more than one group helps achieve better results. Therefore, we train sequentially over the groups. Formally, for each group:</p><formula xml:id="formula_2">L cosPlace = L lmcl (G uvw )<label>(3)</label></formula><p>where L lmcl is the LCML loss as defined in <ref type="bibr" target="#b48">[49]</ref> Training. Regarding hyperparameters, we set M = 10 meters, ? = 30?, N = 5 and L = 2. We define an epoch as 10k iterations over a group. We perform an epoch over the first group, then an epoch over the second group and so on, for a total of 50 epochs (i.e., 500k iterations with batch size of 32). To ensure that the same group is seen more than once during training, we use only 8 of the N ? N ? L (i.e., 5 ? 5 ? 2 = 50) groups. Validation is performed after each epoch, and once training is finished, testing is performed using the model that obtained the best performance on the validation set. In Appendix B.2, we provide further implementation details, and we discuss how CosPlace not only reduces the number of hyperparameters in comparison to previous VG methods but also that the ones it introduces have an intuitive meaning (see <ref type="figure" target="#fig_2">Fig. 4</ref>).</p><formula xml:id="formula_3">, u ? {0, ... , N }, v ? {0, ... , N }, W ? {0, ... ,</formula><p>With respect to NetVLAD-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref>, which represent the entirety of the state of the art of the past five years, our method does not require multiple steps, whereas the NetVLAD layer requires for features clusters to be computed before starting to train the network, and, optionally, PCA to be computed afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with other methods</head><p>In this section, we compare CosPlace with previous methods in visual geo-localization. For a proper assessment of the results, we test on 7 datasets, namely Pitts250k <ref type="bibr" target="#b0">[1]</ref>, Pitts30k <ref type="bibr" target="#b0">[1]</ref>, Tokyo 24/7 <ref type="bibr" target="#b45">[46]</ref>, St Lucia <ref type="bibr" target="#b33">[34]</ref>, Mapillary Street Level Sequences (MSLS) <ref type="bibr" target="#b49">[50]</ref>, and our proposed SF-XL test v1 and SF-XL test v2. Of these datasets, MSLS and St Lucia are composed of frontal view images taken with cars, while the rest rely on Google StreetView imagery to build the database. For MSLS, given that the test set labels have not been publicly released yet, we test on the validation set as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. As a metric, we use the recall@N with a 25 meters threshold, i.e., the percentage of queries for which at least one of the first N predictions is within a 25 meters distance from the query, following standard procedure <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>We perform experiments with NetVLAD <ref type="bibr" target="#b0">[1]</ref>, CRN <ref type="bibr" target="#b26">[27]</ref>, GeM <ref type="bibr" target="#b39">[40]</ref>, SARE <ref type="bibr" target="#b31">[32]</ref> and SFRS <ref type="bibr" target="#b17">[18]</ref>. These methods are trained on the popular Pitts30k <ref type="bibr" target="#b0">[1]</ref> dataset, and we apply color jittering for all, following SFRS. Although for some of them, it is intractable to train on a large-scale dataset such as MSLS or SF-XL (e.g., SFRS's code 2 relies on pairwise distance computation in features space between any pair of query-database image, which makes space and time complexity grow quadratically with the number of images), we are able to train GeM and NetVLAD also on MSLS and SF-XL using the partial mining from <ref type="bibr" target="#b49">[50]</ref>, which scales to larger datasets. With this mining technique, we perform training with two different database/queries splits of SF-XL (note that previous methods, unlike CosPlace, require the dataset to be split): SF-XL* uses images after 2010 as queries, and the rest as database, while SF-XL** uses images after 2015 as queries and before 2015 as database (resulting in a denser database). We also report results with other methods such as SPE-VLAD <ref type="bibr" target="#b52">[53]</ref>, SRALNet <ref type="bibr" target="#b35">[36]</ref>, APPSVR <ref type="bibr" target="#b36">[37]</ref> and APANet <ref type="bibr" target="#b56">[57]</ref>, although the code for these 2 https://github.com/yxgeee/OpenIBL works is not publicly available and we could not independently reproduce the results. -While other methods fail to reach competitive results with compact descriptors, we find that CosPlace reaches stateof-the-art with 512-D outputs, paving the way for scalable, robust, and efficient real-world applications. In Appendix B.3.2, we provide additional results by applying PCA to high-dimensionality descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>-Methods besides CosPlace do not benefit from training on SF-XL. This is likely due to the partial mining <ref type="bibr" target="#b49">[50]</ref> (given that standard mining <ref type="bibr" target="#b0">[1]</ref> is unfeasible on a large scale).</p><p>-For methods other than CosPlace, the way database and queries are partitioned in the training set makes a big difference: training on SF-XL* versus SF-XL**, which use different database/queries partitions, achieve quite different results. This is not a problem for CosPlace since it does not need a database/queries split for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fairness of comparisons.</head><p>While it can be argued that comparisons in Tab. 3 and Tab. 4 are not fair, given that most other methods are trained on Pitts30k, and CosPlace on a much larger dataset, we want to point out that i) CosPlace cannot be trained on Pitts30k nor MSLS (given that they lack heading labels), and ii) training previous state-of-theart methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> on SF-XL would require a huge amount of resources, and it is practically unfeasible: computing the cache (which contains all database and queries descriptors) involves a forward pass on 40M images, with a memory requirement of 5TB, and it would have to be computed periodically every 1000 iterations.</p><p>With these considerations in mind, we were able to use the partial mining method from <ref type="bibr" target="#b49">[50]</ref> to train GeM and NetVLAD, which have already been shown to perform competitively with such mining <ref type="bibr" target="#b49">[50]</ref>, whereas more modern techniques, such as SARE <ref type="bibr" target="#b31">[32]</ref> or SFRS <ref type="bibr" target="#b17">[18]</ref>, cannot be used with partial mining without requiring significant changes to their algorithm. However, the fact that GeM and NetVLAD do not benefit from training on the larger scale   of SF-XL proves that our superior results do not come simply from the dataset size and that the proposed algorithm is needed to succeed in city-wide geo-localization. Moreover, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref> we did not use as comparisons re-ranking based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45]</ref>, given that they perform an extra post-processing step, while methods like CosPlace simply rely on a more efficient retrieval through a nearest neighbor search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Computational efficiency</head><p>Train-time memory footprint. Compared to previous methods that use a triplet loss and require to keep the descriptors for each image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref> (or a large number of them <ref type="bibr" target="#b49">[50]</ref>) in a cache, CosPlace circumvents this by posing the problem as a classification task without mining/caching. This effectively means that it can scale to very large datasets, whereas previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref> would require massive amounts of memory to train on largescale datasets. Note that a cache for a large dataset could be extremely memory demanding: NetVLAD descriptors for an image weigh about 32k ? 4B = 128KB, while a JPEG image of dimension 480 ? 640 is about 70KB; therefore, the cache for a whole dataset is almost twice as heavy as the dataset itself.</p><p>GPU requirements. While previous state of the art (i.e., SFRS <ref type="bibr" target="#b17">[18]</ref>) requires four 11GB GPUs to train, CosPlace is much lighter. Using the same backbone (a VGG-16 <ref type="bibr" target="#b42">[43]</ref>), we only require 7.5 GB on a single GPU to obtain the results shown in Tab. 3 and Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptors dimension.</head><p>Since the advent of the groundbreaking NetVLAD paper <ref type="bibr" target="#b0">[1]</ref>, state of the art relied on the NetVLAD layer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, which produces highdimensional descriptors. Considering that the compactness of the descriptors is perhaps one of the most important factors when choosing an algorithm in the real world (given that for efficient retrieval, all database descriptors should be kept in memory), we produce much more compact vectors w.r.t. previous methods. For instance, while <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref> use the full NetVLAD dimension of 32k, and <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>  Training and testing speed. The network takes roughly one day to train on SF-XL, similarly to SFRS's <ref type="bibr" target="#b17">[18]</ref> training time on Pitts30k. At test time, with a VGG-16-based model, a V100 GPU is able to extract descriptors for 80 images per second, although we find that the speed largely depends on the backbone and is very similar among the methods of Tab. 3, given that all rely on a VGG-16. However, for largescale datasets, the time required for the k-Nearest Neighbors search dwarfs the extraction time (considering a realworld scenario where database descriptors are extracted offline). Given that exhaustive kNN's execution time linearly depends on the dimensionality of the descriptors, our 512-D network is 8 times faster than 4096-D SFRS for inference on large datasets and 64 times faster than 32k-D NetVLAD.  <ref type="table">Table 5</ref>. Ablation. Recall@1 with varying values of hyperparameters M , ?, N and L. The bottom row represents CosPlace, and the other rows represent CosPlace without some components: for example using ? = 360?means that heading labels are not used (i.e. all images within any given cell belong to the same class); similarly using N = 1 and L = 1 means that all classes belong to the same group (and only one group exists, resulting in no group separation). A more thorough ablation is in the appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation</head><p>Hyperparameters. To better understand how the choices made in Sec. 4 impact the results, we perform a series of experiments, reported in Tab. 5. The results show that any naive split of the dataset into classes fails to achieve competitive results. The fact that the last row (which represents the only experiment computed without any intra-group adjacent classes) achieves considerably better results than the other rows clearly proves the benefit of using CosPlace groups.</p><p>Backbones and descriptors dimensionality. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we explore the use of different backbones, such as ResNets <ref type="bibr" target="#b22">[23]</ref> and Transformers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. We find that CosPlace performs well with a variety of backbones, with no need to apply any changes to hyperparameters or layers. This is in contrast with NetVLAD-based architectures (e.g. ResNets followed by NetVLAD perform best when the backbone is cropped at the fourth residual layer instead of the last one <ref type="bibr" target="#b3">[4]</ref>  <ref type="table">Table 6</ref>. Limited downward scalability. The first column represents the number of images which are effectively used at training time (sampled from SF-XL), and the other columns show the re-call@1 on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Limitations</head><p>Heading labels. Unlike previous VG methods, which solely rely on GPS/UTM coordinates, we also take advantage of heading labels. This is a drawback of CosPlace, although we argue that heading labels are really inexpensive, and just like GPS coordinates, they can be collected simply through a sensor, without requiring any manual annotation whatsoever. However, some commonly used datasets such as Pitts30k and MSLS do not provide heading labels for their images, making CosPlace not trainable on such datasets.</p><p>Limited downward scalability. Although CosPlace has been designed for a large-scale dataset, in this paragraph we investigate how the use of a smaller training dataset influences the results. In Tab. 6, we show how recalls vary for logarithmically decreasing sizes of the training dataset. We find that CosPlace needs a large number of training images to reach SOTA performance, and therefore it is not suited to be used for training on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we study the task of visual geo-localization (VG) in large-scale applications. Using the newly proposed San Francisco eXtra Large (SF-XL) dataset, we find that current methods based on contrastive learning are hardly scalable to train on large quantities of data. To address this problem, we propose a new method, CosPlace, which allows us to train on large quantities of data efficiently. We demonstrate that very simple architectures trained on SF-XL using CosPlace can surpass the current state of the art while using an 8x smaller descriptor. Most importantly, CosPlace is remarkably simple, needs much less training resources than the contrastive-based approaches, and generalizes extremely well to other domains. Although CosPlace has some limitations, i.e., it is not suited for training on small datasets nor datasets without orientation labels, it paves the way for a new strategy to tackle VG in large-scale applications.</p><p>A. Further information on SF-XL General information. In <ref type="figure" target="#fig_4">Fig. 6</ref> we show the density of the training set (i.e. number of panoramas within each cell), in <ref type="figure" target="#fig_5">Fig. 7</ref> is its temporal distribution, and in <ref type="figure" target="#fig_6">Fig. 8</ref> we show the temporal variability of SF-XL test v1's queries. The StreetView images composing the train set, val set and test database, are 512 ? 512 images cropped from 360?panoramas. SF-XL test v1. While the database from SF-XL test v1 is very homogeneous, given that StreetView images are all taken at daytime with the same camera and good weather, the queries present large degrees of domain changes: there are night images, grayscale, with heavy changes in viewpoint and occlusions. Coming from the crowd-sourced platform Flickr, these queries are collected by a large number of users, also ensuring variety in the typologies of cameras. We resized these images so that their shorter side is 480 pixels. In <ref type="figure">Fig. 9</ref>, we show more examples of queries, besides the ones shown in <ref type="figure" target="#fig_0">Fig. 2</ref> of the main paper.</p><p>SF-XL test v2. While the database of SF-XL test v2 is the same as SF-XL test v1, the two sets use different queries. With the advantage of having 6 DoF labels, SF-XL test v2 can also be used   for pose estimation. The downside of this set is the homogeneity among its images, given that almost all are taken during sunny days, with clear views and without heavy occlusions. Some examples of the queries are shown in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Further ablations</head><p>In this section, we provide further results obtained by changing the hyperparameters of CosPlace to better understand their correlations to the final results.</p><p>In <ref type="figure" target="#fig_7">Fig. 10</ref> we report an extensive ablation obtained by changing the parameters used to split the dataset into groups and classes, namely M , ?, N and L, as well as using a different number of groups for training. Among other results, we see in the rightmost <ref type="figure">Figure 9</ref>. Examples from SF-XL. The first two rows of images are from the train set, the next two from the queries of SF-XL test v1, and the last two rows from the queries of SF-XL test v2. plot that using just a single group for training the model leads to a drop in recall@1 of just 1%, and that the optimal results are achieved using 8 of the 50 groups.</p><p>To better understand the importance that the GeM pooling <ref type="bibr" target="#b39">[40]</ref> has within the architecture used for CosPlace, we provide a set of experiments by replacing it with the average or max pooling in Tab. 7. From the table, we can see that CosPlace would outperform the previous state-of-the-art even with a standard architecture used for classification, made of a CNN backbone, a max pooling, and a fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Further implementation details</head><p>Regarding CosPlace training, to ensure that each class is well represented, only cells with at least 10 panoramas are considered for training, effectively discarding about 15% of the images. The hyperparameters of M = 10, ? = 30, N = 5, and L = 2 lead to the creation of 50 groups, where each group ends up with roughly 35k classes, and each class contains on average 19.8 images. As explained in the main paper, we only train on 8 (out of 50) groups, which together contain roughly 5.6M images. Note that the total size of the SF-XL training set is 41.2M (i.e., we only use 13.6% of the images), meaning that train-time scalability is a factor that can still be vastly improved in future works.</p><p>We use the Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with a learning rate of 0.00001, and a batch size of 32 images. We use color jittering as in <ref type="bibr" target="#b17">[18]</ref>. For results to be fair with <ref type="bibr" target="#b17">[18]</ref>, which uses a smart region cropping method, we also employ random cropping. Finally, the margin of the cosFace loss is set to 0.40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of hyperparameters.</head><p>Although CosPlace introduces a considerable amount of hyperparameters, we also note that there is no more need for many other ones used in previous state-ofthe-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>, such as the number of negatives per query (usually set to 10), refresh rate of the cache (1000), pool size of randomly sampled negatives (1000), threshold distance for train-time potential positives (10 meters) and the number of cluster in NetVLAD layer (64). Moreover, the intuitive meaning of the hyperparameters in CosPlace in comparison to the less obvious mining hyperparameters makes it easier to set them using common sense: for example, it is clear that a small M (or ?) leads to little intra-class spatial variations, while a large M (or ?) may cause two images of the same class to be too different; similarly, using a small value for N leads to a higher similarity between interclass (but same group) images, while using a very high N leads to classes being very geographically spread out, which can be a problem with smaller datasets (because groups would have few classes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Exploratory experiments B.3.1 Further results on backbones and descriptors dimensionality.</head><p>Given that previous methods (as recent as 2021) in Visual geolocalization rely on relatively old VGG-16 <ref type="bibr" target="#b42">[43]</ref> or AlexNet <ref type="bibr" target="#b30">[31]</ref> backbones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b56">57]</ref>, we believe that this is widening an already large gap between research and real-world applications, where one would want to obtain the best possible results with the lowest computational complexity. To narrow such a gap, we investigate how the use of more recent backbones can enhance CosPlace and lead to better results, smaller descriptors, and faster computation. To this end, we train CosPlace using a number of backbones, namely VGG-16 <ref type="bibr" target="#b42">[43]</ref>, ResNet-18, ResNet-50, ResNet-101 <ref type="bibr" target="#b22">[23]</ref>, ViT <ref type="bibr" target="#b15">[16]</ref>, CCT224 and CCT384 <ref type="bibr" target="#b19">[20]</ref>. All CNN backbones (i.e., VGG-16 and ResNets) are followed by a GeM pooling <ref type="bibr" target="#b39">[40]</ref> and a fully connected layer, Moreover, we experiment with various powers of 2 (from 32-D to 2048-D) as output dimension. Regarding transformers-based neural networks, we use the 384-D SeqPool output of CCT as descriptors, and for ViT, we obtain the 768-D output by feeding the CLS token to a multi-layer perceptron with tanh, following its original implementation <ref type="bibr" target="#b15">[16]</ref>. Preliminary results showed that directly using ViT's CLS token led to lower recalls. Results from <ref type="figure" target="#fig_8">Fig. 11</ref> clearly show that CosPlace presents encouraging results regardless of the depth of the backbone, and we argue that future works should focus on more modern architectures, such as the ResNets, which are generally faster, lighter and achieve comparable or better results than the commonly used VGG-16. We also see that CosPlace is able to reach remarkable recalls and robustness with very low dimensions; for example, we see that any 128-D architecture trained with CosPlace outperforms 4096-D NetVLAD (which is trained on Pitts30k) on any test dataset.</p><p>While transformers achieve lower results, we want to point out that we used the same hyperparameters for all experiments (e.g., same learning rate and optimizer), and we believe that performing a proper hyperparameter tuning independently for each backbone can increase the results shown in <ref type="figure" target="#fig_8">Fig. 11</ref>, at the cost of a large number of experiments. Moreover, while we used a resolution of 512 ? 512 for CNNs, transformers require a smaller size, respectively 224?224 for ViT and CCT224, and 384?384 for CCT384, and this can provide a further explanation of the lower results with transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 Comparison with other methods using same descriptors dimensionality.</head><p>Given that CosPlace uses much lower dimensionality of descriptors, in Tab. 8 and Tab. 9 we report the equivalent experiments of Tab. 3 and Tab. 4 of the main paper, but using the same (512) dimensionality for all methods. We can see that in this scenario, the advantages of CosPlace w.r.t. previous works are even more noticeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.3 Comparison with models trained on Google</head><p>Landmark.</p><p>In Tab. 10, we compare models trained using CosPlace on SF-XL with models trained on two popular landmark retrieval (LR) datasets, namely the Google Landmark (GLD) and SfM120k <ref type="bibr" target="#b39">[40]</ref>. Models trained on GLD and SfM120k are downloaded from the official repository of <ref type="bibr" target="#b39">[40]</ref> 3 , which relied on a triplet loss for training. CosPlace can't be used on such landmark retrieval datasets, as they lack GPS coordinates and heading labels.   Note that these experiments are not aimed at providing a rigorous comparison of CosPlace vs triplet losses or SF-XL vs standard retrieval datasets, given that the underlying tasks (i.e. VG and LR) present many differences; we just want to provide an intuition on how popular models trained for LR fare on VG datasets.  <ref type="table">Table 10</ref>. Comparison with models trained on large landmark retrieval datasets. The models trained on SF-XL is trained with CosPlace, while models trained on GLD and SfM120k rely on a triplet loss. All models are equivalent (i.e. ResNets followed by a GeM pooling and a fully connected layer with output dimensionality 512).  <ref type="figure" target="#fig_0">Figure 12</ref> shows some qualitative results of retrieved images with CosPlace compared to previous SOTA methods such as NetVLAD <ref type="bibr" target="#b0">[1]</ref>, CRN <ref type="bibr" target="#b26">[27]</ref>, SARE <ref type="bibr" target="#b31">[32]</ref> and SFRS <ref type="bibr" target="#b17">[18]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples of queries from SF-XL's test set v1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Grid of the map showing how cells are formed based on UTM coordinates. Each cell (of side lenght M ) is identified by a pair of values (for readability only a few are shown on the right). On each of the four images is written its triplet {UTM east, UTM north, heading} in white between braces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visual representation of CosPlace groups. The orange triangles represent one of the N ? N ? L different groups, while the purple triangles represent another group. For clarity, only two groups are shown (namely G000 and G111). Each triangle represents one class, which contains all images within the respective cell with the proper orientation. The meaning for each of the four hyperparameters that define how groups are built (i.e., M , ?, N and L) is visually shown. In the figure, ? = 60?, N = 3, L = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Results on Pitts250k of CosPlace using different backbones and dimensionality of descriptors, compared with SFRS and NetVLAD trained on Pitts30k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Histogram showing how many cells contain a given number of panorama. We can see that cells with only one panorama (which are discarded at train time as explained in Appendix B.2) are the most common. Note that the y axis is in logarithmic scale. The side of each cell (i.e. the hyperparameter M ) is M = 10 meters, as in our final experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Number of 360?panorama of SF-XL for each given year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Number of queries from SF-XL test v1 for each given year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Full ablation on each hyperparameter. On the x axis are values for the hyperparameters, and on the y axis their respective recall@1 on the SF-XL val set, computed with a ResNet-18. Values in bold are the chosen ones for all experiments besides ablations, and the red line represents their recall@1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Further results on backbones and descriptors dimensionality. Results on a number of datasets of CosPlace using different backbones and dimensionalities, compared with SFRS and NetVLAD trained on Pitts30k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative comparisons of retrieved images for a number of methods. B.3.4 Comparison with other methods: qualitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Number of images for each subset of SF-XL. The train set is not split into database and queries.</figDesc><table><row><cell cols="4">SF-XL SF-XL SF-XL test v1 SF-XL test v2</cell></row><row><cell>(Train)</cell><cell>(Val)</cell><cell>(Test)</cell><cell>(Test)</cell></row><row><cell>Database 41.2M Queries</cell><cell>8k 8k</cell><cell>2.8M 1000</cell><cell>2.8M 598</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>L}. In practice, we train one epoch on G 000 , the next one on G 001 , and so on, iterating over the groups. The remarkable advantage of this procedure w.r.t. the methods based on contrastive losses commonly used in visual geo-localization, is that no mining nor caching is required, making it a much more scalable option. At validation and test time, we use the model not to classify the query, but rather to extract image descriptors as in<ref type="bibr" target="#b48">[49]</ref> for a classic retrieval over the database. This allows for the model to be used also on other datasets from unseen geographical areas (see Tab. 3). CosPlace is architecture-agnostic, i.e. it can be applied on virtually any image-based model. For most experiments, we rely on a simple network made of a standard CNN backbone followed by a GeM pooling and a fully connected layer with output dimension 512. Note that such a simple architecture is in contrast with the trend of the last five years of research in Visual Geo-localization<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref>, where the architectures rely on a more complex (w.r.t. our architecture) NetVLAD layer<ref type="bibr" target="#b0">[1]</ref>, and some even add a number of blocks on top of it<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, resulting in heavier and slower architectures. Given that previous methods rely on a VGG-16 backbone<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref>, we use a VGG-16 in Sec. 5.2, for fair comparisons with other methods. For our ablations and preliminary experimentations, we relied on a ResNet-18, which achieves similar results to the VGG-16 at a fraction of the training time and memory requirements. In Sec. 5.4 and Appendix B.3.1, we investigate the use of more recent backbones (ResNets<ref type="bibr" target="#b22">[23]</ref> and transformer-based<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>) with varying output dimensionality, finding that CosPlace reaches encouraging results with a wide variety of architectures and thus demonstrates a great flexibility. For example, we find that CosPlace with a ResNet-101 backbone and 128-D descriptors outperforms current SOTA, which uses 4096-D descriptors.</figDesc><table><row><cell>5. Experiments</cell></row><row><cell>5.1. Implementation details</cell></row><row><cell>Architecture.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>of results. Results are reported in Tab. 3 and Tab. 4, and all rely on VGG-16-based architectures. The results can be summarized in a few points: -CosPlace achieves best results on average, outperforming the second-best method by 8.5% of R@1 averaged over five datasets. -CosPlace shows strong robustness to datasets coming from other sources. On the other hand, other methods either perform well on datasets with StreetView-sourced database (i.e., Pitts30k, Pitts250k, Tokyo 24/7) or on datasets with frontal view images (i.e., MSLS and St Lucia). For example, the best performing method on MSLS and St Lucia, namely NetVLAD trained on MSLS, falls short of 10.0% of recall@1 on Pitts250k w.r.t. CosPlace; similarly, the best performing on Pitts30k, namely SFRS trained on Pitts30k, is outperformed by CosPlace by 21.8% on St Lucia.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of various methods on popular datasets. Two values of recalls are used as metric (R@1, R@5), with threshold distance for positives of 25 meters. Results rely on a CNN with a VGG-16 backbone, and are averaged over 3 runs with seeds 0, 1 and 2. * images taken after 2010 are used as queries, and the rest as database. ** images taken after 2015 are used as queries, and the rest as database. ?these results could not be independently verified given that no public code implementation is available and we were unable to reproduce the results. The results (when available) were taken from the respective papers.</figDesc><table><row><cell>Method</cell><cell cols="2">Desc. dim. Train set</cell><cell cols="3">SF-XL test v1 R@1 R@5 R@10</cell><cell cols="3">SF-XL test v2 R@1 R@5 R@10</cell></row><row><cell>NetVLAD [1]</cell><cell>32768</cell><cell>Pitts30k</cell><cell>40.0</cell><cell>48.8</cell><cell>52.8</cell><cell>71.1</cell><cell>82.4</cell><cell>84.8</cell></row><row><cell>NetVLAD [1]</cell><cell>32768</cell><cell>MSLS</cell><cell>22.9</cell><cell>32.0</cell><cell>37.1</cell><cell>48.2</cell><cell>64.7</cell><cell>69.7</cell></row><row><cell>NetVLAD [1]</cell><cell>32768</cell><cell>SF-XL*</cell><cell>38.3</cell><cell>47.4</cell><cell>51.7</cell><cell>70.7</cell><cell>83.4</cell><cell>87.0</cell></row><row><cell>NetVLAD [1]</cell><cell>32768</cell><cell cols="2">SF-XL** 28.4</cell><cell>38.6</cell><cell>43.1</cell><cell>60.9</cell><cell>74.6</cell><cell>78.6</cell></row><row><cell>CRN [27]</cell><cell>32768</cell><cell>Pitts30k</cell><cell>45.8</cell><cell>56.9</cell><cell>60.7</cell><cell>76.4</cell><cell>85.3</cell><cell>87.8</cell></row><row><cell>SARE [32]</cell><cell>4096</cell><cell>Pitts30k</cell><cell>45.5</cell><cell>56.5</cell><cell>60.0</cell><cell>78.8</cell><cell>87.6</cell><cell>90.5</cell></row><row><cell>SFRS [18]</cell><cell>4096</cell><cell>Pitts30k</cell><cell>51.2</cell><cell>62.2</cell><cell>66.6</cell><cell>83.1</cell><cell>90.5</cell><cell>93.5</cell></row><row><cell>GeM [40]</cell><cell>512</cell><cell>Pitts30k</cell><cell>21.7</cell><cell>30.3</cell><cell>34.4</cell><cell>43.1</cell><cell>63.7</cell><cell>69.2</cell></row><row><cell>GeM [40]</cell><cell>512</cell><cell>MSLS</cell><cell>8.1</cell><cell>15.6</cell><cell>20.2</cell><cell>29.3</cell><cell>46.3</cell><cell>53.8</cell></row><row><cell>GeM [40]</cell><cell>512</cell><cell>SF-XL*</cell><cell>9.8</cell><cell>17.6</cell><cell>21.2</cell><cell>34.8</cell><cell>55.5</cell><cell>63.0</cell></row><row><cell>CosPlace (Ours)</cell><cell>512</cell><cell>SF-XL</cell><cell>64.7</cell><cell>73.3</cell><cell>76.6</cell><cell>83.4</cell><cell>91.6</cell><cell>94.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Comparisons on SF-XL test v1 and SF-XL test v2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>reduce it to 4k, CosPlace achieves SOTA results with a dimension of just 512. Moreover, in Sec. 5.4 and Appendix B.3.1, we investigate how results depend on the dimensionality of the descriptors (and the underlying backbones), showing that CosPlace with a ResNet-101 and 128-D descriptors outperforms previous SOTA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Ablation over different pooling layers. This table shows results obtained by replacing the GeM layer with a max or average pooling. Results refer to the recall@1 obtained with a ResNet-18.</figDesc><table><row><cell cols="6">Pooling Pitts250k Pitts30k Tokyo 24/7 MSLS St Lucia</cell></row><row><cell>Average</cell><cell>88.5</cell><cell>87.6</cell><cell>73.7</cell><cell>78.5</cell><cell>98.7</cell></row><row><cell>Max</cell><cell>90.8</cell><cell>89.3</cell><cell>78.1</cell><cell>80.5</cell><cell>98.7</cell></row><row><cell>GeM</cell><cell>90.4</cell><cell>89.5</cell><cell>81.6</cell><cell>81.8</cell><cell>98.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of various methods on popular datasets with 512-D descriptors. This table is the equivalent of Tab. 3 in the main paper, but here all descriptors have the same dimensionality.</figDesc><table><row><cell>Method</cell><cell cols="4">Desc. dim. Train set</cell><cell cols="3">Pitts250k R@1 R@5</cell><cell></cell><cell cols="2">Pitts30k R@1 R@5</cell><cell cols="2">Tokyo 24/7 R@1 R@5</cell><cell>R@1</cell><cell>MSLS</cell><cell>R@5</cell><cell>St Lucia R@1 R@5</cell></row><row><cell cols="5">GeM [40] Pitts30k</cell><cell>84.8</cell><cell></cell><cell>93.5</cell><cell></cell><cell>-</cell><cell>-</cell><cell>60.6</cell><cell>76.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>APPSVR [37]  ?</cell><cell></cell><cell>512</cell><cell cols="2">Pitts30k</cell><cell>85.3</cell><cell></cell><cell>94.0</cell><cell></cell><cell>-</cell><cell>-</cell><cell>62.0</cell><cell>76.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CosPlace (Ours)</cell><cell>512</cell><cell cols="2">SF-XL</cell><cell cols="3">89.3? 0.2 96.2? 0.3</cell><cell cols="3">88.5? 0.1 94.5? 0.2</cell><cell cols="2">82.2? 0.5 88.9? 0.9</cell><cell>79.6? 0.5 87.2? 0.4</cell><cell>94.1? 0.8 97.4? 0.1</cell></row><row><cell>Method</cell><cell cols="3">Desc. dim. Train set</cell><cell cols="3">SF-XL test v1 R@1 R@5 R@10</cell><cell cols="3">SF-XL test v2 R@1 R@5 R@10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeM</cell><cell>512</cell><cell></cell><cell cols="2">Pitts30k 21.7</cell><cell>30.3</cell><cell>34.4</cell><cell>43.1</cell><cell>63.7</cell><cell>69.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeM</cell><cell>512</cell><cell></cell><cell>MSLS</cell><cell>8.1</cell><cell>15.6</cell><cell>20.2</cell><cell>29.3</cell><cell>46.3</cell><cell>53.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeM</cell><cell>512</cell><cell></cell><cell>SF-XL*</cell><cell>9.8</cell><cell>17.6</cell><cell>21.2</cell><cell>34.8</cell><cell>55.5</cell><cell>63.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NetVLAD</cell><cell>512</cell><cell></cell><cell cols="2">Pitts30k 27.4</cell><cell>38.1</cell><cell>43.6</cell><cell>66.7</cell><cell>79.3</cell><cell>82.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NetVLAD</cell><cell>512</cell><cell></cell><cell>MSLS</cell><cell>14.5</cell><cell>21.0</cell><cell>28.9</cell><cell>40.5</cell><cell>59.7</cell><cell>64.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NetVLAD</cell><cell>512</cell><cell></cell><cell cols="2">SF-XL* 25.4</cell><cell>32.9</cell><cell>40.5</cell><cell>66.9</cell><cell>78.6</cell><cell>82.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CRN</cell><cell>512</cell><cell></cell><cell cols="2">Pitts30k 31.4</cell><cell>43.0</cell><cell>49.7</cell><cell>68.2</cell><cell>81.3</cell><cell>83.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SARE</cell><cell>512</cell><cell></cell><cell cols="2">Pitts30k 30.8</cell><cell>42.1</cell><cell>46.5</cell><cell>69.2</cell><cell>81.1</cell><cell>83.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SFRS</cell><cell>512</cell><cell></cell><cell cols="2">Pitts30k 35.6</cell><cell>49.7</cell><cell>54.8</cell><cell>78.1</cell><cell>88.5</cell><cell>91.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CosPlace (Ours)</cell><cell>512</cell><cell></cell><cell>SF-XL</cell><cell>65.1</cell><cell>73.6</cell><cell>77.6</cell><cell>83.4</cell><cell>92.1</cell><cell>94.8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Comparisons of various methods on SF-XL test v1and SF-XL test v2 with 512-D descriptors. This table is the equivalent of Tab. 4 in the main paper.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">UTM coordinates are defined by a system used to identify locations on earth in meters, where 1 UTM unit corresponds to 1 meter. They can be extracted from GPS coordinates (i.e., latitude and longitude) and allow approximating a restricted area of the earth's surface on a flat surface.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https : / / github . com / filipradenovic / cnnimageretrieval-pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We acknowledge the CINECA award under the ISCRA initiative, for the availability of high performance computing resources.This work was supported by CINI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the following, Appendix A provides additional information on San Francisco eXtra Large (SF-XL), Appendix B.1 presents a thorough ablation over all CosPlace hyperparameters, and Appendix B.2 provides further implementation details on CosPlace. Finally, in Appendix B.3, we provide a large set of extra results, comprising experiments on changing the backbones and the descriptors dimensionality, and further comparisons with other methods.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding how camera configuration and environmental conditions affect appearance-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hern?n</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium Proceedings</title>
		<meeting><address><addrLine>Dearborn, MI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Viewpoint invariant dense matching for visual geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Paolicelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep visual geo-localization benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Mereu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Trivigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive-attentive geolocalization from few queries: A hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Paolicelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2918" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying deep local and global features for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="726" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">University of Michigan North Campus long-term vision and lidar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Ushani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">City-scale landmark identification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pylv?n?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roimela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning features at scale for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3223" to="3230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning context flexible attention model for long-term visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4015" to="4022" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Only look once, mining distinctive landmarks from ConvNet for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maffra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unifying visual localization and scene recognition for people with visual impairment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64284" to="64296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highly scalable appearanceonly slam -FAB-MAP 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4685" to="4694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable place recognition under appearance change for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-D</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9319" to="9328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Recognition at Scale. ArXiv, abs/2010.11929, 2021. 5</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic-geometric visual place recognition: a new perspective for reconciling opposing views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suenderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervising fine-grained region similarities for large-scale image localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Escaping the Big Data Paradigm with Compact Transformers. ArXiv, abs/2104.05704</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="14141" to="14152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-process fusion: Visual place recognition using multiple image processing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inside out visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ibrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Nanne Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Alpherts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting the earth&apos;s spherical geometry to geolocate images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Izbicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Tsotras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -European Conference, ECML PKDD 2019</title>
		<editor>Arno J. Knobbe, Marloes H. Maathuis, and C?line Robardet</editor>
		<meeting><address><addrLine>W?rzburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11907</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A holistic visual place recognition approach using lightweight CNNs for significant viewpoint and appearance changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khaliq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonald-Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="561" to="569" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learned contextual feature reweighting for image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo</forename><forename type="middle">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Avoiding confusing features in place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Leveraging efficientnet and contrastive learning for accurate global-scale location estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Galopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mapping a suburb with a single camera using a biologically inspired slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1038" to="1053" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geolocation estimation of photos using a hierarchical model and scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>M?ller-Budack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kader</forename><surname>Pustu-Iren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Ewerth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11216</biblScope>
			<biblScope unit="page" from="575" to="592" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XII</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic reinforced attention learning for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentional pyramid pooling of salient visual residuals for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021-10-02" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fine-tuning CNN Image Retrieval with No Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A performance comparison of loss functions for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnav</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv Ram</forename><surname>Dubey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Instancelevel image retrieval using reranking transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="271" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual place recognition with repetitive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2346" to="2359" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Are large-scale 3d models really necessary for accurate visual localization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mapillary street-level sequences: A dataset for lifelong place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Warburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soren</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Google landmarks dataset v2 -a large-scale benchmark for instance-level recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ara?jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2572" to="2581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Planet -photo geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial pyramid-enhanced netvlad with weighted triplet loss for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">VPR-Bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubariz</forename><surname>Zaffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mcdonald-Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Ehsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2136" to="2174" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image geo-localization based on multiple nearest neighbor feature matching using generalized graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Largescale image geo-localization using dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyasu</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Tariku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1702.01238</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention-based pyramid aggregation network for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sralnet</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
