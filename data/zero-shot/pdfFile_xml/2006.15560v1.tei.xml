<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Sampling Networks for Efficient Action Recognition in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Dong</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Limin</roleName><forename type="first">Tong</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>IEEE</roleName><forename type="first">Wang</forename><surname>Member</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Sampling Networks for Efficient Action Recognition in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Dynamic sampling networks</term>
					<term>Reinforcement learning</term>
					<term>Efficient action recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing action recognition methods are mainly based on clip-level classifiers such as two-stream CNNs or 3D CNNs, which are trained from the randomly selected clips and applied to densely sampled clips during testing. However, this standard setting might be suboptimal for training classifiers and also requires huge computational overhead when deployed in practice. To address these issues, we propose a new framework for action recognition in videos, called Dynamic Sampling Networks (DSN), by designing a dynamic sampling module to improve the discriminative power of learned clip-level classifiers and as well increase the inference efficiency during testing. Specifically, DSN is composed of a sampling module and a classification module, whose objective is to learn a sampling policy to on-the-fly select which clips to keep and train a clip-level classifier to perform action recognition based on these selected clips, respectively. In particular, given an input video, we train an observation network in an associative reinforcement learning setting to maximize the rewards of the selected clips with a correct prediction. We perform extensive experiments to study different aspects of the DSN framework on four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.3. The experimental results demonstrate that DSN is able to greatly improve the inference efficiency by only using less than half of the clips, which can still obtain a slightly better or comparable recognition accuracy to the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A CTION recognition in videos has drawn enormous attention from the computer vision community in the past decades <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, partially due to its applications in many areas such as video surveillance, content-based analysis, and human-computer interaction. Recently, deep learning methods have witnessed great success for many image-based tasks, such as image classification <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b12">[13]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14]</ref>, and pose estimation <ref type="bibr" target="#b14">[15]</ref>. These deep models have also been introduced into the video domain for action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which has shown superior performance to the traditional hand-crafted representations <ref type="bibr" target="#b0">[1]</ref>. However, unlike the static image, the extra temporal dimension of the video increases the difficulty of developing an effective deep action recognition approach. When proposing an action recognition solution, it is necessary to carefully consider the inherent properties of temporal information.</p><p>Currently, deep learning methods for action recognition are mostly based on short-term classifiers, due to modeling Y. Zheng, Z. Liu, T. Lu, and L. Wang are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China.</p><p>Y. Zheng and Z. Liu equally contribute to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High Jump</head><p>High Jump High Jump <ref type="figure">Fig. 1</ref>. Dynamic clip sampling for action recognition. DSN framework dynamically selects a subset of clips from video for action recognition. It not only improves the accuracy of the final recognition but also greatly improves the inference efficiency during the testing phase.</p><p>complexity and computational overhead of directly learning a long-term video-level classifier. These short-term classifiers are with architectures of 2D CNNs <ref type="bibr" target="#b15">[16]</ref> or 3D CNNs <ref type="bibr" target="#b16">[17]</ref>, which operate on a single frame (e.g., two-stream CNNs <ref type="bibr" target="#b1">[2]</ref>) or a short clip of multiple consecutive frames (e.g. C3D <ref type="bibr" target="#b3">[4]</ref>, I3D <ref type="bibr" target="#b5">[6]</ref>, ARTNet <ref type="bibr" target="#b7">[8]</ref>). In the training phase, these shortterm classifiers are optimized on a single randomly sampled clip <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b1">[2]</ref> or a set of uniformly sampled clips from the whole video <ref type="bibr" target="#b4">[5]</ref>. In the test phase, to gather the information from the whole video, these trained short-term classifiers are applied in a fully convolutional manner on densely sampled video clips. The recognition results from densely testing are simply averaged to yield the final recognition results. However, there are two critical issues in the above framework when training short-term classifiers randomly and testing these classifiers densely.</p><p>First, training these short-term classifiers from randomly sampled clips may not be the best approach from the training perspective. As depicted in <ref type="figure">Fig. 1</ref>, the distribution of actionrelated information is uneven across the video. In this sense, some clips in a video may contribute more to action recognition than others. Furthermore, the distribution of clip-level importance may vary among different input videos. Therefore, given an input video, we need to on-the-fly select the most important clips to recognize the action in the video. This dynamic and adaptive sampling module will force short-term classifiers to focus on these more discriminative clips which are also beneficial to train more powerful classifiers.</p><p>Second, from the testing perspective, it is expensive to apply these trained short-term classifiers in a fully convolutional manner on densely sampled clips. Meanwhile, simply averaging from these dense prediction scores may also be problematic because these recognition results from less relevant clips will over-smooth the final recognition result. It is well known the clips in the video are redundant and semantics varies slowly along the temporal dimension. Therefore, it is possible to test these short-term classifiers only on a subset of densely sampled clips, which can greatly improve inference efficiency and possibly improve the final recognition accuracy.</p><p>To mitigate the above issues, we propose a new principled framework called as Dynamic Sampling Network (DSN) for action recognition in videos based on a section based selection policy. The primary objective is to train an optimized sampling strategy that is able to maintain the accuracy of action recognition while select very few clips based on each input video. Specifically, the DSN framework is composed of a sampling module and a classification module, which are optimized in an alternating manner. The sampling module is composed of an observation network and a policy maker. The observation network takes several clips as inputs and generates a posterior probability, and the policy maker determines which clips to select based on the posterior probability distribution. The selected clips will be fed into the classification module, and the rewards based on the predictions will guide the training of the observation network. In addition, based on the sampling module output, the classification module will be fine-tuned on the video dataset, by focusing more on these clips with high probabilities. In implementation, to improve inference efficiency, the complexity of the sampling module is much smaller than that of the classification module. When a deploying DSN in practice, the trained sampling module will dynamically determine a small subset of clips to be fed into the classification module, thus greatly improving the efficiency of the whole action recognition system.</p><p>Concerning the training of our DSN, we formulate the framework of DSN within an associative reinforcement learning <ref type="bibr" target="#b17">[18]</ref> where all the decisions are made in a single step given the context. Associative reinforcement learning is different from full reinforcement learning <ref type="bibr" target="#b18">[19]</ref>. Full reinforcement learning is a multi-step Markov decision process. Through multi-step exploration, agent continuously observes the environment and makes decisions based on state history, and gets rewards. Associative reinforcement learning is also called Narmed contextual bandit <ref type="bibr" target="#b19">[20]</ref> which is a simplified reinforcement learning algorithm derived from the bandit problem. In N -armed contextual bandit setting, the agent selects one from N levers by observing the environment and gets a reward. When the environment changes, the reward of each lever also changes. Since a full reinforcement learning requires complex reward design and is prone to over-fitting with a single label, it is difficult to train a multi-step Markov decision process solely with single label supervision. We reduce our designed sampling policy to a single-step Markov decision process, which could be easily optimized with an associative reinforcement learning. The training objective of the sampling module is to maximize the reward for the selected clip, where the reward is calculated based on the action classifier output and groundtruth.</p><p>In experiment, we use ResNet2D-18 as the observation network and use ResNet3D-18 or R(2+1)D-34 as the shortterm action classifier to implement DSN framework. We test DSN on the datasets of UCF101 <ref type="bibr" target="#b20">[21]</ref>, HMDB51 <ref type="bibr" target="#b21">[22]</ref>, THUMOS14 <ref type="bibr" target="#b22">[23]</ref> and ActivityNet v1.3 <ref type="bibr" target="#b23">[24]</ref>. Since the dataset is small, DSN has been pre-trained on Kinetics. DSN obtains a performance improvement over previous state-of-theart approaches. More importantly, DSN applies the short-term classifier only on a smaller number of frames, which greatly reduces the computational overhead while still yields slightly better recognition accuracy than previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning in Action Recognition</head><p>The breakthrough by AlexNet <ref type="bibr" target="#b11">[12]</ref> in image classification started the booming of deep learning in computer vision. Researchers were inspired to exploit deep network architectures for action recognition in videos <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Ji et al. <ref type="bibr" target="#b16">[17]</ref> extended 2D CNNs to 3D domain for action recognition and tested the proposed models on small datasets. To support a huge amount of data needed for training deep networks, Karpathy et al. collected a large-scale dataset (Sports-1M) <ref type="bibr" target="#b2">[3]</ref> with weak tag label from Youtube. Simonyan et al. <ref type="bibr" target="#b1">[2]</ref> proposed a twostream network of spatial and temporal networks that capture appearance and motion information, respectively. Carreira et al. <ref type="bibr" target="#b5">[6]</ref> proposed I3D by inflating the 2D filters in 2D CNNs into spatiotemporal kernels. Huang et al. <ref type="bibr" target="#b25">[26]</ref> studied the effect of short-term temporal information on action recognition. Tran et al. <ref type="bibr" target="#b3">[4]</ref> studied 3D CNNs on realistic and large-scale video datasets. Then they further decomposed the 3D convolutional filters into separate spatial and temporal components to obtain R(2+1)D <ref type="bibr" target="#b6">[7]</ref>, which can improve recognition accuracy.</p><p>Meanwhile, several works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> attempted to utilize long-term temporal information for action recognition. Ng et al. <ref type="bibr" target="#b28">[29]</ref> and Donahue et al. <ref type="bibr" target="#b29">[30]</ref> combined information across the whole video over longer time periods by recurrent neural networks (LSTM). Zhu et al. <ref type="bibr" target="#b30">[31]</ref> proposed KVMF to filter out irrelevant information by identifying key volumes in the sequences. Wang et al. <ref type="bibr" target="#b4">[5]</ref> designed a temporal segment network (TSN) that used sparsely sampled frames to aggregate the temporal information of video. They also developed an UntrimmedNet <ref type="bibr" target="#b31">[32]</ref> which utilized the attention model to aggregate information for learning from the untrimmed datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficient Action Recognition</head><p>Neural networks are typically cumbersome, and there is significant redundancy for deep learning models. To run deep neural network models on mobile devices, we must effectively reduce the storage and computational overhead of the networks. Parameter pruning and sharing <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> reduced redundant parameters which are not sensitive to the performance. Knowledge distillation <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, [42], <ref type="bibr" target="#b42">[43]</ref> trained a compact neural network with distilled knowledge of a large model.</p><p>Efficient network architectures proposed recently such as MobileNet <ref type="bibr" target="#b43">[44]</ref> and -ResNet <ref type="bibr" target="#b44">[45]</ref> were developed for training compact networks. Wu et al. <ref type="bibr" target="#b37">[38]</ref> used REINFORCE to learn optimal block dropping policies in ResNet. ECO <ref type="bibr" target="#b45">[46]</ref> greatly improved recognition speed by manually designing the stacking of sample frames. SlowFast <ref type="bibr" target="#b27">[28]</ref> obtained spatial semantics by constructing a slow pathway and obtains action semantics through a very lightweight Fast pathway, which greatly reduces computational overhead compared to traditional two-stream networks. Yu et al. <ref type="bibr" target="#b46">[47]</ref> utilized local appearance and structural information to achieve real-time action recognition. Korbar et al. <ref type="bibr" target="#b47">[48]</ref> constructed an oracle sampler and trains a clip sampler with performance close to the oracle sampler by using knowledge distillation. Although their idea is similar to ours, they train sampler and classifier in a separate way.</p><p>These methods mainly focus on reducing the complexity of the network architecture or reducing the overhead of inference through some artificially designed methods to improve the speed of inference of the network. Our work provides a novel perspective for reducing the computational overhead of inference by training a lightweight clip sampler within a new reinforcement learning framework. This new framework allows us to jointly train the clip sampler and classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reinforcement Learning in Videos</head><p>Deep reinforcement learning has made many dramatic advances in the field of video understanding. Gao et al. <ref type="bibr" target="#b48">[49]</ref> used reinforcement learning to train an encoder-decoder network for action anticipation. Yeung et al. <ref type="bibr" target="#b49">[50]</ref> utilized reinforcement learning for action detection. Fan et al. <ref type="bibr" target="#b32">[33]</ref> proposed an endto-end reinforcement approach that they developed an agent to decide which frame to watch at the next time step or to stop watching the video to make classification decision at the current time step. Wang et al. <ref type="bibr" target="#b50">[51]</ref> proposed a novel hierarchical reinforcement learning framework for video captioning. Wu et al. <ref type="bibr" target="#b51">[52]</ref> proposed the AdaFrame framework to determine which frames based on the LSTM <ref type="bibr" target="#b52">[53]</ref> module, and the whole framework was trained by policy gradient methods. Dong et al. <ref type="bibr" target="#b53">[54]</ref> used BiLSTM and reinforcement learning to select clips to improve performance.</p><p>Similar to the these approaches, our DSN also uses reinforcement learning to train our selection module for efficient action recognition. Notably different from them, we carefully design a section based selection policy to leverage the uniform prior to regularize the selection process. In addition, our designed selection policy will reduce to a single-step Markov decision process, which greatly relieves the training difficulty of reinforcement learning algorithm merely with a single label. Attention-Aware Sampling <ref type="bibr" target="#b53">[54]</ref> proposed an agent algorithm to select discriminative clips based on the clip-level features. It only improved performance and failed to save computational overhead as it passed all clips into networks for feature extraction. However, due to the sampling module is lightweight, the computational overhead of the sampling module is very small compared with the classification module. So, our DSN can greatly reduce computational overhead while still be able to maintain or even improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DYNAMIC SAMPLING NETWORKS</head><p>In this section, we give a detailed description on our proposed Dynamic Sampling Networks (DSN). First, we provide <ref type="figure">Fig. 3</ref>. Sampling module and classification module in each section. First, N clips are uniformly sampled from each section. Then these clips are fed into the sampling module separately to detect and rank the important clips. The sampling module evaluates the importance of each clip and decides to keep or discard each clip. After that, the kept clips are fed into the classification module.</p><p>indicates that clips are discarded or kept according to the sampling action. Predictions from all sections are fused to produce the final prediction. The dynamic sampling network is designed to yield the final recognition results with better performance by keeping as few clips as possible. a general overview of our DSN framework. Then, we present the technical details of the sampling and classification module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of DSN</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, given a video V , we first divide the video into M sections {S 1 , S 2 , ? ? ? , S M } of equal duration without overlap and then uniformly sample N short clips</p><formula xml:id="formula_0">C m = {c m 1 , c m 2 , ..., c m N } in section S m .</formula><p>We sample a total of M ? N clips from the whole video. The DSN framework is composed of a sampling module and a classification module, which performs clip selection in each section and recognize the action class of selected clips, respectively.</p><p>Specifically, for the sampling module, it takes the sampled clips from each section as inputs and selects a single clip in each section independently:</p><formula xml:id="formula_1">G m = G(c m 1 , c m 2 , ? ? ? , c m N ; ?),<label>(1)</label></formula><p>where G m is the selected clip in section S m by the sampling module G, and ? is the model parameter of the sampling module. Then the output of the sampling module will be fed into the classification module to perform action recognition:</p><formula xml:id="formula_2">H = H(G 1 , ? ? ? , G M ; ?),<label>(2)</label></formula><p>where H is the output of classification module H, representing the prediction result of the video V , and ? is the model parameter of classification module. The sampling module dynamically selects a subset of clips for each video and enables the classification module to concentrate on these most discriminative clips. It can greatly improve the inference efficiency when deploying the trained models in practice. We will present the technical details of modules G and H, and then describe the training process for optimizing model parameters ? and ?. Discussion. Our DSN presents a dynamic clip sampling policy, called as section based selection, for efficient video recognition. This section based clip selection is mainly inspired by the work of TSN <ref type="bibr" target="#b4">[5]</ref>, and our DSN could be viewed as a dynamic version of TSN. This dynamic clip sampling exhibits several important properties: First, similar to the TSN framework, our DSN leverages a uniform prior to our selection process. We argue that an action instance could be divided into different courses, and each course contains its own important and discriminative information for action recognition. Therefore, our section based selection aims to regularize the selected clips to cover the entire duration of the whole video. Second, we perform clip selection for each section independently in our designed sampling policy. We analyze that clips of different sections describe different courses of an action instance, and the selection result in a section would have little effect on the other sections. In this sense, our sampling module only needs to focus on distinguishing similar clips in a local section, which would reduce the complexity of the sampling module and increase its flexibility for easy deployment. Finally, our designed sampling policy will reduce to a single-step Markov decision process, which could be easily optimized with an associative reinforcement learning algorithm as described in Section IV. Instead, without our section based selection, it is difficult to train a multi-step Markov decision process solely with single label supervision, as a normal reinforcement learning requires complex reward design and is also prone to over-fitting with training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sampling Module</head><p>The sampling module is designed to select the most discriminative and relevant clip from each section and feed them into the classification module. As shown in <ref type="figure">Fig. 3</ref>, it consists of an observation network (F s ) and a policy maker that determine which clips to select.</p><p>The observation network aims to output the sampling probability of each clip. Formally, it can be formulated as follows:</p><formula xml:id="formula_3">P m = ?(f c(F s (c m 1 , ? ? ? , c m N ))),<label>(3)</label></formula><p>where F s denotes the backbone of observation network and f c denotes a fully connected layer. F s takes the clips (c m 1 , ? ? ? , c m N ) in section S m as inputs and produces the feature maps for each clip. Then these feature maps are concatenated and fed into a fully connected layer f c followed by softmax function ?. In sampling module, we can obtain the probabili-</p><formula xml:id="formula_4">ties P m = {p m 1 , ..., p m N } of sampling clips C m in section S m , where p m n ? [0, 1]</formula><p>. After obtaining the probabilities P m , the policy maker will determine which clip to keep. Specifically, it will output an action A m = {a m 1 , a m 2 , ? ? ? , a m N } based on the estimated probabilities P m . A m is a binary vector and a m n ? {0, 1}, where a m n = 1 indicates the n th clip will be kept in section S m , otherwise this clip will be discarded. Since the sampling module selects one clip from each section, only one element in vector A m will be set to 1, and the rest will be set to 0. During the training phase, we randomly select a clip from each section based on the probabilities P m , and in the testing phase, we directly choose the clip with the highest probability. This slight difference will enable the sampling module to explore more clips and add randomness to the training process to its generalization ability.</p><p>It should be noted that the policy makes the independent decision for each section. In this sense, we keep the same number of clips for each section and thus the selected clips can cover the entire video. In addition, this independence assumption will reduce the decision process in each section into a singlestep Markov decision process. Therefore, there are only N possible actions in each section, and the sampling module can also be viewed as a N -armed contextual bandit whose agent selects one from N levers by observing the environment and gets a reward. The rewards obtained by these selected M clips will guide the training of the sampling module through the reinforcement learning algorithm (REINFORCE), which will be explained in section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Module</head><p>The classification module output the action prediction results of each selected clip. Then rewards will be calculated from prediction and ground-truth label to guide the training of the sampling module.</p><p>3D Convolutional Neural Networks (3D CNNs) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> have become an effective clip-level action recognition model. Therefore, as shown in <ref type="figure">Fig. 3</ref>, we choose 3D CNNs as the backbone of our classification module. In experiments, to demonstrate the robustness of DSN framework, we try two kinds of network architectures: (1) more efficient one: R3D-18, and (2) more powerful one: R(2+1)D-34. R3D is an inflated 3D ResNet whose input is a short clip of stacking multiple consecutive frames. R(2+1)D is a competitive shortterm classifier that decomposes a 3D convolution into separate spatial and temporal convolutions, making its optimization easier than original 3D CNN. Different network architectures can demonstrate the generalization ability of DSN.</p><p>Prediction of DSN. After the training of sampling module and classification module, our DSN keeps M clips for each video. We first perform action recognition for each clip independently, and then report video-level results via an average fusion:</p><formula xml:id="formula_5">DSN(V ) = H(G 1 , ? ? ? , G M ; ?) = 1 M M m=1 F(G m ; ?), (4)</formula><p>where F represents the 3D convolutional networks, ? is its weight, and G m is the sampling module output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LEARNING OF DYNAMIC SAMPLING NETWORKS</head><p>In this section, we describe the details on the optimization of DSN framework. First, we provide a detailed description on the training process of DSN framework. Then, we present the important implementation details of learning DSN from trimmed and untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimization of DSN</head><p>Since DSN framework is composed of the sampling module and the classification module coupled with each other, and involves a non-differentiable operation of selection, we need to design a special training scheme to optimize the two modules. Specifically, we propose two strategies to optimize their model parameters. In the first scheme, we first train the classification module in advance and then tune the parameters of the sampling module by fixing the classification module parameters. In the second scheme, we propose an alternate optimization scheme learning between classification module and sampling module. In particular, as shown in Algorithm 1, if we fix the pre-trained classification module, we skip the process of updating the classification module parameters. Otherwise, we first optimize the parameters ? of the classification module by fixing the parameters of the sampling module. Then, we update the model parameters ? of the sampling module by using REINFORCE. The total iteration size is denoted by epochs. We will give a detailed explanation of these two steps in the following.</p><p>Classification module optimization. Given the training set {V l , y l } L l=1 where y l is the label of the video V l . The sampling module outputs G l = {G 1 , G 2 , ? ? ? , G M } for each video V l and the training objective classification module is defined as follow:</p><formula xml:id="formula_6">(?) = L l=1 J j=1 y l j log H l j + ? ? 2 2 ,<label>(5)</label></formula><p>where H l = H(G l ; ?) is the prediction score of video V l as defined in Equation <ref type="formula" target="#formula_2">(2)</ref> and Equation <ref type="formula">(4)</ref>, J is the total number of categories, H l j is the j th element of H l , ? is a hyper-parameter controlling the cross-entropy loss and L 2 regularization term. The above objective function can be easily optimized using stochastic gradient descent (SGD). As mentioned in section III-C, if we sample more than one clip in G, we will average these prediction scores of selected clips to obtain a video-level score, and use this score for training.</p><p>Sampling module optimization. After the training of the classification module, we move to optimize the parameter of if not fixing the classification module then 6:</p><formula xml:id="formula_7">G = (G 1 , . . . , G M ) ? G(c 1 1 , . . . , c M N ; ?) 7:</formula><p>H ? H(G; ?) </p><formula xml:id="formula_8">P m = ?(f c(F s (c m 1 , ..., c m N )))</formula><p>12:</p><p>Obtain A m and B m based on P m</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Evaluate reward R(A m ) and R(B m ) by Eqn. 6 <ref type="bibr">14:</ref> Backpropagate gradients computed by Eqn. 11 <ref type="bibr">15:</ref> end for 16: end for the sampling module, namely, the weights of the observation network. Since the selection operation is non-differentiable, we intend to use the reinforcement learning algorithm for optimization. The objective of reinforcement learning is to learn a policy ? that decides actions by maximizing future rewards. The reward is calculated by an evaluation metric that compares the generated prediction to the corresponding groundtruth. Only the selected clips will be fed into the classification module. To encourage the policy to be more discriminative, we associate the actions taken with the following reward function:</p><formula xml:id="formula_9">R(A m ) = r m if G m can be classified correctly, ?? otherwise.<label>(6)</label></formula><p>where r m represents the reward if the selected clip G m is classified correctly, which is the score of the correctly classified actions by the classification module. On the other hand, we penalize it with a constant ?? if the classification module cannot make a correct prediction for clip G m .</p><p>The training objective of sampling module is to maximize the expected reward over all possible actions <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_10">J(?) = E A m ?? ? [R(A m )].<label>(7)</label></formula><p>REINFORCE. In order to maximize the expected reward of the sampling module, we use the Monte Carlo policy gradient algorithm REINFORCE <ref type="bibr" target="#b18">[19]</ref> to train the sampling module. In contrast to value function methods, REINFORCE algorithm can select actions without consulting the value function. With Monte-Carlo sampling using all samples in a mini-batch, the expected gradient can be approximated as follow:</p><formula xml:id="formula_11">?J(?) = E ? [R(A m )? ? log?(A m |C m , ?)] = E ? [R(A m )? ? N n=1 log(p m n a m n )].<label>(8)</label></formula><p>where ? is the parameters of the sampling module and C m is the collection of sampled clips in section S m . As mentioned in section III-B, to obtain A m in Equation <ref type="formula" target="#formula_11">(8)</ref>, we sample the clip based on P m . When the clip c m n is chosen, its corresponding a m n will be set to 1, and otherwise 0. REINFORCE with Baseline. As REINFORCE may be of high variance and learn slowly, we utilize the REINFORCE with Baseline <ref type="bibr" target="#b18">[19]</ref> to reduce variance: <ref type="bibr" target="#b8">(9)</ref> where the baseline b can be an arbitrary function, as long as it does not depend on the action A m . The equation is still valid because we subtract the zero value. It is proved as follows:</p><formula xml:id="formula_12">?J(?) = E ? [(R(A m ) ? b)? ? log?(A m |C m , ?)] ,</formula><formula xml:id="formula_13">E ? [b? ? log?(A m |C m , ?)] = ?(A m |C m , ?)b? ? log?(A m |C m , ?)d? = b? ? ?(A m |C m , ?)d? =b? ? ?(A m |C m , ?)d? =b? ? 1 = 0.<label>(10)</label></formula><p>Update Parameter. We select the largest probability p * from P m to construct R(B m ) as baseline. B m = {b m 1 , ..., b m N } is the action in which b m n = 1 if p m n = p * , and b m n = 0 otherwise. Using REINFORCE with the baseline, we rewrite the estimate of the gradient ?J(?) in Equation <ref type="formula" target="#formula_11">(8)</ref> as:</p><formula xml:id="formula_14">?J(?) =E ? [(R(A m ) ? R(B m ))? ? N n=1 log(p m n a m n )].<label>(11)</label></formula><p>To optimize the expected reward J(?), we update weight ? by ?J(?) as follow:</p><formula xml:id="formula_15">? t+1 = ? t + ??J(? t ),<label>(12)</label></formula><p>where t is the iteration number and ? is the learning rate. Discussion on end-to-end training. DSN can be also optimized in an end-to-end scheme, that is, to update the parameters of two modules at the same time. In this case, the classification module can hardly recognize the action correctly in the early stage of training. For any clip, the sampling module gets almost negative rewards and the whole system is hard to optimize properly. Therefore, the alternating optimization between the two modules is a practical solution, and pre-training classification module before the optimization of the sampling module is helpful to improve the training efficiency of the sampling module in the early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DSN in Practice</head><p>Finally, we give detailed descriptions about the implementations of DSN in practice. We use the notation of M : N to represent the sampling scheme in DSN that each video is divided into M sections of equal duration without overlap and N clips are sampled from each section. Our DSN is a general framework, which could be applied on both trimmed and untrimmed video datasets. To fully investigate DSN framework, we report the results of DSN on both trimmed and untrimmed datasets in experiments. Considering that the temporal duration of videos is different between trimmed and untrimmed datasets, we use different settings in our DSN implementation on the two types of datasets. For trimmed video datasets (e.g., UCF101, HMDB51), considering that the duration of each video is normally very short, we set M = 1 and N = 3 during the training phase. In testing, we use M = 4 to yield better recognition results. The N is consistent whether in training or testing. For untrimmed video datasets (e.g., THUMOS, ActivityNet), we try to use a larger M under the constraint GPU memory. It is expected that action instances might only occupy a small portion of the whole untrimmed video. Larger numbers of sampling clips would help DSN to select the most action-relevant clips. Specifically, when we utilize a lightweight backbone of ResNet3D-18, we set M = 6 and N = 3 during training. In testing, we keep N = 3 and study with different values of M to find a trade-off between performance and computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we first describe the datasets used in experiments and the implementation details. Then we perform some exploration studies on different aspects of DSN. After that, we compare the performance of DSN with state-of-theart methods on four datasets. Finally, we visualize the learned policy of DSN to provide some insights about our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We report performance on two popular untrimmed datasets, ActivityNet v1.3 <ref type="bibr" target="#b23">[24]</ref> and THUMOS14 <ref type="bibr" target="#b22">[23]</ref>. ActivityNet v1.3 covers a wide range of complex human activities divided into 200 classes, which contains 10,024 training videos, 4,926 validation videos, and 5,044 testing videos. Since annotations of action instances in untrimmed videos are hardly available in most cases, we only use video-level labels to train the model. We adopt the Top-1 as the evaluation metric on ActivityNet v1.3 in ablation study and both Top-1 and mean average precision (mAP) as the evaluation metric comparing with the state-of-the-art models. Due to the lack of annotated groundtruth of the test set, here we only report the performance on the validation dataset. THUMOS14 has 101 classes for action recognition, which consists of four parts: training set, validation set, test set, and background set. For the ablation study, we only train models on the validation set and evaluate their mAP on the test set. When comparing with other methods, we train DSN on the combination of the training and validation set.</p><p>We also validate DSN on two trimmed video datasets, UCF101 <ref type="bibr" target="#b20">[21]</ref> and HMDB51 <ref type="bibr" target="#b21">[22]</ref>. UCF101 has 101 action classes and 13K videos. HMDB51 consists of 6.8K videos divided into 51 action categories. In trimmed video datasets, Top-1 is employed as an evaluation metric for all models. We take the official evaluation protocol for these trimmed video datasets: adopting the three training/testing splits provided by the organizers and measuring average accuracy over these splits for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For classification module, we use ResNet3D-18 <ref type="bibr" target="#b54">[55]</ref> and R(2+1)D-34 <ref type="bibr" target="#b6">[7]</ref> as the backbone networks. To make the sampling module as small as possible, we choose the ResNet2D-18 <ref type="bibr" target="#b6">[7]</ref> as the backbone of the observation network. For ResNet2D-18 to process the same input length as R(2+1)D-34, we take the middle 8 frames in a single clip as input. It is worth noting that the computation required for ResNet2D-18 is only 9.4% of the ResNet3D-18 and 1.2% of R(2+1)D-34, respectively. In the experiment, we use SGD with momentum 0.9 as the optimizer. ResNet3D-18, R(2+1)D-34 and ResNet2D-18 are pre-trained on Kinetics <ref type="bibr" target="#b5">[6]</ref>. When training ResNet3D-18, the initial learning rate is set to 0.001, which is reduced at 45, 60 epochs by a factor of 10 and stops at 70 epochs. Considering the R(2+1)D-34 is a heavier model, the whole training procedure is done in 120 epochs, starting at a learning rate of 0.001 and decreased at 90, 110 epochs by a factor of 10. Video frames are scaled to the size of 128 ? 171 and then patches are generated by randomly cropping window of size T ? 112 ? 112 with scale jittering. The temporal dimension T depends on the backbone architecture of classification module (i.e., 8 for ResNet3D-18 and 32 for R(2+1)D-34). When training our models, the batch size is 128 for ResNet3D-18 and 32 for R(2+1)D-34. We use Farnebacks method <ref type="bibr" target="#b55">[56]</ref> to compute optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Exploration Studies</head><p>First, we are ready to perform ablation studies for DSN framework on trimmed and untrimmed video datasets. In this exploration study, we choose ResNet3D-18 as the backbone network for the classification module and ResNet2D-18 for the observation network. We use M : N to denote training  <ref type="figure">Fig. 4</ref>. Results of different selection methods by using different numbers of clips. We report the Top-1 accuracy on the UCF101 (split1), HMDB51 (split1) and ActivityNet, and mAP on the THUMOS14 dataset. M : N is set as 1 : 3 for training on trimmed datasets and 6 : 3 on untrimmed datasets. We set N = 3 and vary M from 1 to 10 for testing.   another different training method of DSN mentioned in section IV called fixed DSN. In this experiment, DSN is trained with the setting of 1 : 3 for trimmed video and 6 : 3 for untrimmed video respectively, and to be consistent with training, we test with the same setting. We choose random sampling and uniform sampling as the baseline. They use the same clip-level classifier, randomly sample M clips and uniformly sample M clips from the whole video in the test phase, respectively. We also compare DSN with a competitive method called max response sampling <ref type="bibr" target="#b25">[26]</ref> which trains another observation network and uses the maximum classification score to sample clips. Then we introduce the fixed DSN in section IV which trains the sampling module with a trained classification module and denote it as DSNf. Finally, we list the results of dense sampling and oracle sampling <ref type="bibr" target="#b25">[26]</ref> which are the most intuitive measurement of the  <ref type="table" target="#tab_1">V  COMPARISONS WITH THE CURRENT STATE-OF-THE-ART MODELS ON THUMOS14. THE DSN MODELS ARE TRAINED ON THE TRAINING SET AND   VALIDATION SET AND TESTED ON THE TEST SET.   Method</ref> Input Clips mAP RGB+EMV <ref type="bibr" target="#b56">[57]</ref> RGB+FLOW -61.5% Two Stream <ref type="bibr" target="#b1">[2]</ref> RGB+FLOW <ref type="bibr" target="#b24">25</ref> 66.1% iDT+CNN <ref type="bibr" target="#b57">[58]</ref> RGB+iDT ALL 62.0% Motion <ref type="bibr" target="#b58">[59]</ref> HOG+HOF+MBH ALL 63.1% Objects+Motion <ref type="bibr" target="#b58">[59]</ref> RGB+HOG+HOF+MBH ALL 71.6% Jain et al. <ref type="bibr" target="#b59">[60]</ref> HOG+HOF+MBH ALL 71.0% ELM <ref type="bibr" target="#b60">[61]</ref> HOG+HOF+MBH ALL 62.3% TSN <ref type="bibr" target="#b4">[5]</ref> RGB+FLOW ALL 80.1% UntrimmedNet (hard) <ref type="bibr" target="#b31">[32]</ref> RGB+FLOW ALL 81.2% UntrimmedNet (soft) <ref type="bibr" target="#b31">[32]</ref> RGB+FLOW ALL 82.2%  performance of the classifier. All results are shown in <ref type="table" target="#tab_1">Table I</ref>. The clip-level classifiers of the baseline methods are trained by randomly sampled frames. When M &gt; 1, we average the predictions of all clips to get the final predictions and train the baseline. As shown in <ref type="table" target="#tab_1">Table I</ref>, on UCF101 and HMDB51, DSN outperforms the random baseline by 2.6% and 5.9% and outperforms the uniform baseline by 1.9% and 2.7%, respectively. For untrimmed video tests, DSN performance improvements are more evident. On THUMOS14 and Activi-tyNet v1.3, DSN is 10.1% and 4.7% higher than the random baseline and 10.3% and 4.1% higher than the uniform baseline. The promising results demonstrate that different sampling policies have a huge impact on the classification performance, and also validate that the sampling module has indeed learned the effective sampling policies.</p><p>The method called max response sampling trains another observation network independently and outputs the classification score for each clip. The max response is defined as the maximum classification score of all the classes. We only select the clip with the highest max response in each section as input to the classification module during testing. For a fair comparison, we use ResNet2D-18 as the backbone of the observation network and sample M clips from M ? N according to maximum score during testing. We observe that DSN is better than max response sampling by 0.6% on UCF101, 3.6% on HMDB51, 6.7% on THUMOS14 and 1.6% on ActivityNet v1.3. This demonstrates that simply using max response as the sampling policy obtains a very limited performance improvement.  is decreased by 0.8%, 1.7%, 5.3%, and 1.8%, respectively. We analyze that the joint training of two modules alternately enables the sampling module and the classification module to perform their duties, focus more on the learning of their respective tasks, and at the same time better couple with each other. Therefore, compared with DSN-f, training DSN model by joint training is more adequate. This fully demonstrates that joint training is indispensable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DSN</head><p>The dense sampling results are the experimental upper bound of the classifier in practice. Currently, the state-of-theart results of most models are obtained by dense sampling. For the trimmed video and untrimmed video datasets, we take 10 clips and 20 clips respectively as input to the classification module. On the trimmed video dataset, DSN uses only one clip as input and achieves performance comparable to dense sampling using 10 clips. On the untrimmed video dataset, DSN using 6 clips as input not only achieves performance comparable to the dense sampling using 20 clips on ActivityNet v1.3 but also has higher mAP on THUMOS14 than the dense sampling. Oracle sampling is the theoretical upper bound of the sampling policy. As long as there is a clip that can be correctly classified by the classifier, oracle sampling will select this clip as the input to the classifier. The results are almost impossible to achieve in the experiment. There is still a gap between DSN and oracle sampling, which shows that there is room for further improvement and optimization of the clip sampling policy.</p><p>Study on different numbers of sampled clips. We have demonstrated the effectiveness of DSN. Now, we are ready to explore the performance curve of DSN by varying the number of sampled clips. We gradually increase the number of sampled clips from 1 to 10 and compare it to the baseline and max response sampling mentioned in <ref type="table" target="#tab_1">Table I</ref>. The experimental results are shown in <ref type="figure">Fig. 4</ref>.</p><p>For trimmed video, when the number of clips is greater than 4, the advantages of DSN over other methods will gradually decrease. We analyze that due to most of the useless frames of the trimmed video have been trimmed and the duration of trimmed videos is short in general, as the number of clips increases, most of the frames will be selected, and all methods will gradually degenerate into dense sampling. For untrimmed video datasets, DSN has an advantage over other methods when using a small number of clips. But when the number of clips is greater than 3 and 5 on THUMOS14 and ActivityNet, the performance gain from increasing clip number gradually decreases. The reason is that the useful frames of untrimmed videos are distributed unevenly and sparsely, and it is difficult to cover them by simply increasing the number of clips. Experimental results show that DSN can almost reach its peak performance when computing a small number of clips, so in practical deployment, when the computing resources are limited, computing a small number of clips can achieve very good results.</p><p>Study on training setting. After evaluating the performance trend of DSN, we are ready to investigate the effect of the different number of sections and the number of clips in each section in the training sampling scheme. In training, we use the training sampling schemes of 1 : 3, 1 : 6, and 3 : 3. In testing, the N is the same as training and M is set to 3 for fair comparison between different sampling schemes. The results are shown in <ref type="table" target="#tab_1">Table II</ref>. When M = 1, simply increasing the number of clips in each section does not bring about significant performance improvement. The reason is that increasing the number of segments means enlarging the exploration space of reinforcement learning, which may lead to under-fitting, so sampling module needs more epoch to explore. With the same number of training epochs, the sampling module is difficult to get enough training. When we increased M from 1 to 3, we achieved 0.7% and 1.9% performance improvement on UCF101 and HMDB51 respectively. This is because more sections mean that the sampling module and the classification module can obtain more global semantic information. This avoids the imbalance of the learning samples caused by repeated local sampling during the training phase. At the same time, in the training phase of the classification module, based on the conclusion of TSN <ref type="bibr" target="#b4">[5]</ref>, the effect of the multi- segments fusion training is significantly better than the singlesegment training. Therefore, when computational resources are sufficient, increasing the number of sections can obtain better performance.</p><p>Quantitative analysis in FLOPs. We perform experiments on computational overhead and the performance of DSN. In the experiments, R3D uses ResNet3D-18 as the backbone and R(2+1)D use ResNet3D-34 as the backbone, respectively. As shown in <ref type="table" target="#tab_1">Table III</ref>, on HMDB51, R3D with DSN uses only 3 clips and one-third of the baseline FLOPS, and the accuracy is 2% higher than the baseline. R(2+1)D uses only 4 clips and less than half of the baseline FLOPs and achieves performance comparable to baseline. The effect is more obvious on THU-MOS14. As shown in <ref type="table" target="#tab_1">Table IV</ref>, R3D with DSN uses only 6 clips, which is 1.6% higher than the baseline using 20 clips, while saving more than half of FLOPs. R(2+1)D requires only one-third of FLOPs and achieves accuracy improvement of 1.2%. This indicates that DSN framework has great advantages in improving accuracy and saving computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the State of the Art</head><p>We compare DSN with the current state-of-the-art methods.</p><p>In the experiments, R3D uses ResNet3D-18 as the backbone and R(2+1)D uses ResNet3D-34 as the backbone, respectively. The number of clips used by other methods is also given in the table.</p><p>Results on untrimmed video datasets. We list the performance of DSN models and other state-of-the-art approaches on THUMOS14 and ActivityNet v1.3. The results of both RGB and optical flow are shown in <ref type="table" target="#tab_1">Table V and Table VI</ref>. DSN is trained with the setting of 6 : 3 for R3D and 1 : 3 for R(2+1)D. R3D and R(2+1)D use the same sampling scheme N = 3 and M = 6 or M = 10 for testing.</p><p>As shown in <ref type="table">Table V</ref>, on THUMOS14, for R3D and R(2+1)D, DSN only uses 6 clips, and their mAP is 1.6% and 1.2% higher than the dense sampling when using RGB as input. The optical flow and two-stream results of R(2+1)D are higher than dense sampling by 4.9% and 2.1%, respectively. R(2+1)D trained with DSN only uses RGB and 6 clips as input, and it achieves better performance than other methods of two-stream results. We give more experimental results on ActivityNet v1.3 in <ref type="table" target="#tab_1">Table VI</ref>. For R3D, both DSN and baseline use 6 clips for training When using RGB as input and sampling 6 clips from each video, the Top-1 accuracy of DSN is 1.1% higher than the baseline using 20 clips. When only half of the input of the baseline is used, that is, 10 clips, the Top-1 accuracy is directly increased by 2.4%. For R(2+1)D, both DSN and baseline use only one clip for training. The Top-1 accuracy of DSN using only 10 clips is 0.3% higher than the baseline using 20 clips. It has also achieved very obvious effects on the optical flow that outperform dense sampling by 1.3% and 2.2% by using 6 clips and 10 clips, respectively. These experimental results demonstrate that DSN framework only uses video-level supervision information for training, which allows the sampling module to select useful clips in the video.</p><p>Results on trimmed video datasets. We furthermore compare the performance of our method with the state-of-the-art approaches on UCF101 and HMDB51 in <ref type="table" target="#tab_1">Table VII</ref>. We only list the accuracy of the models using RGB as input. The DSN models are trained with N = 3, and testing sampling scheme is set as 4 : 3.</p><p>On UCF101, for R3D backbone with 1 : 3 training scheme, the Top-1 accuracy of DSN using 4 clips is comparable to the baseline using 10 clips. When using 3 : 3 training scheme, the input of 3 clips can improve accuracy by nearly 1%. For R(2+1)D, DSN uses only 4 clips to get the same performance as the baseline using 10 clips. The experimental results on HMDB51 are more obvious. After R3D and R(2+1)D are trained by DSN and with 1 : 3 training scheme, the results of the model using 4 clips are 0.9% and 0.7% higher respectively than the baseline using 10 clips. These experimental results demonstrate that although the trimmed datasets have removed most of the irrelevant frames that do not contain actions, DSN framework can still filter out the useless clips so that the performance is further improved when the performance has already saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization and Discussion</head><p>After analyzing the quantitative results in both aspects of recognition accuracy and computational overhead, we present the visualization results of the sampling policies in this section. <ref type="figure" target="#fig_5">Fig. 5</ref> illustrates sampling frames sorted by the output of the observation network in the sampling module. We show the top 2 frames with the highest confidence and last 2 frames with the lowest confidence. It is seen that the sampling module in DSN framework can select discriminative action clips and avoid irrelevant clips that might degrade the classifier training. For some videos, the duration of action is usually a short clip through the whole video but interspersed with some irrelevant pictures. For example, sometimes it needs some useless preparations before the action happens. Even during the action is happening, the camera shot may turn to other irrelevant scenes. Those useless clips not only bring a large waste of computing but also degenerate the performance in final recognition. From <ref type="figure" target="#fig_5">Fig. 5</ref>, this observation shows that the motivation of DSN is of practical significance that it is necessary to sample frames based on some optimal policy to train a better clip-level action classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have presented a new framework for learning action recognition models, called Dynamic Sampling Networks (DSN). DSN is composed of a sampling module and a classification module, whose objectives are to dynamically select discriminative clips given an input video and to perform action recognition on the selected clips, respectively. The sampling module in DSN could be optimized within an associative reinforcement learning algorithm. The classification module can be optimized by SGD with cross-entropy loss. Meanwhile, we provide two different schemes to optimize DSN framework. One iteratively optimizes the sampling module and classification module, while the other only trains the sampling module while fixing the classification module. We conduct extensive experiments on several action recognition datasets, including trimmed and untrimmed video recognition. As demonstrated on four challenging action recognition benchmarks, DSN can greatly reduce the computational overhead yet is still able to achieve slightly better or comparable accuracy to the previous state-of-the-art approaches.</p><p>In the future, we still have many directions to improve the DSN framework. Compared with the current alternating optimization scheme between sampling module and classification module, we can utilize the reparameterization trick and Gumbel-Softmax to discretize the output of the sampling module, so as to solve the non-differentiable problem of clip selection operation and realize an end-to-end training of the DSN framework. In addition, the lastest reinforcement learning algorithms such as PPO <ref type="bibr" target="#b65">[66]</ref> could be used to improve sample efficiency and robustness. Finally, we can also use temporal models such as LSTM <ref type="bibr" target="#b52">[53]</ref> to better exploit temporal information across different sections for better clip selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Dynamic Sampling Network (DSN). We devise a dynamic clip sampling strategy, termed as section based selection, to build our DSN framework. The DSN first divides each video into several sections of equal duration, and then performs dynamic sampling in each section for efficient video recognition. The sampling module and classification module share weights across all sections. Details on the design of sampling module and classification module could be found inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 1 :</head><label>11</label><figDesc>The training algorithm of DSN framework. Input: Videos and their labels {V l , y l } L l=1 Set epochs, M and N 2: Initialize the weight ? and ? 3: for s ? 1 to epochs do 4: Sample (c 1 1 , . . . , c M N ) from V l 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-f is a simplified training version of DSN. Compared to DSN, the parameters of the classification module of DSN-f are fixed and no longer updated during training. The parameters of the classification module of DSN-f are consistent with the baseline. Without joint training, the performance of DSNf is lower than that of DSN. On the four datasets, DSN-f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the frames chosen by the sampling module on THUMOS14. Each column represents frames sampled from the same video. For each video, we show two frames with the highest confidence on the top and two frames with the lowest confidence on the bottom, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>Study on the DSN framework. WE LIST SOME COMPETITIVE METHODS TO DEMONSTRATE THE EFFECTIVENESS OF DSN. THE SPECIFIC EVALUATION METRIC IS DESCRIBED IN SECTION V-A. FOR UCF101 AND HMDB51, THE RESULTS ARE REPORTED ON SPLIT1. M : N IS SET AS 1 : 3 FOR TRAINING ON TRIMMED DATASETS AND 6 : 3 ON UNTRIMMED DATASETS. THE TESTING SETTING IS THE SAME WITH TRAINING.</figDesc><table><row><cell>Methond</cell><cell>UCF101</cell><cell>HMDB51</cell><cell>THUMOS14</cell><cell>ActivityNet v1.3</cell></row><row><cell>Baseline (random)</cell><cell>83.9%</cell><cell>53.6%</cell><cell>62.5%</cell><cell>62.0%</cell></row><row><cell>Baseline (uniform)</cell><cell>84.6%</cell><cell>56.8%</cell><cell>62.3%</cell><cell>62.6%</cell></row><row><cell>Max response</cell><cell>85.9%</cell><cell>55.9%</cell><cell>65.9%</cell><cell>65.1%</cell></row><row><cell>DSN-f</cell><cell>85.7%</cell><cell>57.8%</cell><cell>67.3%</cell><cell>64.9%</cell></row><row><cell>DSN</cell><cell>86.5%</cell><cell>59.5%</cell><cell>72.6%</cell><cell>66.7%</cell></row><row><cell>Dense sampling</cell><cell>88.6%</cell><cell>59.5%</cell><cell>65.6%</cell><cell>67.5%</cell></row><row><cell>Oracle sampling</cell><cell>90.1%</cell><cell>66.8%</cell><cell>73.1%</cell><cell>88.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EXPLORATION</head><label>II</label><figDesc>OF THE OPTIMAL SAMPLING SCHEME IN THE TRAINING PHASE. ALL CLASSIFICATION MODELS USE RESNET3D-18 AS THE BACKBONE. THE RESULTS ARE REPORTED ON SPLIT1. 1% 98.6% 61.5% 89.8%or testing setting that each video is divided into M sections and each section has N clips without overlap. The sampling module keeps M clips as inputs to the classification module.All ablation experiments use RGB as input.</figDesc><table><row><cell></cell><cell cols="2">UCF101</cell><cell cols="2">HMDB51</cell></row><row><cell>Training Setting</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>1:3</cell><cell cols="4">88.4% 98.5% 59.6% 88.0%</cell></row><row><cell>1:6</cell><cell cols="4">88.7% 98.8% 59.2% 88.6%</cell></row><row><cell>3:3</cell><cell>89.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF FLOPS AND ACCURACY ON HMDB51. THE ACCURACY IS REPORTED ON SPLIT1.</figDesc><table><row><cell>Model</cell><cell>Clips</cell><cell>FLOPs</cell><cell>Top-1</cell></row><row><cell>R3D</cell><cell>10</cell><cell>203.6G</cell><cell>59.5%</cell></row><row><cell>R3D &amp; DSN (M = 1)</cell><cell>4</cell><cell>104.6G</cell><cell>60.9%</cell></row><row><cell>R3D &amp; DSN (M = 3)</cell><cell>3</cell><cell>78.5G</cell><cell>61.5%</cell></row><row><cell>R(2+1)D</cell><cell>10</cell><cell cols="2">1528.8G 75.5%</cell></row><row><cell>R(2+1)D &amp; DSN (M = 1)</cell><cell>4</cell><cell>631.2G</cell><cell>75.</cell></row></table><note>8% Study on DSN. To demonstrate the effectiveness of DSN framework, we compare it to five different methods and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF FLOPS AND ACCURACY ON THUMOS14. THE MODELS ARE TRAINED ON THE TRAINING SET AND VALIDATION SET, AND TESTED ON THE TEST SET.</figDesc><table><row><cell>Model</cell><cell>Clips</cell><cell>FLOPs</cell><cell>mAP</cell></row><row><cell>R3D &amp; TSN (S = 3)</cell><cell>20</cell><cell>407.2G</cell><cell>73.6%</cell></row><row><cell>R3D &amp; DSN (M = 3)</cell><cell>6</cell><cell>157.0G</cell><cell>75.2%</cell></row><row><cell>R(2+1)D</cell><cell>20</cell><cell cols="2">3057.6G 81.9%</cell></row><row><cell>R(2+1)D &amp; DSN (M = 1)</cell><cell>6</cell><cell>946.8G</cell><cell>83.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISONS</head><label>VI</label><figDesc>WITH THE STATE-OF-THE-ART MODELS ON ACTIVITYNET V1.3. WE LIST SOME COMPETITIVE MODELS TO COMPARE WITH OUR DSN. THE M IN THE FIRST COLUMN DENOTES HOW MUCH CLIPS ARE USED DURING THE DSN TRAINING.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Pre-trained</cell><cell>Clips</cell><cell>Top-1</cell><cell>mAP</cell></row><row><cell>IDT [1]</cell><cell>-</cell><cell>ImageNet</cell><cell>ALL</cell><cell>64.7%</cell><cell>68.7%</cell></row><row><cell>C3D [62]</cell><cell>-</cell><cell>Sports1M</cell><cell>10</cell><cell>65.8%</cell><cell>67.7%</cell></row><row><cell>P3D [62]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>20</cell><cell>75.1%</cell><cell>78.9%</cell></row><row><cell>RRA [63]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>25</cell><cell>78.8%</cell><cell>83.4%</cell></row><row><cell>MARL [64]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>25</cell><cell>79.8%</cell><cell>83.8%</cell></row><row><cell>MARL [64]</cell><cell>ResNet-152</cell><cell>Kinetics</cell><cell>25</cell><cell>80.2%</cell><cell>83.5%</cell></row><row><cell>MARL [64]</cell><cell>SEResNeXt152</cell><cell>Kinetics</cell><cell>25</cell><cell>85.7%</cell><cell>90.1%</cell></row><row><cell>R3D-RGB &amp; TSN (6 seg)</cell><cell></cell><cell></cell><cell>20</cell><cell>65.6%</cell><cell>68.7%</cell></row><row><cell>R3D-RGB &amp; DSN (M = 6)</cell><cell>ResNet-18</cell><cell>Kinetics</cell><cell>6</cell><cell>66.7%</cell><cell>71.4%</cell></row><row><cell>R3D-RGB &amp; DSN (M = 6)</cell><cell></cell><cell></cell><cell>10</cell><cell>68.0%</cell><cell>71.7%</cell></row><row><cell>R(2+1)D-RGB</cell><cell></cell><cell></cell><cell>20</cell><cell>78.2%</cell><cell>83.6%</cell></row><row><cell>R(2+1)D-FLOW</cell><cell></cell><cell></cell><cell>20</cell><cell>76.9%</cell><cell>80.7%</cell></row><row><cell>R(2+1)D-RGB+FLOW</cell><cell></cell><cell></cell><cell>20</cell><cell>81.3%</cell><cell>87.1%</cell></row><row><cell>R(2+1)D-RGB &amp; DSN (M = 1)</cell><cell></cell><cell></cell><cell>6</cell><cell>76.5%</cell><cell>82.6%</cell></row><row><cell>R(2+1)D-FLOW &amp; DSN (M = 1)</cell><cell>ResNet-34</cell><cell>Kinetics</cell><cell>6</cell><cell>78.2%</cell><cell>82.9%</cell></row><row><cell>R(2+1)D-RGB+FLOW &amp; DSN (M = 1)</cell><cell></cell><cell></cell><cell>6</cell><cell>82.1%</cell><cell>87.8%</cell></row><row><cell>R(2+1)D-RGB &amp; DSN (M = 1)</cell><cell></cell><cell></cell><cell>10</cell><cell>78.5%</cell><cell>83.5%</cell></row><row><cell>R(2+1)D-FLOW &amp; DSN (M = 1)</cell><cell></cell><cell></cell><cell>10</cell><cell>79.1%</cell><cell>84.3%</cell></row><row><cell>R(2+1)D-RGB+FLOW &amp; DSN (M = 1)</cell><cell></cell><cell></cell><cell>10</cell><cell>82.6%</cell><cell>87.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>WITH THE CURRENT STATE-OF-THE-ART MODELS ON UCF101 AND HMDB51. THE MODELS EQUIPPED WITH DSN ARE TRAINED BY SAMPLING SCHEME M : 3 WHERE M IS PRESENTED IN THE TABLE. THE ACCURACY IS REPORTED AS AVERAGE OVER THE THREE SPLITS. WE ONLY COMPARE WITH THE MODELS USING RGB AS INPUT.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Clips</cell><cell>Backbone</cell><cell>UCF101</cell><cell>HMDB51</cell></row><row><cell>TSN RGB [5]</cell><cell>[1 ? 3 ? 224 ? 224]</cell><cell>25</cell><cell>Inception V2</cell><cell>91.1%</cell><cell>-</cell></row><row><cell>TSN RGB [5]</cell><cell>[1 ? 3 ? 229 ? 229]</cell><cell>25</cell><cell>Inception V3</cell><cell>93.2%</cell><cell>-</cell></row><row><cell>C3D [4]</cell><cell>[16 ? 3 ? 112 ? 112]</cell><cell>10</cell><cell>VGGNet-11</cell><cell>82.3%</cell><cell>51.6%</cell></row><row><cell>C3D [55]</cell><cell>[16 ? 3 ? 112 ? 112]</cell><cell>10</cell><cell>ResNet-18</cell><cell>85.8%</cell><cell>54.9%</cell></row><row><cell>I3D-RGB [6]</cell><cell>[64 ? 3 ? 224 ? 224]</cell><cell>ALL</cell><cell>Inception V1</cell><cell>95.6%</cell><cell>74.8%</cell></row><row><cell>P3D [62]</cell><cell>[32 ? 3 ? 299 ? 299]</cell><cell>20</cell><cell>ResNet-152</cell><cell>88.6%</cell><cell>-</cell></row><row><cell>MF-Net [65]</cell><cell>[16 ? 3 ? 224 ? 224]</cell><cell>50</cell><cell>-</cell><cell>96.0%</cell><cell>74.6%</cell></row><row><cell>ARTNet with TSN [8]</cell><cell>[24 ? 3 ? 112 ? 112]</cell><cell>25</cell><cell>ResNet-18</cell><cell>94.3%</cell><cell>70.9%</cell></row><row><cell>S3D-G [9]</cell><cell>[64 ? 3 ? 224 ? 224]</cell><cell>ALL</cell><cell>Inception</cell><cell>96.8%</cell><cell>75.9%</cell></row><row><cell>R3D-RGB</cell><cell>[8 ? 3 ? 112 ? 112]</cell><cell>10</cell><cell></cell><cell>88.6%</cell><cell>58.8%</cell></row><row><cell>R3D-RGB &amp; DSN (M = 1)</cell><cell>[8 ? 3 ? 112 ? 112]</cell><cell>4</cell><cell>ResNet-18</cell><cell>88.7%</cell><cell>59.7%</cell></row><row><cell>R3D-RGB &amp; DSN (M = 3)</cell><cell>[8 ? 3 ? 112 ? 112]</cell><cell>3</cell><cell></cell><cell>89.5%</cell><cell>61.6%</cell></row><row><cell>R(2+1)D-RGB [7]</cell><cell>[32 ? 3 ? 112 ? 112]</cell><cell>10</cell><cell></cell><cell>96.8%</cell><cell>74.5%</cell></row><row><cell>R(2+1)D-RGB (ours)</cell><cell>[32 ? 3 ? 112 ? 112]</cell><cell>10</cell><cell>ResNet-34</cell><cell>96.7%</cell><cell>74.8%</cell></row><row><cell>R(2+1)D-RGB &amp; DSN (M = 1)</cell><cell>[32 ? 3 ? 112 ? 112]</cell><cell>4</cell><cell></cell><cell>96.8%</cell><cell>75.5%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6450" to="6459" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="318" to="335" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associative search network: A reinforcement learning associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Brouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1054" to="1054" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The epoch-greedy algorithm for contextual multi-armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What makes a video a video: Analyzing temporal information in video understanding models and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7366" to="7375" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Watching a small portion could be as good as watching all: Towards efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="705" to="711" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data-free parameter pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast convnets using group-wise brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2554" to="2564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8817" to="8826" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6550</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face model compression by distilling knowledge from neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3560" to="3566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning strict identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4432" to="4440" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ECO: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time action recognition by spatiotemporal semantic and structural forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RED: reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4213" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention-aware sampling via deep reinforcement learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8247" to="8254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1708.05038</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA, ser</title>
		<editor>J. Big?n and T. Gustavsson</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2749</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">What do 15, 000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">University of amsterdam at thumos challenge 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Challenge</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Extreme learning machine for large-scale action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Finegrained video categorization with redundancy reduction attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="139" to="155" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="364" to="380" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
