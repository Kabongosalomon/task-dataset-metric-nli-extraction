<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Financial Aspect-Based Sentiment Analysis using Deep Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yang</surname></persName>
							<email>steve.yang@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rosenfeld</surname></persName>
							<email>jrosenfeld@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Makutonin</surname></persName>
							<email>jacquesmakutonin@berkeley.edu</email>
						</author>
						<title level="a" type="main">Financial Aspect-Based Sentiment Analysis using Deep Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The topic of aspect-based sentiment analysis (ABSA) has been explored for a variety of industries, but it still remains much unexplored in finance. The recent release of data for an open challenge (FiQA) from the companion proceedings of WWW '18 has provided valuable finance-specific annotations. FiQA contains high quality labels, but it still lacks data quantity to apply traditional ABSA deep learning architecture. In this paper, we employ highlevel semantic representations and methods of inductive transfer learning for NLP. We experiment with extensions of recently developed domain adaptation methods and target task fine-tuning that significantly improve performance on a small dataset. Our results show an 8.7% improvement in the F1 score for classification and an 11% improvement over the MSE for regression on current state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis (ABSA) is a way to systematically mine opinions given a body of text. Unlike regular sentiment analysis, ABSA allows for far more granular levels of opinion mining. For example, one common application of ABSA is to dissect product or service reviews and determine sentiment on sometimes unrelated aspects, such as a quality or price. Rarely are product reviews as simple as good or bad. They are nuanced with conflicting positive and negative opinions based on what aspect of the product is being reviewed.</p><p>We find the field of finance to be a significantly under-explored domain for ABSA. Similar to product reviews, financial investment opportu-nities are commonly written as free-form essays; these write-ups are generally nuanced with positive and negative opinions on specific aspects of a certain investment opportunity. Being able to identify these topics and to subsequently determine the associated sentiment could be beneficial in downstream models to auto-summarize predefined aspects, allowing users to obtain structured information from an unstructured set of write-ups. Another use case could be to employ the aspectbased sentiments as features to classify future performance or volatility of investment ideas.</p><p>The under-explored nature of financial related ABSA also manifests itself in a lack of large, highquality datasets on which to train. Current ABSA techniques and model architectures do not accurately scale down to small data sizes, presenting an opportunity for transfer learning that can leverage larger, domain-related datasets. Successful inductive transfer learning allows these larger datasets that have no sentiment or aspect annotations to be used to improve results on the main ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>ABSA is not a new idea. There have been extensive bodies of work, starting from the original rulebased methods <ref type="bibr" target="#b19">(Thet et al., 2010)</ref> to more recent deep learning methods <ref type="bibr" target="#b20">(Wang and Liu, 2015)</ref>. In general, the state-of-the-art consists of a few subtasks. First, a model identifies the entity and its aspects. Then, sentiment analysis is performed on the body of text before combining the two tasks.</p><p>Sentiment analysis for finance -without considering aspects -has also been explored <ref type="bibr" target="#b4">(Cortis et al., 2017)</ref>. Unlike ABSA, more general sentiment analysis cannot determine if a statement is positive or negative by each aspect -it must choose an overall sentiment.</p><p>Until recently, it has been difficult to perform the kind of analysis in <ref type="bibr" target="#b20">Wang and Liu (2015)</ref> in the financial domain due to both a lack of wellannotated datasets and the necessary quantity of data. Unlike the larger product review dataset <ref type="bibr" target="#b18">(Pontiki et al., 2016)</ref> used in Wang's study, we speculate it would be expensive to annotate as many examples for the financial domain, as it would require extensive domain expertise. However, as part of the companion proceedings for WWW '18 conference, <ref type="bibr" target="#b11">Maia et al. (2018)</ref> released a very small dataset (FiQA) with a call for papers. FiQA contains the particular labels for which we are interested, but it lacks data quantity. Still, we reference the submissions to this open challenge as a response to FiQA Task 1. Many of the submissions <ref type="bibr" target="#b10">(Jangid et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2018;</ref><ref type="bibr"></ref> de Fran?a Costa and da Silva, 2018) use a neural architecture similar to <ref type="bibr" target="#b20">Wang and Liu (2015)</ref>, despite very few training examples. We show these results along with our own in Section 7.1.</p><p>The problem of few training examples brings us to the topic of inductive transfer learning <ref type="bibr" target="#b15">(Pan et al., 2010)</ref>. In general, transfer learning allows us to perform training on some source task with the ultimate goal of optimizing the loss of some target task. Word embeddings such as word2vec  or GloVe <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> are an early form of transfer learning in NLP. Just beyond that are more sophisticated vector representations of words, such as ELMo <ref type="bibr" target="#b17">(Peters et al., 2018)</ref>. ELMo embeddings offer a solution to the challenges of complex word use across linguistic context (i.e. polysemy, syntax, etc.) and limited training data. In ELMo, each word is assigned a representation which is a function of the entire corpus sentences to which they belong. At a high-level, the embeddings are computed from internal states of a two-layer biLSTM, bidirectional Language Model (biLM). Despite ELMo's important improvements on "traditional" word embeddings, any approach utilizing ELMo embeddings still requires a custom architecture further downstream.</p><p>In this paper, we explore a recent method called ULMFiT <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref>, which allows us to not only transfer word representations with a vector, but use a single, pre-trained model architecture (AWD-LSTM <ref type="bibr" target="#b12">Merity et al., 2017)</ref> for all intended tasks. Similar to ELMo, the key benefit of a higher level representation is that it allows semantically meaningful starting points of the input words for training. Using ELMo, one ultimately concatenates the output of each trained layer and uses it as a fixed embedding for some downstream task, whereas ULMFiT fine-tunes an entire language model to some target domain and then directly connects a downstream target task. This concept itself is not entirely new (Dai and Le, 2015), but <ref type="bibr" target="#b9">Howard and Ruder (2018)</ref> contribute novel techniques (Gradual Unfreezing, Discriminative Learning Rates and Slanted Triangular Learning Rates) to make this possible on small datasets without all prior learning being forgotten. Similar to chain-thaw <ref type="bibr" target="#b7">(Felbo et al., 2017)</ref>, gradual unfreezing offers another approach to the transfer learning training and fine-tuning process. Chainthaw first tunes any new layers in a model until convergence; this is followed by a sequential tuning of each layer individually. Finally, chain-thaw fine-tunes all layers together. In contrast, gradual unfreezing fine-tunes all layers in reverse, adding a 'thawed' layer instead of fine-tuning single layers individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contributions</head><p>The following are the primary contributions of this paper:</p><p>? We assess the performance of recent NLP inductive transfer learning techniques such as ULMFiT on a new dataset (FiQA) which has the problem of limited training examples.</p><p>? We use FiQA as our target task to conduct experiments with varying intermediary tasks, at least one of which appears to be novel, to our knowledge.</p><p>? We extend state-of-the-art on FiQA task 1 (aspect 2 classification and sentiment scoring) by using our novel intermediary task.</p><p>The rest of this paper is structured as follows. In Section 4 we introduce the critical datasets used for both the primary and intermediary tasks (outlined in Section 5). We propose our novel variation to model architecture and hyper-parameter tuning in Section 6. In Section 7 we discuss our results and analysis of our experiments. Finally, we conclude with some future direction of research related to this paper in Section 8. , and our Level 2 Aspect label takes on one of twenty-seven possible labels (Appointment, Risks, Dividend Policy, Financial, Legal, Volatility, Coverage, Price Action, etc.). The original dataset contained a small number of multilabel examples, however, we considered this number too few to train a meaningful multilabel classifier. Thus, we slightly stray from the original WWW '18 task for the purpose of this research. Finally, sentiment score takes on a continuous value between -1 and 1 -most negative to most positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Value Investors Club (VIC)</head><p>VIC is an online investment forum where fund managers and skilled investors submit in-depth investment recommendations on a daily basis. We scraped roughly 6,800 documents with investment theses along with attributes of a particular thesis, such as long or short stock position. Each document has an average of approximately 1,950 words which gives us a corpus size of about 13M. This is a crucial dataset that we use to adapt the domain of our general pre-trained models to our tasks in FiQA subsection 6.2. We find the high quality of VIC to be of particular interest. Although most recommendations and investment ideas shared online lack credibility or accuracy, VIC investment ideas empirically outperform the market on aver-age and over time <ref type="bibr" target="#b8">(Gray et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-Trained Models</head><p>Although we do not directly train on "1B Word Benchmark" corpus <ref type="bibr" target="#b2">(Chelba et al., 2013)</ref> or wikitext-103 <ref type="bibr" target="#b13">(Merity et al., 2016)</ref>, we thought it was important to note the underlying source for the pre-trained models released by <ref type="bibr" target="#b9">Howard and Ruder (2018)</ref> and <ref type="bibr" target="#b17">Peters et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tasks</head><p>Using this outlined data, we have two primary subtasks: classification for determining sentence aspects and regression for continuous sentiment. Namely, FiQA Level 2 Aspect and FiQA sentiment score are our ultimate target values of interest. We measure our models by using error-rate and F1-score for the classification task, and with MSE and R-squared for the regression task. These tasks can be seen in <ref type="figure">Figure 1</ref> We also have a number of intermediary tasks, the purpose of which is to better adapt the pretrained models to our primary tasks. First, we use VIC to train a language model on the 13M word corpus. We also train VIC on a binary classifier which learns if the thesis of a given body of text is that of a long position or short position. We also use FiQA Level 1 Aspect as an intermediary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline</head><p>Our most naive baseline on the performance of the primary tasks are logistic regression for aspect classification and linear regression for sentiment. We use a simple sparse representation of words, rather than introduce any type of continuous embedding.</p><p>We also create a simple ELMo embedding baseline from the Peters et al. <ref type="formula">(2018)</ref> pre-trained biLM. Our model computes a fixed mean-pooling of all contextualized word representations for each input sentence which is then passed through a single dense layer. Note that we do not claim this methodology to be comparable to our more rigorous implementation of ULMFiT. We use this as a neural baseline, but reserve any in-depth analysis for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">ULMFiT</head><p>We largely use the methodology and architecture used in the ULMFiT <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref> Figure 1: Our custom ULMFiT training process consists of several stages: for all final tasks, our base models begin with a language model pretrained on Wikitext-103 followed by a fine-tuning step trained on the VIC corpus. For further steps, the language model's decoder is removed and replaced with a linear head. FiQA Aspect Level 1 classification is first trained before then passing that same model onto training for FiQA Aspect Level 2 classification. For our regression task, the fine-tuned language model performs an intermediary classification task on known attributes of the VIC write-ups (namely, whether a write-up advocates for a long or short position). Lastly, the model is trained as a regressor on FiQA sentiment scores.</p><p>paper and experiment with different methods of domain adaptation, model fine-tuning, and hyperparameter tuning.</p><p>Typically, ULMFiT is composed of three phases of training. First, a language model is trained using a large general corpus. In particular, <ref type="bibr" target="#b9">Howard and Ruder (2018)</ref> released a pre-trained general language model which we use as our starting point. Second, the weights from the first phase are fine-tuned to a general task using a corpus from the target domain. Third, the weights from the second phase are fine-tuned to the primary task using the same upstream architecture as the first two phases, but by passing the output of the tuned LSTM model into a new fully-connected layer.</p><p>We propose a few different variations of the typical framework and apply it to FiQA:</p><p>1. FiQA does not contain enough volume of data to meaningfully apply LM fine-tuning (Phase 2); Thus, we use VIC as our target corpus for the language model fine-tuning. In this phase, we apply no change in model architecture.</p><p>2. We further exploit VIC labels of long/short positions and perform an additional round of fine-tuning in Phase 2. The decoder of the Language Model is no longer used, and, in its place, we add a fully-connected layer with a binary output.</p><p>3. We train the FiQA primary classification and regression tasks using both variations of phase two, attaching classification and regression fully-connected layers, respectively. 4. We train FiQA Aspect Level 1 using both variations of phase two, then transfer the weights further downstream for FiQA Aspect Level 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hyper-parameters</head><p>As suggested in <ref type="bibr" target="#b9">Howard and Ruder (2018)</ref>, there are a number of techniques used to ensure that the training process during fine-tuning does not cause the model to "forget" what was previously learned. We experiment with both gradual unfreezing (2018) and chain-thaw (2017) and compare results on varying sub-sample sizes. We also experiment with some early-stoppage of chainthaw, which we think may be beneficial to avoid catastrophic forgetting and to not overly fine-tune.</p><p>In addition, we utilize slanted triangular learning rates, concat pooling, and bptt for text classification as described in <ref type="bibr" target="#b9">Howard and Ruder (2018)</ref>. While we did initially perform some exploratory work on different hyper-parameter values of these latter techniques, we keep the values constant for our experiments and results. We were able to ultimately outperform the current FiQA Task 1 state-of-the-art by using the ULM-FiT framework in conjunction with our modified intermediary tasks (see <ref type="table" target="#tab_1">Table 2</ref>). In some cases we achieve close to state-of-the-art scores, even without much fine-tuning. In Aspect Level 2 classification, although we can match state-of-the-art results with a typical ULMFiT process, we are unable to exceed stateof-the-art until we use Aspect Level 1 as our intermediary task. We conclude that since Aspect Level 1 and Level 2 are hierarchical, the internal states from fine-tuning on Aspect Level 1 lead to a much better starting point for training on Aspect Level 2. We want to be clear that this method does not train with Aspect Level 1 as a feature, we simply transfer the internal state. Thus, it will work at inference time without Aspect Level 1 labels. We also believe that this is not simply a result of additional training as the prior metrics for Aspect Level 2 had already converged.</p><p>While Aspect Level 1 was not our primary task, we note some interesting results here as well. We saw the best performance from using chain-thaw. For this particular task, we see that the other models actually perform worse than using the Phase 1 language model without any domain adaptation. It is possible that these methods of training here are leading to catastrophic forgetting <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref>, in which we lose the utility of the pre-trained language model.</p><p>For the regression task, we outperform the current FiQA state-of-the-art in terms of MSE, but not in terms of R-squared. It is interesting to note that the top-performer was from Phase 2 training on whether or not VIC advocates for a long or short position. It seems rational that transfer learning from a more closely related task leads to better starting internal states for our primary sentiment task.</p><p>We also note that in all cases, even our Phase 1 language model is comparable, if not superior, to our naive baseline models. In this case, no additional training time is required to produce these predictions. This framework is also relatively universal in the sense that not much prepreprocessing is required other than standard tokenization. Out-of-vocabulary words are also easily handled by using the mean representation of all other vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Hyper-parameter results</head><p>The only hyper-parameter we vary for each of the models above is the method in which layers are unfrozen. We report gradual unfreezing for all models, but only report the best version of chainthaw for each model for brevity. Roughly speaking, our primary tasks perform the best with full chain-thaw, but our Aspect Level 1 classification performs best with partial chain-thaw. More research is needed on this topic, but we speculate that this may be due to over-learning on the target task.</p><p>Also, we compare model performance for both primary tasks with random subsamples of the training data (see <ref type="figure">Figure 2</ref>). We see that chainthaw tends to have a smoother decrease in error. It appears the steepest portion of the curve occurs when there are even fewer training examples than <ref type="figure">Figure 2</ref>: Validation F1-scores on Aspect 2 classification and R2 on sentiment regression across a range of training examples. We find chain thaw to be a more stable and predictable learning process at smaller training sample sizes. our case. While it's difficult to further forecast the error rates, this shows some evidence our current methodology does help with the few training examples issue. However, the error rates do not yet appear to have hit a point of saturation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Aspect-based sentiment analysis is not broadly used in the finance industry. Due to a lack of well annotated data, it is necessary to apply transfer learning techniques to leverage data from a large general corpus for the training of tasks on specific domain data.</p><p>Because the dataset for ABSA training (FiQA) is so small, we modify the ULMFiT steps. We fine tuned the language model with another dataset containing target domain specific context (VIC) and perform additional fine-tuning on a classification task (long/short position of VIC documents). Another contribution is the improvement of the FiQA Aspect 2 classification task by leveraging a pre-trained model on Aspect 1 classification.</p><p>Despite improved performance using a modified ULMFiT process for ABSA training on a very small sample, more research is needed to generalize this methodology to multi-task and multilabel learning. Moreover, in order to improve the methodology, experiments using other pre-trained models of large corpora as well as different setups of hyper-parameters should be evaluated.</p><p>The adapted ULMFiT methodology to ABSA on a very small, domain-specific dataset such as FiQA is an important cornerstone for learning subsequent tasks related to predicting financial perfor-mance. The methodology can also be expanded to other domain specific tasks when the size of data available represents a challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test error rates and F1 scores for classification, test MSE and R2 for regression.</figDesc><table><row><cell>7 Results and Discussions</cell></row><row><cell>7.1 Model Results</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association of computational linguistics</title>
		<meeting>the 45th annual meeting of the association of computational linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of financial tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3191824</idno>
		<ptr target="https://doi.org/10.1145/3184558.3191824" />
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018. International World Wide Web Conferences Steering Committee, Republic and Canton of</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1943" to="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Cortis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Daudert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Huerlimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1511.01432</idno>
		<ptr target="http://arxiv.org/abs/1511.01432" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inf-ufg at fiqa 2018 task 1: Predicting sentiments and aspects on financial tweets and news headlines</title>
		<idno type="DOI">10.1145/3184558.3191828</idno>
		<ptr target="https://doi.org/10.1145/3184558.3191828" />
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018. International World Wide Web Conferences Steering Committee, Republic and Canton of</title>
		<editor>Dayan de Fran?a Costa and Nadia Felix Felipe da Silva</editor>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1967" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Do fund managers identify and share profitable ideas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Wesley R Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aspect-based financial sentiment analysis using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitkul</forename><surname>Jangid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivangi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajiv Ratn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1961" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Www&apos;18 open challenge: Financial opinion mining and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1941" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<ptr target="http://arxiv.org/abs/1708.02182" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Smadi</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orph?e</forename><surname>De Clercq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016)</title>
		<meeting>the 10th international workshop on semantic evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis of movie reviews on discussion boards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Cheon</forename><surname>Tun Thura Thet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Sg</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of information science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="823" to="848" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
