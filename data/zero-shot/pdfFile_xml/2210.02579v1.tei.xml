<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DigiFace-1M: 1 Million Digital Face Images for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangbin</forename><surname>Bae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><forename type="middle">Gorce</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><forename type="middle">Baltru?aitis</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><forename type="middle">Hewitt</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Chen</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><forename type="middle">Valentin</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Shen</surname></persName>
							<email>jinshen@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DigiFace-1M: 1 Million Digital Face Images for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a largescale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline 1 . We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to Syn-Face, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning-based face recognition models <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b16">18]</ref> use Deep Neural Networks (DNNs) to encode the given face image into an embedding vector of fixed di- <ref type="bibr" target="#b0">1</ref> DigiFace-1M dataset can be downloaded from https://github. com/microsoft/DigiFace1M mension (e.g., 512). These embeddings can then be used for various tasks, such as face identification (who is this person) and verification (are they the same person). To learn diverse, discriminative embeddings, the training dataset should contain a large number of unique identities. To learn robust embeddings, i.e., which are not sensitive to the changes in pose, expression, accessories, camera and lighting, the dataset should also contain a sufficient number of images per identity with these variations.</p><p>Publicly available face recognition datasets satisfy both. MS1MV2 <ref type="bibr" target="#b6">[8]</ref> contains 5.8M images of 85K identities (approx. 68 images per ID). Recently released Web-Face260M <ref type="bibr" target="#b41">[43]</ref> contains 260M images of 4M identities (approx. 65 images per ID). While such datasets have driven recent advances in face recognition models, there are several problems associated with them.</p><p>(1) Ethical issues. Large-scale face recognition datasets are often criticized for ethical issues including privacy violation and the lack of informed consent. For example, datasets like <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b41">43]</ref> are obtained by crawling web images of celebrities without consent. To increase the number of identities, some datasets exploited the term "celebrities" to include anyone with online presence. Datasets like <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b24">26]</ref> collected face images of the general public (including children) from Flickr <ref type="bibr">[3]</ref>. Projects like MegaPixels <ref type="bibr" target="#b2">[4]</ref> are exposing the ethical problems of such web-crawled face recognition datasets. Following severe criticism, public access to several datasets has been removed <ref type="bibr" target="#b1">[2]</ref>.</p><p>(2) Label noise. Web images collected by searching the names of celebrities often contain label errors. For example, the Labeled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b12">[14]</ref> contains several known errors including: (1) mislabeled images; <ref type="bibr" target="#b1">(2)</ref> distinct persons with the same name labeled as the same per- son; and (3) the same person that goes by different names labeled as different persons.</p><p>(3) Data bias. Face recognition models are generally trained and tested on celebrity faces, many of which are taken with strong lighting and make-up. Celebrity faces also have imbalanced racial distribution (e.g., 84.5% of the faces in CASIA-WebFace <ref type="bibr" target="#b37">[39]</ref> are Caucasian faces <ref type="bibr" target="#b32">[34]</ref>), leading to poor recognition accuracy for the under-represented racial groups <ref type="bibr" target="#b32">[34]</ref>.</p><p>In order to circumvent all these issues that affect the existing real face datasets, we introduce a new large-scale face recognition dataset consisting only of photo-realistic digital face images rendered using a computer graphics pipeline and make this dataset available to the community. Specifically, we build upon the face generation pipeline introduced by Wood et al. <ref type="bibr" target="#b34">[36]</ref>, tailoring the amount of variability for each attribute (e.g., pose and accessories) for our recognition task, and generate 1.22M images with 110K unique identities. Each identity is generated by randomizing the facial geometry and texture as well as the hair style. The generated face is then rendered with different poses, expressions, hair color, hair thickness and density, accessories (including clothes, make-ups, glasses, and head/face wear), cameras and environments, to encourage the network to learn a robust embedding. <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of synthetic face images in this new dataset. We generated 1.22M images, but in practice the number of identities and images you can generate with synthetics pipeline is only limited by the cost of generating and storing these images.</p><p>Digital synthetic faces can solve the aforementioned problems associated with the real face datasets. Firstly, the generated faces are free of label noise. Secondly, the bias in lighting, make-up and skin color can be reduced as we have full control over those attributes. Most importantly, the face generation pipeline does not rely on any privacy-sensitive data obtained without consent. This is a critical difference from the GAN-generated synthetic faces; face GANs rely (either directly or indirectly) on large-scale real face datasets to train some components of their pipeline, leaving unresolved ethical problems. For ex-ample, a recent method called SynFace <ref type="bibr" target="#b26">[28]</ref> was trained on synthetic faces generated using DiscoFaceGAN <ref type="bibr" target="#b7">[9]</ref>. While the generated face images are free of label noise, millions of real face images were used for training DiscoFaceGAN. The GANs may also inherit any bias that exists in the real face images used to train them. For our dataset, only 511 face scans, obtained with consent, were used to build a parametric model of face geometry and texture library <ref type="bibr" target="#b34">[36]</ref>. From this limited source data, we can generate infinite number of identities, making our approach easily scalable.</p><p>Our contributions can be summarized as below:</p><p>? We release a new large-scale synthetic dataset for face recognition that is free from privacy violations and lack of consent. To the best of our knowledge, our dataset, containing 1.22M images of 110K identities, is the largest public synthetic dataset for face recognition.</p><p>? Compared to SynFace <ref type="bibr" target="#b26">[28]</ref>, which is trained on GANgenerated faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). For five popular benchmarks <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b40">42]</ref>, the average error rate is reduced by 46.0% (accuracy from 74.75% to 86.37%).</p><p>? We demonstrate how the proposed synthetic dataset can be used in conjunction with a small number of real face images to substantially improve the accuracy. This simulates a scenario where a small number of curated (i.e., no label noise and reduced bias) real face images are collected with consent. By fine-tuning our network with only 120K real face images (i.e., 2% of the commonly-used MS1MV2 dataset <ref type="bibr" target="#b6">[8]</ref>), we achieve 99.33% accuracy on LFW and 93.61% on average across the five benchmarks, which is comparable to the methods trained on millions of real face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face recognition datasets with real face images. Major tech companies can utilize private data to train their face recognition models. Google used 100M-200M images of 8M identities to train FaceNet <ref type="bibr" target="#b27">[29]</ref>, and Facebook used 500M images of 10M identities <ref type="bibr" target="#b29">[31]</ref>. It is challenging to construct datasets of comparable size using face images that are publicly available. Public datasets generally rely on celebrity images <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b41">43]</ref> or web images that are posted with Creative Commons license <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b24">26]</ref>. As discussed in section 1, such datasets have ethical issues and suffer from label noise and data bias. Synthetic faces generated using deep generative models.</p><p>Deep generative models such as GANs <ref type="bibr" target="#b9">[11]</ref> can produce photo-realistic images and have been used to generate synthetic data to train face recognition <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b26">28]</ref>. While traditional generators (e.g., <ref type="bibr" target="#b14">[16]</ref>) generate a face image from a single latent vector that changes both the identity and its appearance, DiscoFaceGAN <ref type="bibr" target="#b7">[9]</ref> learned disentangled latent representations for identity, pose, expression and illumination. SynFace <ref type="bibr" target="#b26">[28]</ref> used DiscoFaceGAN to generate a synthetic dataset for face recognition, consisting of 10K identities and 500K images. SynFace achieved 91.93% accuracy on LFW dataset <ref type="bibr" target="#b12">[14]</ref>, and by mixing the synthetic dataset with 2K real identities (20 images each), the accuracy was pushed up to 97.23%. However, their performance is poor for large-pose-variation datasets (e.g., 75.03% on CFP-FP <ref type="bibr" target="#b28">[30]</ref> and 70.43% on CPLFW <ref type="bibr" target="#b39">[41]</ref>). This is mainly because it is challenging to train a 2D GAN to produce images that preserve 3D geometric consistency <ref type="bibr" target="#b8">[10]</ref>. Synthetic faces generated using 3D parametric models. Classical 3D parametric face models such as morphable models <ref type="bibr" target="#b3">[5]</ref> explicitly model the identity independently from other parameters which makes them well suited for generating face recognition datasets. However, previous results obtained with this kind of synthetic images have shown limited performance <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b17">19]</ref> unless combined with a large number of real images. This can be due to the lack of realism and variability in the models that have been used to generate the faces. Wood et al. <ref type="bibr" target="#b34">[36]</ref> introduced a pipeline for generating and rendering diverse and photo-realistic 3D face models. A generative face model, learned from the 3D scans of 511 individuals, is used to generate a random 3D face. The face is then combined with artist-created assets (e.g., texture, hair, accessories) and is rendered under a random environment (simulated with HDRIs -high dynamic range images). The rendered synthetic face images (and the corresponding autogenerated ground truth annotations) were used to learn various face analysis tasks such as face parsing <ref type="bibr" target="#b34">[36]</ref>, landmark localization <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref> and face reconstruction <ref type="bibr" target="#b35">[37]</ref>, demonstrating state-of-the-art performance. In this paper, we aim to demonstrate that such photo-realistic rendered synthetic faces can be used to tackle face recognition.</p><p>Accessory #1 Accessory #2 Accessory #3 Accessory #4 <ref type="figure">Figure 2</ref>. Each row shows the same identity rendered with different accessory setups. Accessories include clothes, glasses, makeup (e.g., eyeshadow and eyeliner), face-wear and head-wear. The color, density and thickness of facial and head hair are also randomized. The hair style is modified only when the sampled accessory conflicts with the original hair style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomize</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Density Thickness</head><p>Randomize Color Density Thickness + Style <ref type="figure">Figure 3</ref>. Randomizing the hair style makes the problem unnecessarily difficult (see the bottom row), as most people maintain similar hair styles. Therefore, we only randomize the color, density and thickness of the hair as shown in the top row (the hair is also randomly flipped horizontally).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Digital Faces for Face Recognition</head><p>This section explains how the proposed dataset is generated. We first explain how digital faces are controlled, rendered and aligned to create the dataset (subsection 3.1). After providing the dataset statistics (subsection 3.2), we introduce the data augmentation details which help in minimizing the synthetic-to-real domain-gap (subsection 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Face Rendering</head><p>We build upon the face generation and rendering pipeline introduced by Wood et al. <ref type="bibr" target="#b34">[36]</ref>. In this section, we explain the modifications we made to the original pipeline to create a large-scale dataset for face recognition.</p><p>We define identity as a unique combination of facial geometry, texture (albedo and displacement), eye color and hair style. For each identity, we render a number of im- ages where all other parameters are varied to encourage the network to learn robust embeddings. While hair style can change for an individual, most people maintain similar hair style (for both facial and head hair) which makes hair style an important cue for the person's identity. Consequently, for the same identity, we randomize only the color, density and thickness of the hair (see <ref type="figure">Figure 3</ref> for examples), and the hair style is only changed when the added head-wear is not compatible with the original hair style to avoid intersection (e.g., third image of top row in <ref type="figure">Figure 2</ref>). For sampling facial geometry, texture and eye color we follow <ref type="bibr" target="#b34">[36]</ref>.</p><p>For a given identity, we sample different accessories including clothing, make-up, glasses, face-wear (e.g., face masks) and head-wear (e.g., hats). After selecting the clothing randomly from the digital wardrobe, other accessories are added with probability p = {0.15, 0.15, 0.01, 0.15} respectively. We also add hands and secondary faces with a small probability (p = 0.01) to simulate the case when (1) the face is occluded by hands and when (2) there are multiple faces in the image. <ref type="figure">Figure 2</ref> shows examples of the sampled identities rendered with different sets of accessories.</p><p>For each accessory setup, we vary the pose, expression, camera and environment (lighting and background) to render multiple images. The camera is rotated around the face, both horizontally and vertically. Horizontal angle is sampled from a truncated zero-mean normal distribution with support ? hori ? [?90 ? , 90 ? ]. The variance is set such that the probability density p(? hori = 90 ? ) equals to 10 ?3 ? p(? hori = 0 ? ). Vertical angle is sampled from a similar truncated normal distribution with support ? vert ? [?30 ? , 30 ? ] and p(? vert = 30 ? ) = 10 ?3 ? p(? vert = 0 ? ). This allows us to render a wide range of poses while making sure that frontal views are rendered more often. Lastly, the face is randomly translated within the viewing frustum to add additional perspective distortion. For pose, expression, and environment sampling, we follow <ref type="bibr" target="#b34">[36]</ref>. <ref type="figure" target="#fig_1">Figure 4</ref> shows the impact of varying the pose, expression, environment and camera for the same identity and accessory setup. Face alignment. The input to the face embedding network should be an aligned crop around the face. Instead of de-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rendered Image</head><p>Landmarks Aligned Image Rendered Image Landmarks Aligned Image <ref type="figure">Figure 5</ref>. For synthetic faces, it is trivial to extract the locations of ground-truth facial landmarks (e.g., eyes, nose-tip and mouth corners) and align the crop around the face. This enables robust face alignment, even when some of the landmarks are not visible.</p><p>tecting facial landmarks using pre-trained DNNs (such as MTCNN <ref type="bibr" target="#b38">[40]</ref> and RetinaFace <ref type="bibr" target="#b5">[7]</ref>), we align the faces using the ground truth landmarks (see <ref type="figure">Figure 5</ref>), which enable robust alignment even when some landmarks are not visible.</p><p>Limitations. The face generation pipeline <ref type="bibr" target="#b34">[36]</ref> we build upon has a number of limitations resulting in domain-gap to real face images. Particularly relevant to face recognition is that we cannot generate the same person at different ages. While we simulate aging to some extent by randomizing the color, density and thickness of the hair (as hair typically becomes grayer, sparser and thinner during aging), more work should be done to faithfully simulate aging. Lack of coverage (e.g., no jewelry and tattoos) may also mean that the distribution of the synthetic data does not match reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Statistics</head><p>The proposed dataset consists of two parts. The first part contains 720K images with 10K identities. For each identity, 4 different sets of accessories are sampled and 18 images are rendered for each set (i.e., 72 images-peridentity). Since many views of the same face are available, the network can learn embedding that is robust to the changes in accessories, camera, pose, expression, and environment. The second part contains 500K images with 100K identities. For each identity, only one set of accessories is sampled and only 5 images are rendered. This part was added to substantially increase the total number of identities with small rendering cost. Ensuring sufficient number of identities is important since the network should learn to distinguish between similar-looking faces of different identities. We show in the experiments that mixing the two parts leads to better accuracy than using one of them <ref type="table">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>The quality of in-the-wild face images can vary significantly. Certain parts of the face may be occluded, and the images are subject to distortion and noise that are specific to each camera. As our synthetic faces are rendered with controlled quality using a perfect pinhole camera, aggressive data augmentation is needed to reduce the synthetic-to-real domain-gap. We first apply random horizontal flipping and cropping, following <ref type="bibr" target="#b16">[18]</ref>. Then, we apply two sets of augmentations -appearance and warping. <ref type="figure">Figure 6</ref> shows train-  <ref type="figure">Figure 6</ref>. Synthetic face images at different stages of data augmentation. Aggressive augmentation helps to simulate effects such as motion blur and distortion common in real-world images and thus improve the robustness of DNNs trained on synthetic images. ing images with these augmentations. Note that we apply the data augmentation on-the-fly during training, i.e., each epoch sees different random augmentations. For each type of augmentation, we indicate its probability p to be applied on a sample image. Appearance augmentation. We apply random Gaussian blur (p = 0.05) and Gaussian noise (p = 0.035). By applying the Gaussian blur along a random direction using an anisotropic covariance, we also simulate motion blur (p = 0.05). Brightness, contrast, hue and saturation are randomized with p = {0.15, 0.3, 0.1, 0.1}. Images are converted into grayscale with p = 0.01. Lastly, the image quality is randomized by downsampling-and-upsampling (p = 0.01) and JPEG compression (p = 0.05). Warping augmentation. Warping is performed by randomly shifting the four corners of the image. Firstly, the aspect ratio is randomized with p = 0.1. Then, all images undergo random scaling, rotation and shift. Lastly, the four corners are shifted differently for additional distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Implementation details. Synthetic faces are rendered using Cycles renderer <ref type="bibr" target="#b0">[1]</ref>, with 256 samples per pixel. The rendering of the full dataset took approximately 10 days, using 300 NVIDIA M60 GPUs. The images are rendered at 256?256 resolution, and the aligned crop around the face is resized into 112?112. We use ResNet-50 <ref type="bibr" target="#b11">[13]</ref> backbone for the experiments in subsection 5.1, 5.2 and 5.3. For comparison against the state-of-the-art methods in subsection 5.4, we use their encoder architecture to ensure fair comparison. For all experiments, the networks are implemented with Py-Torch <ref type="bibr" target="#b25">[27]</ref> and are trained for 40 epochs using SGD. The batch size is set to 256 and the networks are trained on four NVIDIA P100 GPUs. We follow the learning rate scheduling of <ref type="bibr" target="#b26">[28]</ref>, and use the training loss from <ref type="bibr" target="#b16">[18]</ref>. Note that all networks are trained from scratch (not pre-trained on, e.g., ImageNet <ref type="bibr" target="#b4">[6]</ref>), to make sure that no real images are used. Evaluation protocol. Following state-of-the-art methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b16">18]</ref>, we report the face verification accuracy on five benchmark datasets -LFW <ref type="bibr" target="#b12">[14]</ref>, CFP-FP <ref type="bibr" target="#b28">[30]</ref>, CPLFW <ref type="bibr" target="#b39">[41]</ref>, AgeDB <ref type="bibr" target="#b23">[25]</ref> and CALFW <ref type="bibr" target="#b40">[42]</ref>. LFW contains 6,000 pairs of in-the-wild face images. CFP-FP and CPLFW have larger pose variation (CFP-FP specifically compares frontal views to profile views). AgeDB and CALFW have larger age variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We run a series of experiments to demonstrate the usefulness of the proposed dataset. Subsection 5.1 compares different data augmentations. In subsection 5.2, we train the network on various different subsets of the full dataset to understand how each attribute sampling in rendering affects the accuracy. In subsection 5.3, we show that our synthetic faces can be used in conjunction with a small number of real faces to substantially improve the accuracy. Lastly, we provide comparison against the state-of-the-art methods in subsection 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Augmentation</head><p>In subsection 3.3, we introduced appearance and warping augmentations. As shown in <ref type="table">Table 1</ref>, both lead to significant improvement across all datasets. We also compare against the augmentation used by AdaFace <ref type="bibr" target="#b16">[18]</ref>, which includes horizontal flipping, cropping and mild color augmentation. For our synthetic face images which are free of imperfection, more aggressive data augmentation is needed to reduce the domain-gap. Notice that the warping augmentation improves the performance especially for the largepose-variation datasets (CFP-FP and CPLFW).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Dataset Composition</head><p>Having full control over the rendering pipeline, we can create a dataset with desired statistics to study how each attribute affects the face recognition accuracy. The results are provided in <ref type="table">Table 2</ref>. Accessory sampling. For 10K synthetic identities, we sampled 4 accessory setups and rendered 18 images for each setup (i.e., 720K images in total). These 18 images have variations in pose, expression, camera, and environment (see <ref type="figure" target="#fig_1">Figure 4</ref>). From this, we can create a subset of 180K images by selecting 18 images per ID with fixed accessory. Similarly, we can select 18 images randomly so that images with different accessories are used during training. When randomizing the accessories, we also randomized the color, thickness and density of the hair to simulate aging <ref type="figure">(Figure 3)</ref>. As a result, the accuracy is improved especially for the large-age-variation datasets (AgeDB and CALFW). For CFP-FP and CPLFW, which has smaller age gap (i.e.,  <ref type="table">Table 2</ref>, increasing the variation in horizontal and vertical angles improved the accuracy especially for the large-posevariation datasets (CFP-FP and CPLFW). For AgeDB and CALFW, which consists mainly of frontal faces, the accuracy was similar. Texture sampling. While we can create infinite number of unique facial geometries, the texture is sampled from a li-brary built from 208 scans of real human faces (obtained with consent). Since we generated 110K identities in total, many of them share the same texture. To see how the number of textures affects the accuracy, we created a dataset of 1200 identities with N textures, by generating 1200/N identities for each texture. As shown in Row 6-9 of <ref type="table">Table 2</ref>, increasing the number of textures did not lead to a meaningful improvement in the accuracy. This is contrary to the intuition that small number of textures and lack of texture generative model are limitations of synthetic data for face recognition. We believe that the appearance variability is a combination of geometry, texture, hair, accessories, environment and image quality. In <ref type="figure">Figure 7</ref>, we show that (1) the texture library already covers diverse skin color and age, (2) an arbitrary number of unique identities can be generated with the same texture, and (3) skin appearance is greatly affected by the environment. Also, the image quality for face recognition task is in general limited due to low resolution and data augmentation. Thus, the contribution of texture variation is likely less important than that of geometry and environment. Balance between # IDs and # images/ID. Ensuring large number of IDs is important for learning diverse discriminative embedding. On the other hand, large number of images per ID (referred to as images/ID) is needed for learning robust embedding (that is not affected by the changes in pose, accessories, expressions, camera and environment). Mixing the two datasets with different number of images/ID can be considered as an efficient way of getting the best of both. This also simulates the long-tailed distribution of real face datasets (i.e., most identities have small number of images).</p><p>The result in <ref type="table">Table 3</ref> shows that mixing the two datasets leads to better accuracy than using one of them.   <ref type="table">Table 3</ref>. Number of IDs and number of images/ID should both be high to learn diverse and robust embedding. Mixing two datasets with large/small number of images/ID can be an efficient way of satisfying both. The overall accuracy becomes higher than relying on one of the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Mixing with Real Faces</head><p>The main problems associated with large-scale real face datasets are ethical issues, label noise and data bias. In this study, we assume a scenario where a small number of real face images are collected with consent. For small number of images, it would also be possible to remove (or reduce) the label noise and data bias.</p><p>For synthetic data, we used 10K identities with 72 images per identity. For real face images, we varied the number of identities from 200 to 2000, with 20 images sampled for each identity (the identities and images were sampled randomly from CASIA-WebFace <ref type="bibr" target="#b37">[39]</ref>).</p><p>We first tried training only on the synthetic data. Secondly, we tried training only on the real data. Then, we explored two different strategies for using both real and synthetic images: (1) dataset mixing and (2) pre-training on synthetic data and fine-tuning on the real data. For finetuning, we reduced the learning rate by 1/10 for the prediction head, and 1/100 for the encoder to avoid catastrophic forgetting. The results are provided in <ref type="figure" target="#fig_2">Figure 8</ref>.</p><p>When the network is trained only on a small number of real face images, the accuracy is worse than the network trained only on our synthetic dataset. Both dataset mixing and pre-training can lead to significantly higher accuracy, especially for the large-pose-variation datasets (CFP-FP and CPLFW). Compared to dataset mixing, pre-training on synthetics followed by fine-tuning on real images led to better accuracy. This can be due to the imbalance between the number of images (we use 720K synthetic images, and a lot fewer real images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison to the State-of-the-Art</head><p>Comparison to SynFace. SynFace <ref type="bibr" target="#b26">[28]</ref> is the current state-of-the-art for face recognition model trained on synthetic faces. They used DiscoFaceGAN <ref type="bibr" target="#b7">[9]</ref> to generate 500K synthetic faces (10K identities &amp; 50 images/ID). To ensure a fair comparison, we trained the same encoder (LResNet50E-IR) with same number of images. We also trained using our full dataset (1.22M images). The results are provided in Row 1-3 of <ref type="table" target="#tab_3">Table 4</ref>. In the second scenario, we additionally used 40K real face images from CASIA-WebFace <ref type="bibr" target="#b37">[39]</ref>. While SynFace mixed their synthetic dataset with the real faces, we instead adopted the two-stage method of pre-training and fine-tuning as discussed in subsection 5.3. The results are provided in Row 4-6 of <ref type="table" target="#tab_3">Table 4</ref>. For both scenarios, we significantly outperform SynFace across all datasets. This suggests that our rendered synthetic faces are better than GAN-generated faces for learning face recognition. While GANs like <ref type="bibr" target="#b7">[9]</ref> can generate realistic face images, the data they generate is not ideal for face recognition, due to following reasons: (1) Identity change. While <ref type="bibr" target="#b7">[9]</ref> is encouraged to preserve the identity when changing other latent variables, there is no guarantee that the identity will be preserved during data generation. (2) Geometric inconsistency. As pointed out by <ref type="bibr" target="#b8">[10]</ref>, the images generated  <ref type="table">Table 5</ref>. Comparison to the state-of-the-art methods trained on real face images (MS1MV2 <ref type="bibr" target="#b6">[8]</ref>). We use the same backbone (ResNet100) for fair comparison. By only using 120K real face images (2% of MS1MV2 <ref type="bibr" target="#b6">[8]</ref>), we achieve accuracy that is comparable to the methods trained on millions of real face images. Since we do not model aging explicitly, our accuracy is worse for large-age-variation datasets (AgeDB and CALFW). Avg ? shows average of LFW, CFP-FP and CPLFW, and on these, we outperform <ref type="bibr" target="#b33">[35]</ref> and are similar to <ref type="bibr" target="#b21">[23]</ref>.</p><p>by <ref type="bibr" target="#b7">[9]</ref> for same identity and different poses lack 3D consistency.</p><p>(3) Lack of accessory change. <ref type="bibr" target="#b7">[9]</ref> cannot randomize accessories. (4) Unresolved ethical concerns. Training the GAN model itself requires large-scale real face dataset. For example, 70K images are used to train <ref type="bibr" target="#b7">[9]</ref>. To learn to preserve identity, they also used a perceptual loss based on <ref type="bibr" target="#b36">[38]</ref>, which is trained on 3M real face images. In Row 2 and 3 of <ref type="table" target="#tab_3">Table 4</ref>, we increase our synthetics dataset size from 500K to 1.22M, and achieve better accuracy. This indicates that the accuracy may not have converged yet and could be improved further by generating more synthetic data. Comparison to methods trained on real faces. Lastly, we compare the accuracy against the methods that are trained on real face images. In <ref type="table">Table 5</ref>, we provide the accuracy of six methods that use ResNet100 as the embedding network and MS1MV2 <ref type="bibr" target="#b6">[8]</ref> as the training data. We trained the same architecture on our synthetic dataset (Row 1). We also tried fine-tuning the network on a small number of real face images (Row 2). When trained only with the proposed synthetic dataset, the network can achieve 96.17% on LFW. For LFW, CFP-FP and CPLFW (excluding the highage-variation datasets), the average accuracy is 89.40%. By fine-tuning the network on just 120K images (2.0% of MS1MV2), the accuracy becomes comparable to the methods trained on MS1MV2 (e.g., average accuracy on LFW, CFP-FP and CPLFW becomes higher than that of SV-AM-Softmax <ref type="bibr" target="#b33">[35]</ref>).</p><p>The performance of our method on AgeDB <ref type="bibr" target="#b23">[25]</ref> and CALFW <ref type="bibr" target="#b40">[42]</ref> has a significantly larger gap than for the other datasets evaluated. This is expected given the lack of aging simulation in our synthetic data. We suspect that other causes of domain-gap, as described at the end of subsection 3.1, are the primary reason for the remaining performance gap for other evaluation datasets. Reducing this domain-gap remains an area of ongoing work for our synthetic data and is likely to result in improved performance for all downstream tasks, including face recognition. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduced a new large-scale synthetic dataset for face recognition by rendering digital faces using a graphics pipeline. We ran extensive experiments to study how data augmentation and various other attributes affect the accuracy. We demonstrated that our synthetic faces are significantly better than the GAN-generated faces for learning face recognition. With a small number of real face images, we achieve accuracy that is comparable to the methods trained on millions of web-crawled face images. We hope this dataset would be a meaningful step towards developing socially responsible face recognition models that do not depend on privacy-sensitive data obtained without consent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of synthetic face images in our dataset. Our dataset captures a wide variety of facial geometry, pose, textures, expressions, accessories and environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of images rendered for the same identity and accessory setup. The same face can look very different depending on the pose, expression, environment (lighting and background) and camera, encouraging the network to learn robust embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Comparison between training with our synthetic data only (black dashed line), with small amount of real data only (red line), with the mixture of the two (blue line), and pre-training on synthetics and fine-tuning with the real data (black line). The number of real identities varies from 200 to 2000, and 20 images are sampled for each identity. When only a small number of real face images are available (e.g., due to ethical issues), the proposed synthetic dataset can substantially improve the accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>The proposed aggressive data augmentation significantly improves the accuracy across all datasets. Dataset composition experiments to study how the sampling of each attribute affects the accuracy.Figure 7. Left: 40 textures selected randomly from the texture library. The library covers diverse skin color and age. Right top row: various identities (facial geometry and hair style) sampled with the same texture. Right bottom row: same identity with the same texture under different environments (taken from<ref type="bibr" target="#b34">[36]</ref>). With large variations in geometry, hair style and environments, rich appearance variation could be achieved with limited textures. positive pairs capture the identity at similar age), fixing the accessory and hair leads to slightly better accuracy. Pose sampling. Similar to the accessory sampling, we can select 18 images for each of the 10K identities by selecting the ones with the smallest horizontal/vertical angles. Then, we can compare them against the 18 images selected randomly. For the randomly selected images, the standard deviation in horizontal and vertical angles were (? hori , ? vert ) = (24.13 ? , 9.20 ? ). For the images with the smallest horizontal/vertical angles, they were (4.71 ? , 8.06 ? ) and (22.02 ? , 1.72 ? ) respectively. As shown in Row 3-5 in</figDesc><table><row><cell>Experiment</cell><cell cols="2">Method</cell><cell cols="4">LFW CFP-FP CPLFW AgeDB CALFW Avg</cell></row><row><cell></cell><cell cols="2">No augmentation</cell><cell>88.07 70.99</cell><cell>66.73</cell><cell>60.92</cell><cell>69.23 71.19</cell></row><row><cell>Data augmentation</cell><cell cols="3">Augmentation from AdaFace [18] 90.12 76.41 Ours (appearance) 94.32 80.00</cell><cell>71.33 74.83</cell><cell>67.17 75.82</cell><cell>74.13 75.83 76.92 80.38</cell></row><row><cell></cell><cell cols="2">Ours (appearance + warping)</cell><cell>94.55 84.86</cell><cell>77.08</cell><cell>76.97</cell><cell>77.20 82.13</cell></row><row><cell>Experiment</cell><cell></cell><cell>Method</cell><cell cols="4">LFW CFP-FP CPLFW AgeDB CALFW Avg</cell></row><row><cell>Accessory sampling</cell><cell></cell><cell>Fix accessory Randomize accessory</cell><cell>93.50 82.16 94.23 82.04</cell><cell>75.75 75.18</cell><cell>73.05 76.43</cell><cell>73.83 79.66 77.22 81.02</cell></row><row><cell></cell><cell></cell><cell cols="2">Minimize horizontal angle 93.42 67.19</cell><cell>66.48</cell><cell>76.78</cell><cell>77.22 76.22</cell></row><row><cell>Pose sampling</cell><cell></cell><cell>Minimize vertical angle</cell><cell>93.67 81.13</cell><cell>74.57</cell><cell>76.57</cell><cell>76.68 80.52</cell></row><row><cell></cell><cell></cell><cell>Random pose</cell><cell>94.23 82.04</cell><cell>75.18</cell><cell>76.43</cell><cell>77.22 81.02</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>89.63 75.04</cell><cell>69.72</cell><cell>69.47</cell><cell>70.10 74.79</cell></row><row><cell>Texture sampling</cell><cell></cell><cell>100</cell><cell>90.83 74.84</cell><cell>70.30</cell><cell>70.62</cell><cell>70.57 75.43</cell></row><row><cell cols="2">(# textures to select from)</cell><cell>150</cell><cell>90.03 73.01</cell><cell>69.63</cell><cell>71.48</cell><cell>70.27 74.89</cell></row><row><cell></cell><cell></cell><cell>200</cell><cell>89.82 73.37</cell><cell>69.37</cell><cell>71.45</cell><cell>70.50 74.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison to SynFace using the same encoder architecture (LResNet50E-IR<ref type="bibr" target="#b26">[28]</ref>). For both scenarios -training only on synthetic faces &amp; using a small number of real faces -we significantly outperform SynFace across all datasets. Avg ? shows average of LFW, CFP-FP and CPLFW, excluding the large-age-variation datasets.</figDesc><table><row><cell>Method</cell><cell cols="2"># Synthetic images (# IDs ? # imgs/ID)</cell><cell cols="2"># Real images (# IDs ? # imgs/ID)</cell><cell cols="3">LFW CFP-FP CPLFW AgeDB CALFW Avg Avg  ?</cell></row><row><cell>SynFace [28]</cell><cell cols="2">500K (10K?50)</cell><cell>0</cell><cell></cell><cell cols="2">91.93 75.03</cell><cell>70.43</cell><cell>61.63</cell><cell>74.73 74.75 79.13</cell></row><row><cell>Ours</cell><cell cols="2">500K (10K?50)</cell><cell>0</cell><cell></cell><cell cols="2">95.40 87.40</cell><cell>78.87</cell><cell>76.97</cell><cell>78.62 83.45 87.22</cell></row><row><cell>Ours</cell><cell cols="2">1.22M (10K?72+100K?5)</cell><cell>0</cell><cell></cell><cell cols="2">95.82 88.77</cell><cell>81.62</cell><cell>79.72</cell><cell>80.70 85.32 88.74</cell></row><row><cell>SynFace [28]</cell><cell cols="2">500K (10K?50)</cell><cell cols="2">40K (2K?20)</cell><cell cols="2">97.23 87.68</cell><cell>80.32</cell><cell>81.42</cell><cell>85.08 86.35 88.41</cell></row><row><cell>Ours</cell><cell cols="2">500K (10K?50)</cell><cell cols="2">40K (2K?20)</cell><cell cols="2">99.05 94.01</cell><cell>87.27</cell><cell>89.77</cell><cell>90.08 92.04 93.44</cell></row><row><cell>Ours</cell><cell cols="2">1.22M (10K?72+100K?5)</cell><cell cols="2">40K (2K?20)</cell><cell cols="2">99.17 94.63</cell><cell>88.10</cell><cell>90.50</cell><cell>90.97 92.67 93.97</cell></row><row><cell cols="2">Method</cell><cell cols="6"># Synthetic images # Real images LFW CFP-FP CPLFW AgeDB CALFW Avg Avg  ?</cell></row><row><cell cols="2">Ours (SX best)</cell><cell>1.22M</cell><cell>0</cell><cell cols="2">96.17 89.81</cell><cell cols="2">82.23</cell><cell>81.10</cell><cell>82.55 86.37 89.40</cell></row><row><cell cols="2">Ours (SX+Real best)</cell><cell>1.22M</cell><cell>120K</cell><cell cols="2">99.33 95.93</cell><cell cols="2">89.47</cell><cell>91.55</cell><cell>91.78 93.61 94.91</cell></row><row><cell cols="2">SV-AM-Softmax [35]</cell><cell></cell><cell></cell><cell cols="2">99.50 95.10</cell><cell cols="2">89.48</cell><cell>95.68</cell><cell>94.38 94.83 94.69</cell></row><row><cell cols="2">SphereFace [23]</cell><cell></cell><cell></cell><cell cols="2">99.67 96.84</cell><cell cols="2">91.27</cell><cell>97.05</cell><cell>95.58 96.08 95.93</cell></row><row><cell cols="2">CosFace [33] ArcFace [8]</cell><cell>0</cell><cell>5.8M</cell><cell cols="2">99.78 98.26 99.81 98.40</cell><cell cols="2">92.18 92.72</cell><cell>98.17 98.05</cell><cell>96.18 96.91 96.74 95.96 96.99 96.98</cell></row><row><cell cols="2">MagFace [24]</cell><cell></cell><cell></cell><cell cols="2">99.83 98.46</cell><cell cols="2">92.87</cell><cell>98.17</cell><cell>96.15 97.10 97.05</cell></row><row><cell cols="2">AdaFace [18]</cell><cell></cell><cell></cell><cell cols="2">99.82 98.49</cell><cell cols="2">93.53</cell><cell>98.05</cell><cell>96.08 97.19 97.28</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? Having full control over the rendering pipeline, we perform extensive experiments to study how each attribute (e.g., variation in facial pose, accessories and textures) affects the face recognition accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://www.cycles-renderer.org" />
		<title level="m">Blender foundation. cycles renderer</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The ethical questions that haunt facial-recognition research</title>
		<idno>d41586-020-03187-3. Accessed: 2022-10-03</idno>
		<ptr target="https://www.nature.com/articles/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Megapixels</surname></persName>
		</author>
		<ptr target="https://ahprojects.com/megapixels-glassroom/" />
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disentangled and controllable face image generation via 3d imitative-contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gram: Generative radiance manifolds for 3d-aware image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on faces in &apos;Real-Life&apos; Images: detection, alignment, and recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curricularface: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaface: Quality adaptive margin for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Can synthetic faces undo the damage of dataset bias to face recognition and facial landmark detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corius</forename><surname>Reyneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08565</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and reducing the damage of dataset bias to face recognition with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic class queue for large scale face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spherical confidence learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Magface: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Synface: Face recognition with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating photo-realistic training data to improve face recognition accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Daniel S?ez Trigueros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Racial faces in the wild: Reducing racial bias by information maximization adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Support vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11317</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fake it till you make it: face analysis in the wild using synthetic data alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dziadzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ivan Stojiljkovic, et al. 3d face reconstruction with dense landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Milosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Webface260m: A benchmark unveiling the power of million-scale deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
