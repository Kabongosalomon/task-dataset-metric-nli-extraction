<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Leng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Soong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text to speech (TTS) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a TTS system can achieve human-level quality, how to define/judge that quality and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a TTS system called NaturalSpeech that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder (VAE) for end-to-end text to waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in VAE. Experiment evaluations on popular LJSpeech dataset show that our proposed NaturalSpeech achieves ?0.01 CMOS (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text to speech (TTS) aims at synthesizing intelligible and natural speech from text <ref type="bibr" target="#b0">[1]</ref>, and has made rapid progress in recent years due to the development of deep learning. Neural network based TTS has evolved from CNN/RNN-based models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> to Transformer-based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, from basic generative models (autoregressive) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> to more powerful models (VAE, GAN, flow, diffusion) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, from cascaded acoustic models/vocoders <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to fully end-to-end models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Building TTS systems with human-level quality has always been the dream of the practitioners in speech synthesis. While current TTS systems achieve high voice quality, they still have quality gap compared with human recordings. To pursue this goal, several questions need to be answered: 1) how to define human-level quality in text to speech synthesis? 2) how to judge whether a TTS system has achieved human-level quality or not? 3) how to build a TTS system to achieve human-level quality? In this paper, we conduct a comprehensive study on these problems in TTS. We first give a formal definition on human-level quality in TTS based on a statistical and measurable way (see <ref type="bibr">Definition 1)</ref>. Then we introduce some guidelines to judge whether a TTS system has achieved human-level quality with a hypothesis test. Using this judge method, we found several previous TTS systems have not achieved it (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>In this paper, we further develop a fully end-to-end text to waveform generation system called NaturalSpeech to bridge the quality gap to recordings and achieve human-level quality. Specifically, inspired by image/video/waveform generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15]</ref>, we leverage variational autoencoder (VAE) <ref type="bibr" target="#b21">[22]</ref> to compress the high-dimensional speech (x) into continuous frame-level representations (denoted as posterior q(z|x)), which are used to reconstruct the waveform (denoted as p(x|z)). The corresponding prior (denoted as p(z|y)) is obtained from the text sequence y. Considering the posterior from speech is more complicated than the prior from text, we design several modules (see <ref type="figure" target="#fig_0">Figure 1</ref>) to match the posterior and prior as close to each other as possible, to enable text to speech synthesis through p(z|y) ? p(x|z):</p><p>? We leverage large-scale pre-training on the phoneme encoder to extract better representations from phoneme sequence (Section 3.2). ? We leverage a fully differentiable durator 2 that consists of a duration predictor and an upsampling layer to improve the duration modeling (Section 3.3). ? We design a bidirectional prior/posterior module based on flow models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> to further enhance the prior p(z|y) and reduce the complexity of posterior q(z|x) (Section 3.4). ? We propose a memory based VAE to reduce the complexity of the posterior needed to reconstruct waveform (Section 3.5).</p><p>Compared to previous TTS systems, NaturalSpeech has several advantages: 1) Reduce traininginference mismatch. In previous cascaded acoustic model/vocoder pipeline <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref> and explicit duration prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>, both mel-spectrogram and duration suffer from training-inference mismatch since ground-truth values are used in training the vocoder and mel-spectrogram decoder while predicted values are used in inference. Our fully end-to-end text to waveform generation and differentiable durator can avoid the training-inference mismatch. 2) Alleviate one-to-many mapping problem. One text sequence can correspond to multiple speech utterances with different variation information (e.g., pitch, duration, speed, pause, prosody, etc). Previous works only using variance adaptor <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11]</ref> to predict pitch/duration cannot well handle the one-to-many mapping problem. Our memory based VAE and bidirectional prior/posterior can reduce the complexity of posterior and enhance the prior, which helps relieve the one-to-many mapping problem. 3) Improve representation capacity. Previous models are not powerful enough to extract good representations from phoneme sequence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> and learn complicated data distribution in speech <ref type="bibr" target="#b17">[18]</ref>. Our large-scale phoneme pre-training and powerful generative models such as flow and VAE can learn better text representations and speech data distributions.</p><p>We conduct experimental evaluations on the widely adopted LJSpeech dataset <ref type="bibr" target="#b25">[26]</ref> to measure the voice quality of our NaturalSpeech system. Based on the proposed judgement guidelines, NaturalSpeech achieves similar quality with human recordings in terms of MOS (mean opinion score) and CMOS (comparative MOS). Specifically, the speech generated by NaturalSpeech achieves ?0.01 CMOS compared to recordings, with p-level p 0.05 under Wilcoxon signed rank test, which demonstrates that NaturalSpeech can generate speech with no statistically significant difference from recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition and Judgement of Human-Level Quality in TTS</head><p>In this section, we introduce the formal definition of human-level quality in text to speech synthesis, and describe how to judge whether a TTS system achieves human-level quality or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition of Human-Level Quality</head><p>We define human-level quality in a statistical and measurable way. Definition 1. If there is no statistically significant difference between the quality scores of the speech generated by a TTS system and the quality scores of the corresponding human recordings on a test set, then this TTS system achieves human-level quality on this test set.</p><p>Note that by claiming a TTS system achieves human-level quality on a test set, we do not mean that a TTS system can surpass or replace human, but the quality of this TTS system is statistically indistinguishable from human recordings on this test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Judgement of Human-Level Quality</head><p>Judgement Guideline While there are some objective metrics to measure the quality gap between the generated speech and human recordings, such as PESQ <ref type="bibr" target="#b26">[27]</ref>, STOI <ref type="bibr" target="#b27">[28]</ref>, SI-SDR <ref type="bibr" target="#b28">[29]</ref>, they are not reliable to measure the perception quality in TTS. Therefore, we use subjective evaluation to measure the voice quality. Previous works usually use mean opinion score (MOS) with 5 points (from 1 to 5) to compare the generated speech with recordings. However, MOS is not sensitive enough to the difference in voice quality since the judge simply rates the quality of each sentence alone from the two systems with no paired comparison. Thus, we choose comparative mean opinion score (CMOS) with 7 points (from ?3 to 3) as the evaluation metric, where each judge measures the voice quality by comparing samples from two systems head by head. We further conduct Wilcoxon signed rank test <ref type="bibr" target="#b29">[30]</ref> to measure whether the two systems are significantly different or not in terms of CMOS evaluation.</p><p>Therefore, we list the judgement guidelines of human-level quality as follows: 1) Each utterance from TTS system and human recordings should be listened and compared side-by-side by more than 20 judges, who should be native language speakers. At least 50 test utterances from each system should be used in the judgement. 2) The speech generated by TTS system has no statistically significant difference from human recordings, if and only if the average CMOS is close to 0 and the p-level of Wilcoxon signed rank test satisfies p &gt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Judgement of Previous TTS Systems</head><p>Based on these guidelines, we test whether current TTS systems can achieve human-level quality or not on the LJSpeech dataset. The systems we study include: 1) FastSpeech 2 <ref type="bibr" target="#b17">[18]</ref> + HiFiGAN <ref type="bibr" target="#b16">[17]</ref>, 2) Glow-TTS [13] + HiFiGAN <ref type="bibr" target="#b16">[17]</ref>, 3) Grad-TTS <ref type="bibr" target="#b13">[14]</ref> + HiFiGAN <ref type="bibr" target="#b16">[17]</ref>, 4) VITS <ref type="bibr" target="#b14">[15]</ref>. We re-produce the results of all these systems by our own, which can match or even beat the quality in their original papers (note that the HiFiGAN vocoder is fine-tuned on the predicted mel-spectrograms for better synthesis quality). We use 50 test utterances, each with 20 judges for MOS and CMOS evaluation. As shown in <ref type="table" target="#tab_0">Table 1</ref>, although the current TTS systems can achieve close MOS with recordings, they have a large CMOS gap to recordings, with Wilcoxon signed rank test at p-level p 0.05, which shows statistically significant difference from human recordings. We further study where the quality gap comes from by analyzing each component in one of the above TTS systems in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of NaturalSpeech System</head><p>To bridge the quality gap to human recordings, we develop NaturalSpeech, a fully end-to-end text to waveform generation model. We first describe the design principle of our system (Section 3.1), and then introduce each module of this system (Section 3.2-3.5) and training/inference pipeline (Section 3.6), and finally explain why our system can bridge the quality gap to human recordings (Section 3.7).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Principle</head><p>Inspired by image/video generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> that uses VQ-VAE <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> to compress high-dimensional image into low-dimensional representations to ease the generation, we leverage VAE <ref type="bibr" target="#b21">[22]</ref> to compress high-dimensional speech x into frame-level representations z (i.e., z is sampled from posterior distribution q(z|x)), which are used to reconstruct the waveform (denoted as p(x|z)). In general formulation of VAE, the prior p(z) is chosen to be standard isotropic multivariate Gaussian. To enable conditional waveform generation from input text in TTS, we predict z from phoneme sequence y, i.e., z is sampled from predicted prior distribution p(z|y). We jointly optimize the VAE and the prior prediction with gradients propogating to both q(z|x) and p(z|y). Derived from the evidence lower bound <ref type="bibr" target="#b21">[22]</ref>, the loss function consists of a waveform reconstruction loss ? log p(x|z) and a Kullback-Leibler divergence loss between the posterior q(z|x) and the prior p(z|y), i.e., KL[q(z|x)||p(z|y)].</p><p>Considering the posterior from speech is more complicated than the prior from text, to match them as close as possible to enable text to waveform generation, we design several modules to simplify the posterior and to enhance the prior, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. First, to learn a good representations of phoneme sequence for better prior prediction, we pre-train a phoneme encoder on a large-scale text corpus using masked language modeling on phoneme sequence (Section 3.2). Second, since the posterior is at the frame level while the phoneme prior is at the phoneme level, we need to expand the phoneme prior according to its duration to bridge the length difference. We leverage a differentiable durator to improve duration modeling (Section 3.3). Third, we design a bidirectional prior/posterior module to enhance the prior or simplify the posterior (Section 3.4). Fourth, we propose a memory based VAE that leverages a memory bank through Q-K-V attention <ref type="bibr" target="#b36">[37]</ref> to reduce the complexity of posterior needed to reconstruct the waveform (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phoneme Encoder</head><p>The phoneme encoder ? pho takes a phoneme sequence y as input and outputs a phoneme hidden sequence. To enhance the representation capability of the phoneme encoder, we conduct large-scale phoneme pre-training. Previous works <ref type="bibr" target="#b37">[38]</ref> conduct pre-training in character/word level and apply the pre-trained model to phoneme encoder, which will cause inconsistency, and the works <ref type="bibr" target="#b38">[39]</ref> directly using phoneme pre-training will suffer from limited capacity due to too small size of phoneme vocabulary. To avoid these issues, we leverage mixed-phoneme pre-training <ref type="bibr" target="#b39">[40]</ref>, which uses both phoneme and sup-phoneme (adjacent phonemes merged together) as the input of the model, as shown in <ref type="figure" target="#fig_2">Figure 2c</ref>. When using masked language modeling <ref type="bibr" target="#b40">[41]</ref>, we randomly mask some sup-phoneme tokens and their corresponding phoneme tokens and predict the masked phoneme and sup-phoneme at the same time. After mixed phoneme pre-training, we use the pre-trained model to initialize the phoneme encoder of our TTS system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differentiable Durator</head><p>The differentiable durator ? dur takes a phoneme hidden sequence as input, and outputs a sequence of prior distribution at the frame level, as shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. We denote the prior distribution as  </p><formula xml:id="formula_0">p(z |y; ? pho , ? dur ) = p(z |y; ? pri ), where ? pri = [? pho , ? dur ].</formula><p>The differentiable durator ? dur consists of several modules: 1) a duration predictor that builds upon the phoneme encoder to predict the duration for each phoneme, 2) a learnable upsampling layer that leverages the predicted duration to learn a projection matrix to extend the phoneme hidden sequence from phoneme level to frame level in a differentiable way <ref type="bibr" target="#b41">[42]</ref>, and 3) two additional linear layers on the expanded hidden sequence to calculate the mean and variance of the prior distribution p(z |y; ? pri ). The detailed formulation of differentiable durator is in Appendix B. We optimize the duration prediction, learnable upsampling layer, and mean/variance linear layers together with the TTS model in a fully differentiable way, which can reduce the training-inference mismatch in previous duration prediction (ground-truth duration is used in training while predicted duration is used in inference) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> and better use duration in a soft and flexible way instead of a hard expansion, hence the side-effect of inaccurate duration prediction is mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bidirectional Prior/Posterior</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2b</ref>, we design a bidirectional prior/posterior module to enhance the capacity of the prior p(z |y; ? pri ) or to reduce the complexity of the posterior q(z|x; ?) where ? is the posterior encoder, since there is information gap between the posterior obtained from speech sequence and the prior obtained from phoneme sequence. We choose a flow model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> as the bidirectional prior/posterior module (denoted as ? bpp ) since it is easy to optimize and has a nice property of invertibility.</p><p>Reduce Posterior q(z|x; ?) with Backward Mapping f ?1 The bidirectional prior/posterior module can reduce the complexity of posterior from q(z|x; ?) to q(z |x; ?, ? bpp ) through the backward</p><formula xml:id="formula_1">mapping f ?1 (z; ? bpp ), i.e., for z ? q(z|x; ?), z = f ?1 (z; ? bpp ) ? q(z |x; ?, ? bpp ).</formula><p>The objective is to match the simplified posterior q(z |x; ?, ? bpp ) to the prior p(z |y; ? pri ) by using the KL divergence loss as follows:</p><formula xml:id="formula_2">L bwd (?, ? bpp , ? pri ) = KL[q(z |x; ?, ? bpp )||p(z |y; ? pri ))] = q(z |x; ?, ? bpp ) ? log q(z |x; ?, ? bpp ) p(z |y; ? pri ) dz = q(z|x; ?)| det ?f ?1 (z; ? bpp ) ?z | ?1 ? log q(z|x; ?)| det ?f ?1 (z;?bpp) ?z | ?1 p(f ?1 (z; ? bpp )|y; ? pri ) ? | det ?f ?1 (z; ? bpp ) ?z |dz = q(z|x; ?) ? log q(z|x; ?) p(f ?1 (z; ? bpp )|y; ? pri )| det ?f ?1 (z;?bpp) ?z | dz = E z?q(z|x;?) (log q(z|x; ?) ? log(p(f ?1 (z; ? bpp )|y; ? pri )| det ?f ?1 (z; ? bpp ) ?z |),<label>(1)</label></formula><p>where the third equality (the second line) in Equation 1 is obtained via the change of variables: dz = | det</p><formula xml:id="formula_3">?f ?1 (z;?bpp) ?z |dz, and q(z |x; ?, ? bpp ) = q(z|x; ?)| det ?f (z ;?bpp) ?z | = q(z|x; ?)| det ?f ?1 (z;?bpp) ?z | ?1 according to inverse function theorem.</formula><p>Enhance Prior p(z |y; ? pri ) with Forward Mapping f The bidirectional prior/posterior module can enhance the capacity of prior from p(z |y; ? pri ) to p(z|y; ? pri , ? bpp ) through the forward mapping f (z ; ? bpp ), i.e., for z ? p(z |y; ? pri ), z = f (z ; ? bpp ) ? p(z|y; ? pri , ? bpp ). The objective is to match the enhanced prior p(z|y; ? pri , ? bpp ) to the posterior q(z|x; ?) using the KL divergence loss as follows: By using backward and forward loss functions, both directions of the flow model are considered in training, which can reduce the training-inference mismatch in the previous flow models that train in backward direction but infer in forward direction. We also provide another formulation of the bidirectional prior/posterior in Appendix C.</p><formula xml:id="formula_4">L fwd (?, ? bpp , ? pri ) = KL[p(z|y; ? pri , ? bpp )||q(z|x; ?)] = p(z|y; ? pri , ? bpp ) ? log p(z|y; ? pri , ? bpp ) q(z|x; ?) dz = p(z |y; ? pri )| det ?f (z ; ? bpp ) ?z | ?1 ? log p(z |y; ? pri )| det ?f (z ;?bpp) ?z | ?1 q(f (z ; ? bpp )|x; ?) ? | det ?f (z ; ? bpp ) ?z |dz = E z ?p(z |y;?pri) (log p(z |y; ? pri ) ? log q(f (z ; ? bpp )|x; ?)| det ?f (z ; ? bpp ) ?z |),<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">VAE with Memory</head><p>The posterior q(z|x; ?) in the original VAE model is used to reconstruct the speech waveform, and thus is more complicated than the prior from the phoneme sequence. To further relieve the burden of prior prediction, we simplify the posterior by designing a memory based VAE model. The high-level idea of this design is that instead of directly using z ? q(z|x; ?) for waveform reconstruction, we just use z as a query to attend to a memory bank, and use the attention result for waveform reconstruction, as shown in <ref type="figure" target="#fig_2">Figure 2d</ref>. In this way, the posterior z is only used to determine the attention weights in the memory bank, and thus is largely simplified. The waveform reconstruction loss based on memory VAE can be formulated as</p><formula xml:id="formula_5">L rec (?, ? dec ) = ?E z?q(z|x;?) [log p(x|Attention(z, M, M ); ? dec )], Attention(Q, K, V ) = [softmax( QW Q (KW K ) T ? h )V W V ]W O ,<label>(3)</label></formula><p>where ? dec denotes the waveform decoder, which covers not only the original waveform decoder but also the model parameters related to the memory mechanism, including the memory bank M and the attention parameters W Q , W K , W V , and W O , where M ? R L?h and W * ? R h?h , L is the size of the memory bank and h is the hidden dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and Inference Pipeline</head><p>Besides the waveform reconstruction loss and bidirectional prior/posterior loss, we additionally conduct a fully end-to-end optimization to take the whole inference procedure in training for better voice quality. The loss function is formulated as follows.</p><formula xml:id="formula_6">L e2e (? pri , ? bpp , ? dec ) = ?E z ?p(z |y;?pri) [log p(x|Attention(f (z ; ? bpp ), M, M ); ? dec )].<label>(4)</label></formula><p>Based on Equation 1, 2, 3, and 4, the total loss function is</p><formula xml:id="formula_7">L = L bwd (?, ? pri , ? bpp ) + L fwd (?, ? pri , ? bpp ) + L rec (?, ? dec ) + L e2e (? pri , ? bpp , ? dec ),<label>(5)</label></formula><p>where ? pri = [? pho , ? dur ]. Note that there are some special explanations of the above loss functions: 1) Since the frame-level prior distribution p(z |y; ? pri ) cannot well align with the ground-truth speech frames due to the intrinsically inaccurate duration prediction in durator, we leverage a soft dynamic time warping (DTW) version of KL loss for L bwd and L fwd . See Appendix D for the detailed formulation of the soft-DTW loss. 2) We write the waveform loss in L rec and L e2e as negative log-likelihood loss for simplicity. Actually following <ref type="bibr" target="#b16">[17]</ref>, L rec consists of GAN loss, feature mapping loss and mel-spectrogram loss, while L e2e consists of only GAN loss. We do not use soft-DTW in L e2e since we found GAN loss can still perform well with mismatched lengths. See Appendix E for the details of the waveform loss.</p><p>There are several different gradient flows in training the model, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>:</p><formula xml:id="formula_8">1) L rec ? ? dec ? ?; 2) L bwd ? ? dur ? ? pho ; 3) L bwd ? ? bpp ? ?; 4) L fwd ? ? bpp ? ? dur ? ? pho ; 5) L fwd ? ?; 6) L e2e ? ? dec ? ? bpp ? ? dur ? ? pho .</formula><p>After training, we discard the posterior encoder ? and only use ? pho , ? bpp , ? dur and ? dec for inference. The training and inference pipeline is summarized in Algorithm 1. Pre-train the phoneme encoder ? pho .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Train the whole model [?, ? pho , ? dur , ? bpp , ? dec ] using loss L defined in Equation 5. 4: Inference:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Sample prior z ? p(z |y; ? pho , ? dur ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Get enhanced prior z = f (z ; ? bpp ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Generate waveform sample x ? p(x|Attention(z, M, M ); ? dec ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Advantages of NaturalSpeech</head><p>We explain how the designs in our NaturalSpeech system can close the quality gap to recordings.</p><p>? Reduce training-inference mismatch. We directly generate waveform from text and leverage a differentiable durator to ensure a fully end-to-end optimization, which can reduce the traininginference mismatch in the cascaded acoustic model/vocoder <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref> and explicit duration prediction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. Note that although VAE and flow can have training-inference mismatch inherently (waveform is reconstructed from the posterior in training while predicted from the prior in inference for VAE, and flow is trained in backward direction and infered in forward direction), we design the backward/forward loss in Equation 1 and 2 and the end-to-end loss in <ref type="bibr">Equation 4</ref> to alleviate this problem. ? Alleviate one-to-many mapping problem. Compared to previous methods using reference encoder <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b10">11]</ref> or pitch/energy extraction <ref type="bibr" target="#b17">[18]</ref> for variation information modeling, our posterior encoder ? in VAE acts like a reference encoder that can extract all the necessary variance information in posterior distribution q(z|x; ?). We do not predict pitch explicitly since it can be learned implicitly in the posterior encoder and the memory bank of VAE. To ensure the prior and posterior can match with each other, on the one hand, we simplify the posterior with memory VAE and backward mapping in the bidirectional prior/posterior module, and on the other hand, we enhance the prior with phoneme pre-training, differentiable durator, and forward mapping in the bidirectional prior/posterior module. Thus, we can alleviate the one-to-mapping problem to a large extent. ? Increase representation capacity. We leverage large-scale phoneme pre-training to extract better representation from the phoneme sequence, and leverage the advanced generative models (flow, VAE, GAN) to capture the speech data distributions better, which can enhance the representation capacity of the TTS models for better voice quality.</p><p>We further list the difference between our NaturalSpeech and previous TTS systems as follows: 1) Compared to previous autoregressive TTS models such as Tacotron 1/2 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, WaveNet <ref type="bibr" target="#b1">[2]</ref>, TransformerTTS <ref type="bibr" target="#b8">[9]</ref>, and Wave-Tacotron <ref type="bibr" target="#b46">[47]</ref>, our NaturalSpeech is non-autoregressive in nature with a fast inference speed. 2) Compared to the previous systems with cascaded acoustic model and vocoder, such as Tacotron 1/2 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, FastSpeech 1/2 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, ParallelTacotron 2 <ref type="bibr" target="#b41">[42]</ref>, Glow-TTS <ref type="bibr" target="#b12">[13]</ref>, and Grad-TTS <ref type="bibr" target="#b13">[14]</ref>, we are fully end-to-end with no cascaded errors. 3) Compared to previous systems with various reference encoders and pitch/duration prediction, such as FastSpeech 2 <ref type="bibr" target="#b17">[18]</ref>, AdaSpeech <ref type="bibr" target="#b44">[45]</ref>, and DelightfulTTS <ref type="bibr" target="#b10">[11]</ref>, we unify all the variance information with a posterior encoder and model the duration in a fully differentiable way. 4) Compared to previous fully end-to-end TTS systems such as EATS <ref type="bibr" target="#b18">[19]</ref>, FastSpeech 2s <ref type="bibr" target="#b17">[18]</ref>, and VITS <ref type="bibr" target="#b14">[15]</ref>, we bridge the quality gap to recordings with advanced model designs to closely match the prior and posterior in the VAE framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We evaluate our proposed NaturalSpeech on the LJSpeech dataset <ref type="bibr" target="#b25">[26]</ref>, which is widely used for benchmarking TTS. LJSpeech is a single speaker English corpus and consists of 13, 100 audios and text transcripts, with a total length of nearly 24 hours at a sampling rate of 22.05kHz. We randomly split the dataset into training set with 12, 500 samples, validation set with 100 samples, and test set with 500 samples. For phoneme pre-training on phoneme encoder, we collect a large-scale text corpus with 200 million sentences from the news-crawl dataset <ref type="bibr" target="#b47">[48]</ref>. Note that we do not use any extra paired text and speech data except for LJSpeech dataset. We conduct several preprocessings on the speech and text sequences: 1) We convert the text/character sequence into phoneme sequence <ref type="bibr" target="#b48">[49]</ref> using a grapheme-to-phoneme tool <ref type="bibr" target="#b49">[50]</ref>. 2) We use linear-spectrograms as the input of the posterior encoder <ref type="bibr" target="#b14">[15]</ref>, instead of original waveform sequence for simplicity. The linear-spectrograms are obtained by short-time Fourier transform (STFT) with FFT size, window size, and hop size of 1024, 1024, and 256, respectively. 3) For the mel-spectrogram loss on the waveform decoder, we obtain the mel-spectrograms by applying 80-dimension mel-filterbanks on the linear-spectrograms of the speech waveform.</p><p>Model Configurations Our phoneme encoder is a stack of 6 Feed-Forward Transformer (FFT) blocks <ref type="bibr" target="#b9">[10]</ref>, where each block consists of a multi-head attention layer and a 1D convolution feedforward layer, with hidden size of 192. In the differentiable durator, the duration predictor consists of 3-layer convolution. We use 4 consecutive affine coupling layers <ref type="bibr" target="#b50">[51]</ref> in our bidirectional prior/posterior module following <ref type="bibr" target="#b14">[15]</ref>. We discard the scaling operation in the affine transform to stabilize the bidirectional training. The shifting in the affine transform is estimated by a 4-layer WaveNet <ref type="bibr" target="#b1">[2]</ref> with a dilation rate of 1. The posterior encoder is based on a 16-layer WaveNet with a kernel size of 5 and a dilation rate of 1. The waveform decoder consists of 4 residual convolution blocks following <ref type="bibr" target="#b16">[17]</ref>, where each block has 3 layers of 1D convolution. We perform transpose convolution for upsampling at every convolution block at a rate of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2]</ref>. The hyperparameters of NaturalSpeech are listed in Appendix G.</p><p>Training Details We train our proposed system on 8 NVIDIA V100 GPUs with 32G memory, with a dynamic batch size of 8, 000 speech frames (under hop size of 256) per GPU, and a total 15k training epochs. We use AdamW optimizer [52] with ? 1 = 0.8, ? 2 = 0.99. The initial learning rate is 2 ? 10 ?4 , with a learning rate decay factor ? = 0.999875 in each epoch, i.e., the learning rate is multiplied by ? in every epoch. We find it is helpful to stabilize the training of our system and achieve better results through a warmup stage with 1k epochs at the beginning of the training, and a tuning stage with 2k epochs at the end of the training. More details about these training stages can be found in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Human Recordings</head><p>We first compare the speech generated by NaturalSpeech with human recordings in terms of both MOS and CMOS evaluation. As described in Section 2, we use 50 test utterances, each with 20 judges for evaluation. As shown in <ref type="table" target="#tab_2">Table 2</ref> and 3, our system achieves similar quality scores with human recordings in both MOS and CMOS. Importantly, our system achieves ?0.01 CMOS compared to recordings, with a Wilcoxon p-value <ref type="bibr" target="#b29">[30]</ref> p 0.05, which demonstrates the speech generated by our system has no statistically significant difference from human recordings <ref type="bibr">3 4</ref> . Thus, our NaturalSpeech achieves human-level quality according to the definition and judgement in Section 2.   <ref type="bibr" target="#b16">[17]</ref>, and 4) VITS <ref type="bibr" target="#b14">[15]</ref>. We re-produce the results of all these systems by our own, which can match or even beat the quality in their original papers (note that the HiFiGAN vocoder is fine-tuned on the predicted mel-spectrograms for better synthesis quality). Both the MOS and CMOS results are shown in <ref type="table" target="#tab_4">Table 4</ref>. It can be seen that our NaturalSpeech achieves better voice quality than these systems in terms of both MOS and CMOS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies and Method Analyses</head><p>Ablation Studies We further conduct ablation studies to verify the effectiveness of each module in our system, as shown in <ref type="table" target="#tab_5">Table 5</ref>. We describe the ablation studies as follows: 1) By removing phoneme pre-training, we do not initialize the phoneme encoder from pre-trained weights but just random initialization, which brings ?0.09 CMOS drop, demonstrating the effectiveness of phoneme pre-training. 2) By removing differentiable durator, we do not use learnable upsampling layer and end-to-end duration optimization, but just use duration predictor for hard expansion. In this way, we use monotonic alignment search <ref type="bibr" target="#b12">[13]</ref> to provide the duration label to train the duration predictor through the whole training process. Removing differentiable durator causes ?0.12 CMOS drop, demonstrates the importance of end-to-end optimization in duration modeling. 3) By removing bidirectional prior/posterior module, we only use L bwd in training and do not use L fwd . It brings ?0.09 CMOS drop, showing the gain by leveraging bidirectional training to bridge the gap between posterior and prior. 4) By removing memory mechanism in VAE, we use original VAE for waveform reconstruction, which causes ?0.06 CMOS drop, showing the effectiveness of memory in VAE to simplify the posterior. Inference Latency We compare the inference speed of our NaturalSpeech with previous TTS systems. We measure the latency by using an NVIDIA V100 GPU with a batch size of 1 sentence and averaging the latency over the sentences in the test set. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. The model components ? pho , ? dur , ? bpp , and ? dec in NaturalSpeech are used in inference, with 28.7M model parameters. Our NaturalSpeech achieves faster or comparable inference speed when compared with the previous systems, and achieves better voice quality.  <ref type="bibr" target="#b16">[17]</ref> 0.021 Grad-TTS <ref type="bibr" target="#b13">[14]</ref> (1000) + HiFiGAN <ref type="bibr" target="#b16">[17]</ref> 4.120 Grad-TTS <ref type="bibr" target="#b13">[14]</ref> (10) + HiFiGAN <ref type="bibr" target="#b16">[17]</ref> 0.082 VITS <ref type="bibr" target="#b14">[15]</ref> 0.014 NaturalSpeech 0.013</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussions</head><p>In this paper, we conduct a systematic study on the problems related to human-level quality in TTS. We first give a formal definition of human-level quality and describe the guidelines to judge it, and further build a TTS system called NaturalSpeech to achieve human-level quality. Specifically, after analyzing the quality gap on several competitive TTS systems, we develop a fully end-to-end text to waveform generation system, with several designs to close the gap to human recordings, including phoneme pre-training, differentiable durator, bidirectional prior/posterior module, and memory mechanism in VAE. Evaluations on the popular LJSpeech dataset demonstrate that our NaturalSpeech achieves human-level quality with CMOS evaluations, with no statistically significant difference from human recordings for the first time on this dataset.</p><p>Note that by claiming our NaturalSpeech system achieves human-level quality on LJSpeech dataset, we do not mean that we can surpass or replace human, but the quality of NaturalSpeech is statistically indistinguishable from human recordings on this dataset. Meanwhile, although our evaluations are conducted on LJSpeech dataset, we believe the technologies in NaturalSpeech can be applied to other languages, speakers, and styles to improve the general synthesis quality. We will further try to achieve human-level quality in more challenging datasets or scenarios, such as expressive voices, longform audiobook voices, and singing voices that have more dynamic, diverse, and contextual prosody in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Study of the Quality Gap of Previous TTS System</head><p>To understand where and how the quality gap to recordings comes from, we conduct a systematic study on the current TTS systems, which can help us to find the problems, and is equally important (if not more) than solving the problems. Specifically, we choose a state-of-the-art TTS system using FastSpeech 2 <ref type="bibr" target="#b17">[18]</ref> as the acoustic model and HiFiGAN <ref type="bibr" target="#b16">[17]</ref> as the vocoder, which consists of four components: phoneme encoder, variance adaptor, mel-spectrogram decoder, and vocoder. We design a series of comparison experiments to measure the quality gap (in terms of CMOS) of each component to its corresponding upper bound. We conduct analyses from this order (from the closest to waveform to the farest): vocoder, mel-spectrogram decoder, variance adaptor, and phoneme encoder. ? Vocoder. We study the quality drop on the vocoder by comparing the two settings: 1) waveform generated by vocoder with ground-truth mel-spectrograms as input; 2) ground-truth waveform (human recordings). The CMOS is shown in <ref type="table" target="#tab_8">Table 7</ref>. It can be seen that when taking ground-truth mel-spectrograms as input, the waveform generated by vocoder has some but not huge gap to human recordings. However, we need to pay attention to the training-inference mismatch in vocoder: in training, vocoder takes ground-truth mel-spectrograms as input, while in inference, it takes predicted mel-spectrograms as input.</p><p>? Mel-spectrogram Decoder. We study the quality drop on the mel-spectrogram decoder by comparing the two settings: 1) mel-spectrograms generated by mel-spectrogram decoder with ground-truth pitch and duration as input 5 ; 2) ground-truth mel-spectrograms (extracted from human recordings). We use the vocoder to convert the mel-sepctrograms in the two settings into waveform for evaluation. As shown in <ref type="table" target="#tab_8">Table 7</ref>, the predicted mel-spectrograms have 0.15 CMOS drop compared to the ground-truth mel-spectrograms.</p><p>? Variance Adaptor. We study the quality drop on the variance adaptor by comparing the predicted pitch/duration with the ground-truth pitch/duration. We need the mel-spectrogram decoder and vocoder to generate the waveform for evaluation in the two settings. As shown in <ref type="table" target="#tab_8">Table 7</ref>, the predicted pitch/duration have 0.14 CMOS drop compared to the ground-truth pitch/duration.</p><p>? Phoneme Encoder. Since it is not straightforward to construct the upper bound of the phoneme encoder, we analyze the approximate quality drop through backward verification, by improving phoneme encoder for better voice quality. We conduct large-scale phoneme pre-training on the phoneme encoder, and fine-tune it with the FastSpeech 2 training pipeline, and achieves a 0.12 CMOS gain, as shown in <ref type="table" target="#tab_8">Table 7</ref>, which demonstrates the phoneme encoder has improvement space.</p><p>According to the above experimental studies, we analyze several reasons causing the quality drop in each component: 1) Training-inference mismatch. Ground-truth mel-spectrogram, pitch, and duration are used in training, while predicted values are used in inference, which causes mismatch in the input of vocoder and mel-spectrogram decoder. Fully end-to-end text to waveform optimization is helpful to eliminate this mismatch. 2) One-to-many mapping problem. Text to speech mapping is one-tomany, where a text sequence can correspond to multiple speech utterances with different variation information (e.g., pitch, duration, speed, pause, prosody, etc). Current systems usually use a variance adaptor to predict variance information (e.g., pitch, duration) to alleviate this problem, which is not enough to well handle this problem. We should rethink previous methods on variance information and come up with some thorough and elegant solutions. 3) Lack of representation capacity. Current models are not powerful enough to extract good representations from phoneme sequence and learn complicated data distribution in speech. More advanced methods such as large-scale pre-training and powerful generative models are critical to enhance the learning capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Differentiable Durator</head><p>To enable end-to-end duration optimization, we design a durator that can upsample a phoneme hidden sequence H n?h into a frame-level hidden sequence O m?h in a differentiable way, where h, n, m is the hidden dimension size, phoneme sequence length and frame sequence length, respectively. The differentiable durator consists of a duration predictor ? dp for phoneme duration prediction and a learnable upsampling layer ? lu for sequence expansion from phoneme level to frame level.</p><p>Duration Predictor The input to the duration predictor ? dp is phoneme hidden sequence H n?h and the output is the estimated phoneme durationd n?1 . The duration predictor ? dp consists of 3 layers of one-dimensional convolution, with ReLU activation, layer normalization, and dropout between each layer.</p><p>Learnable Upsampling Layer The learnable upsampling layer ? lu takes phoneme duration d as input and upsamples phoneme hidden sequence H to frame-level sequence O <ref type="bibr" target="#b41">[42]</ref>. First, we calculate the duration start and end matrices S m?n and E m?n by</p><formula xml:id="formula_9">S i,j = i ? j?1 k=1 d k , E i,j = j k=1 d k ? i,<label>(6)</label></formula><p>where S i,j indexes the (i, j)-th element in the matrix. We calculate the primary attention matrix W m?n?q and auxiliary context matrix C m?n?p following <ref type="bibr" target="#b41">[42]</ref>: <ref type="figure">(Proj(H)</ref>))])),</p><formula xml:id="formula_10">W = Softmax(MLP 10?q ([S, E, Expand(Conv1D</formula><formula xml:id="formula_11">C = MLP 10?p ([S, E, Expand(Conv1D(Proj(H)))]),<label>(7)</label></formula><p>where Proj(?) represents one linear layer with input and output dimensions of h. Conv1D(?) is one-dimensional convolution operation with layer normalization and Swish activation <ref type="bibr">[53]</ref>. The input and output dimensions of Conv1D(?) are h and 8. Expand(?) means adding an extra dimension by repeating the input matrix by m times.</p><p>[?] stands for matrix concatenation along the hidden dimension, and gets a hidden dimension of 10 = 1 + 1 + 8. MLP(?) is a two-layer full-connected network with Swish activations. The numbers underneath MLP denote the input and output hidden dimensions. We set p = 2 and q = 4. The Softmax(?) operation is performed on the sequence time dimension. We calculate the frame-level hidden sequence output O m?d with the following equation:</p><formula xml:id="formula_13">O = Proj qh?h (W H) + Proj qp?h (Einsum(W , C)),<label>(9)</label></formula><p>where Einsum(?) represents the einsum operation ('qmn, mnp ? qmp', W , C). We first permute W from m ? n ? q to q ? m ? n for computation, and after we get W H with shape q ? m ? h and Einsum(W , C) with shape q ? m ? p, we reshape them to m ? qh and m ? qp respectively for final projection to dimension m ? h. Finally, we map O with a mean and variance linear layer to get the frame-level prior distribution parameter ?(y; ? pri ) and ?(y; ? pri ), and get the prior distribution p(z |y; ? pri ) = N (z ; ?(y; ? pri ), ?(y; ? pri )).</p><p>Compared to simply repeating each phoneme hidden sequence with the predicted duration in a hard way, the learnable upsampling layer enables more flexible duration adjustment for each phoneme. Also, the learnable upsampling layer makes the phoneme to frame expansion differentiable, and thus can be jointly optimized with other modules in the TTS system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Alternative Formulation of Bidirectional Prior/Posterior</head><p>We provide another formulation of the backward loss L bwd in Equation 1 and forward loss L fwd in Equation 2 by directly using KL loss to match two distributions.</p><p>For the backward loss, we directly match the posterior q(z|x; ?) to the prior p(z|y; ? pri ): For the forward loss, we directly match the prior p(z |y; ? pri ) to the posterior q(z |x; ?): where r i,j is the KL divergence loss between the enhanced prior p(z|y; ? pri , ? bpp ) from frame 1 to frame i and the posterior q(z|x; ?) from frame 1 to frame j with the best alignment. KL[p(z * |y; ? pri , ? bpp )||q(z * |x; ?)] is defined in Equation 2. p(z i |y; ? pri , ? bpp ) is the i-th frame of the enhanced prior, and q(z j |x; ?) is the j-th frame of the posterior.</p><formula xml:id="formula_14">L bwd (?, ? bpp , ? pri ) = KL[q(z|x; ?)||p(z|y; ? pri ))] = E z?q(z|x;?) (log q(z|x; ?) ? log p(z|y; ? pri )) = E z?q(z|x;?) (log q(z|x; ?) ? log p(f ?1 (z; ? bpp )|y; ? pri ))| det ?f ?1 (z; ? bpp ) ?z |),<label>(10)</label></formula><formula xml:id="formula_15">L fwd (?, ? bpp , ? pri ) = KL[p(z |y; ? pri )||q(z |x; ?)] = E z ?p(z |y;?pri) (log p(z |y; ? pri ) ? log q(z |x; ?)) = E z ?p(z |y;?pri) (log p(z |y; ? pri ) ? log q(f (z ; ? bpp )|x; ?)| det ?f (z ; ? bpp ) ?z |),<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Waveform Decoder Loss</head><p>Instead of using negative log-likelihood loss in waveform reconstruction and prediction in Equation 3 and 4, we use GAN loss, feature mapping loss, and mel-spectrogram loss as used in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Loss</head><p>The GAN loss follows LS-GAN [54], which is defined as follows. The generator is trained to minimize the loss function while the discriminator is train to maximize it:</p><formula xml:id="formula_16">E x [(D(x) ? 1) 2 ] + E z [D(G(z)) 2 ]<label>(14)</label></formula><p>where is x the ground-truth waveform and z is the input of waveform decoder. We follow <ref type="bibr" target="#b14">[15]</ref> for the design of discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Mapping Loss</head><p>The feature mapping loss consists of the L1 distance between real samples and fake samples in terms of the intermediate feature in each layer of the discriminator, which can be formulated as:</p><formula xml:id="formula_17">E (x,z) [ l 1 N l ||D l (x) ? D l (G(z))|| 1 ]<label>(15)</label></formula><p>where l is the layer index in discriminator, D l (?) and N l are the features and the number of features in the l-th layer of the discriminator, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mel-Spectrogram Loss</head><p>The mel-spectrogram loss is L1 distance between the mel-spectrogram of ground-truth waveform and that of generated waveform, which can be defined as:</p><formula xml:id="formula_18">E (x,z) = ||S(x) ? S(G(z))|| 1<label>(16)</label></formula><p>where S(?) is the function that converts the waveform into corresponding mel-spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Training Details of NaturalSpeech</head><p>Phoneme Pre-training We pre-train our phoneme encoder on 200M phoneme sequences, which is converted from text with grapheme-to-phoneme conversion. The size of the phoneme dictionary is 182. We learn the sup-phoneme using Byte-Pair Encoding (BPE) [55] with a sup-phoneme dictionary size of 30, 088. We conduct the pre-training on 8 NVIDIA A100 GPUs with 80G memory (we only use A100 for phoneme pre-training, and use V100 for the remaining training of NaturalSpeech), with a total batch size of 1, 024 sentences for 120k training steps. The mask ratio for sup-phoneme is 15%.</p><p>Duration Predictor In the warmup stage (the first 1k epochs), we obtain the duration label to train the duration predictor to speed up the convergence of differentiable durator. We can choose any tools to provide duration label, such as Montreal forced alignment [56]. Here we choose monotonic alignment search (MAS) <ref type="bibr" target="#b12">[13]</ref>, which estimates the optimal alignment between the phoneme prior distribution p(z |y; ? pho ) = N (z ; ?(y; ? pho ), ?(y; ? pho )) and simplified frame-level posterior q(z |x; ?, ? bpp ), where ?(y; ? pho ), ?(y; ? pri ) are the mean and variance parameters obtained from the phoneme hidden sequence by two linear layers. The monotonic and non-skipping constraints of MAS provide the inductive bias that human read words in orders without skipping. The optimal alignment search result A can be formulated as A = arg max A ? m i=1 N (z i ; ?(y; ? pho ) A(i) , ?(y; ? pho ) A(i) ),</p><p>where A(i) denotes the aligned phoneme index of the i-th frame z i from q(z |x; ?, ? bpp ). We search the alignment result using dynamic programming. Let Q i,j denote the probability of z i belongs to the prior distribution of the j-th phoneme, then we can formulate Q i,j recursively with Q i?1,j?1 and Q i,j?1 with the following equation:</p><p>Q i,j = max(Q i?1,j?1 , Q i?1,j ) + log N (z i ; ?(y; ? pho ) j , ?(y; ? pho ) j ).</p><p>(18) We calculate all the Q i,j from i = 0, j = 0 to i = m, j = n. Since the best alignment path is determined by the highest Q value, we utilize all the cached Q value to backtrack from Q m,n to Q 0,0 for the most probable alignment A.</p><p>Note that in the warmup training stage, the duration d comes from MAS. After the warmup stage, the input duration comes from the duration predictord. During the whole training process, we apply gradient stop operation on the input of duration predictor.</p><p>Bidirectional Prior/Posterior For the two loss terms L bwd and L fwd in bidirectional prior/posterior module, we only use L bwd during the warmup stage to learn a reasonable prior distribution, and then add L fwd to the loss function for bidirectional optimization after the warmup stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE with Memory</head><p>In the warmup stage, we do not use the memory bank in VAE training, i.e., z ? q(z|x; ?) is directly taken as the input of the waveform decoder. After the warmup stage, we initialize the memory banks M as follows: we first get the posterior z ? q(z|x; ?) of each frame of the utterances in the training set, and then conduct K-means clustering on these z to get 1K clusters, and use the cluster center to initialize the memory bank M . After the initialization, we jointly train the memory mechanism with the whole TTS system.</p><p>In the tuning stage (the last 2k epochs), we only use L e2e to tune the model. We freeze the parameters of posterior encoder, waveform decoder, phoneme encoder, and bidirectional prior/posterior, and only update the durator for fully end-to-end duration optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hyper-Parameters of NaturalSpeech</head><p>The hyper-parameters of NaturalSpeech are listed in <ref type="table" target="#tab_9">Table 8</ref>. The number of model parameters for ? pho , ? dur , ? bpp , and ? dec is 28.7M, for the posterior encoder ? is 7.2M, and for the discriminators is 46.7M. Note that only ? pho , ? dur , ? bpp , and ? dec with 28.7M model parameters are used in inference. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System overview of NaturalSpeech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Memory mechanism in VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The designed modules in NaturalSpeech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where the third equality (the second line) in Equation 2 is obtained via the change of variables: dz = | det ?f (z ;?bpp) ?z |dz , and p(z|y; ? pri , ? bpp ) = p(z |y; ? pri )| det ?f ?1 (z;?bpp) ?z | = p(z |y; ? pri )| det ?f (z ;?bpp) ?z | ?1 according to inverse function theorem, similar to that in Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Gradient flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Training and inference of NaturalSpeech 1: Training: 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where f ?1 (z; ? bpp ) = z , and p(z|y; ? pri )) = p(f ?1 (z; ? bpp )|y; ? pri ))| det ?f ?1 (z;?bpp) ?z |) according to the change of variable rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>where f (z ; ? bpp ) = z, and q(z |x; ?)) = q(f (z ; ? bpp )|x; ?)| det ?f (z ;?bpp) ?z |) according to the change of variable rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :is conducted using Wilcoxon rank sum test [30], instead of the Wilcoxon signed rank test in CMOS, due to no paired comparison in MOS evaluation. For Grad-TTS, we use 1000 steps for inference.</head><label>1</label><figDesc>The MOS and CMOS comparisons between previous TTS systems and human recordings. Note that the Wilcoxon p-value in MOS</figDesc><table><row><cell>System</cell><cell>MOS</cell><cell cols="3">Wilcoxon p-value CMOS Wilcoxon p-value</cell></row><row><cell>Human Recordings</cell><cell>4.52 ? 0.11</cell><cell>-</cell><cell>0</cell><cell>-</cell></row><row><cell cols="2">FastSpeech 2 [18] + HiFiGAN [17] 4.32 ? 0.10</cell><cell>1.0e-05</cell><cell>?0.30</cell><cell>5.1e-20</cell></row><row><cell>Glow-TTS [13] + HiFiGAN [17]</cell><cell>4.33 ? 0.10</cell><cell>1.3e-06</cell><cell>?0.23</cell><cell>8.7e-17</cell></row><row><cell>Grad-TTS [14] + HiFiGAN [17]</cell><cell>4.37 ? 0.10</cell><cell>0.0127</cell><cell>?0.23</cell><cell>1.2e-11</cell></row><row><cell>VITS [15]</cell><cell>4.49 ? 0.10</cell><cell>0.2429</cell><cell>?0.19</cell><cell>2.9e-04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Phoneme Wave Decoder Posterior Encoder Bidirectional Prior/Posterior Differentiable Durator</head><label></label><figDesc></figDesc><table><row><cell>Only in training</cell><cell>Waveform</cell></row><row><cell>Training &amp; inference</cell><cell></cell></row><row><cell></cell><cell>(with memory)</cell></row><row><cell>( ?| )</cell><cell>( | )</cell></row><row><cell>Phoneme Encoder</cell><cell></cell></row><row><cell>(with phoneme pre-training)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MOS comparison between NaturalSpeech and human recordings. Wilcoxon rank sum test is used to measure the p-value in MOS evaluation.</figDesc><table><row><cell cols="3">Human Recordings NaturalSpeech Wilcoxon p-value</cell></row><row><cell>4.58 ? 0.13</cell><cell>4.56 ? 0.13</cell><cell>0.7145</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CMOS comparison between NaturalSpeech and human recordings. Wilcoxon signed rank test is used to measure the p-value in CMOS evaluation.</figDesc><table><row><cell cols="3">Human Recordings NaturalSpeech Wilcoxon p-value</cell></row><row><cell>0</cell><cell>?0.01</cell><cell>0.6902</cell></row></table><note>4.3 Comparison with Previous TTS Systems We compare our NaturalSpeech with previous TTS systems, including: 1) FastSpeech 2 [18] + HiFiGAN [17], 2) Glow-TTS [13] + HiFiGAN [17], 3) Grad-TTS [14] + HiFiGAN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>MOS and CMOS comparisons between NaturalSpeech and previous TTS systems.</figDesc><table><row><cell>System</cell><cell>MOS</cell><cell>CMOS</cell></row><row><cell cols="3">FastSpeech 2 [18] + HiFiGAN [17] 4.32 ? 0.15 ?0.33</cell></row><row><cell>Glow-TTS [13] + HiFiGAN [17]</cell><cell cols="2">4.34 ? 0.13 ?0.26</cell></row><row><cell>Grad-TTS [14] + HiFiGAN [17]</cell><cell cols="2">4.37 ? 0.13 ?0.24</cell></row><row><cell>VITS [15]</cell><cell cols="2">4.43 ? 0.13 ?0.20</cell></row><row><cell>NaturalSpeech</cell><cell>4.56 ? 0.13</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on each design in NaturalSpeech .</figDesc><table><row><cell>Setting</cell><cell>CMOS</cell></row><row><cell>NaturalSpeech</cell><cell>0</cell></row><row><cell>? Phoneme Pre-training</cell><cell>?0.09</cell></row><row><cell>? Differentiable Durator</cell><cell>?0.12</cell></row><row><cell cols="2">? Bidirectional Prior/Posterior ?0.09</cell></row><row><cell>? Memory in VAE</cell><cell>?0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>steps in</cell></row></table><note>Inference speed comparison. RTF (real-time factor) means the time (in seconds) to synthesize a 1-second waveform. Grad-TTS (1000) and Grad-TTS (10) mean using 1000 and 10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>[52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.[53] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv:1710.05941, 2017.</figDesc><table><row><cell>[54] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.</cell></row><row><cell>Least squares generative adversarial networks. In Proceedings of the IEEE international</cell></row><row><cell>conference on computer vision, pages 2794-2802, 2017.</cell></row><row><cell>[55] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words</cell></row><row><cell>with subword units. arXiv preprint arXiv:1508.07909, 2015.</cell></row><row><cell>[56] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.</cell></row><row><cell>Montreal forced aligner: Trainable text-speech alignment using kaldi. In Interspeech, volume</cell></row><row><cell>2017, pages 498-502, 2017.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The CMOS of each component to its upper bound. Negative CMOS means this component setting is worse than its upper bound.</figDesc><table><row><cell>Component</cell><cell>Setting</cell><cell>Upper Bound</cell><cell>CMOS</cell></row><row><cell>Vocoder</cell><cell>GT Mel?Vocoder</cell><cell>Human Recordings</cell><cell>?0.04</cell></row><row><cell>Mel Decoder</cell><cell cols="2">GT Pitch/Duration?Mel Decoder GT Mel</cell><cell>?0.15</cell></row><row><cell>Variance Adaptor</cell><cell>Predicted Pitch/Duration</cell><cell>GT Pitch/Duration</cell><cell>?0.14</cell></row><row><cell cols="2">Phoneme Encoder Phoneme Encoder</cell><cell cols="2">Phoneme Encoder + Pre-training ?0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters of NaturalSpeech.</figDesc><table><row><cell>Module</cell><cell>Hyper-Parameter</cell><cell>Value</cell></row><row><cell></cell><cell>Phoneme Encoder Embedding Dimension</cell><cell>192</cell></row><row><cell></cell><cell>Phoneme Encoder Blocks</cell><cell>6</cell></row><row><cell></cell><cell>Phoneme Encoder Multi-Head Attention Hidden Dimension</cell><cell>192</cell></row><row><cell>Phoneme Encoder ? pho</cell><cell>Phoneme Encoder Multi-Head Attention Heads</cell><cell>2</cell></row><row><cell></cell><cell>Phoneme Encoder Conv Kernel Size</cell><cell>3</cell></row><row><cell></cell><cell>Phoneme Encoder Conv Filter Size</cell><cell>768</cell></row><row><cell></cell><cell>Phoneme Encoder Dropout</cell><cell>0.1</cell></row><row><cell></cell><cell>Duration Predictor Kernel Size</cell><cell>3</cell></row><row><cell></cell><cell>Duration Predictor Filter Size</cell><cell>192</cell></row><row><cell>Durator ? dur</cell><cell>Duration Predictor Dropout</cell><cell>0.5</cell></row><row><cell></cell><cell>Upsampling Layer Kernel Size</cell><cell>3</cell></row><row><cell></cell><cell>Upsampling Layer Filter Size</cell><cell>8</cell></row><row><cell></cell><cell>Flow Model Affine Coupling Layers</cell><cell>4</cell></row><row><cell></cell><cell>Flow Model Affine Coupling Dilation</cell><cell>1</cell></row><row><cell>Prior/Posterior ? bpp</cell><cell>Flow Model Affine Coupling Kernel Size</cell><cell>5</cell></row><row><cell></cell><cell>Flow Model Affine Coupling Filter Size</cell><cell>192</cell></row><row><cell></cell><cell>Flow Model Affine Coupling WaveNet Layers</cell><cell>4</cell></row><row><cell></cell><cell>Waveform Decoder ConvBlocks</cell><cell>4</cell></row><row><cell></cell><cell>Waveform Decoder ConvBlock Hidden</cell><cell>[256, 128, 64, 32]</cell></row><row><cell>Waveform Decoder ? dec</cell><cell>Waveform Decoder ConvBlock Upsampling Ratio Waveform Decoder ConvLayers</cell><cell>[8, 8, 2, 2] 3</cell></row><row><cell></cell><cell>Waveform Decoder ConvLayer Kernel Size</cell><cell>[3, 7, 11]</cell></row><row><cell></cell><cell>Waveform Decoder Conv Dilation</cell><cell>[1, 3, 5]</cell></row><row><cell></cell><cell>Memory Banks Size</cell><cell>1000</cell></row><row><cell></cell><cell>Memory Banks Hidden Dimension</cell><cell>192</cell></row><row><cell></cell><cell>Memory Banks Attention Heads</cell><cell>2</cell></row><row><cell></cell><cell>Posterior Encoder WaveNet Layers</cell><cell>16</cell></row><row><cell></cell><cell>Posterior Encoder Dilation</cell><cell>1</cell></row><row><cell>Posterior Encoder ?</cell><cell>Posterior Encoder Conv Kernel Size</cell><cell>5</cell></row><row><cell></cell><cell>Posterior Encoder Conv Filter Size</cell><cell>192</cell></row><row><cell>Discriminator D</cell><cell>Multi-Period Discriminator Periods</cell><cell>[1, 2, 3, 5, 7, 11]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since duration is very important in TTS, especially in non-autoregressive TTS, we name the module related to duration modeling as durator, including but not limited to the functionalities of duration prediction and hidden expansion. It is common to come up with new term to revolutionize the concept in speech community, such as vocoder, cepstrum.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Audio samples can be found in https://speechresearch.github.io/naturalspeech/ 4 Note that some human recordings in LJSpeech dataset may contain strange rhythm ups and downs that affect the rating score. To ensure the human recordings used for evaluation are of good quality, we let judges to exclude the recordings with strange rhythms from evaluation. Otherwise, our NaturalSpeech will achieve better CMOS than human recordings. In a CMOS test without excluding bad recordings, NaturalSpeech achieves +0.09 CMOS better than recordings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Ideally, we should also use ground-truth phoneme hidden sequence as input. However, ground-truth hidden sequence cannot be obtained. Thus, this comparison setting is just a approximation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Soft Dynamic Time Warping in KL loss</head><p>Since the frame-level prior distribution p(z |y; ? pri ) usually has different lengths from the groundtruth speech frames, the standard KL loss cannot be applied. Therefore, we use a soft dynamic time warping (Soft-DTW) of KL loss for L bwd and L fwd to circumvent this mismatch.</p><p>The Soft-DTW version of the KL loss for L bwd can be obtained by recursive calculation:</p><p>where r i,j is the KL divergence loss between the simplified posterior q(z |x; ?, ? bpp ) from frame 1 to frame i and the prior p(z |y; ? pri ) from frame 1 to frame j with the best alignment. KL[q(z * |x; ?, ? bpp )||p(z * |y; ? pri )] is defined in Equation 1. min ? a soft-min operator, which is defined as min ? (a 1 , ..., a n ) = ?? log ? i e ? a i ? and ? = 0.01. warp is a warp penalty for not choosing the diagonal path and is set as 0.07. q(z i |x; ?, ? bpp ) is the i-th frame of the simplified posterior, and p(z j |y; ? pri ) is the j-th frame of the prior.</p><p>The Soft-DTW version of KL loss for L fwd is similar to that of L bwd , which can be defined as: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15561</idno>
		<title level="m">A survey on neural speech synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep voice: Real-time neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Sercan ? Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongguo</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep voice 2: Multi-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">Frederick</forename><surname>Sercan ?mer Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideyuki</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuya</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4784" to="4788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delightfultts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12612</idno>
		<title level="m">The microsoft speech synthesis system for blizzard challenge 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glow-tts: A generative flow for text-to-speech via monotonic alignment search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradtts: A diffusion probabilistic model for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Gogoryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnima</forename><surname>Sadekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Kudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8599" to="8608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhee</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5530" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fastspeech 2: Fast and high-quality end-to-end text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Endto-end adversarial text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glow: generative flow with invertible 1? 1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">Mastering text-to-image generation via transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">N?wa: Visual synthesis pre-training for neural visual world creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10704</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Latent video transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving prosody with linguistic and bert derived features in multi-speaker based mandarin chinese neural tts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiping</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6704" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Png bert: Augmented bert on phonemes and graphemes for neural tts. Proc. Interspeech 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17190</idno>
		<title level="m">Mixed-phoneme bert: Improving bert with mixed phoneme and sup-phoneme representations for text to speech</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Parallel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14574</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj-Skerry</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5180" to="5189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaspeech: Adaptive text to speech for custom voice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00436</idno>
	</analytic>
	<monogr>
		<title level="m">Adaptive text to speech in zero-shot scenarios</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wave-tacotron: Spectrogram-free end-to-end text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5679" to="5683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Machine Translation Group at UEDIN. The news-crawl dataset</title>
		<ptr target="https://data.statmt.org/news-crawl/en/" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tokenlevel ensemble distillation for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Phonemizer: Text to phones transcription for multiple languages in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Titeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">68</biblScope>
			<biblScope unit="page">3958</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
