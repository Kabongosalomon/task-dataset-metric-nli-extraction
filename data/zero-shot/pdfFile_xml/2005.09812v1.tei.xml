<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Speakers in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n Alc?zar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
							<email>malong@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
							<email>perazzi@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
							<email>jolee@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
							<email>pa.arbelaez@uniandes.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>3bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff2">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active Speakers in Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current methods for active speak er detection focus on modeling short-term audiovisual information from a single speaker. Although this strategy can be enough for addressing single-speaker scenarios, it prevents accurate detection when the task is to identify who of many candidate speakers are talking. This paper introduces the Active Speaker Context, a novel representation that models relationships between multiple speakers over long time horizons. Our Active Speaker Context is designed to learn pairwise and temporal relations from an structured ensemble of audiovisual observations. Our experiments show that a structured feature ensemble already benefits the active speaker detection performance. Moreover, we find that the proposed Active Speaker Context improves the state-of-the-art on the AVA-ActiveSpeaker dataset achieving a mAP of 87.1%. We present ablation studies that verify that this result is a direct consequence of our long-term multi-speaker analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Active speaker detection is a multi-modal task that relies on the careful integration of audiovisual information. It aims at identifying active speakers, among a set of possible candidates, by analyzing subtle facial motion patterns and carefully aligning their characteristic speech wave-forms. Although it has a long story in computer vision <ref type="bibr" target="#b10">[11]</ref>, and despite its many applications such as speaker diarization or video re-framing, detecting active speakers in-the-wild remains an open problem. Towards that goal, the recently released AVA Active-Speaker benchmark <ref type="bibr" target="#b30">[31]</ref> provides an adequate experimental framework to study the problem.</p><p>Recent approaches for active speaker detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> have focused on developing sophisticated 3D convolutional models to fuse local audiovisual patterns that estimate binary labels over short-term sequences. These methods perform well on scenarios with a single speaker, but they meet their limits when multiple speakers are present. We argue that this limitation stems from the insufficiency of audio cues to fully solve the problem and from the high ambiguity of visual cues when considered in isolation <ref type="bibr" target="#b30">[31]</ref>. Our goal is to identify the active speaker at a reference time. Let us assume we only have access to a short audiovisual sample from a single speaker (a). By looking at the lips of the speaker, it is hard to tell if he is talking, but the audio indicates that someone at that moment is talking. We have no other option than provide an educated guess. To increase our success prediction chances, let us leverage multi-speaker context (b). We now observe all speakers in the scene during longterm. From this enriched observation, we can infer two things. First, Speaker B is not talking over the whole sequence, and instead, he is listening to Speaker A. Second, looking at Speaker A (e.g. his lips) for the long-term helps us to smooth out local uncertainties. We propose a new representation, the Active Speaker Context, which learns long-term relationships between multiple speakers to make accurate active speaker detections.</p><p>In a multi-speaker scenario, an appropriate disambiguation strategy would exploit rich, long-term, contextual information extracted from each candidate speaker. <ref type="figure" target="#fig_0">Figure  1</ref> illustrates the challenges in active speaker detection when there is more than one candidate speaker. Intuitively, we can fuse information from multiple speakers to disambiguate single speaker predictions. For instance, by analyzing a speaker for an extended period, we can smooth out wrong speech activity predictions coming from short filler words. Likewise, observing multiple candidate speakers, jointly, enables us to understand conversational patterns, e.g. that a natural two-speaker conversation consists of an interleaved sequence of the speakers' utterances.</p><p>In this paper, we introduce the Active Speaker Context, a novel representation that models long-term interac-tions between multiple speakers for in-the-wild videos. Our method estimates active speaker scores by integrating audiovisual cues from every speaker present in a conversation (or scene). It leverages two-stream architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> to encode short-term audiovisual observations, sampled from the speakers in the conversation, thus creating a rich context ensemble. Our experiments indicate that this context, by itself, helps improve accuracy in active speaker detection. Furthermore, we propose to refine the computed context representation by learning pairwise relationships via self-attention <ref type="bibr" target="#b32">[33]</ref> and by modeling the temporal structure with a sequence-to-sequence model <ref type="bibr" target="#b16">[17]</ref>. Our model not only improves the state-of-the-art but also exhibits robust performance for challenging scenarios that contain multiple speakers in the scene. Contributions. In this work we design and validate a model that learns audiovisual relationships among multiple speakers. To this end, our work brings two contributions. <ref type="bibr" target="#b0">1</ref> (1) We develop a model that learns non-local relationships between multiple speakers over long timespans (Section 3).</p><p>(2) We observe that this model improves the state-of-theart in the AVA-ActiveSpeaker dataset by 1.6%, and that this improvement is a direct result of modeling long-term multispeaker context (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-modal learning aims at fusing multiple sources of information to establish a joint representation, which models the problem better than any single source in isolation <ref type="bibr" target="#b26">[27]</ref>. In the video domain, a form of modality fusion with growing interest in the scientific community involves the learning of joint audiovisual representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. This setting includes problems such as person re-identification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref>, audio-visual synchronization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, speaker diarization <ref type="bibr" target="#b37">[38]</ref>, bio-metrics <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>, and audio-visual source separation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. Active speaker detection, the problem studied in this paper, is an specific instance of audiovisual source separation, in which the sources are persons in a video (candidate speakers), and the goal is to assign a segment of speech to an active speaker, or none of the available sources.</p><p>Several studies have paved the way for enabling active speaker detection using audiovisual cues <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Cutler and Davis pioneered the research <ref type="bibr" target="#b10">[11]</ref> in the early 2000s. Their work proposed a time-delayed neural network to learn the audiovisual correlations from speech activity. Alternatively, other methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> opted for using visual information only, especially the lips motion, to address the task. Recently, rich alignment between audio and visual information has been re-explored with methods that leverage audio as supervision <ref type="bibr" target="#b2">[3]</ref>, or jointly train an audiovisual embedding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>, that enables more accurate active speaker detection. Although these previous approaches were seminal to the field, the lack of large-scale data for training and benchmark limited their application to in-the-wild active speaker detection in movies or consumer videos.</p><p>To overcome the lack of diverse and in-the-wild data, Roth et al. <ref type="bibr" target="#b30">[31]</ref>, introduced AVA-ActiveSpeaker, a largescale video dataset devised for the active speaker detection task. With the release of the dataset and its baseline -a twostream network that learns to detect active speakers within a multi-task setting-a few novel approaches have started to emerge. In the AVA-ActiveSpeaker challenge of 2019, Chung et al. <ref type="bibr" target="#b4">[5]</ref> improved the core architecture of their previous work <ref type="bibr" target="#b8">[9]</ref> by adding 3D convolutions and leveraging large-scale audiovisual pre-training. The submission of Zhang et al. <ref type="bibr" target="#b38">[39]</ref> also relied on a hybrid 3D-2D architecture, with large-scale pre-training on two multi-modal datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Their method achieved the best performance when the feature embedding was refined using a contrastive loss <ref type="bibr" target="#b14">[15]</ref>. Both approaches improved the representation of a single speaker, but ignored the rich contextual information from co-occurring speaker relationships, and intrinsic temporal structures that emerge from dialogues.</p><p>Our approach starts from the baseline of a two-stream modality fusion but explores an orthogonal research direction. Instead of improving the performance of a short-term architecture, we aim at modeling the conversational context of speakers, i.e. to leverage active speaker context from long-term inter-speaker relations. Context modeling has been widely studied in computer vision tasks such as object classification <ref type="bibr" target="#b22">[23]</ref>, video question answering <ref type="bibr" target="#b39">[40]</ref>, person re-identification <ref type="bibr" target="#b21">[22]</ref>, or action detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>. Despite the existence of many works harnessing context to improve computer vision systems, our model is unique and tailored to detect active speakers accurately. To the best of our knowledge, our work is the first to address the task of active speaker detection in-the-wild using contextual information from multiple speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Active Speakers in Context</head><p>This section describes our approach to active speaker detection, which focuses on learning long-term and interspeaker relationships. At its core, our strategy estimates an active speaker score for an individual face (target face) by analyzing the target itself, the current audio input, and multiple faces detected at the current timestamp.</p><p>Instead of holistically encoding long time horizons and multi-speaker interactions, our model learns relationships following a bottom-up strategy where it first aggregates fine-grained observations (audiovisual clips), and then maps these observations into an embedding that allows the analysis of global relations between clips. Once the individual  <ref type="figure">Figure 2</ref>. Active Speaker Context. Our approach first splits the video data into short clips (? seconds) composed by a stack of face crops and their associated audio. It encodes each of these clips using a two-stream architecture (Short-Term Encoder) to generate a lowdimensional audiovisual encoding. Then, it stacks the high-level audiovisual features from all the clips and all the speakers sampled within a window of size T (T &gt; ? ) centered at a reference time t. We denote this stack of features as Ct. Then, using self-attention, our approach refines the representation by learning a pairwise attention between all elements. Finally, an LSTM mines temporal relationships across the refined features. This final output is our Active Speaker Context, which we use to classify speech activity of a candidate at time t. embeddings have been estimated, we aggregate them into a context-rich representation which we denote as the Active Speaker Ensemble. This ensemble is then refined to explicitly model pairwise relationships, and to explicitly model long-term structures over the clips, we name this refined ensemble the Active Speaker Context. <ref type="figure">Figure 2</ref> presents an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Aggregating Local Video Information</head><p>Our proposal begins by analyzing audiovisual information from short video clips. The visual information is a stack of k consecutive face crops 2 sampled from a time interval ? . The audio information is the raw wave-form sampled over the same ? interval. We refer to these clips as a tuples c s,? = {v s , a ? }, where v s is a crop stack of a speaker s, and a ? is the corresponding audio. For every clip c s,? in a video sequence, we compute an embedding u s,? using a short-term encoder ?(c s,? ) whose role is twofold. First, it creates a low-dimensional representation that fuses the audiovisual information. Second, it ensures that the embedded representation is discriminative enough for the active speaker detection task. Short-term Encoder (?). Following recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>, we approximate ? by means of a two-stream convolutional architecture. Instead of using compute-intensive 3D convolutions as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>, we opt for 2D convolutions in both streams. The visual stream takes as input a tensor v ? R H?W ?(3k) , where H and W are the width and height of k face crops. On the audio stream, we convert the raw audio waveform into a Mel-spectrogram represented as a ? R Q?P , where Q and P depend on the length of the 2 Our method leverages pre-computed face tracks (consecutive face crops) at training and testing time.</p><p>interval ? . On a forward pass the visual sub-network estimates a visual embedding u v ? R dv , while the audio sub-network computes an audio embedding u a ? R da . We compose an audiovisual feature embedding u ? R d by concatenating the output embedding of each stream.</p><p>Structured Context Ensemble. Once the clip features u ? R d have been estimated, we proceed to assemble these features into a set that encodes contextual information. We denote this set as the Active Speaker Ensemble. To construct this ensemble, we first define a long interval T (T &gt; ? ) centered at a reference time t, and designate one of the speakers present at t as the reference speaker and every other speaker is designated as context speaker.</p><p>We proceed to compute u s,? for every speaker s = 1, . . . , S present at t over L different ? intervals throughout temporal window T . This sampling scheme yields a tensor C t with dimensions L ? S ? d, where S is the total number of speakers analyzed. <ref type="figure">Figure 3</ref> contains a detailed example on the sampling process.</p><p>We assemble C t for every possible t in a video. Since temporal structures are critical in the active speaker problem, we strictly preserve the temporal order of the sampled features. As C t is defined for a reference speaker, we can generate as many ensembles C t as speakers are present at time t. In practice, we always locate the feature set of the reference speaker as the first element along the S axis of C t . Context speakers are randomly stacked along the remaining positions on the S axis. This enables us to directly supervise the label of the reference speaker regardless of the number or order of the context speakers. <ref type="figure">Figure 3</ref>. Building Context Tensors. We build a context ensemble given a reference speaker (Speaker 1 in this example), and a reference time t. First, we define a long-term sampling window T containing L + 1 clips centered at time t, T = {0, 1, ..., t, ..., L ? 1, L}. We select as context speakers those that overlap with the reference speaker at t (speakers 2 and 3). Finally, we sample cliplevel features u l throughout the whole sampling window T from the reference speaker and all the speakers designated as context. If the temporal span of the speaker does not entirely match the interval T, we pad it with the initial or final speaker features. For instance, Speaker 2 is absent between 0 and i, so we pad left with ui. Similarly, for speaker 3, we pad right with u k . Notice that, by our definition, Speakers 2 and 3 could switch positions, but Speaker 1 must remain at the bottom of the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Refinement</head><p>After constructing the context ensemble C t , we are left with the task of classifying the speaking activity of the designated reference speaker. A naive approach would finetune a fully-connected layer over C t with binary output classes i.e. speaking and silent. Although such a model already leverages global information beyond clips, we found that it tends not to encode useful relationships between speakers and their temporal patterns, which emerge from conversational structures. This limitation inspires us to design our novel Active Speaker Context (ASC) model. ASC consists of two core components. First, it implements a multi-modal self-attention mechanism to establish pairwise interactions between the audiovisual observations on C t . Second, it incorporates a long-term temporal encoder, which exploits temporal structures in conversations.</p><p>Pairwise Refinement. We start from the multi-modal context ensemble C t , and model pairwise affinities between observations in C t regardless of their temporal order or the speaker they belong to. We do this refinement by following a strategy similar to Vaswani et al. <ref type="bibr" target="#b32">[33]</ref>. We compute selfattention over long-term sequences and across an arbitrary number of candidate speakers.</p><p>In practice, we adapt the core idea of pair-wise attention from the non-local framework <ref type="bibr" target="#b34">[35]</ref> to work over multimodal high-level features, thereby estimating a dense attention map over the full set of clips contained in the sampling window T . We avoid using this strategy over low or midlevel features as there is no need to relate distributed information on the spatial or temporal domains of a clip i.e. in the active speaker detection task, meaningful information is tightly localized on the visual (lips region) and audio (speech snippets) domains.</p><p>We implement a self-attention module that first estimates a pairwise affinity matrix B with dimension LS ? LS and then uses its normalized representation as weights for the input C t :</p><formula xml:id="formula_0">B = ?((W ? * C t ) ? (W ? * C t ) )</formula><p>(1)</p><formula xml:id="formula_1">C ? t = W ? * (B ? (W ? * C t )) + C t<label>(2)</label></formula><p>Where ? is a softmax operation, {W ? , W ? , W ? , W ? } are learnable 1 ? 1 ? 1 weights that adapt the channel dimensions as needed, and the second term in Equation 2 (+C t ) denotes a residual connection. The output C ? t is a tensor with identical dimensions as the input C t (L ? S ? d), but it now encodes the pairwise relationships.</p><p>Temporal Refinement. The goal of this long-term pooling step is two-fold. First, to refine the weighted features in C ? t by directly attending to their temporal structure. Second, to reduce the dimensionality of the final embedding to d (d &gt; d ), allowing us to use a smaller fully-connected prediction layer. Given the inherent sequential structure of the task, we implement this refinement using an LSTM model <ref type="bibr" target="#b16">[17]</ref>. We cast its input by squeezing the speaker and time dimension of C ? t into (L ? S) ? d; thus we input the LSTM time steps t i ? {1, . . . , L ? S}, with a feature vector z i ? R d . In practice, we use a single uni-directional LSTM unit with d = 128, and keep the LSTM memory as it passes over the sequence. Thus, we create a sequenceto-sequence mapping between tensor C ? t ? R (L?S)?d and a our final Active Speaker Context representation ASC t ? R (L?S)?d .</p><p>Our final step consists of estimating the presence of an active speaker given ASC t . We resort to a simple fully-connected layer with binary output (active speaker and silent). We establish the final confidence score using a softmax operator over the outputs and select the value of the speaking class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Implementation Details</head><p>We use a two-stream (visual and audio) convolutional encoder based on the Resnet-18 architecture <ref type="bibr" target="#b15">[16]</ref> for the Short-Term Feature extraction (STE). Following <ref type="bibr" target="#b30">[31]</ref>, we re-purpose the video stream to accept a stack of N face crops by replicating the weights on the input layer N times. The audio stream input is a Mel-spectrogram calculated from an audio snippet, which exactly matches the time interval covered by the visual stack. Since Mel-spectrograms are 2D tensors, we re-purpose the input of the audio stream to accept a L ? P ? 1 tensor by averaging channel-specific weights at the input layer.</p><p>Training the Short-term Encoder We train the STE using the Pytorch library <ref type="bibr" target="#b28">[29]</ref> for 100 epochs. We choose the ADAM optimizer <ref type="bibr" target="#b20">[21]</ref> with an initial learning rate of 3 ? 10 ?4 and learning rate annealing ? = 0.1 every 40 epochs. We resize every face crop to 124?124 and perform random flipping and corner cropping uniformly along the visual input stack. We drop the large-scale multi-modal pre-training of <ref type="bibr" target="#b4">[5]</ref>, in favor of standard Imagenet <ref type="bibr" target="#b11">[12]</ref> pretraining for the initialization.</p><p>Since we want to favor the estimation of discriminative features on both streams, we follow the strategy presented by Roth et al. <ref type="bibr" target="#b30">[31]</ref> and add two auxiliary supervision sources, and place them on top of both streams before the feature fusion operation, this creates two auxiliary loss functions L a , L v . Our final loss function is L = L av + L a + L v . We use the standard Cross-entropy loss for all three terms.</p><p>Training the Active Speaker Context Model We also optimize the ASC using the Pytorch library and the ADAM optimizer with an initial learning rate of 3?10 ?6 and learning rate annealing ? = 0.1 every 10 epochs. We train the full ASC module from scratch and include batch normalization layers to favor faster convergence <ref type="bibr" target="#b17">[18]</ref>. Similar to the STE, we use Cross-entropy loss to train ASC, but in this scenario, the loss consists of a single term L av .</p><p>The ASC processes a fixed number of speakers S to construct C t . Given that not every reference time t contains the same number of speaker detections, there are three scenarios for J overlapping speakers and an ensemble of size S. If J ? S, we randomly sample S ? 1 context speakers (one is already assigned as reference). If J &lt; S, we select a reference, and randomly sample (with replacement) S ? 1 context speakers from the remaining J ? 1 speakers. In the extreme case where J = 1, the reference speaker is replicated S ? 1 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section evaluates our method's ability to detect active speakers in untrimmed videos. We conduct the experiments using the large-scale AVA-ActiveSpeaker dataset <ref type="bibr" target="#b30">[31]</ref>. We divide the experiment analyses into three parts. First, we compare our approach with the existing state-ofthe-art approaches. Then, we ablate our method and inspect the contributions of each of its core components. Finally, we do a performance breakdown and analyze success and failure modes.</p><p>AVA-ActiveSpeaker dataset. The AVA-ActiveSpeaker dataset <ref type="bibr" target="#b30">[31]</ref> contains 297 Hollywood movies, with 133 of those for training, 33 for validation and 131 for testing. The dataset provides normalized bounding boxes for 5.3 million faces (2.6M training, 0.76M validation, and 2.0M testing) detected over 15-minute segments from each movie. These detections occur at an approximate rate of 20fps and are manually linked over time to produce face tracks depicting a single identity (actor). Each face detection in the dataset is augmented with a speaking or non-speaking attribute. Thus, the task at inference time is to produce a confidence score that indicates the chance of speaking for each given face detection. In our experiments, we use the dataset official evaluation tool, which computes the mean average precision (mAP) metric over the validation (ground-truth available) and test sets (ground-truth withheld). Unless mentioned otherwise, we evaluate active speaker detection on the AVA-ActiveSpeaker validation subset.</p><p>Dataset sampling at training time. As noted by Roth et al. <ref type="bibr" target="#b30">[31]</ref>, AVA-ActiveSpeaker has a much smaller variability in comparison to natural image datasets with a comparable size. For the training of the STE, we prevent over-fitting by randomly sampling a single clip with k time contiguous crops from every face track instead of densely sampling every possible time contiguos clip of size k in the tracklet. Therefore, our epoch size correlates with the number of face tracks rather than the number of face detections. To train our context refinement models, we use standard dense sampling over the training set, as we do not observe any significant over-fitting in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with the State-of-the-art</head><p>We compare our method's performance to the state-ofthe-art and summarize these results in <ref type="table">Table 1</ref>. We set L = 11 and S = 3 for the experiment. We report results on the validation and testing subsets. The latter is withheld for the AVA-ActiveSpeaker task in the ActivityNet challenge <ref type="bibr" target="#b1">[2]</ref>.</p><p>We observe that our method outperforms all existing approaches in the validation subset. This result is very favorable as the other methods rely on 3D convolutions and large scale pre-training, while our model relies exclusively on contextual information built from 2D models. The best existing approach, Chung et al. <ref type="bibr" target="#b4">[5]</ref>, obtains 85.5%. Even though their method uses a large-scale multi-modal dataset for pre-training, our context modeling outperforms their solution by 1.6%.</p><p>As <ref type="table">Table 1</ref> shows, our method achieves competitive results in the testing subset. Even though our model discards 3D convolutions and model ensembles <ref type="bibr" target="#b4">[5]</ref>, we rank 2nd in the AVA-ActiveSpeaker 2019 Leaderboard 3 . The overall results on the AVA-ActiveSpeaker validation and testing subsets validate the effectiveness of our approach. We empirically demonstrate that it improves the state-of-the-art, but a question remains. What makes our approach strong? We answer that question next via ablation studies.  <ref type="bibr" target="#b4">[5]</ref> 85.1 Zhang et al. <ref type="bibr" target="#b38">[39]</ref> 84.0</p><p>ActivityNet Challenge Leaderboard 2019 Naver Corporation <ref type="bibr" target="#b4">[5]</ref> 87.8 Active Speakers Context (Ours) 86.7 University of Chinese Academy of Sciences <ref type="bibr" target="#b38">[39]</ref> 83.5 Google Baseline <ref type="bibr" target="#b30">[31]</ref> 82.1 <ref type="table">Table 1</ref>. Comparison with the State-of-the-art. We report the performance of state-of-the-art methods in the AVA Active Speakers validation and testing subsets. Results in the validation set are obtained using the official evaluation tool published by <ref type="bibr" target="#b30">[31]</ref>, test set metrics are obtained using the the ActivityNet challenge evaluation server. In the validation subset, we improve the performance of previous approaches by 1.6%, without using large-scale multimodal pre-training. In the test subset, we achieve 86.7% and rank second in the leaderboard, without using 3D convolutions, sophisticated post-processing heuristics or assembling multiple models.  <ref type="table">Table 2</ref>. Effect of context refinement. We ablate the contributions of our method's core components. We begin with a baseline that does not include any context, which achieves 79.5%. Then, by simply leveraging context with a linear prediction layer, we observe a significant boost of 4.9%. Additionally, we find that adding pairwise and temporal refinement further improves the performance by 0.8% and 1.3% respectively. The ASC best performance is achieved only if both refinement steps are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context &amp; Refinement mAP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Analysis</head><p>Does context refinement help? We first assess the effectiveness of the core components of our approach. <ref type="table">Table  2</ref> compares the performance of the baseline, a two-stream network (No Context) that encodes a single speaker in a short period, a naive context prediction using a single linear layer (Context + No Refinement), and three ablated variants of our method, two of these variants verify the individual contributions of the two ASC refinement steps (Context + Pairwise Refinement and Context + Temporal Refinement), the third (Context + Pairwise Refinement + MLP) has a two layer perceptron which yields about the same number of parameters as the ASC, it is useful to test if the increased performance derives from the increased size of the network.</p><p>While the initial assembly of the context tensor already improves the baseline performance, our results show that context refinement brings complementary gains. That is, the active speaker detection task benefits not only from the presence of additional clip information in the context, but also profits from directly modeling speaker relationships and temporal structures. We observe that our whole context refinement process leads to an average of 4.73% mAP increase over the context tensor and a naive prediction. These results validate our design choice of distilling context via the pairwise and temporal refinement modules.</p><p>Are there alternatives for temporal refinement? We now compare our temporal refinement strategy against a baseline strategy for temporal refinement. During the recent ActivityNet challenge, Chung et al. <ref type="bibr" target="#b4">[5]</ref> explored the moving average strategy, reporting an increase of 1.3% mAP using a median filter over prediction scores. A key difference is that <ref type="bibr" target="#b4">[5]</ref> processes short-term windows (0.5s), whereas we consider windows of 2.25s. We found that smoothing long temporal windows negatively impacts the performance of our method. <ref type="table">Table 3</ref> shows that there is a negligible increase (+0.02%) using short temporal averages, and a drastic drop (?11.64%) using long averages.  <ref type="table">Table 3</ref>. Moving average vs. temporal refinement (mAP). We observe only marginal benefits when replacing the proposed temporal smoothing step with a moving average, in fact this operation has a large penalty when smoothing longer sampling windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does context size matter?</head><p>We continue the ablation by analyzing the influence of context size on the final performance of our method. <ref type="table">Table 4</ref> summarizes the two dimensions of this analysis, where we vary the temporal support (i.e. vary L from 1 to 11 clips), or alter the number of context speakers (i.e. vary S from 1 to 3 speakers). Overall, extended temporal contexts and more cooccurring speakers at training time favor the performance of our method. These results indicate that the proposed approach utilizes both types of context to disambiguate predictions for a single speaker. We observe a larger gap in performance when switching between one to two speakers (1.8% on average) than when switching between 2 and 3 (0.15% on average). This behavior might be due to the relative scarcity of samples containing more than three speakers at training time. Regarding temporal support, we observe gradual improvements by increasing L. However, as soon as L reaches 11, we see diminishing returns that seem to be correlated with the average length of face tracks in the training subset. The context size analysis performed here supports our central hypothesis that context from long-time horizons and multiple-speakers is crucial for making accurate active speaker detections.  <ref type="table">Table 4</ref>. Impact of context size. We investigate the effect of different sizes of temporal support and the number of speakers used to construct our context representation. To that end, We report the mAP obtained by different context size configurations. We observe that both types of context play a crucial role at boosting performance. Using our longest temporal support, L = 11 (2.25 seconds), our method improves the baseline (L = 1 / S = 1) by 6.1%. Moreover, when combined with context from multiple speakers, i.e. L = 11 / S = 3, we achieve an additional boost of 1.5% resulting in our best performance of 87.1%. In short, our findings reveal the importance of sampling context from long time horizons and multiple speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Distortion Type</head><p>Temporal Order Surrounding None mAP 77.8 84.5 87.1 <ref type="table">Table 5</ref>. Effect of context sampling distortion. We observe that our method looses 2.6% mAP when the context speakers are randomly sampled across the video. It also drastically drops (?9.3%) when the context temporal order is scrambled. These results validate the importance of sampling context for the target face within the right surrounding and preserving its temporal order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does context sampling matter?</head><p>We now evaluate the effect of tempering the temporal structure when constructing C t . We also assess the effectiveness of 'in-context' speaker information, i.e. we study if sampling 'out-ofcontext' speakers degrades the performance of our approach. For the first experiment, we build C t exactly as outlined in Section 3.3, but randomly shuffle the temporal sequence of all speakers except clips at reference time t. For the second experiment we replace the context speakers with a set of speakers sampled from a random time t such that t = t. We report the results in <ref type="table">Table 5</ref>.</p><p>Let us analyze the two sampling distortions one at a time. First, the ablation results highlight the importance of the temporal structure. If such a structure is altered, the effectiveness of our method drops below that of the baseline to 77.8%. Second, it is also important to highlight that incorporating out-of-context speakers in our pipeline is worse than using only the reference speaker (84.5% vs. 87.1%). In other words, temporal structure and surrounding speakers provide unique contextual cues that are difficult to replace with random information sampled from a video. Baseline Ours <ref type="figure">Figure 4</ref>. Performance breakdown. We analyze the performance of the baseline approach (w/o context) and our proposed method (Active Speaker Context) under two different visual characteristics of the samples at inference time: number of faces (left) and face size (right). For the number of faces, we split the dataset into three exclusive buckets: one, two, and three faces, which altogether cover &gt; 90% of the dataset. Similarly, we split the dataset into three face sizes: Small (S), Medium (M), Large (L), corresponding to face crops of width &lt;= 64, &gt; 64 but &lt;= 128, and &gt; 128 pixels, respectively. In all scenarios, we observe that our approach outperforms the baseline, with those gains being more pronounced in challenging scenarios. For instance, when we compare their performance for three (3) faces, our method offers a significant boost of 13.2%. Moreover, for the hard case of small faces (S), we achieve an improvement of 11.3% over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results Analysis</head><p>Performance Breakdown. Following recent works <ref type="bibr" target="#b0">[1]</ref>, we break down our model's and baseline performances in terms of relevant characteristics of the AVA Active Speaker dataset, namely number of faces and face size, which we present in <ref type="figure">Figure 4</ref>. We also analyze the impact of noise in speech and find that both our method and the baseline are fairly robust to altered speech quality;</p><p>The performance breakdown for the number of faces in <ref type="figure">Figure 4</ref> (left) reveals the drawbacks of the baseline approach, and the benefits of ASC. We split the validation frames into three mutually exclusive groups according to the number of faces in the frame. For each group, we compute the mAP of the baseline and our approach. Although both follow a similar trend with performance decreasing as the number of faces increases, our method is more resilient. For instance, in the challenging case of three faces, our method outperforms the baseline by 13.2%. This gain could be due to our method leverages information from multiple speakers at training time, making it aware of conversational patterns and temporal structures unseen by the baseline.</p><p>Dealing with small faces is a challenge for active speaker detection methods <ref type="bibr" target="#b30">[31]</ref>. <ref type="figure">Figure 4</ref> (right) presents how the baseline and our ASC method are affected by face size. <ref type="figure">Figure 5</ref>. Qualitative results. The attention within the pairwise refinement step has some characteristic activation patterns. We highlight the reference speaker in a yellow bounding box and represent the attention score with a heat-map growing from light-blue (no attention) to red (the highest-attention). The first row shows a typical activation pattern for two silent speakers. The attention model focuses exclusively on the reference speaker (highlighted in yellow) at the reference time. In the cases where there is an active speaker (second row), the attention concentrates on the reference speaker over an extended time interval. In the third row, the reference speaker is also active, but in this case, his facial gestures are ambiguous; thus, the attention also looks at the context speaker.</p><p>We divide the validation set into three splits: (S) small faces with width less than 64 pixels, (M) medium faces with width 64 and 128 pixels, and (L) large faces with width more than 128 pixels. There is a correlation between the performance of active speaker detection and face size. Smaller faces are usually harder to label as active speakers. However, our approach exhibits less performance degradation than the baseline as face size decreases. In the most challenging case, i.e. small faces, our method outperforms the baseline by 11.3%. We hypothesize that our method aggregates information from larger faces via temporal context, which enhances predictions for small faces.</p><p>Qualitative results. We analyze the pairwise relations built on the matrix C t on a model trained with only two speakers. <ref type="figure">Figure 5</ref> showcases three sample sequences centered at a reference time t, each containing two candidate speakers. We highlight the reference speaker in yellow and represent the attention score with a heat-map growing from light-blue (no attention) to red (the highest-attention).</p><p>Overall we notice three interesting patterns. First, sequences labeled as silent generate very sparse activations focusing on a specific timestamp (see top row). We hypothesize that identifying the presence of speech is a much simpler task than detecting the actual active speaker. Therefore, our model reliably decides by only attending a short time span. Second, for sequences with an active speaker, our pairwise refinement tends to distribute the attention towards a single speaker throughout the temporal window (see the second row). Besides, the attention score tends to have a higher value near the reference time and slowly decades as it approaches the limit of the time interval. Third, we find many cases in which our model attends to multiple speakers in the scene. This behavior often happens when the facial features of the reference speaker are difficult to observe or highly ambiguous. For example, the reference speaker in the third row is hard to see due to insufficient lighting and face orientation in the scene. Hence, the network attends simultaneously to both the reference and the context speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a context-aware model for active speaker detection that leverages cues from co-occurring speakers and long-time horizons. We have shown that our method outperforms the state-of-the-art in active speaker detection, and works remarkably well in challenging scenarios when many candidate speakers or only small faces are on-screen. We have mitigated existing drawbacks, and hope our method paves the way towards more accurate active speaker detection. Future explorations include using speaker identities as a supervision source as well as learning to detect faces and their speech attribute jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional Results Analysis</head><p>To complement the experimental validation of section 4, we assess the robustness of our method in presence of audio noise and analyze in detail the pairwise refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Performance breakdown</head><p>Following the experimental setup of Roth et al. <ref type="bibr" target="#b30">[31]</ref>, we analyze the performance of our method in the presence of different qualities of speech: Music (common due to Hollywood soundtracks) and other types of noise, <ref type="figure" target="#fig_4">figure 6</ref> summarizes this results. As outlined by <ref type="bibr" target="#b30">[31]</ref>, the presence of music in the audio stream is not a significant source of error. Our ASC model reduces its performance by 3.7%, and the baseline drops by 5.4%. We hypothesize the ASC is slightly more resilient to noise as it can look over more extended periods, where clean speech might be present.</p><p>Noisy speech makes the active speaker detection problem more difficult, where the ASC performance drops by 6% and the baseline by 6.6%. Again we believe ASC profits from longer sequences where clean speech patterns are present. Although these error sources have an impact on performance, they are a smaller source of error when compared to the face size and the number of candidate speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean Noise Music</head><p>Speech Quality  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Pairwise Refinement Visualization</head><p>We conclude by visualizing the activation patterns in the pairwise refinement module to better understand the relationships being built amongst the speakers. We use a smaller version of ASC (S = 2 and L = 9) than the one in the main paper with only two candidate speakers for easier visualization over nine short-term clips. We plot the activation values of the matrix B (after the soft-max operation), for the pairwise attention over 9 time steps and 2 speakers. In figures 7, 8, 9 and 10 we zoom in-between time steps 2 and 6 for simpler plots, and use light-blue for no activation (0.0) and red for maximum activations (1.0).</p><p>We expand the explanation given in Section 4.3 and highlight the activation patterns for 3 common scenarios.</p><p>Silence: As outlined in the main paper, when both speakers are silent the pair-wise module usually generates sparse activations. This behavior indicates that a single (or few) elements in the Active Speaker Context embedding are enough to reliably estimate the active speaker score. <ref type="figure">Figure 7</ref> left and right, show typical activation patterns for this situation, where few elements inside the ensemble have a large attention score.</p><p>Our hypothesis is that local features that correlate to silent scenarios are strong predictors; therefore, inference over the ensemble can be very reliable even if looking at a small number of cues. Overall these are the only scenarios where we find activation values of 1.0. In other cases, we observe that the activation patterns are distributed across the matrix B.</p><p>Active Speaker without context speaker: When there is a single candidate speaker in the scene, the ensemble is composed from replicas of the same speaker. In this scenario, we observe some particular activation patterns over B along straight lines. Such activation patterns indicate that the attention over the ensemble is distributed along the temporal dimension.</p><p>We interpret this behavior as the way our pairwise refinement attends to useful temporal information. Since context and reference speakers have the same information, any line pattern aggregates temporal information of the reference speaker. In other words, the attention mechanism looks for evidence that indicates the speaker is currently talking beyond the reference timestamp. <ref type="figure">Figure 8</ref> visualizes 2 sample activations for this scenario.</p><p>Active Speaker with context speaker: The activation patterns for multiple candidate speakers are more complex and harder to characterize as the pairwise attention does not focus on a particular speaker or time-stamp. However, it seems to focus on several time steps establishing relations between the active speaker and the context speaker; notice the activation's on the top right and bottom left of the figure, these areas correspond to inter-speaker attention and contain large activation patterns, while top left and bottom right are self-attention and have less significant activations. <ref type="figure">Figure 9</ref> contains two examples of this activation pattern.</p><p>Overall, the activation pattern for this scenario can be a lot more diverse, and do not always focus on the relation between speakers. In <ref type="figure" target="#fig_0">Figure 10</ref> we plot some of this scenarios. We will further note that occasionally the activation patterns in this scenario slightly resemble the line activation pattern of a single active speaker. <ref type="figure">Figure 7</ref>. Activation patterns for silence When there are no active speakers, it is common to see activation patterns similar to those pictured above, where very sparse activations are observed over matrix B, we think local cues that indicate silence are strong predictors, thus it is not necessary to look at many. <ref type="figure">Figure 8</ref>. Activation patterns for an active speaker with no context speakers, When there is a single active speaker in the scene we commonly observe activations patterns along a straight line. This patterns suggest the network looks for temporal consistency along several clips in order to predict a high active speaker score. <ref type="figure">Figure 9</ref>. Activation Patterns for an active speaker with context speaker. Activation patterns are much more diverse with two speakers, there are some scenarios where the networks focuses on the areas of B where inter-speaker relations are modelled. <ref type="figure" target="#fig_0">Figure 10</ref>. Activation Patterns for an active speaker with context speaker. With two speaker activations patterns are far more diverse than in any other scenario. While the sections of the matrix that attend at inter-speaker relations are highlighted there is also presence of self-attention (see right), and line pattern activations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Active Speakers in Context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Chung et al. (Temporal Convolutions) [5] 85.5 Chung et al. (LSTM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Performance breakdown by speech quality Out of the two sources of speech quality characterized in AVA-ActiveSpeaker, Music is the less relevant one reducing about 3.7% mAP for ASC. Noise in speech is a significant source of error, where the proposed approach loses about 6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Short-Term Encoder (STE) STE Ct STE Pairwise RefinementC ? t Audio CNN Visual CNN . . . . . . Temporal Refinement LSTM . . .</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>t -T/2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>W ? :1x1x1</cell><cell>Softmax</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell>W ? :1x1x1</cell><cell>Transpose</cell></row><row><cell>Time</cell><cell>t</cell><cell>W :1x1x1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>W ? :1x1x1</cell><cell>z1 z2</cell><cell>zi</cell><cell>Active Speaker</cell></row><row><cell></cell><cell>t + T/2</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To enable reproducibility and promote future research, code has been made available at: https://github.com/fuankarion/ active-speakers-context</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://activity-net.org/challenges/2019/evaluation.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Who&apos;s speaking? audio-supervised classification of active speakers in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayeh</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active speaker detection with audio-visual co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Naver at activitynet challenge 2019-task b active speaker detection (ava)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10555</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">You said that? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perfect match: Improved cross-modal embeddings for audiovisual synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Goo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taking the bite out of automated naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Carl Doersch, and Andrew Zisserman. Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural predictive coding using convolutional neural networks toward unsupervised learning of speaker characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Jati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1577" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On learning associations of faces and voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Hijung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kaspar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using the forest to see the trees: A graphical model relating features, objects, and scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learnable pins: Cross-modal embeddings for person identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seeing voices and hearing faces: Cross-modal biometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ava-activespeaker: An audiovisual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01342</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kevin Wilson, James Glass, and Trevor Darrell. Visual speech recognition with loosely synchronized feature streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Siracusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning discriminative features for speaker identification and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully supervised speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-task learning for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
