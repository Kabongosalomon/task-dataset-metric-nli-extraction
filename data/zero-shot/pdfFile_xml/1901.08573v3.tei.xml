<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theoretically Principled Trade-off between Robustness and Accuracy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
							<email>hongyanz@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
							<email>jiantao@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
							<email>elghaoui@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">CMU &amp; TTIC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">CMU &amp; Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Theoretically Principled Trade-off between Robustness and Accuracy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of~2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean 2 perturbation distance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In response to the vulnerability of deep neural networks to small perturbations around input data [SZS + 13], adversarial defenses have been an imperative object of study in machine learning [HPG + 17], computer vision [SKN + 18, XWZ + 17, MC17], natural language processing [JL17], and many other domains. In machine learning, study of adversarial defenses has led to significant advances in understanding and defending against adversarial threat [HWC + 17]. In computer vision and natural language processing, adversarial defenses serve as indispensable building blocks for a range of security-critical systems and applications, such as autonomous cars and speech recognition authorization. The problem of adversarial defenses can be stated as that of learning a classifier with high test accuracy on both natural and adversarial examples. The adversarial example for a given labeled data (x, y) is a data point x that causes a classifier c to output a different label on x than y, but is "imperceptibly similar" to x. Given the difficulty of providing an operational definition of "imperceptible similarity," adversarial examples typically come in the form of restricted attacks such as -bounded perturbations [SZS + 13], or unrestricted attacks such as adversarial rotations, translations, and deformations [BCZ + 18, ETT + 17, GAG + 18, XZL + 18, AAG19, ZCS + 19]. The focus of this work is the former setting, though our framework can be generalized to the latter.</p><p>Despite a large literature devoted to improving the robustness of deep-learning models, many fundamental questions remain unresolved. One of the most important questions is how to trade off adversarial robustness The problem of adversarial defense becomes more challenging when computational issues are considered. For example, the straightforward empirical risk minimization (ERM) formulation of robust classification involves minimizing the robust 0-1 loss max x : x ?x ? 1{c(x ) = y}, a loss which is NP-hard to optimize even if = 0 in general. Hence, it is natural to expect that some prior work on adversarial defense replaced the 0-1 loss 1(?) with a surrogate loss [MMS + 18, KGB17, UOKvdO18]. However, there is little theoretical guarantee on the tightness of this approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our methodology and results</head><p>We begin with an example that illustrates the trade-off between accuracy and adversarial robustness in Section 2.4. This phenomenon was first theoretically demonstrated by [TSE + 19]. We construct another toy example where the Bayes optimal classifier achieves natural error 0% and robust error 100%, while the trivial all-one classifier achieves both natural error and robust error 50% <ref type="table">(Table 1)</ref>. While a large literature on the analysis of robust error in terms of generalization [SST + 18, <ref type="bibr" target="#b10">CBM18,</ref><ref type="bibr" target="#b41">YRB18]</ref> and computational complexity <ref type="bibr" target="#b6">[BPR18,</ref><ref type="bibr" target="#b5">BLPR18]</ref>, in this work we focus on how to address the trade-off between the natural error and the robust error.</p><p>We show that the robust error can in general be bounded tightly using two terms: one corresponds to the natural error measured by a surrogate loss function, and the other corresponds to how likely the input features are close to the -extension of the decision boundary, termed as the boundary error. We then minimize the differentiable upper bound. Our theoretical analysis naturally leads to a new formulation of adversarial defense which has several appealing properties; in particular, it inherits the benefits of scalability to large datasets exhibited by Tiny ImageNet, and the algorithm achieves state-of-the-art performance on a range of benchmarks while providing theoretical guarantees. For example, while the defenses overviewed in <ref type="bibr" target="#b1">[ACW18]</ref> achieve robust accuracy no higher than~47% under white-box attacks, our method achieves robust accuracy as high as 57% in the same setting. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge where we won first place out of~2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean 2 perturbation distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Summary of contributions</head><p>Our work tackles the problem of trading accuracy off against robustness and advances the state-of-the-art in multiple ways.</p><p>? Theoretically, we characterize the trade-off between accuracy and robustness for classification problems via decomposing the robust error as the sum of the natural error and the boundary error. We provide differentiable upper bounds on both terms using the theory of classification-calibrated loss, which are shown to be the tightest upper bounds uniform over all probability distributions and measurable predictors.</p><p>? Algorithmically, inspired by our theoretical analysis, we propose a new formulation of adversarial defense, TRADES, as optimizing a regularized surrogate loss. The loss consists of two terms: the term of empirical risk minimization encourages the algorithm to maximize the natural accuracy, while the regularization term encourages the algorithm to push the decision boundary away from the data, so as to improve adversarial robustness (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>? Experimentally, we show that our proposed algorithm outperforms state-of-the-art methods under both black-box and white-box threat models. In particular, the methodology won the final round of the NeurIPS 2018 Adversarial Vision Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We illustrate our methodology using the framework of binary classification, but it can be generalized to other settings as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>We will use bold capital letters such as X and Y to represent random vector, bold lower-case letters such as x and y to represent realization of random vector, capital letters such as X and Y to represent random variable, and lower-case letters such as x and y to represent realization of random variable. Specifically, we denote by x ? X the sample instance, and by y ? {?1, +1} the label, where X ? R d indicates the instance space. sign(x) represents the sign of scalar x with sign(0) = +1. Denote by f : X ? R the score function which maps an instance to a confidence value associated with being positive. It can be parametrized, e.g., by deep neural networks. The associated binary classifier is sign(f (?)). We will frequently use 1{event}, the 0-1 loss, to represent an indicator function that is 1 if an event happens and 0 otherwise. For norms, we denote by x a generic norm. Examples of norms include x ? , the infinity norm of vector x, and x 2 , the 2 norm of vector x. We use B(x, ) to represent a neighborhood of x: {x ? X :</p><p>x ? x ? }. For a given score function f , we denote by DB(f ) the decision boundary of f ; that is, the set {x ? X : f (x) = 0}. The set B(DB(f ), ) denotes the neighborhood of the decision boundary of f :</p><formula xml:id="formula_0">{x ? X : ?x ? B(x, ) s.t. f (x)f (x ) ? 0}.</formula><p>For a given function ?(u), we denote by ? * (v) := sup u {u T v ? ?(u)} the conjugate function of ?, by ? * * the bi-conjugate, and by ? ?1 the inverse function. We will frequently use ?(?) to indicate the surrogate of 0-1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Robust (classification) error</head><p>In the setting of adversarial learning, we are given a set of instances x 1 , ..., x n ? X and labels y 1 , ..., y n ? {?1, +1}. We assume that the data are sampled from an unknown distribution (X, Y ) ? D. To characterize </p><formula xml:id="formula_1">!(#) 0 1 % % 1/2 # 1</formula><formula xml:id="formula_2">= E (X,Y )?D 1{?X ?B(X, ) s.t. f (X )Y ? 0}.</formula><p>This is in sharp contrast to the standard measure of classifier performance-the natural (classification) error</p><formula xml:id="formula_3">R nat (f ) := E (X,Y )?D 1{f (X)Y ? 0}.</formula><p>We note that the two errors satisfy R rob (f ) ? R nat (f ) for all f ; the robust error is equal to the natural error when = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Boundary error</head><p>We introduce the boundary error defined as</p><formula xml:id="formula_4">R bdy (f ) := E (X,Y )?D 1{X ? B(DB(f ), ), f (X)Y &gt; 0}.</formula><p>We have the following decomposition of R rob (f ):</p><formula xml:id="formula_5">R rob (f ) = R nat (f ) + R bdy (f ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Trade-off between natural and robust errors</head><p>Our study is motivated by the trade-off between natural and robust errors. [TSE + 19] theoretically showed that training models to be robust may lead to a reduction of standard accuracy by constructing a toy example. To illustrate the phenomenon, we provide another toy example here.</p><p>Example. Consider the case (X, Y ) ? D, where the marginal distribution over the instance space is a uniform distribution over [0, 1], and for k = 0, 1, ..., 1 2 ? 1 ,</p><formula xml:id="formula_6">?(x) := Pr(Y = 1|X = x) = 0, x ? [2k , (2k + 1) ), 1, x ? ((2k + 1) , (2k + 2) ].</formula><p>(2) See <ref type="figure" target="#fig_1">Figure 2</ref> for the visualization of ?(x). We consider two classifiers: a) the Bayes optimal classifier sign(2?(x) ? 1); b) the all-one classifier which always outputs "positive." <ref type="table">Table 1</ref> displays the trade-off between natural and robust errors: the minimal natural error is achieved by the Bayes optimal classifier with large robust error, while the optimal robust error is achieved by the all-one classifier with large natural error.</p><p>Our goal. In practice, one may prefer to trade-off between robustness and accuracy by introducing weights in (1) to bias more towards the natural error or the boundary error. Noting that both the natural error and the boundary error involve 0-1 loss functions, our goal is to devise tight differentiable upper bounds on both of these terms. Towards this goal, we utilize the theory of classification-calibrated loss. <ref type="table">Table 1</ref>: Comparisons of natural and robust errors of Bayes optimal classifier and all-one classifier in example (2). The Bayes optimal classifier has the optimal natural error while the all-one classifier has the optimal robust error. Bayes Optimal Classifier All-One Classifier R nat 0 (optimal) 1/2 R bdy 1 0 R rob 1 1/2 (optimal) </p><formula xml:id="formula_7">(?) = 1 2 (1 ? ?) log 2 (1 ? ?) + 1 2 (1 + ?) log 2 (1 + ?). Loss ?(?) ?(?) Hinge max{1 ? ?, 0} ? Sigmoid 1 ? tanh(?) ? Exponential exp(??) 1 ? ? 1 ? ? 2 Logistic log 2 (1 + exp(??)) ? log (?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classification-calibrated surrogate loss</head><p>Definition. Minimization of the 0-1 loss in the natural and robust errors is computationally intractable and the demands of computational efficiency have led researchers to focus on minimization of a tractable surrogate loss,</p><formula xml:id="formula_8">R ? (f ) := E (X,Y )?D ?(f (X)Y ).</formula><p>We then need to find quantitative relationships between the excess errors associated with ? and those associated with 0-1 loss. We make a weak assumption on ?: it is classification-calibrated <ref type="bibr" target="#b4">[BJM06]</ref>. Formally, for ? ? [0, 1], define the conditional ?-risk by</p><formula xml:id="formula_9">H(?) := inf ??R C ? (?) := inf ??R (??(?) + (1 ? ?)?(??)) ,</formula><p>and define H ? (?) := inf ?(2??1)?0 C ? (?). The classification-calibrated condition requires that imposing the constraint that ? has an inconsistent sign with the Bayes decision rule sign(2? ? 1) leads to a strictly larger ?-risk:</p><p>Assumption 1 (Classification-Calibrated Loss). We assume that the surrogate loss ? is classification-calibrated, meaning that for any ? = 1/2, H ? (?) &gt; H(?).</p><p>We argue that Assumption 1 is indispensable for classification problems, since without it the Bayes optimal classifier cannot be the minimizer of the ?-risk. Examples of classification-calibrated loss include hinge loss, sigmoid loss, exponential loss, logistic loss, and many others (see <ref type="table" target="#tab_0">Table 2</ref>).</p><p>Properties. Classification-calibrated loss has many structural properties that one can exploit. We begin by introducing a functional transform of classification-calibrated loss ? which was proposed by <ref type="bibr" target="#b4">[BJM06]</ref>. Define the function ? : Below we state useful properties of the ?-transform. We will frequently use the function ? to bound R rob (f ) ? R * nat .</p><p>Lemma 2.1 ([BJM06]). Under Assumption 1, the function ? has the following properties: ? is non-decreasing, continuous, convex on [0, 1] and ?(0) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relating 0-1 loss to Surrogate Loss</head><p>In this section, we present our main theoretical contributions for binary classification and compare our results with prior literature. Binary classification problems have received significant attention in recent years as many competitions evaluate the performance of robust models on binary classification problems [BCZ + 18]. We defer the discussion of multi-class problems to Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Upper bound</head><p>Our analysis leads to a guarantee on the performance of surrogate loss minimization. Intuitively, by Eqn. (1),</p><formula xml:id="formula_10">R rob (f ) ? R * nat = R nat (f ) ? R * nat + R bdy (f ) ? ? ?1 (R ? (f ) ? R * ? ) + R bdy (f ),</formula><p>where the last inequality holds because we choose ? as a classification-calibrated loss <ref type="bibr" target="#b4">[BJM06]</ref>. This leads to the following result.</p><formula xml:id="formula_11">Theorem 3.1. Let R ? (f ) := E?(f (X)Y ) and R * ? := min f R ? (f ).</formula><p>Under Assumption 1, for any nonnegative loss function ? such that ?(0) ? 1, any measurable f : X ? R, any probability distribution on X ? {?1}, and any ? &gt; 0, we have 1</p><formula xml:id="formula_12">R rob (f ) ? R * nat ? ? ?1 (R ? (f )?R * ? )+Pr[X?B(DB(f ), ), f (X)Y &gt; 0] ? ? ?1 (R ? (f )?R * ? ) + E max X ?B(X, ) ?(f (X )f (X)/?).</formula><p>Quantity governing model robustness. Our result provides a formal justification for the existence of adversarial examples: learning models are vulnerable to small adversarial attacks because the probability that data lie around the decision boundary of the model,</p><formula xml:id="formula_13">Pr[X ? B(DB(f ), ), f (X)Y &gt; 0]</formula><p>, is large. As a result, small perturbations may move the data point to the wrong side of the decision boundary, leading to weak robustness of classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lower bound</head><p>We now establish a lower bound on R rob (f ) ? R * nat . Our lower bound matches our analysis of the upper bound in Section 3.1 up to an arbitrarily small constant. </p><formula xml:id="formula_14">? ? ? E max X ?B(X, ) ?(f (X )f (X)/?) ? R ? (f ) ? R * ? ? ? ? ? E max X ?B(X, ) ?(f (X )f (X)/?) + ?.</formula><p>Theorem 3.2 demonstrates that in the presence of extra conditions on the loss function, i.e., lim x?+? ?(x) = 0, the upper bound in Section 3.1 is tight. The condition holds for all the losses in <ref type="table" target="#tab_0">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithmic Design for Defenses</head><p>Optimization. Theorems 3.1 and 3.2 shed light on algorithmic designs of adversarial defenses. In order to minimize R rob (f ) ? R * nat , the theorems suggest minimizing 2</p><formula xml:id="formula_15">min f E ?(f (X)Y ) for accuracy + max X ?B(X, ) ?(f (X)f (X )/?) regularization for robustness .<label>(3)</label></formula><p>We name our method TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization).</p><p>Intuition behind the optimization. Problem (3) captures the trade-off between the natural and robust errors: the first term in (3) encourages the natural error to be optimized by minimizing the "difference" between f (X) and Y , while the second regularization term encourages the output to be smooth, that is, it pushes the decision boundary of classifier away from the sample instances via minimizing the "difference" between the prediction of natural example f (X) and that of adversarial example f (X ). This is conceptually consistent with the argument that smoothness is an indispensable property of robust models [CBG + 17]. The tuning parameter ? plays a critical role on balancing the importance of natural and robust errors. To see how the ? affects the solution in the example of Section 2.4, problem (3) tends to the Bayes optimal classifier when ? ? +?, and tends to the all-one classifier when ? ? 0.</p><p>Comparisons with prior work. We compare our approach with several related lines of research in the prior literature. One of the best known algorithms for adversarial defense is based on robust optimization [MMS + 18, KW18, WSMK18, RSL18a, RSL18b]. Most results in this direction involve algorithms that approximately minimize</p><formula xml:id="formula_16">min f E max X ?B(X, ) ?(f (X )Y ) ,<label>(4)</label></formula><p>where the objective function in problem (4) serves as an upper bound of the robust error R rob (f ). In complex problem domains, however, this objective function might not be tight as an upper bound of the robust error, and may not capture the trade-off between natural and robust errors. A related line of research is adversarial training by regularization [MMIK18, KGB17, RDV17, ZSLG16]. There are several key differences between the results in this paper and those of [KGB17, <ref type="bibr" target="#b20">RDV17,</ref><ref type="bibr" target="#b46">ZSLG16]</ref>. Firstly, the optimization formulations are different. In the previous works, the regularization term either measures the "difference" between f (X ) and Y [KGB17], or its gradient <ref type="bibr" target="#b20">[RDV17]</ref>. In contrast, our regularization term measures the "difference" between f (X) and f (X ). While [ZSLG16] generated the adversarial example X by adding random Gaussian noise to X, our method simulates the adversarial example by solving the inner maximization problem in Eqn. (3). Secondly, we note that the losses in [MMIK18, KGB17, RDV17, ZSLG16] lack of theoretical guarantees. Our loss, with the presence of the second term in problem (3), makes our theoretical analysis significantly more subtle. Moreover, our algorithm takes the same computational resources as [KGB17], which makes our method scalable to large-scale datasets. We defer the experimental comparisons of various regularization based methods to <ref type="table" target="#tab_4">Table 5</ref>.</p><p>Differences with Adversarial Logit Pairing. We also compare TRADES with Adversarial Logit Pairing (ALP) [KKG18, <ref type="bibr" target="#b15">EIA18]</ref>. The algorithm of ALP works as follows: given a fixed network f in each round, the algorithm firstly generates an adversarial example X by solving argmax X ?B(X, ) ?(f (X )Y ); ALP then updates the network parameter by solving a minimization problem</p><formula xml:id="formula_17">min f E ??(f (X )Y ) + (1 ? ?)?(f (X)Y ) + f (X) ? f (X ) 2 /? ,</formula><p>where 0 ? ? ? 1 is a regularization parameter; the algorithm finally repeats the above-mentioned procedure until it converges. We note that there are fundamental differences between TRADES and ALP. While ALP simulates adversarial example X by the FGSM k attack, TRADES simulates X by solving argmax X ?B(X, ) ?(f (X)f (X )/?). Moreover, while ALP uses the 2 loss between f (X) and f (X ) to regularize the training procedure without theoretical guarantees, TRADES uses the classification-calibrated loss according to Theorems 3.1 and 3.2.</p><p>Heuristic algorithm. In response to the optimization formulation (3), we use two heuristics to achieve more general defenses: a) extending to multi-class problems by involving multi-class calibrated loss; b) approximately solving the minimax problem via alternating gradient descent. For multi-class problems, a surrogate loss Algorithm 1 Adversarial training by TRADES 1: Input:</p><p>Step sizes ? 1 and ? 2 , batch size m, number of iterations K in inner optimization, network architecture parametrized by ? 2: Output: Robust network f ? 3: Randomly initialize network f ? , or initialize network with pre-trained configuration 4: repeat 5:</p><p>Read mini-batch B = {x 1 , ..., x m } from training set 6:</p><p>for i = 1, ..., m (in parallel) do 7:</p><formula xml:id="formula_18">x i ? x i + 0.001 ? N (0, I),</formula><p>where N (0, I) is the Gaussian distribution with zero mean and identity variance 8:</p><p>for k = 1, ..., K do 9: </p><formula xml:id="formula_19">x i ? ? B(x i , ) (? 1 sign(? x i L(f ? (x i ), f ? (x i ))) + x i ),</formula><formula xml:id="formula_20">? ? ? ? ? 2 m i=1 ? ? [L(f ? (x i ), y i ) + L(f ? (x i ), f ? (x i ))/?]/m 13: until training converged</formula><p>is calibrated if minimizers of the surrogate risk are also minimizers of the 0-1 risk <ref type="bibr" target="#b18">[PS16]</ref>. Examples of multi-class calibrated loss include cross-entropy loss. Algorithmically, we extend problem (3) to the case of multi-class classifications by replacing ? with a multi-class calibrated loss L(?, ?):</p><formula xml:id="formula_21">min f E L(f (X), Y ) + max X ?B(X, ) L(f (X), f (X ))/? ,<label>(5)</label></formula><p>where f (X) is the output vector of learning model (with softmax operator in the top layer for the cross-entropy loss L(?, ?)), Y is the label-indicator vector, and ? &gt; 0 is the regularization parameter. One can also exchange f (X) and f (X ) in the second term of (5). The pseudocode of adversarial training procedure, which aims at minimizing the empirical form of problem (5), is displayed in Algorithm 1.</p><p>The key ingredient of the algorithm is to approximately solve the linearization of inner maximization in problem (5) by the projected gradient descent (see Step 7). We note that x i is a global minimizer with zero gradient to the objective function g(x ) := L(f (x i ), f (x )) in the inner problem. Therefore, we initialize x i by adding a small, random perturbation around x i in Step 5 to start the inner optimizer. More exhaustive approximations of the inner maximization problem in terms of either optimization formulations or solvers would lead to better defense performance.</p><p>Semi-supervised learning. We note that TRADES problem (5) can be straightforwardly applied to the semisupervised learning framework, as the second term in problem (5) does not depend on the label Y . Therefore, with more unlabeled data points, one can approximate the second term (in the expectation form) better by the empirical loss minimization. There are many interesting recent works which explore the benefits of invloving unlabeled data [CRS + 19, SFK + 19, ZCH + 19].</p><p>Acceleration. Adversarial training is typically more than 10x slower than natural training. To resolve this issue for TRADES, [SNG + 19, ZZL + 19] proposed new algorithms to solve problem (5) at negligible additional cost compared to natural training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we verify the effectiveness of TRADES by numerical experiments. We denote by A rob (f ) = 1 ? R rob (f ) the robust accuracy, and by A nat (f ) = 1 ? R nat (f ) the natural accuracy on test dataset. We (3), and use the hinge loss in <ref type="table" target="#tab_0">Table 2</ref> as the surrogate loss ?, where the associated ?-transform is ?(?) = ?.</p><p>To verify the tightness of our upper bound, we calculate the left hand side in Theorem 3.1, i.e.,</p><formula xml:id="formula_22">? LHS = R rob (f ) ? R * nat ,</formula><p>and the right hand side, i.e.,</p><formula xml:id="formula_23">? RHS = (R ? (f ) ? R * ? ) + E max X ?B(X, ) ?(f (X )f (X)/?).</formula><p>As we cannot have access to the unknown distribution D, we approximate the above expectation terms by test dataset. We first use natural training method to train a classifier so as to approximately estimate R * nat and R * ? , where we find that the naturally trained classifier can achieve natural error R * nat = 0%, and loss value R * ? = 0.0 for the binary classification problem. Next, we optimize problem (3) to train a robust classifier f . We take perturbation = 0.1, number of iterations K = 20 and run 30 epochs on the training dataset. Finally, to approximate the second term in ? RHS , we use FGSM k (white-box) attack (a.k.a. PGD attack) [KGB17] with 20 iterations to approximately calculate the worst-case perturbed data X .</p><p>The results in <ref type="table" target="#tab_2">Table 3</ref> show the tightness of our upper bound in Theorem 3.1. It shows that the differences between ? RHS and ? LHS under various ?'s are very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sensitivity of regularization hyperparameter ?</head><p>The regularization parameter ? is an important hyperparameter in our proposed method. We show how the regularization parameter affects the performance of our robust classifiers by numerical experiments on two datasets, MNIST and CIFAR10. For both datasets, we minimize the loss in Eqn. (5) to learn robust classifiers for multi-class problems, where we choose L as the cross-entropy loss.</p><p>MNIST setup. We use the CNN which has two convolutional layers, followed by two fully-connected layers. The output size of the last layer is 10. We set perturbation = 0.1, perturbation step size ? 1 = 0.01, number of iterations K = 20, learning rate ? 2 = 0.01, batch size m = 128, and run 50 epochs on the training dataset. To evaluate the robust error, we apply FGSM k (white-box) attack with 40 iterations and 0.005 step size. The results are in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>CIFAR10 setup. We apply ResNet-18 [HZRS16] for classification. The output size of the last layer is 10. We set perturbation = 0.031, perturbation step size ? 1 = 0.007, number of iterations K = 10, learning rate ? 2 = 0.1, batch size m = 128, and run 100 epochs on the training dataset. To evaluate the robust error, we apply FGSM k (white-box) attack with 20 iterations and the step size is 0.003. The results are in <ref type="table" target="#tab_3">Table 4</ref>. We observe that as the regularization parameter 1/? increases, the natural accuracy A nat (f ) decreases while the robust accuracy A rob (f ) increases, which verifies our theory on the trade-off between robustness and accuracy. Note that for MNIST dataset, the natural accuracy does not decrease too much as the regularization term 1/? increases, which is different from the results of CIFAR10. This is probably because the classification task for MNIST is easier. Meanwhile, our proposed method is not very sensitive to the choice of ?. Empirically, when we set the hyperparameter 1/? in [1, 10], our method is able to learn classifiers with both high robustness and high accuracy. We will set 1/? as either 1 or 6 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adversarial defenses under various attacks</head><p>Previously, <ref type="bibr" target="#b1">[ACW18]</ref> showed that 7 defenses in ICLR 2018 which relied on obfuscated gradients may easily break down. In this section, we verify the effectiveness of our method with the same experimental setup under both white-box and black-box threat models.</p><p>MNIST setup. We use the CNN architecture in <ref type="bibr" target="#b12">[CW17]</ref> with four convolutional layers, followed by three fully-connected layers. We set perturbation = 0.3, perturbation step size ? 1 = 0.01, number of iterations K = 40, learning rate ? 2 = 0.01, batch size m = 128, and run 100 epochs on the training dataset.</p><p>CIFAR10 setup. We use the same neural network architecture as [MMS + 18], i.e., the wide residual network WRN-34-10 [ZK16]. We set perturbation = 0.031, perturbation step size ? 1 = 0.007, number of iterations K = 10, learning rate ? 2 = 0.1, batch size m = 128, and run 100 epochs on the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">White-box attacks</head><p>We summarize our results in <ref type="table" target="#tab_4">Table 5</ref> together with the results from <ref type="bibr" target="#b1">[ACW18]</ref>. We also implement methods in [ZSLG16, KGB17, RDV17] on the CIFAR10 dataset as they are also regularization based methods. For MNIST dataset, we apply FGSM k (white-box) attack with 40 iterations and the step size is 0.01. For CIFAR10 dataset, we apply FGSM k (white-box) attack with 20 iterations and the step size is 0.003, under which the defense model in [MMS + 18] achieves 47.04% robust accuracy. <ref type="table" target="#tab_4">Table 5</ref> shows that our proposed defense method can significantly improve the robust accuracy of models, which is able to achieve robust accuracy as high as 56.61%. We also evaluate our robust model on MNIST dataset under the same threat model as in <ref type="bibr" target="#b24">[SKC18]</ref> (C&amp;W white-box attack <ref type="bibr" target="#b12">[CW17]</ref>), and the robust accuracy is 99.46%. See appendix for detailed information of models in <ref type="table" target="#tab_4">Table 5</ref>.    </p><formula xml:id="formula_24">A nat (f ) A rob (f ) [BRRG18] gradient mask [ACW18] CIFAR10 0.031 ( ? ) - 0% [MLW + 18] gradient mask [ACW18] CIFAR10 0.031 ( ? ) - 5% [DAL + 18] gradient mask [ACW18] CIFAR10 0.031 ( ? ) - 0% [SKN + 18] gradient mask [ACW18] CIFAR10 0.031 ( ? ) - 9% [NKM17] gradient mask [ACW18] CIFAR10 0.015 ( ? ) - 15% [WSMK18] robust opt. FGSM 20 (PGD) CIFAR10 0.031 ( ? ) 27.07% 23.54% [MMS + 18] robust opt. FGSM 20 (PGD) CIFAR10 0.031 ( ? ) 87.30% 47.04% [ZSLG16] regularization FGSM 20 (PGD) CIFAR10 0.031 ( ? ) 94.64% 0.15% [KGB17] regularization FGSM 20 (PGD) CIFAR10 0.031 ( ? ) 85.25% 45.89% [RDV17] regularization FGSM 20 (PGD) CIFAR10 0.031 ( ? ) 95.34% 0% TRADES (1/? = 1) regularization FGSM 1,000 (PGD) CIFAR10 0.031 ( ? ) 88.64% 48.90% TRADES (1/? = 6) regularization FGSM 1,000 (PGD) CIFAR10 0.031 ( ? )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Black-box attacks</head><p>We verify the robustness of our models under black-box attacks. We first train models without using adversarial training on the MNIST and CIFAR10 datasets. We use the same network architectures that are specified in the beginning of this section, i.e., the CNN architecture in <ref type="bibr" target="#b12">[CW17]</ref> and the WRN-34-10 architecture in <ref type="bibr" target="#b45">[ZK16]</ref>. We denote these models by naturally trained models (Natural For both datasets, we use FGSM k (black-box) method to attack various defense models. For MNIST dataset, we set perturbation = 0.3 and apply FGSM k (black-box) attack with 40 iterations and the step size is 0.01. For CIFAR10 dataset, we set = 0.031 and apply FGSM k (black-box) attack with 20 iterations and the step size is 0.003. Note that the setup is the same as the setup specified in Section 5.3.1. We summarize our results in <ref type="table" target="#tab_6">Table 6 and Table 7</ref>. In both tables, we use two source models (noted in the parentheses) to generate adversarial perturbations: we compute the perturbation directions according to the gradients of the source models on the input images. It shows that our models are more robust against black-box attacks transfered from naturally trained models and [MMS + 18]'s models. Moreover, our models can generate stronger adversarial examples for black-box attacks compared with naturally trained models and [MMS + 18]'s models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case study: NeurIPS 2018 Adversarial Vision Challenge</head><p>Competition settings. In the adversarial competition, the adversarial attacks and defenses are under the black-box setting. The dataset in this competition is Tiny ImageNet, which consists of 550,000 data (with  our data augmentation) and 200 classes. The robust models only return label predictions instead of explicit gradients and confidence scores. The task for robust models is to defend against adversarial examples that are generated by the top-5 submissions in the un-targeted attack track. The score for each defense model is evaluated by the smallest perturbation distance that makes the defense model fail to output correct labels.</p><p>Competition results. The methodology in this paper was applied to the competition, where our entry ranked the 1st place. We implemented our method to train ResNet models. We report the mean 2 perturbation distance of the top-6 entries in <ref type="figure" target="#fig_4">Figure 3</ref>. It shows that our method outperforms other approaches with a large margin. In particular, we surpass the runner-up submission by 11.41% in terms of mean 2 perturbation distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we study the problem of adversarial defenses against structural perturbations around input data. We focus on the trade-off between robustness and accuracy, and show an upper bound on the gap between robust error and optimal natural error. Our result advances the state-of-the-art work and matches the lower bound in the worst-case scenario. </p><formula xml:id="formula_25">x := x + sign(? x ?(f (x)y)),</formula><p>where x is the input instance, y is the label, f : X ? R is the score function (parametrized by deep nerual network for example) which maps an instance to its confidence value of being positive, and ?(?) is a surrogate of 0-1 loss. A more powerful yet natural extension of FGSM is the multi-step variant FGSM k (also known as PGD attack) [KGB17]. FGSM k applies projected gradient descent by k times: They can be adapted to the purpose of black-box attacks by running the algorithms on another similar network which is white-box to the algorithms [TKP + 18]. Though defenses that cause obfuscated gradients defeat iterative optimization based attacks, <ref type="bibr" target="#b1">[ACW18]</ref> showed that defenses relying on this effect can be circumvented.</p><formula xml:id="formula_26">x t+1 := ? B(x, ) (x t + sign(? x ?(f (x t )y))),</formula><p>Other attack methods include MI-FGSM [DLP + 18] and LBFGS attacks <ref type="bibr" target="#b34">[TV16]</ref>.</p><p>Robust optimization based defenses. Compared with attack methods, adversarial defense methods are relatively fewer. Robust optimization based defenses are inspired by the above-mentioned attacks. Intuitively, the methods train a network by fitting its parameters to the adversarial examples:</p><formula xml:id="formula_27">min f E max X ?B(X, ) ?(f (X )Y ) .<label>(6)</label></formula><p>Following this framework, [HXSS15, SYN15] considered one-step adversaries, while [MMS + 18] worked with multi-step methods for the inner maximization problem. There are, however, two critical differences between the robust optimization based defenses and the present paper. Firstly, robust optimization based defenses lack of theoretical guarantees. Secondly, such methods do not consider the trade-off between accuracy and robustness.</p><p>Relaxation based defenses. We mention another related line of research in adversarial defenses-relaxation based defenses. Given that the inner maximization in problem <ref type="formula" target="#formula_27">(6)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of Main Results</head><p>In this section, we provide the proofs of our main results.</p><p>B.1 Proof of Theorem 3.1</p><formula xml:id="formula_28">Theorem 3.1 (restated). Let R ? (f ) := E?(f (X)Y ) and R * ? := min f R ? (f ).</formula><p>Under Assumption 1, for any non-negative loss function ? such that ?(0) ? 1, any measurable f : X ? R, any probability distribution on X ? {?1}, and any ? &gt; 0, we have</p><formula xml:id="formula_29">R rob (f ) ? R * nat ? ? ?1 (R ? (f ) ? R * ? ) + Pr[X ? B(DB(f ), ), f (X)Y &gt; 0] ? ? ?1 (R ? (f ) ? R * ? ) + E max X ?B(X, ) ?(f (X )f (X)/?). Proof. By Eqn. (1), R rob (f ) ? R * nat = R nat (f ) ? R * nat + R bdy (f ) ? ? ?1 (R ? (f ) ? R * ? ) + R bdy (f ),</formula><p>where the last inequality holds because we choose ? as a classification-calibrated loss <ref type="bibr" target="#b4">[BJM06]</ref>. This leads to the first inequality.</p><p>Also, notice that</p><formula xml:id="formula_30">Pr[X ? B(DB(f ), ), f (X)Y &gt; 0] ? Pr[X ? B(DB(f ), )] = E max X ?B(X, ) 1{f (X ) = f (X)} = E max X ?B(X, ) 1{f (X )f (X)/? &lt; 0} ? E max X ?B(X, ) ?(f (X )f (X)/?).</formula><p>This leads to the second inequality. </p><formula xml:id="formula_31">? ? ? E max X ?B(X, ) ?(f (X )f (X)/?) ? R ? (f ) ? R * ? ? ? ? ? E max X ?B(X, ) ?(f (X )f (X)/?) + ?.</formula><p>Proof. The first inequality follows from Theorem 3.1. Thus it suffices to prove the second inequality. Fix &gt; 0 and ? ? [0, 1]. By the definition of ? and its continuity, we can choose ?, ? 1 , ? 2 ? [0, 1] such that ? = ?? 1 + (1 ? ?)? 2 and ?(?) ? ??(? 1 ) + (1 ? ?)?(? 2 ) ? /3. For two distinct points x 1 , x 2 ? X , we set P X such that Pr[X = x 1 ] = ?, Pr[X = x 2 ] = 1 ? ?, ?(x 1 ) = (1 + ? 1 )/2, and ?(x 2 ) = (1 + ? 2 )/2. By the definition of H ? , we choose function f : R d ? R such that f (x) &lt; 0 for all x ? X , C ?(x 1 ) (f (x 1 )) ? H ? (?(x 1 )) + /3, and C ?(x 2 ) (f (x 2 )) ? H ? (?(x 2 )) + /3. By the continuity of ?, there is an &gt; 0 such that ?(?) ? ?(? ? 0 ) + /3 for all 0 ? 0 &lt; . We also note that there exists an ? 0 &gt; 0 such that for any 0 &lt; ? &lt; ? 0 , we have</p><formula xml:id="formula_32">0 ? E max X ?B(X, ) ?(f (X )f (X)/?) &lt; .</formula><p>Thus, we have</p><formula xml:id="formula_33">R ? (f ) ? R * ? = E?(Y f (X)) ? inf f E?(Y f (X)) = ?[C ?(x 1 ) (f (x 1 )) ? H(?(x 1 ))] + (1 ? ?)[C ?(x 2 ) (f (x 2 )) ? H(?(x 2 ))] ? ?[H ? (?(x 1 )) ? H(?(x 1 ))] + (1 ? ?)[H ? (?(x 2 )) ? H(?(x 2 ))] + /3 = ??(? 1 ) + (1 ? ?)?(? 2 ) + /3 ? ?(?) + 2 /3 ? ? ? ? E max X ?B(X, ) ?(f (X )f (X)/?) + .</formula><p>Furthermore, by Lemma C.6,</p><formula xml:id="formula_34">R rob (f ) ? R * nat = E[1{sign(f (X)) = sign(f * (X)), X ? B(DB(f ), ) ? }|2?(X) ? 1|] + Pr[X ? B(DB(f ), ), sign(f * (X)) = Y ] = E|2?(X) ? 1| = ?(2?(x 1 ) ? 1) + (1 ? ?)(2?(x 2 ) ? 1) = ?,</formula><p>where f * is the Bayes optimal classifier which outputs "positive" for all data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extra Theoretical Results</head><p>In this section, we provide extra theoretical results for adversarial defenses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Adversarial vulnerability under log-concave distributions</head><p>provided that the marginal distribution over X is products of log-concave measures. A measure is log-concave if the logarithm of its density is a concave function. The class of log-concave measures contains many well-known (classes of) distributions as special cases, such as Gaussian and uniform measure over ball. Our results are inspired by the isoperimetric inequality of log-concave distributions by the work of <ref type="bibr" target="#b2">[Bar01]</ref>. Intuitively, the isoperimetric problem consists in finding subsets of prescribed measure, such that its measure increases the less under enlargement. Our analysis leads to the following guarantee on the quantity (7).  The isoperimetric function is</p><formula xml:id="formula_36">I ? = inf{? + (A) : ?(A) = 1/2}.<label>(8)</label></formula><p>Before proceeding, we cite the following results from <ref type="bibr" target="#b2">[Bar01]</ref>. If M (x) is a convex function, then for every integer d, we have I ? ?d ? I ? ?d , where ? is the Gaussian measure with mean 0 and variance 1/(2?). In particular, among sets of measure 1/2 for ? ?d , the halfspace [0, ?) ? R d?1 is solution to the isoperimetric problem (8).</p><p>Now we are ready to prove Theorem C.1.</p><p>Proof. We note that</p><formula xml:id="formula_37">Pr[X ? B(DB(f ), )] = Pr[X ? B(DB(f ), ), sign(f (X)) = +1] + Pr[X ? B(DB(f ), )</formula><p>, sign(f (X)) = ?1].</p><p>To apply Lemma C.2, we set the A in Lemma C.2 as the event {sign(f (X)) = +1}. Therefore, the set</p><formula xml:id="formula_38">A = {X ? B(DB(f ), )</formula><p>, sign(f (X)) = ?1}.</p><p>By Lemma C.2, we know that for linear classifier f 0 which represents the halfspace [0, ?) ? R d?1 , and any classifier f ,</p><formula xml:id="formula_39">lim inf ?+0 Pr X?? ?d [X ? B(DB(f ), ), sign(f (X)) = ?1] ? Pr[sign(f (X)) = +1] ? lim inf ?+0 Pr X?? ?d [X ? B(DB(f 0 ), ), sign(f 0 (X)) = ?1] ? Pr[sign(f 0 (X)) = +1] .<label>(9)</label></formula><p>Similarly, we have</p><formula xml:id="formula_40">lim inf ?+0 Pr X?? ?d [X ? B(DB(f ), ), sign(f (X)) = +1] ? Pr[sign(f (X)) = ?1] ? lim inf ?+0 Pr X?? ?d [X ? B(DB(f 0 ), ), sign(f 0 (X)) = +1] ? Pr[sign(f 0 (X)) = ?1] .<label>(10)</label></formula><p>Adding both sides of Eqns. (9) and (10), we have</p><formula xml:id="formula_41">lim inf ?+0 Pr X?? ?d [X ? B(DB(f ), )] ? lim inf ?+0 Pr X?? ?d [X ? B(DB(f 0 ), )] ? c.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Margin based generalization bounds</head><p>Before proceeding, we first cite a useful lemma. We say that function f 1 : R ? R and f 2 : R ? R have a ? separator if there exists a function f 3 :</p><formula xml:id="formula_42">R ? R such that |h 1 ? h 2 | ? ? implies f 1 (h 1 ) ? f 3 (h 2 ) ? f 2 (h 1 ).</formula><p>For any given function f 1 and ? &gt; 0, one can always construct f 2 and f 3 such that f 1 and f 2 have a ?-separator f 3 by setting f 2 (h) = sup |h?h |?2? f 1 (h ) and f 3 (h) = sup |h?h |?? f 1 (h ).</p><p>Lemma C.3 (Corollary 1, <ref type="bibr" target="#b44">[Zha02]</ref>). Let f 1 be a function R ? R. Consider a family of functions f ? 2 : R ? R, parametrized by ?, such that 0 ? f 1 ? f ? 2 ? 1. Assume that for all ?, f 1 and f ? 2 has a ? separator. Assume also that f ? 2 (z) ? f ? 2 (z) when ? ? ? . Let ? 1 &gt; ? 2 &gt; ... be a decreasing sequence of parameters, and p i be a sequence of positive numbers such that ? i=1 p i = 1, then for all ? &gt; 0, with probability of at least 1 ? ? over data:</p><formula xml:id="formula_43">E (X,Y )?D f 1 (L(w, X, Y )) ? 1 n n i=1 f ? 2 (L(w, x i , y i )) + 32 n ln 4N ? (L, ? i , x 1:n ) + ln 1 p i ?</formula><p>for all w and ?, where for each fixed ?, we use i to denote the smallest index such that ? i ? ?.</p><p>Lemma C.4 (Theorem 4, <ref type="bibr" target="#b44">[Zha02]</ref>). If x p ? b and w q ? a, where 2 ? p &lt; ? and 1/p + 1/q = 1, then ?? &gt; 0, log 2 N ? (L, ?, n) ? 36(p ? 1) a 2 b 2 ? 2 log 2 [2 4ab/? + 2 + 1].</p><p>Theorem C.5. Suppose that the data is 2-norm bounded by x 2 ? b. Consider the family ? of linear classifier</p><formula xml:id="formula_44">w with w 2 = 1. Let R rob (w) := E (X,Y )?D 1[?X rob ? B 2 (X, ) such that Y w T X rob ? 0].</formula><p>Then with probability at least 1 ? ? over n random samples (x i , y i ) ? D, for all margin width ? &gt; 0 and w ? ?, we have</p><formula xml:id="formula_45">R rob (w) ? 1 n n i=1 1(?x rob i ? B(x i , ) s.t. y i w T x rob i ? 2?) + C n b 2 ? 2 ln n + ln 1 ? .</formula><p>Proof. The theorem is a straightforward result of Lemmas C.3 and C.4 with L(w, x, y) = min</p><formula xml:id="formula_46">x rob ?B(x, ) yw T x rob , f 1 (g) = 1(g ? 0) and f ? 2 (h) = sup |g?h|&lt;2? f 1 (g) = f 1 (g ? 2?) = 1(g ? 2?),</formula><p>and ? i = b/2 i and p i = 1/2 i .</p><p>We note that for the 2 ball B 2 (x, ) = {x :</p><p>x ? x 2 ? }, we have</p><formula xml:id="formula_47">1(?x rob i ? B(x i , ) s.t. y i w T x rob i ? 2?) = max x rob i ?B(x i , ) 1(y i w T x rob i ? 2?) = 1(y i w T x i ? 2? + ).</formula><p>Therefore, we can design the following algorithm-Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Adversarial Training of Linear Separator via Structural Risk Minimization</head><p>Input: Samples (x 1:n , y 1:n ) ? D, a bunch of margin parameters ? 1 , ..., ? T . 1: For k = 1, 2, ..., T 2: Solve the minimax optimization problem:</p><p>L k (w * k , x 1:n , y 1:n ) = min</p><formula xml:id="formula_48">w?S(0,1) 1 n n i=1 max x rob i ?B(x i , ) 1(y i w T x rob i ? 2? k ) = min w?S(0,1) 1 n n i=1 1(y i w T x i ? 2? k + ).</formula><p>3: End For 4: k * = argmin k L k (w * k , x 1:n , y 1:n ) + C n b 2 ? 2 k ln n + ln 1 ? . Output: Hypothesis w k * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 A lemma</head><p>We denote by f * (?) := 2?(?) ? 1 the Bayes decision rule throughout the proofs.</p><p>Lemma C.6. For any classifier f , we have </p><formula xml:id="formula_49">= E[1{Y = 1}1{?X ? B(X, ) s.t. sign(f (X )) = ?1}|X = x] + E[1{Y = ?1}1{?X ? B(X, ) s.t. sign(f (X )) = 1}|X = x] = 1{?x ? B(x, ) s.t. sign(f (x )) = ?1}E1{Y = 1|X = x} + 1{?x ? B(x, ) s.t. sign(f (x )) = 1}E1{Y = ?1|X = x} = 1{?x ? B(x, ) s.t. sign(f (x )) = ?1}?(x) + 1{?x ? B(x, ) s.t. sign(f (x )) = 1}(1 ? ?(x)) = 1, x ? B(DB(f ), ), 1{sign(f (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x)), otherwise.</formula><p>Therefore,</p><formula xml:id="formula_50">R rob (f ) = X Pr[?X ? B(X, ) s.t. sign(f (X )) = Y |X = x]d Pr X (x) = B(DB(f ), ) Pr[?X ? B(X, ) s.t. sign(f (X )) = Y |X = x]d Pr X (x) + B(DB(f ), ) ? Pr[?X ? B(X, ) s.t. sign(f (X )) = Y |X = x]d Pr X (x) = Pr(X ? B(DB(f ), )) + B(DB(f ), ) ? [1{sign(f (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x))]d Pr X (x).</formula><p>We have </p><formula xml:id="formula_51">R rob (f ) ? R nat (f * ) = Pr(X ? B(DB(f ), )) + B(DB(f ), ) ? [1{sign(f (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x))]d Pr X (x) ? B(DB(f ), ) ? [1{sign(f * (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x))]d Pr X (x) ? B(DB(f ), ) [1{sign(f * (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x))]d Pr X (x) = Pr(X ? B(DB(f ), )) ? B(DB(f ), ) [1{sign(f * (x)) = ?1}(2?(x) ? 1) + (1 ? ?(x))]d Pr X (x) + E[1{sign(f (X)) = sign(?(X) ? 1/2), X ? B(DB(f ), ) ? }|2?(X) ? 1|] = Pr(X ? B(DB(f ), )) ? E[1{X ? B(DB(f ), )} min{?(X), 1 ? ?(X)}]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extra Experimental Results</head><p>In this section, we provide extra experimental results to verify the effectiveness of our proposed method TRADES.</p><p>D.1 Experimental setup in Section 5.3.1</p><p>We use the same model, i.e., the WRN-34-10 architecture in <ref type="bibr" target="#b45">[ZK16]</ref>, to implement the methods in <ref type="bibr" target="#b46">[ZSLG16]</ref>, [KGB17] and <ref type="bibr" target="#b20">[RDV17]</ref>. The experimental setup is the same as TRADES, which is specified in the beginning of Section 5. For example, we use the same batch size and learning rate for all the methods. More specifically, we find that using one-step adversarial perturbation method like FGSM in the regularization term, defined in [KGB17], cannot defend against FGSM k (white-box) attack. Therefore, we use FGSM k with the cross-entropy loss to calculate the adversarial example X in the regularization term, and the perturbation step size ? 1 and number of iterations K are the same as in the beginning of Section 5.</p><p>As for defense models in <ref type="table" target="#tab_4">Table 5</ref>, we implement the 'TRADES' models, the models trained by using other regularization losses in [KGB17, <ref type="bibr" target="#b20">RDV17,</ref><ref type="bibr" target="#b46">ZSLG16]</ref>, and the defense model 'Madry' in the antepenultimate line of <ref type="table" target="#tab_4">Table 5</ref>. We evaluate [WSMK18]'s model based on the checkpoint provided by the authors. The rest of the models in <ref type="table" target="#tab_4">Table 5</ref> are reported in <ref type="bibr" target="#b1">[ACW18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Extra attack results in Section 5.3.1</head><p>Extra white-box attack results are provided in <ref type="table" target="#tab_12">Table 8</ref>. DeepFool ( 2 ) CIFAR10 0.031 ( ? ) 88.64% 84.49% TRADES (1/? = 6.0) FGSM CIFAR10 0.031 ( ? ) 84.92% 61.06% TRADES (1/? = 6.0) DeepFool ( 2 ) CIFAR10 0.031 ( ? ) 84.92% 81.55%</p><p>The attacks in <ref type="table" target="#tab_4">Table 5</ref> and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Extra attack results in Section 5.3.2</head><p>Extra black-box attack results are provided in <ref type="table" target="#tab_14">Table 9 and Table 10</ref>. We apply black-box FGSM attack on the MNIST dataset and the CIFAR10 dataset.  D.5 Interpretability of the robust models trained by TRADES</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5.1 Adversarial examples on MNIST and CIFAR10 datasets</head><p>In this section, we provide adversarial examples on MNIST and CIFAR10. We apply foolbox 3 [RBB17] to generate adversarial examples, which is able to return the smallest adversarial perturbations under the ? norm distance. The adversarial examples are generated by using FGSM k (white-box) attack on the models described in Section 5, including Natural models, Madry's models and TRADES models. Note that the FGSM k attack is foolbox.attacks.LinfinityBasicIterativeAttack in foolbox. See <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref> for the adversarial examples of different models on MNIST and CIFAR10 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5.2 Adversarial examples on Bird-or-Bicycle dataset</head><p>We find that the robust models trained by TRADES have strong interpretability. To see this, we apply a (spatial-tranformation-invariant) version of TRADES to train ResNet-50 models in response to the unrestricted adversarial examples of the bird-or-bicycle dataset [BCZ + 18]. The dataset is bird-or-bicycle, which consists of 30,000 pixel-224 ? 224 images with label either 'bird' or 'bicycle'. The unrestricted threat models include structural perturbations, rotations, translations, resizing, 17+ common corruptions, etc. We show in <ref type="figure" target="#fig_13">Figures 7 and 8</ref> the adversarial examples by the boundary attack with random spatial transformation on our robust model trained by the variant of TRADES. The boundary attack [BRB18] is a black-box attack method which searches for data points near the decision boundary and attack robust models by these data points. Therefore, the adversarial images obtained by boundary attack characterize the images around the decision boundary of robust models. We attack our model by boundary attack with random spatial transformations, a baseline in the competition. The classification accuracy on the adversarial test data is as high as 95% (at 80% coverage), even though the adversarial corruptions are perceptible to human. We observe that the robust model trained by TRADES has strong interpretability: in <ref type="figure" target="#fig_13">Figure 7</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left figure: decision boundary learned by natural training method. Right figure: decision boundary learned by our adversarial training method, where the orange dotted line represents the decision boundary in the left figure. It shows that both methods achieve zero natural training error, while our adversarial training method achieves better robust training error than the natural training method. against natural accuracy. Statistically, robustness can be be at odds with accuracy [TSE + 19]. This has led to an empirical line of work on adversarial defense that incorporates various kinds of assumptions [SZC + 18, KGB17]. On the theoretical front, methods such as relaxation based defenses [KW18, RSL18a] provide provable guarantees for adversarial robustness. They, however, ignore the performance of classifier on the non-adversarial examples, and thus leave open the theoretical treatment of the putative robustness/accuracy trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Counterexample given by Eqn. (2). the robustness of a score function f : X ? R, [SST + 18, CBM18, BPR18] defined robust (classification) error under the threat model of bounded perturbation: R rob (f ) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[0, 1] ? [0, ?) by ? = ? * * , where ?(?) := H ? 1+? 2 ? H 1+? 2 . Indeed, the function ?(?) is the largest convex lower bound on H ? 1+? 2 surrogate loss ? is to the class of non-classification-calibrated losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 3. 2 .</head><label>2</label><figDesc>Suppose that |X | ? 2. Under Assumption 1, for any non-negative loss function ? such that ?(x) ? 0 as x ? +?, any ? &gt; 0, and any ? ? [0, 1], there exists a probability distribution on X ? {?1}, a function f : R d ? R, and a regularization parameter ? &gt; 0 such that R rob (f ) ? R * nat = ? and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Top-6 results (out of~2,000 submissions) in the NeurIPS 2018 Adversarial Vision Challenge. The vertical axis represents the mean 2 perturbation distance that makes robust models fail to output correct labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where x t is the t-th iteration of the algorithm with x 0 := x and ? B(x, ) is the projection operator onto the ball B(x, ). Both FGSM and FGSM k are approximately solving (the linear approximation of) maximization problem: max x ?B(x, ) ?(f (x )y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>B. 2</head><label>2</label><figDesc>Proof of Theorem 3.2 Theorem 3.2 (restated). Suppose that |X | ? 2. Under Assumption 1, for any non-negative loss function ? such that ?(x) ? 0 as x ? +?, any ? &gt; 0, and any ? ? [0, 1], there exists a probability distribution on X ? {?1}, a function f : R d ? R, and a regularization parameter ? &gt; 0 such that R rob (f ) ? R * nat = ? and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 3 .</head><label>3</label><figDesc>1 states that for any classifier f , the value Pr[X ? B(DB(f ), )] characterizes the robustness of the classifier. In this section, we show that among all classifiers such that Pr[sign(f (X)) = +1] = 1/2, linear classifier minimizes lim inf ?+0 Pr[X ? B(DB(f ), )] ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>&lt;</head><label></label><figDesc>Red area of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Left figure: boundary neighborhood of linear classifier. Rightfigure:boundary neighborhood of non-linear classifier. Theorem C.1 shows that the mass of S linear is smaller than the mass of S non-linear , provided that the underlying distribution over the instance space is the products of log-concave distribution on the real line.Theorem C.1. Let ? be an absolutely continuous log-concave probability measure on R with even density function and let ? ?d be the products of ? with dimension d.Denote by d? = e ?M (x) , where M : R ? [0, ?] is convex. Assume that M (0) = 0. If M (x) isa convex function, then for every integer d and any classifier f with Pr[sign(f (X)) = +1] = 1/2, we have lim inf ?+0 Pr X?? ?d [X ? B(DB(f ), )] ? c for an absolute constant c &gt; 0. Furthermore, among all such probability measures and classifiers, the linear classifier over products of Gaussian measure with mean 0 and variance 1/(2?) achieves the lower bound. Theorem C.1 claims that under the products of log-concave distributions, the quantity Pr[X ? B(DB(f ), )] increases with rate at least ?( ) for all classifier f , among which the linear classifier achieves the minimal value. C.1.1 Proofs of Theorem C.1 For a Borel set A and for &gt; 0, denote by A = {x : d(x, A) ? }. The boundary measure of A is then defined as ? + (A) = lim inf ?+0 ?(A ) ? ?(A) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma C. 2 (</head><label>2</label><figDesc>Theorem 9, [Bar01]). Let ? be an absolutely continuous log-concave probability measure on R with even density function. Denote by d? = e ?M (x) , where M : R ? [0, ?] is convex. Assume that M (0) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>R</head><label></label><figDesc>rob (f ) ? R * nat =E[1{sign(f (X)) = sign(f * (X)), X ? B(DB(f ), ) ? }|2?(X) ? 1|] + Pr[X ? B(DB(f ), ), sign(f * (X)) = Y ].Proof. For any classifier f , we havePr(?X ? B(X, ) s.t. sign(f (X )) = Y |X = x) = Pr(Y = 1, ?X ? B(X, ) s.t. sign(f (X )) = ?1|X = x) + Pr(Y = ?1, ?X ? B(X, ) s.t. sign(f (X )) = 1|X = x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>+</head><label></label><figDesc>E[1{sign(f (X)) = sign(?(X) ? 1/2), X ? B(DB(f ), ) ? }|2?(X) ? 1|] = E[1{X ? B(DB(f ), )} max{?(X), 1 ? ?(X)}] + E[1{sign(f (X)) = sign(?(X) ? 1/2), X ? B(DB(f ), ) ? }|2?(X) ? 1|] = Pr[X ? B(DB(f ), ), sign(f * (X)) = Y ] + E[1{sign(f (X)) = sign(f * (X)), X ? B(DB(f ), ) ? }|2?(X) ? 1|].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>all of adversarial images have obvious feature of 'bird', while in Figure 8 all of adversarial images have obvious feature of 'bicycle'. This shows that images around the decision boundary of truly robust model have features of both classes. (a) clean example (b) adversarial example by boundary attack with random spatial transformation (c) clean example (d) adversarial example by boundary attack with random spatial transformation (e) clean example (f) adversarial example by boundary attack with random spatial transformation Adversarial examples by boundary attack with random spatial transformation on the ResNet-50 model trained by a variant of TRADES. The ground-truth label is 'bicycle', and our robust model recognizes the adversarial examples correctly as 'bicycle'. It shows in the second column that all of adversarial images have obvious feature of 'bird' (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>adversarial example by boundary attack with random spatial transformation (c) clean example (d) adversarial example by boundary attack with random spatial transformation (e) clean example (f) adversarial example by boundary attack with random spatial transformation Adversarial examples by boundary attack with random spatial transformation on the ResNet-50 model trained by a variant of TRADES. The ground-truth label is 'bird', and our robust model recognizes the adversarial examples correctly as 'bird'. It shows in the second column that all of adversarial images have obvious feature of 'bicycle' (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Examples of classification-calibrated loss ? and associated ?-transform. Here ? log</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where ? is the projection operator</figDesc><table><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell>end for</cell></row><row><cell>12:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Theoretical verification on the optimality of Theorem 3.1. We verify the tightness of the established upper bound in Theorem 3.1 for binary classification problem on MNIST dataset. The negative examples are '1' and the positive examples are '3'.Here we use a Convolutional Neural Network (CNN) with two convolutional layers, followed by two fully-connected layers. The output size of the last layer is 1. To learn the robust classifier, we minimize the regularized surrogate loss in Eqn.</figDesc><table><row><cell cols="2">? A rob (f ) (%)</cell><cell>R ? (f )</cell><cell>? = ? RHS ? ? LHS</cell></row><row><cell>2.0</cell><cell>99.43</cell><cell>0.0006728</cell><cell>0.006708</cell></row><row><cell>3.0</cell><cell>99.41</cell><cell>0.0004067</cell><cell>0.005914</cell></row><row><cell>4.0</cell><cell>99.37</cell><cell>0.0003746</cell><cell>0.006757</cell></row><row><cell>5.0</cell><cell>99.34</cell><cell>0.0003430</cell><cell>0.005860</cell></row><row><cell cols="4">release our code and trained models at https://github.com/yaodongyu/TRADES.</cell></row><row><cell cols="2">5.1 Optimality of Theorem 3.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Sensitivity of regularization hyperparameter ? on MNIST and CIFAR10 datasets.MNIST CIFAR10 1/? A rob (f ) (%) A nat (f ) (%) A rob (f ) (%) A nat (f ) (%) 0.191.09 ? 0.0385 99.41 ? 0.0235 26.53 ? 1.1698 91.31 ? 0.0579 0.2 92.18 ? 0.0450 99.38 ? 0.0094 37.71 ? 0.6743 89.56 ? 0.2154 0.4 93.21 ? 0.0660 99.35 ? 0.0082 41.50 ? 0.3376 87.91 ? 0.2944 0.6 93.87 ? 0.0464 99.33 ? 0.0141 43.37 ? 0.2706 87.50 ? 0.1621 0.8 94.32 ? 0.0492 99.31 ? 0.0205 44.17 ? 0.2834 87.11 ? 0.2123 1.0 94.75 ? 0.0712 99.28 ? 0.0125 44.68 ? 0.3088 87.01 ? 0.2819 2.0 95.45 ? 0.0883 99.29 ? 0.0262 48.22 ? 0.0740 85.22 ? 0.0543 3.0 95.57 ? 0.0262 99.24 ? 0.0216 49.67 ? 0.3179 83.82 ? 0.4050 4.0 95.65 ? 0.0340 99.16 ? 0.0205 50.25 ? 0.1883 82.90 ? 0.2217 5.0 95.65 ? 0.1851 99.16 ? 0.0403 50.64 ? 0.3336 81.72 ? 0.0286</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of TRADES with prior defense models under white-box attacks.</figDesc><table><row><cell>Defense</cell><cell>Defense type Under which attack</cell><cell>Dataset</cell><cell>Distance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of TRADES with prior defenses under black-box FGSM 40 attack on the MNIST dataset. The models inside parentheses are source models which provide gradients to adversarial attackers. We provide the average cross-entropy loss value L(f (X), Y ) of each defense model in the bracket. The defense model 'Madry' is the same model as in the antepenultimate line ofTable 5. The defense model 'TRADES' is the same model as in the penultimate line ofTable 5.</figDesc><table><row><cell>Defense Model</cell><cell cols="2">Robust Accuracy A rob (f )</cell></row><row><cell>Madry</cell><cell cols="2">97.43% [0.0078484] (Natural)</cell></row><row><cell>TRADES</cell><cell cols="2">97.63% [0.0075324] (Natural)</cell></row><row><cell>Madry</cell><cell>97.38% [0.0084962]</cell><cell>(Ours)</cell></row><row><cell>TRADES</cell><cell cols="2">97.66% [0.0073532] (Madry)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of TRADES with prior defenses under black-box FGSM 20 attack on the CIFAR10 dataset. The models inside parentheses are source models which provide gradients to adversarial attackers.</figDesc><table><row><cell>Defense Model</cell><cell cols="2">Robust Accuracy A rob (f )</cell></row><row><cell>Madry</cell><cell cols="2">84.39% [0.0519784] (Natural)</cell></row><row><cell>TRADES</cell><cell cols="2">87.60% [0.0380258] (Natural)</cell></row><row><cell>Madry</cell><cell>66.00% [0.1252672]</cell><cell>(Ours)</cell></row><row><cell>TRADES</cell><cell cols="2">70.14% [0.0885364] (Madry)</cell></row></table><note>We provide the average cross-entropy loss value of each defense model in the bracket. The defense model 'Madry' is implemented based on [MMS + 18], and the defense model 'TRADES' is the same model as in the 11th line of Table 5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The bounds motivate us to minimize a new form of regularized surrogate loss, TRADES, for adversarial training. Experiments on real datasets and adversarial competition demonstrate the effectiveness of our proposed algorithms. It would be interesting to combine our methods with other related line of research on adversarial defenses, e.g., feature denoising technique [XWvdM + 18] and network architecture design [CBG + 17], to achieve more robust learning systems. [GAG + 18] Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. Motivating the rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732, 2018. Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. [HPG + 17] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017. [HWC + 17] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defenses: Ensembles of weak defenses are not strong. arXiv preprint arXiv:1706.04701, 2017. Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Although deep neural networks have achieved great progress in various areas [ZSS19, ZXJ + 18], they are brittle to adversarial attacks. Adversarial attacks have been extensively studied in the recent years. One of the baseline attacks to deep nerual networks is the Fast Gradient Sign Method (FGSM) [GSS15]. FGSM computes an adversarial example as</figDesc><table><row><cell cols="2">[GSS15] Ian J Goodfellow, [HXSS15] Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv?ri. Learning with a strong</cell></row><row><cell></cell><cell>adversary. arXiv preprint arXiv:1511.03034, 2015.</cell></row><row><cell>[HZRS16]</cell><cell>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image</cell></row><row><cell></cell><cell>recognition. In IEEE conference on computer vision and pattern recognition, pages 770-778,</cell></row><row><cell></cell><cell>2016.</cell></row><row><cell>[MDFF16]</cell><cell>Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple</cell></row><row><cell></cell><cell>and accurate method to fool deep neural networks. In IEEE Conference on Computer Vision</cell></row><row><cell></cell><cell>and Pattern Recognition, pages 2574-2582, 2016.</cell></row><row><cell>[MLW + 18]</cell><cell>Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Michael E Houle,</cell></row><row><cell></cell><cell>Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using</cell></row><row><cell></cell><cell>local intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.</cell></row><row><cell>[MMIK18]</cell><cell>Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training:</cell></row><row><cell></cell><cell>a regularization method for supervised and semi-supervised learning. IEEE Transactions on</cell></row><row><cell></cell><cell>Pattern Analysis and Machine Intelligence, 2018.</cell></row><row><cell>[MMS + 18]</cell><cell>Aleksander Madry,</cell></row></table><note>[JL17] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Empirical Methods in Natural Language Processing, 2017.[KGB17] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In International Conference on Learning Representations, 2017.[KKG18] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.[KW18] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, 2018.[MC17] Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In ACM SIGSAC Conference on Computer and Communications Security, pages 135-147, 2017.[NKM17] Taesik Na, Jong Hwan Ko, and Saibal Mukhopadhyay. Cascade adversarial machine learning regularized with a unified embedding. arXiv preprint arXiv:1708.02582, 2017.A Other Related Works Attack methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>might be hard to solve due to the nonconvexity nature of deep neural networks, [KW18] and<ref type="bibr" target="#b21">[RSL18a]</ref> considered a convex outer approximation of the set of activations reachable through a norm-bounded perturbation for one-hidden-layer neural networks.[WSMK18] later scaled the methods to larger models, and<ref type="bibr" target="#b22">[RSL18b]</ref> proposed a tighter convex approximation. [SND18, VNS + 18] considered a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball. These approaches, however, do not apply when the activation function is ReLU.Theoretical progress. Despite a large amount of empirical works on adversarial defenses, many fundamental questions remain open in theory. There are a few preliminary explorations in recent years.<ref type="bibr" target="#b17">[FFF18]</ref> derived upper bounds on the robustness to perturbations of any classification function, under the assumption that the data is generated with a smooth generative model. From computational aspects,<ref type="bibr" target="#b6">[BPR18,</ref><ref type="bibr" target="#b5">BLPR18]</ref> showed that adversarial examples in machine learning are likely not due to information-theoretic limitations, but rather it could be due to computational hardness. From statistical aspects, [SST + 18] showed that the sample complexity of robust training can be significantly larger than that of standard training. This gap holds irrespective of the training algorithm or the model family.<ref type="bibr" target="#b10">[CBM18]</ref> and<ref type="bibr" target="#b41">[YRB18]</ref> studied the uniform convergence of robust error R rob (f ) by extending the classic VC and Rademacher arguments to the case of adversarial learning, respectively. A recent work demonstrates the existence of trade-off between accuracy and robustness [TSE + 19], without providing a practical algorithm to address it.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Results of our method TRADES under different white-box attacks.</figDesc><table><row><cell>Defense</cell><cell>Under which attack</cell><cell>Dataset</cell><cell>Distance</cell><cell>A nat (f ) A rob (f )</cell></row><row><cell>TRADES (1/? = 1.0)</cell><cell>FGSM</cell><cell cols="3">CIFAR10 0.031 ( ? ) 88.64% 56.38%</cell></row><row><cell>TRADES (1/? = 1.0)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc>include FGSM k [KGB17], DeepFool ( ? ) [MDFF16], LBFGSAttack [TV16], MI-FGSM [DLP + 18], C&amp;W [CW17], FGSM [KGB17], and DeepFool ( 2 ) [MDFF16].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of TRADES with prior defense models under black-box FGSM attack on the MNIST dataset. The models inside parentheses are source models which provide gradients to adversarial attackers.</figDesc><table><row><cell>Defense Model</cell><cell cols="2">Robust Accuracy A rob (f )</cell></row><row><cell>Madry</cell><cell>97.68% (Natural)</cell><cell>98.11% (Ours)</cell></row><row><cell>TRADES</cell><cell cols="2">97.75% (Natural) 98.44% (Madry)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Comparisons of TRADES with prior defense models under black-box FGSM attack on the CIFAR10 dataset. The models inside parentheses are source models which provide gradients to adversarial attackers.The robust accuracy of [MMS + 18]'s CNN model is 96.01% on the MNIST dataset. The robust accuracy of [MMS + 18]'s WRN-34-10 model is 47.66% on the CIFAR10 dataset. Note that we use the same white-box attack method introduced in the Section 5.3.1, i.e., FGSM 20 , to evaluate the robust accuracies of Madry's models.</figDesc><table><row><cell>Defense Model</cell><cell cols="2">Robust Accuracy A rob (f )</cell></row><row><cell>Madry</cell><cell>84.02% (Natural)</cell><cell>67.66% (Ours)</cell></row><row><cell>TRADES</cell><cell cols="2">86.84% (Natural) 71.52% (Madry)</cell></row><row><cell cols="2">D.4 Experimental setup in Section 5.3.2</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We study the population form of the risk functions, and mention that by incorporating the generalization theory for classificationcalibrated losses<ref type="bibr" target="#b4">[BJM06]</ref> one can extend the analysis to finite samples. We leave this analysis for future research.2 For simplicity of implementation, we do not use the function ? ?1 and rely on ? to approximately reflect the effect of ? ?1 , the trade-off between the natural error and the boundary error, and the tight approximation of the boundary error using the corresponding surrogate loss function.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Link: https://foolbox.readthedocs.io/en/latest/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Maria-Florina Balcan, Avrim Blum, Zico Kolter, and Aleksander M?dry for valuable comments and discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perturbed Image</head> <ref type="figure">Figure 5</ref><p>: Adversarial examples on MNIST dataset. In each subfigure, the image in the first row is the original image and we list the corresponding correct label beneath the image. We show the perturbed images in the second row. The differences between the perturbed images and the original images, i.e., the perturbations, are shown in the third row. In each column, the perturbed image and the perturbation are generated by FGSM k (white-box) attack on the model listed below. The labels beneath the perturbed images are the predictions of the corresponding models, which are different from the correct labels. We record the smallest perturbations in terms of ? norm that make the models predict a wrong label.  <ref type="figure">Figure 6</ref>: Adversarial examples on CIFAR10 dataset. In each subfigure, the image in the first row is the original image and we list the corresponding correct label beneath the image. We show the perturbed images in the second row. The differences between the perturbed images and the original images, i.e., the perturbations, are shown in the third row. In each column, the perturbed image and the perturbation are generated by FGSM k (white-box) attack on the model listed below. The labels beneath the perturbed images are the predictions of the corresponding models, which are different from the correct labels. We record the smallest perturbations in terms of ? norm that make the models predict a wrong label (best viewed in color).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ADef: an iterative algorithm to construct adversarial deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rima</forename><surname>Alaifari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">S</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tandri</forename><surname>Gauksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extremal properties of central half-spaces for product measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Barthe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="107" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08352</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Unrestricted adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial examples from cryptographic pseudo-random generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Tat</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Razenshteyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06418</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Razenshteyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10204</idno>
		<title level="m">Adversarial examples from computational constraints</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PAC-learning in the presence of adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><forename type="middle">Nitin</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="228" to="239" />
		</imprint>
	</monogr>
	<note>CRS + 19</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Duchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13736</idno>
		<title level="m">Unlabeled data improves adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10272</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A rotation and a translation suffice: Fooling CNNs with simple transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1186" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiclass classification calibration functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Bernardo ?vila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szepesv?ri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06385</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Foolbox v0. 8.0: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09404</idno>
		<title level="m">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semidefinite relaxations for certifying robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10899" to="10909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Are labels required for improving adversarial robustness?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13725</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SKN + 18</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free! arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>M?dry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5019" to="5031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding adversarial training: Increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy? -a comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5025" to="5034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5339" to="5349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Jh Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Xwvdm + 18] Cihang Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03411</idno>
		<title level="m">Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising for improving adversarial robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>XWZ + 17</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatially transformed adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11914</idno>
		<title level="m">Rademacher complexity for adversarially robust generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adversarially robust generalization just requires more unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtian</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00555</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The limitations of adversarial training and the blind-spot attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Covering number bounds of certain regularized linear function classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="527" to="550" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep neural networks with multibranch architectures are intrinsically less non-convex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1099" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08010</idno>
		<title level="m">Stackelberg GAN: Towards provable minimax equilibrium via multi-generator architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
