<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Multi-View 3D Human Pose Estimation using Semantic Feedback to Smart Edge Sensors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bultmann</surname></persName>
							<email>bultmann@ais.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Intelligent Systems</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
							<email>behnke@cs.uni-bonn.de</email>
							<affiliation key="aff1">
								<orgName type="department">Autonomous Intelligent Systems</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Multi-View 3D Human Pose Estimation using Semantic Feedback to Smart Edge Sensors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted for Robotics: Science and Systems (RSS), July 2021.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method for estimation of 3D human poses from a multi-camera setup, employing distributed smart edge sensors coupled with a backend through a semantic feedback loop. 2D joint detection for each camera view is performed locally on a dedicated embedded inference processor. Only the semantic skeleton representation is transmitted over the network and raw images remain on the sensor board. 3D poses are recovered from 2D joints on a central backend, based on triangulation and a body model which incorporates prior knowledge of the human skeleton. A feedback channel from backend to individual sensors is implemented on a semantic level. The allocentric 3D pose is backprojected into the sensor views where it is fused with 2D joint detections. The local semantic model on each sensor can thus be improved by incorporating global context information. The whole pipeline is capable of realtime operation. We evaluate our method on three public datasets, where we achieve state-of-the-art results and show the benefits of our feedback architecture, as well as in our own setup for multi-person experiments. Using the feedback signal improves the 2D joint detections and in turn the estimated 3D poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Accurate perception of humans is a challenging task with many applications in robotics and computer vision. It is a prerequisite e.g., for safe navigation and anticipative movement of robots in the vicinity of people and can enable human-robot interaction or augmented reality scenarios.</p><p>In this work, we address the task of 3D human pose estimation in allocentric world coordinates from a calibrated multicamera setup. Most state-of-the-art methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b9">9]</ref> follow a two-step approach: First, 2D pose detections are generated for each available view (cf. <ref type="figure" target="#fig_0">Fig. 1 bottom)</ref>. Second, detections from multiple views are fused into a 3D human pose estimate and post-processed using a skeleton model (cf. <ref type="figure" target="#fig_0">Fig. 1</ref> top-left). Many recent methods focus more on accuracy than efficiency. They are thus difficult to employ in real-world scenarios with real-time constraints.</p><p>We propose a novel architecture for real-time multi-view 3D human pose estimation using distributed smart edge sensors for 2D pose estimation. Each camera view is interpreted locally using an embedded inference accelerator. The 2D human poses are streamed over a network to a central backend, where data association, triangulation and post-processing are performed to fuse the 2D detections into 3D skeletons. Furthermore, we propose a semantic feedback channel from backend to smart edge sensors. The allocentric 3D pose estimate is backprojected into the respective local views where it is combined with the joint detections. Thus, global context information can be incorporated into the local 2D semantic model of the individual smart edge sensor, which improves the pose estimation result.</p><p>The use of distributed smart edge sensors has several advantages over the centralized approaches more common in literature. As the images are processed directly on the sensor boards, raw images are not sent to the backend and only the 2D pose information has to be transmitted over the network. This significantly reduces the required communication bandwidth and furthermore mitigates privacy issues, as the abstract semantic information contains no personal details. Moreover, using a dedicated inference accelerator for each camera lessens hardware requirements on the backend side, which, in a centralized architecture, can quickly become the bottleneck. On the other hand, using an embedded sensor platform poses challenges, as the employed vision models need to meet the limitations of the hardware. For this, we propose a lightweight 2D pose estimation model for efficient image arXiv:2106.14729v1 [cs.CV] 28 Jun 2021 processing locally on the sensors, on the edge of the network.</p><p>In summary, the main contributions of our work are:</p><p>? a new real-time method for multi-view 3D human pose estimation dividing the computation between smart edge sensors performing image analysis locally for each camera view and a backend fusing the semantic interpretations of individual views and using a computationally efficient skeleton model to incorporate prior knowledge, ? a novel 3D / 2D feedback architecture enabling bidirectional communication on a semantic level between sensors and backend, and ? an extensive evaluation of the proposed approach on the single-person H3.6M dataset <ref type="bibr" target="#b15">[15]</ref>, the multi-person Campus and Shelf datasets <ref type="bibr" target="#b1">[2]</ref>, and in own multi-person experiments in a less-controlled environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Human pose estimation from multi-camera input has been investigated for many years in the computer vision and robotics communities. Early works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">5]</ref> use manually designed image features, such as HOG descriptors <ref type="bibr" target="#b8">[8]</ref>, for 2D part detection and combine multiple views using a graphbased body model. With the increasing success of deeplearning methods, more recent approaches <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b9">9]</ref> employ 2D convolutional neural networks (CNNs) for human joint detection <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b30">30]</ref> and recover the 3D pose using variants of the Pictorial Structures Model (PSM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">5]</ref>. In these approaches, the body model consists of a graph with 3D joint locations as nodes and pairwise articulation constraints on the edges. While the PSM body model recovers 3D poses accurately, it is computationally very expensive and generally not realtime capable, due to a large volumetric grid used as discrete state space for optimization. In our work, we also employ a graph-based body model but use a fast iterative optimization scheme <ref type="bibr" target="#b18">[18]</ref>, achieving real-time operation.</p><p>Qiu et al. <ref type="bibr" target="#b24">[24]</ref> present an approach for cross-view fusion to improve the estimated 2D poses of individual camera views. 3D poses are recovered using an offline recursive PSM implementation with a processing time of several seconds per frame <ref type="bibr" target="#b26">[26]</ref>. In our work, we take up the idea of across-sensor viewpoint fusion but propose a different formulation. Qiu et al. <ref type="bibr" target="#b24">[24]</ref> implement the fusion between perspectives on a purely 2D basis, using epipolar constraints. Hence, a 2D joint in one view will be associated with all features on the corresponding epipolar lines of other views, which can be ambiguous. In contrast, we implement a semantic 3D / 2D feedback channel from backend to sensors based on reprojection of the estimated 3D skeleton into the individual camera views.</p><p>Several recent methods with a focus on computational efficiency have been proposed <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b26">26]</ref>. In these approaches, 3D pose estimation is based on direct triangulation of 2D joint detections without usage of an expensive body model. Chen et al. <ref type="bibr" target="#b7">[7]</ref> propose a fast iterative triangulation scheme but assume 2D pose detections as given. Remelli et al. <ref type="bibr" target="#b26">[26]</ref> consider the whole pipeline including 2D keypoint estimation but use a fully centralized approach while our method employs distributed sensors for 2D pose estimation.</p><p>Naikal et al. <ref type="bibr" target="#b21">[21]</ref> proposed a system for human joint detection and action recognition using a network of smart cameras transmitting only abstract image features to a central processing station. However, at the time, no CNN-based vision models were available for pose estimation on mobile devices, limiting the performance of their framework. Furthermore, their communication channel is purely feed-forward-no feedback for viewpoint fusion is implemented.</p><p>Research interest in computer vision models that run efficiently also on mobile and embedded devices has significantly increased in recent years. MobiPose <ref type="bibr" target="#b32">[32]</ref> investigates human pose estimation on smartphone SoCs without dedicated inference accelerators, using motion vector-based tracking. Xiao et al. <ref type="bibr" target="#b30">[30]</ref> propose a simple CNN architecture consisting only of a feature extractor and a deconvolutional head but use a standard ResNet backbone <ref type="bibr" target="#b12">[12]</ref>. Popular lightweight backbone architectures include EfficientNet <ref type="bibr" target="#b27">[27]</ref> and MobileNets <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref>. These architectures greatly reduce the number of parameters w.r.t. standard CNN feature extractors like ResNet, e.g., by replacing convolutions with depthwise-separable convolutions. Moreover, tensor processors for inference acceleration, like the Google Edge TPU <ref type="bibr" target="#b31">[31]</ref>, can be employed to efficiently run a CNN vision model within a limited size and energy budget. For compatibility with the Edge TPU, weights and activations of the model need to be quantized to 8-bit integer values using a quantization scheme as proposed by Jacob et al. <ref type="bibr" target="#b16">[16]</ref>.</p><p>To the best of our knowledge, the proposed framework is the first approach for real-time 3D human pose estimation using multiple smart edge sensors which perform 2D pose estimation on-device and incorporate global context via semantic feedback from the backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>An overview of our proposed approach is given in <ref type="figure" target="#fig_1">Fig. 2</ref>. We consider scenarios where N calibrated cameras with known projection matrices P i perceive a scene with one or several individuals from multiple viewpoints. Our method is described for the single person case in the following. Extensions to handle multiple persons are described in Sec. III-E. 2D locations of a fixed set of J human joints {u j i } J j=1 in camera view i, corresponding confidence values c j i and covariance matrices ? j i are calculated directly on the respective smart edge sensor board using the vision model described in Sec. III-A. The 2D pose information is then transmitted over a network to a central backend, using the robot operation system (ROS) <ref type="bibr" target="#b25">[25]</ref> as middleware for communication.</p><p>The clocks of sensors and backend are software-synchronized and each 2D pose message includes a timestamp representing the capture time of the corresponding image. Sets of N corresponding messages, one for each view, are determined based on these timestamps, and raw 3D poses are recovered via triangulation as detailed in Sec. III-B.</p><p>A skeleton model (cf. Sec. III-C), incorporating prior information on the typical bone-lengths of the human skeleton, is  then applied and outputs the final estimated 3D pose.</p><p>A semantic feedback channel from backend to sensors is implemented as described in Sec. III-D, which enables each individual view to benefit from the fused 3D information. For this, first, a prediction step is performed to compensate for the pipeline delay. Second, the predicted 3D skeleton is reprojected into each camera view and sent to the sensors where it is incorporated into the local 2D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Human Pose Estimation on Smart Edge Sensor</head><p>The smart edge sensor platform employed in this work (cf. <ref type="figure" target="#fig_0">Fig. 1</ref> top-right) is based on the Google Edge TPU Dev Board <ref type="bibr" target="#b10">[10]</ref>, equipped with an ARM Cortex-A53 quad-core processor, the Edge TPU inference accelerator and 1 GB of shared RAM. A 5 MP RGB camera is connected to the board via the MIPI-CSI2 interface.</p><p>We adopt the CNN architecture of Xiao et al. <ref type="bibr" target="#b30">[30]</ref> for 2D human pose estimation, consisting of a backbone feature extractor and three transposed convolution layers to extract heatmaps from image features. To achieve real-time performance on the mobile sensor platform, we exchange the ResNet backbone used by Xiao et al. <ref type="bibr" target="#b30">[30]</ref> with the significantly more lightweight MobileNetV3 feature extractor <ref type="bibr" target="#b13">[13]</ref>. Furthermore, for execution on the Edge TPU, the model is quantized for 8-bit integer inference using post-training quantization <ref type="bibr" target="#b16">[16]</ref> as implemented in the TensorFlow ML framework <ref type="bibr" target="#b0">[1]</ref>. In multiperson scenarios, a detector is also run on the sensor boards to provide person crops for the pose estimation network. It is based on the Single Shot Detector (SSD) architecture <ref type="bibr" target="#b20">[20]</ref>, also using the MobileNetV3 backbone.</p><p>The output heatmaps H det of the pose estimation model are a multi-channel image with one channel per joint, encoding the confidence of a joint being present at the pixel location. 2D joint locations u j = [u j , v j ] T are inferred as global maxima of the resp. heatmap channel, as single person crops are processed. The value of the heatmap at the joint position gives the corresponding confidence c j . Only joints with confidence above a minimum threshold are considered as valid detections.</p><p>The covariance matrices ? j are determined as proposed by Pasqualetto et at. <ref type="bibr" target="#b22">[22]</ref>: Heatmap pixels with values above a threshold contribute to the empirical covariance with their xand y-locations, weighted by the respective confidence:</p><formula xml:id="formula_0">? j = ? 2 xx ? 2 xy ? 2 xy ? 2 yy ,<label>(1)</label></formula><formula xml:id="formula_1">? 2 xy = 1 K K k=1 c j k x k ? u j ? y k ? v j ,<label>(2)</label></formula><p>where K is the number of contributing pixels and the mean is replaced by the peak u j to model a distribution about the detected 2D joint location. Some representative examples of heatmaps and extracted covariances are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The uncertainty in the heatmaps, including their directionality, is well captured by the covariance ellipses. Note, that the dispersion for asymmetric heatmaps, as in the third example of <ref type="figure" target="#fig_2">Fig. 3</ref>, is overestimated by the proposed procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-View Fusion</head><p>The 3D positionx j of each joint j is recovered from a set of 2D detections {u j i } N i=1 via triangulation using the Direct Linear Transform (DLT) <ref type="bibr" target="#b11">[11]</ref>. The relationship between 2D points u i = [u i , v i ] T from camera view i ? {1, . . . , N } and 3D pointx can be written as:</p><formula xml:id="formula_2">Ax = 0 ,<label>(3)</label></formula><p>with</p><formula xml:id="formula_3">A = ? ? ? ? ? ? u 1 p T 1,3 ? p T 1,1 v 1 p T 1,3 ? p T 1,2 ? ? ? u N p T N,3 ? p T N,1 v N p T N,3 ? p T N,2 ? ? ? ? ? ? ? R 2N ?4 ,<label>(4)</label></formula><p>wherex ? R 4 are the homogeneous coordinates ofx and p T i,k denotes the k-th row of projection matrix P i ? R 3?4 . According to the DLT algorithm <ref type="bibr" target="#b11">[11]</ref>, (3) is solved by a singular value decomposition (SVD) on A, taking the unit singular vector corresponding to the smallest singular value of A as solution forx. Finally,x is divided by its fourth coordinate to obtain the 3D vectorx =x/(x) <ref type="bibr" target="#b4">4</ref> .</p><p>The above formulation <ref type="formula" target="#formula_2">(3)</ref> assumes that all 2D detections make a similar contribution to the triangulation. However, 2D joint positions cannot be estimated reliably on some views, e.g., due to occlusions, which in turn degrades the result. The reliability of a detection is expressed by the heatmap confidence value c i and can be incorporated into the DLT by multiplying each row of A with the corresponding element of a weight vector w, reformulating (3) as:</p><formula xml:id="formula_4">(w ? A)x = 0 ,<label>(5)</label></formula><p>with</p><formula xml:id="formula_5">w = c 1 a T 1 , c 1 a T 2 , . . . , c N a T 2N ?1 , c N a T 2N<label>(6)</label></formula><p>and ? the Hadamard product, similar to the approach of Chen et al. <ref type="bibr" target="#b7">[7]</ref>. The confidence values in w are divided by the L 2norm of the corresponding row of A to compensate for the different image locations of the joints in each view.</p><p>To obtain the 3D joint positionx j and its covariance? j 3D , deterministic samples are propagated through the triangulation according to the Unscented Transform <ref type="bibr" target="#b17">[17]</ref>. Sigma points are generated from the mean vector ? j = [u jT 1 , . . . , u jT N ] T and the block-diagonal matrix containing the 2D covariances ? j i extracted from each heatmap. Each set of samples is triangulated according to (5) andx j and? j 3D are determined as sample mean and covariance of the resulting points using weights given by the Unscented Transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Skeleton Model</head><p>We employ a factor graph model <ref type="bibr" target="#b18">[18]</ref> representing the tree structure of the human body, with 3D joint positions x j as nodes connected by unary and pairwise factors on the edges.</p><p>The unary constraints are given by the triangulated joint positionsx j and covariances? j 3D and follow a 3D Gaussian noise model:</p><formula xml:id="formula_6">f x j ? N x j |x j ,? j 3D .<label>(7)</label></formula><p>The pairwise factors model typical limb-lengths of the human body and also follow a Gaussian noise model:</p><formula xml:id="formula_7">g x j , x k ? N x j ? x k | l j,k , ? l ,<label>(8)</label></formula><p>with x j ? x k the Euclidean distance between joints x j and x k . l j,k and ? l denote mean and standard deviation of the length of the corresponding limb determined from the statistics of the H3.6M dataset <ref type="bibr" target="#b15">[15]</ref>. The final 3D human poses are obtained by optimizing the factor graph using the Levenberg-Marquardt algorithm and the gtsam-framework <ref type="bibr" target="#b18">[18]</ref>. The optimization is initialized with the poses from the previous frame, predicted using a linear velocity model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Semantic Feedback</head><p>To enable the local semantic models of each sensor to benefit from the globally fused 3D pose, a feedback channel from backend to sensors is implemented in our framework.</p><p>First, the motion of the 3D skeleton is predicted using a linear velocity model for each joint to compensate for the pipeline delay ?t. Second, predicted 2D joint positions {? j i } and their image-plane covariances {? j i } are determined by reprojecting the predicted 3D pose and its covariance extracted from the factor graph into each sensor view i using the projection matrix P i and the Unscented Transform <ref type="bibr" target="#b17">[17]</ref>.</p><p>The reprojected feedback skeleton is sent to the smart edge sensors, where a feedback heatmap H fb is rendered to be fused with the detected heatmap H det of the current image crop. For each joint? j , a 2D Gaussian blob is rendered in the corresponding heatmap channel according to the reprojected covariance matrix? j .</p><p>The heatmaps are fused via weighted addition of detection, feedback, and their element-wise multiplication:</p><formula xml:id="formula_8">H fused = s ((1????) H det + ?H fb + ? (H fb ? H det )) ,<label>(9)</label></formula><p>with ? + ? &lt; 1. The scale s is set as (1 ? ? ? ?) ?1 to ensure that positive feedback always increases the joint confidence. The feedback gains ? and ? are important design parameters of our method. A sufficient weight must be accorded to the feedback to improve the raw detections, but too high gains can cause instability. Hence, the feedback gains are learned using a hyper-parameter search <ref type="bibr" target="#b4">[4]</ref> optimizing the 3D pose error.</p><p>The above formulation models an arbitrary combination of additive and multiplicative feedback and can efficiently be executed on the embedded processor of the sensor board. Examples of the heatmap fusion are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Through the feedback loop, evidence for joint occurrence from detection and feedback is combined in the fused heatmap, improving the accuracy of the joint locations and reducing their uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-Person Pose Estimation</head><p>To handle real-world scenes (cf. Sec. IV-C and IV-D), we extend our method to estimate the poses of multiple persons at a time. Person detections are associated across camera views based on the epipolar distance of their joints using the efficient iterative greedy matching proposed by Tanke et al. <ref type="bibr" target="#b28">[28]</ref>. The rest of the pipeline is then run for each person observed in at least two views to compute 3D poses and feedback. A feedback skeleton is associated to its corresponding 2D detection based on the IoU overlap of their bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION AND EXPERIMENTS</head><p>We evaluate the proposed approach on three widely-used public datasets: The Human 3.6M dataset <ref type="bibr" target="#b15">[15]</ref> and the multi-person Campus and Shelf datasets <ref type="bibr" target="#b1">[2]</ref>, as well as on own data from experiments in our lab. We make our C++ implementation publicly available at https://github.com/AIS-Bonn/SmartEdgeSensor3DHumanPose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Metrics</head><p>1) Human 3.6M: The Human 3.6M dataset <ref type="bibr" target="#b15">[15]</ref> is a largescale public dataset for single-person multi-view 3D human pose estimation. It contains 3.6 million frames of 11 different actors, captured by four synchronized cameras together with ground truth 2D and 3D poses.</p><p>We measure the 2D pose estimation accuracy as the percentage of correctly detected joints, the Joint Detection Rate (JDR). A joint is correctly detected when its distance towards the corresponding ground-truth annotation is smaller than a threshold. We set the JDR threshold to half the head size as proposed by Qiu et al. <ref type="bibr" target="#b24">[24]</ref>.</p><p>The 3D pose accuracy is measured by the Mean Per Joint Position Error (MPJPE) between estimated 3D joints x j and ground truth 3D joints x j gt : MPJPE = 1 J J j=1 x j ? x j gt . 2) Campus and Shelf: The Campus dataset <ref type="bibr" target="#b1">[2]</ref> consists of three people interacting outdoors, captured by three calibrated cameras. The Shelf dataset <ref type="bibr" target="#b1">[2]</ref> consists of four people interacting and disassembling a shelf in a small indoor area, captured by five cameras. It is a more complex setting compared to Campus, as frequent occlusions occur between persons and with the shelf. The same evaluation protocol as in previous works <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> is used, employing the 3D Percentage of Correct Parts (PCP) metric <ref type="bibr" target="#b5">[5]</ref>. A body part is considered as correctly estimated if the average of the Euclidean distances of start and end point of the limb with the ground-truth is smaller than half the limb-length.</p><p>B. Evaluation on the H3.6M Dataset 1) Implementation Details: We adopt the network for pose estimation described in Sec. III-A and use two different training schemes: (i) training solely on H3.6M training data and (ii) pretraining the network on person keypoints from the COCO dataset <ref type="bibr" target="#b19">[19]</ref> and finetuning on H3.6M. The input resolution is set to 256?256 pixels.</p><p>As is common practice in literature <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29]</ref>, we use subjects 1, 5, 6, 7, 8 for training and subjects 9 and 11 for testing. Input images are cropped using the provided groundtruth bounding box and evaluation is performed for every 5 th frame as subsequent frames are highly similar at the original frame rate of 50 Hz.</p><p>All four image streams are processed simultaneously, each on its own smart edge sensor board. The estimated 2D skeletons are transmitted to the backend, where they are triangulated, and the skeleton model is applied. We report evaluation results with and without using the proposed feedback channel. The parameters for the heatmap fusion (9) are determined as ? = 0.15 and ? = 0.75.</p><p>2) Quantitative Results: Tab. I shows evaluation results for the accuracy of the 2D pose estimation calculated on the smart edge sensors, depending on the employed feedback mode and training data. Our experiments indicate that using the feedback channel (cf. Sec. III-D) significantly improves the JDR accuracy. The improvement is highest for the often-occluded wrist and ankle joints, 5.7 % resp. 5.8 % for the H3.6M-only model. For the better visible joint classes, detection is easier also without feedback and the improvement is smaller.</p><p>Pretraining the model on the COCO keypoint dataset generally improves performance, as the model trained on a larger and more varying dataset generalizes better to unknown scenes. For the stronger model, the gain from using the feedback signal is smaller, but still amounts to 2 % for the wrists which are the most difficult joints to detect.</p><p>The improved 2D joint detections in turn lead to more accurate 3D poses, as becomes apparent from the results in Tab. II, where 3D pose error is shown. As in the 2D case, the improvement from the feedback channel is more significant for the weaker model and highest for ankles and wrists, around 11 mm resp. 6 mm for the H3.6M-only network.</p><p>In Tab. III, the MPJPE 3D pose error is shown per action    <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b26">[26]</ref> as well as our method provide significant improvements over the older methods <ref type="bibr" target="#b23">[23]</ref> and <ref type="bibr" target="#b29">[29]</ref>. Comparing the models trained on H3.6M only, the results of our approach using feedback are better than Qiu et al. <ref type="bibr" target="#b24">[24]</ref> and Remelli et al. <ref type="bibr" target="#b26">[26]</ref> for 10 of the 15 action categories and reduce the average error. The proposed semantic feedback channel is key to this improvement over the literature. When using additional training data, our method also achieves state-of-the-art results. In Tab. IV, we compare the inference time per frame set (i.e., for a set of four images) and model size of our approach with the recent approaches <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b26">[26]</ref>. The approach of Qiu et al. <ref type="bibr" target="#b24">[24]</ref> is an offline method with a runtime of several seconds. The approach of Remelli et al. <ref type="bibr" target="#b26">[26]</ref> achieves near real-time performance on a powerful desktop GPU. The runtime of our method is still about 40 % lower while running on efficient embedded sensor boards and a backend that doesn't require a GPU. Our pose estimation model, optimized for the Edge TPU inference accelerator, requires only 12 MB of memory, significantly less than the models of other approaches.</p><p>Results of an ablation study on the impacts of different parts of our proposed pipeline are shown in Tab. V. Using the skeleton model to post-process the raw 3D poses obtained by triangulation significantly improves the average MPJPE. Employing the directional covariances extracted from the heatmaps, instead of modeling uncertainties only by the confidence value, again reduces the error. The semantic feedback further improves the result, where the proposed combination of additive and multiplicative feedback is more efficient than using only a single type. The impact of the feedback signal for each action class can also be observed in Tab. III. It improves the results for all actions for the H3.6M-only trained model, with an average improvement of 3.1 mm. When using the stronger pose estimation model trained on additional data, the average improvement amounts to 0.5 mm. The feedback signal is more important when the raw pose estimates are less accurate but reduces the average 3D pose error in both cases.</p><p>3) Qualitative Results: In addition, we qualitatively show how the proposed feedback loop improves the pose estimation result. <ref type="figure" target="#fig_4">Fig. 4</ref> shows three example situations, where the feedback heatmap helps to recover from incorrect or imprecise 2D joint detections. The images are overlaid with the respective heatmaps for a specific joint. In the first and second row, the left wrist of the actors is occluded by their body and the detected heatmap is very inaccurate. However, from the perspectives of other cameras, the joint is visible, and its 3D position can be estimated. This is reflected in the feedback heatmap which predicts the joint detection close to the ground truth location. The resulting fused heatmap, obtained by combining detection and feedback according to <ref type="bibr" target="#b9">(9)</ref>, permits to accurately estimate the respective joint despite the imprecise detection. In Row 3 of <ref type="figure" target="#fig_4">Fig. 4</ref>, a similar situation is shown, but for the right elbow, which here cannot be distinguished from the left elbow due to the challenging pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation on the Campus and Shelf Datasets</head><p>1) Implementation Details: To process multi-person scenes, a person detector is employed together with the pose estimation model (cf. Sec. III-A). The detector is trained for 130 Epochs on the person class of the COCO dataset <ref type="bibr" target="#b19">[19]</ref>, for input images of 640?480 px and achieves a mAP of 44.6 %.  validation set using ground-truth detections. Note, that the generic detector and pose estimation networks are employed without any fine-tuning on the evaluated datasets. The three or five image streams of the respective dataset are processed simultaneously on the sensor boards. The entire image is passed to the detector and image crops of the detected persons are analyzed by the pose estimation network. To improve the processing speed, the detector is only run once per second. In between, the crops are determined based on the detections of the previous frame. This is necessary, as alternating between models is inefficient on the Edge TPU, as parameter caching cannot come into effect in this case <ref type="bibr" target="#b31">[31]</ref>.</p><p>On the backend, the estimated 2D poses are synchronized based on their timestamps and the framework is run in multiperson mode as detailed in Sec. III-E. The feedback delay amounts to one frame during dataset processing.</p><p>2) Quantitative Results: We report evaluation results of PCP score and runtime on the Campus and Shelf datasets in Tab. VI. and compare our method with other approaches: Belagiannis et al. <ref type="bibr" target="#b2">[3]</ref> were among the first to propose 3D PSM-based multi-person pose estimation and exploit temporal consistency in videos. Dong et al. <ref type="bibr" target="#b9">[9]</ref> propose to reduce the PSM state-space and exploit appearance information for data association. Chen et al. <ref type="bibr" target="#b7">[7]</ref> propose a fast iterative triangulation scheme performing data association in 3D space.</p><p>In terms of PCP score, our method largely outperforms the older method <ref type="bibr" target="#b2">[3]</ref> and is on par with the recent approaches <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b7">7]</ref>. The overall result is improved by our method using feedback for both Campus and Shelf dataset in comparison to the literature. The improvement is most significant for Actor2 of the Shelf dataset, whose arms are often severely occluded, which can be resolved by the semantic feedback signal.</p><p>In terms of processing speed, our method does not reach the high frame rates of Chen et al. <ref type="bibr" target="#b7">[7]</ref> but achieves significant improvements over <ref type="bibr" target="#b9">[9]</ref> and <ref type="bibr" target="#b2">[3]</ref>. Furthermore, 2D poses are estimated online, at run-times of 30-40 ms per frame, while other methods use offline pre-computed keypoint detections. Our method is the only approach in the comparison providing a fully online multi-person pose estimation.  3) Qualitative Results: <ref type="figure" target="#fig_6">Fig. 5</ref> shows an exemplary scene of the Shelf dataset. The proposed semantic feedback improves the estimation of occluded wrist joints in 2D and 3D. Annotations for evaluation are only provided for two of the four actors in this scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments in Multi-Person Scenes</head><p>We further evaluate the proposed framework in online experiments in multi-person scenarios in our lab.</p><p>1) Implementation Details: 16 sensor boards are mounted under the ceiling of our lab in a roughly 12?16 m area. The cameras face downwards towards the center and run at 30 Hz and VGA resolution. We conduct experiments with a subset of 4 cameras, similar to the setting of the H3.6M dataset, as well as with all 16 sensors to demonstrate the scalability of our method to large-scale camera systems. The same detection and pose estimation models as for the Campus and Shelf dataset are employed in the experiments and the pipeline runs in multiperson mode (cf. Sec. III-E).</p><p>2) Quantitative Results: To analyze the consistency of the online pose estimation, we evaluate the error between detected 2D poses and fused 3D poses reprojected into the camera views in Tab. VII. The reprojection error decreases for all joints when using semantic feedback, indicating that the locally estimated 2D poses are more consistent with the globally fused 3D poses through the proposed feedback architecture. The error is slightly higher with 16 than with 4 sensors, probably due to the more difficult camera calibration and synchronization in the large-scale setup.  3) Qualitative Results: An exemplary real-world scene from the experiments conducted in our lab is shown in <ref type="figure" target="#fig_7">Fig. 6</ref>. The 16 camera views contain up to six persons and complex, cluttered backgrounds. Estimated 3D poses are reprojected onto the corresponding images to provide a visual evaluation. The reprojected skeletons closely fit the persons in the images, indicating that 3D and 2D poses are reliably estimated. People are reliably detected also at large distances to the cameras and occlusions by objects or other people can be resolved through the multi-view architecture and the semantic feedback.</p><p>The human poses are estimated online, in real time, and could directly be used, e.g., for human-robot interaction. Note, that the camera images are not transmitted during operation of our framework but are only shown for visualization. A video of the experiments is available on our website 1 . 4) Run-time Analysis: The average processing time per image crop on the sensor boards consists of 4.5 ms for pose estimation on the TPU and 6 ms on the ARM-CPU for preand post-processing and sums to 10.5 ms per detected person. Once per second, the person detector requires additional 20 ms on the TPU. Up to three persons can thus be tracked at the full camera frame rate of 30 Hz, six persons still at 15 Hz.</p><p>The backend processing on a desktop PC with an Intel i9-9900K CPU takes 10.7 ms in average per frame set for the 4-camera setup and 60.8 ms during the experiments with 16 cameras and six persons. Especially the computational load of multi-view triangulation grows with larger number of cameras.</p><p>Camera images and semantic feedback are processed asynchronously on the sensors during the online experiments, the frequencies of the feedback and feed-forward parts of the pipeline do not need to be balanced. The most recent feedback message not older than a threshold is used for a camera image. The average pipeline delay ?t including processing on sensors and backend as well as network and synchronization delays sums to 89 ms in the 4-camera setup and to 200 ms with 16 1 https://www.ais.uni-bonn.de/videos/RSS 2021 Bultmann cameras. This delay does not limit the feed-forward frequency of pose inference due to the asynchronous parallel processing. The latency is compensated by the prediction step in the feedback channel (cf. Sec. III-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Network Bandwidth and Power Consumption:</head><p>The network usage when processing a 30 Hz video stream only amounts to 15 kB/s per detected person, as only semantic skeletons are transmitted between sensors and backend. This is an over 99 % reduction of bandwidth compared to 27 MB/s when transmitting the raw VGA images. The power consumption of a sensor board was measured as approx. 7 W when running inference on the 30 Hz multi-person video stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this work, we proposed a novel method for real-time 3D human pose estimation using a network of smart edge sensors. Our main idea is to process each camera view ondevice, transmit only semantic information to a backend where it is fused into a 3D skeleton, and implement a 3D / 2D semantic feedback channel which lets the local semantic models incorporate fused multi-view information. The pipeline is able to track up to three persons at 30 Hz and up to six persons at 15 Hz, achieving real-time performance. It is evaluated on the H3.6M, Campus, and Shelf datasets where it is shown to achieve state-of-the-art results, as well as on own data in scenarios with up to 16 cameras and six persons.</p><p>In future research, we plan to use the estimated human pose information to enable safe human-robot interaction and anticipative robot behavior in a workspace shared with people. Mobile robots carrying a smart sensor board can participate in the network for collaborative perception and add further viewpoints. The semantic scene model could be extended to also include objects and scene geometry. Furthermore, using a more elaborate motion model in the prediction step could compensate better for the pipeline delay and improve the feedback signal, especially for fast motions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-view 3D human pose estimation using smart edge sensors: Sensor board with attached camera (top-right). 2D pose detections from four views of the H3.6M dataset [15] (bottom). Estimated 3D human skeleton (top-left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed pipeline for 3D human pose estimation using smart edge sensors and semantic feedback. Images are analyzed locally on the sensor boards. Semantic pose information is transmitted to the backend where multiple views are fused into a 3D skeleton. The 3D pose is reprojected into local views and sent to sensors as semantic feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Heatmaps and derived covariances (3? ellipses).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>V: Ablation study of the impact of various components of our approach on MPJPE 3D pose error (mm). * skeleton model without directional heatmap covariances category and compared to other approaches from literature. 3D pose errors after application of the resp. post-processing step or skeleton model are reported. The recent approaches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Samples of the heatmap fusion approach for left wrist (Rows 1-2) and right elbow (Row 3): Detected heatmap (b) and feedback heatmap (c) are combined into the fused heatmap (d). In difficult situations such as occlusions (Rows 1-2) or leftright inversions (Row 3), using feedback results in a heatmap closer to the ground-truth (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The pose estimation network is trained for 140 epochs on COCO for person crops of 192?256 px. It achieves a mAP of 69.6 % in FP32-mode and 68.4 % in INT8-mode on the COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Evaluation on Shelf dataset: 2D pose detections and estimated 3D pose without (top) and with feedback (bottom). 3D annotations for Actor1 (red) and Actor2 (orange). Highlighted are improvements due to the feedback signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Evaluation in multi-person scenarios: Estimated 3D poses reprojected into the corresponding camera image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>2D Joint Detection Rate (JDR) (%) for different joint classes, feedback modes and training data. Feedback Training Data Hips Knees Ankls Shlds Elbs Wrists Avg w/o fb H3.6M 99.2 96.1 90.3 93.3 93.3 89.1 95.1 w fb H3.6M 99.5 97.6 96.1 97.2 96.5 94.8 97.5 w/o fb COCO + H3.6M 99.3 97.1 96.9 98.9 96.2 92.8 97.6 w fb COCO + H3.6M 99.3 98.0 97.8 99.0 97.1 94.8 98.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>3D pose error (mm) for different joint classes, feedback modes and training data.</figDesc><table><row><cell cols="3">Feedback Training Data Hips Knees Ankls Shlds Elbs Wrists Avg</cell></row><row><cell>w/o fb</cell><cell>H3.6M</cell><cell>22.2 29.4 58.6 40.5 43.8 39.8 32.9</cell></row><row><cell>w fb</cell><cell>H3.6M</cell><cell>22.1 28.0 47.2 36.7 38.6 33.9 29.8</cell></row><row><cell>w/o fb</cell><cell cols="2">COCO + H3.6M 19.2 25.5 38.0 25.6 30.7 29.4 24.0</cell></row><row><cell>w fb</cell><cell cols="2">COCO + H3.6M 19.2 24.9 36.9 25.5 29.9 28.3 23.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Evaluation result on H3.6M dataset: MPJPE 3D pose error (mm) per action type. 3D poses after application of the resp. post-processing or skeleton model are reported. + denotes using additional training data.Method Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Pavlakos et al. [23] 41.2 49.2 42.8 43.4</figDesc><table><row><cell></cell><cell></cell><cell>55.6</cell><cell>46.9</cell><cell>40.3</cell><cell>63.7</cell><cell>97.6 119.9</cell><cell>52.1</cell><cell>42.7</cell><cell>51.9</cell><cell>41.8</cell><cell>39.4</cell><cell>56.9</cell></row><row><cell>Tome et al. [29]</cell><cell>43.3 49.6 42.0 48.8</cell><cell>51.1</cell><cell>64.3</cell><cell>40.3</cell><cell>43.3</cell><cell>66.0 95.2</cell><cell>50.2</cell><cell>52.2</cell><cell>51.1</cell><cell>43.9</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell>Qiu et al. [24]</cell><cell>28.9 32.5 26.6 28.1</cell><cell>28.3</cell><cell>29.3</cell><cell>28.0</cell><cell>36.8</cell><cell>42.0 30.5</cell><cell>35.6</cell><cell>30.0</cell><cell>28.3</cell><cell>30.0</cell><cell>30.5</cell><cell>31.2</cell></row><row><cell>Remelli et al. [26]</cell><cell>27.3 32.1 25.0 26.5</cell><cell>29.3</cell><cell>35.4</cell><cell>28.8</cell><cell>31.6</cell><cell>36.4 31.7</cell><cell>31.2</cell><cell>29.9</cell><cell>26.9</cell><cell>33.7</cell><cell>30.4</cell><cell>30.2</cell></row><row><cell>Ours, w/o fb</cell><cell>27.7 36.5 27.8 27.1</cell><cell>33.9</cell><cell>33.1</cell><cell>29.3</cell><cell>33.6</cell><cell>41.3 42.5</cell><cell>32.8</cell><cell>33.5</cell><cell>33.3</cell><cell>27.8</cell><cell>27.2</cell><cell>32.9</cell></row><row><cell>Ours, w fb</cell><cell>27.1 29.9 27.0 26.5</cell><cell>31.3</cell><cell>28.9</cell><cell>27.1</cell><cell>29.8</cell><cell>36.5 36.0</cell><cell>30.8</cell><cell>29.3</cell><cell>29.7</cell><cell>27.3</cell><cell>26.3</cell><cell>29.8</cell></row><row><cell>Qiu et al. [24] +</cell><cell>24.0 26.7 23.2 24.3</cell><cell>24.8</cell><cell>22.8</cell><cell>24.1</cell><cell>28.6</cell><cell>32.1 26.9</cell><cell>31.0</cell><cell>25.6</cell><cell>25.0</cell><cell>28.1</cell><cell>24.4</cell><cell>26.2</cell></row><row><cell>Ours + , w/o fb</cell><cell>22.4 24.3 22.4 21.7</cell><cell>24.6</cell><cell>24.7</cell><cell>22.4</cell><cell>22.6</cell><cell>26.8 28.4</cell><cell>25.0</cell><cell>23.1</cell><cell>24.5</cell><cell>22.0</cell><cell>21.5</cell><cell>24.0</cell></row><row><cell>Ours + , w fb</cell><cell>22.4 24.0 22.2 21.7</cell><cell>24.0</cell><cell>23.9</cell><cell>22.1</cell><cell>22.6</cell><cell>26.0 26.8</cell><cell>24.5</cell><cell>22.8</cell><cell>24.6</cell><cell>21.8</cell><cell>21.3</cell><cell>23.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Average inference time and model size for H3.6M dataset (Values for Qiu et al.<ref type="bibr" target="#b24">[24]</ref> taken from<ref type="bibr" target="#b26">[26]</ref>).</figDesc><table><row><cell></cell><cell>Qiu et al. [24]</cell><cell>Remelli et al. [26]</cell><cell>Ours</cell></row><row><cell>Inference Time</cell><cell>8.4 s</cell><cell>0.040 s</cell><cell>0.024 s</cell></row><row><cell>Model Size</cell><cell>2.1 GB</cell><cell>251 MB</cell><cell>4 ? 12 MB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Evaluation result on Campus and Shelf dataset: Percentage of Correct Parts (PCP) (%) and average run-time of 2D and 3D pose inference. '-' means offline pre-computation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PCP (%)</cell><cell cols="2">Inference Time</cell></row><row><cell>Campus</cell><cell cols="5">Actor1 Actor2 Actor3 Avg 2D pose 3D pose</cell></row><row><cell cols="2">Belagiannis et al. [3] 83.0</cell><cell>73.0</cell><cell>78.0 78.0</cell><cell>-</cell><cell>1 s</cell></row><row><cell>Dong et al. [9]</cell><cell>97.6</cell><cell>93.3</cell><cell>98.0 96.3</cell><cell>-</cell><cell>105 ms</cell></row><row><cell>Chen et al. [7]</cell><cell>97.1</cell><cell>94.1</cell><cell>98.6 96.6</cell><cell>-</cell><cell>1.6 ms</cell></row><row><cell>Ours, w/o fb</cell><cell>98.8</cell><cell>93.4</cell><cell cols="2">97.5 96.6 30 ms</cell><cell>8.8 ms</cell></row><row><cell>Ours, w fb</cell><cell>99.2</cell><cell>93.6</cell><cell cols="2">98.3 97.0 30 ms</cell><cell>8.8 ms</cell></row><row><cell>Shelf</cell><cell cols="5">Actor1 Actor2 Actor3 Avg 2D pose 3D pose</cell></row><row><cell cols="2">Belagiannis et al. [3] 75.0</cell><cell>67.0</cell><cell>86.0 76.0</cell><cell>-</cell><cell>1 s</cell></row><row><cell>Dong et al. [9]</cell><cell>98.8</cell><cell>94.1</cell><cell>97.8 96.9</cell><cell>-</cell><cell>105 ms</cell></row><row><cell>Chen et al. [7]</cell><cell>99.6</cell><cell>93.2</cell><cell>97.5 96.8</cell><cell>-</cell><cell>3.1 ms</cell></row><row><cell>Ours, w/o fb</cell><cell>99.4</cell><cell>94.6</cell><cell cols="2">96.8 96.9 40 ms</cell><cell>20 ms</cell></row><row><cell>Ours, w fb</cell><cell>99.3</cell><cell>95.7</cell><cell cols="2">97.3 97.4 40 ms</cell><cell>20 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Evaluation in own experiments with up to 16 cameras and 6 persons: Reprojection error (px) per joint class between detected 2D poses and fused 3D poses.</figDesc><table><row><cell cols="9">Feedback Cams Pers Hips Knees Ankls Shlds Elbs Wrists Avg</cell></row><row><cell>w/o fb</cell><cell>4</cell><cell cols="2">1-4 5.4</cell><cell>4.6</cell><cell>5.0</cell><cell>2.8 4.0</cell><cell>5.2</cell><cell>4.2</cell></row><row><cell>w fb</cell><cell>4</cell><cell cols="2">1-4 4.4</cell><cell>3.5</cell><cell>3.4</cell><cell>2.3 3.2</cell><cell>3.7</cell><cell>3.3</cell></row><row><cell>w/o fb</cell><cell>16</cell><cell>6</cell><cell>5.4</cell><cell>5.2</cell><cell>6.4</cell><cell>3.9 5.1</cell><cell>6.4</cell><cell>5.1</cell></row><row><cell>w fb</cell><cell>16</cell><cell>6</cell><cell>4.3</cell><cell>3.8</cell><cell>4.7</cell><cell>3.4 4.0</cell><cell>4.9</cell><cell>4.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was funded by grant BE 2556/16-2 of the German Research Foundation (DFG), a Google faculty research award, and Fraunhofer IAIS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX symposium on operating systems design and implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mykhaylo Andriluka, Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3D pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-16178-5_52</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.5555/3042817.3042832</idno>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">and Stefan Carlsson. 3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-view tracking for multi-human 3D pose estimation at over 100 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3279" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3D pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://coral.ai/docs/dev-board/datasheet,2020" />
		<imprint>
			<biblScope unit="page" from="2021" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unscented filtering and nonlinear estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey K Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="422" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">iSAM2: Incremental smoothing and mapping using the Bayes tree. The Intl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hordur</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorela</forename><surname>Ila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dellaert</surname></persName>
		</author>
		<idno type="DOI">https:/journals.sagepub.com/doi/10.1177/0278364911430419</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-10602-1_48</idno>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-46448-0_2</idno>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint detection and recognition of human actions in wireless surveillance camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Lajevardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4747" to="4754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CNN-based pose estimation system for close-proximity operations around uncooperative spacecraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lorenzo Pasqualetto Cassinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eberhard</forename><surname>Fonod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">Gil</forename><surname>Ahrns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez</surname></persName>
		</author>
		<idno type="DOI">https:/arc.aiaa.org/doi/10.2514/6.2020-1457</idno>
	</analytic>
	<monogr>
		<title level="m">AIAA Scitech 2020 Forum</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1457</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4341" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ROS: An open-source robot operating system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gerkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tully</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Leibs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation (ICRA) Workshop on Open Source Software</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3D pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6040" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural metworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iterative greedy matching for 3D human pose tracking from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-33676-9_38</idno>
	</analytic>
	<monogr>
		<title level="m">German Conf. on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3D: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-01231-1_29</idno>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An evaluation of Edge TPU accelerators for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10423</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MobiPose: Real-time multi-person pose estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fucheng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3384419.3430726</idno>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Embedded Networked Sensor Systems (SenSys)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="136" to="149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
