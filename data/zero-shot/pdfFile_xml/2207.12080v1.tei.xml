<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intention-Conditioned Long-Term Human Egocentric Action Forecasting @ EGO4D Challenge 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteve</forename><surname>Valls Mascar?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Systems</orgName>
								<orgName type="institution">Technische Universit?t Wien (TU Wien)</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyemin</forename><surname>Ahn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ulsan National Institute of Science and Technology (UNIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongheui</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Systems</orgName>
								<orgName type="institution">Technische Universit?t Wien (TU Wien)</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Robotics and Mechatronics</orgName>
								<orgName type="institution">German Aerospace Center (DLR)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intention-Conditioned Long-Term Human Egocentric Action Forecasting @ EGO4D Challenge 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To anticipate how a human would act in the future, it is essential to understand the human intention since it guides the human towards a certain goal. In this paper, we propose a hierarchical architecture which assumes a sequence of human action (low-level) can be driven from the human intention (high-level). Based on this, we deal with Long-Term Action Anticipation task in egocentric videos. Our framework first extracts two level of human information over the N observed videos human actions through a Hierarchical Multitask MLP Mixer (H3M). Then, we condition the uncertainty of the future through an Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stable predictions of the next Z = 20 actions that the observed human might perform. By leveraging human intention as high-level information, we claim that our model is able to anticipate more time-consistent actions in the long-term, thus improving the results over baseline methods in EGO4D Challenge. This work ranked first in the EGO4D LTA Challenge by providing more plausible anticipated sequences, improving the anticipation of nouns and overall actions. The code is available at https://github. com/Evm7/ego4dlta-icvae</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Reasoning about the next actions to take for performing a certain task is essential in the humans everyday life and has direct applications in task planning or in robot collaborative scenarios. The fundamental challenges of anticipating human actions is the inherent uncertainty of the future, which results in high variability in the task execution. However, the future often has only a limited number of plausible predictions. We aim then to reduce the arbitrariness of events by means of conditions. These conditions are hypotheses extracted from past observations of the human that allow to guide the human in its actions: the intention. For instance, if we observe a human cutting a tomato and we know that the intention is to prepare a salad, the variety of ingredients that the human might interact with are reduced, and actually differ if the intention is to cook a hamburger. Our method aims at capturing the human intention, as the highest level concept that defines the objective pursued by the human, to narrow the inherent uncertainty of the future. Moreover, low-level actions are also predicted, which each define exactly the concrete steps followed by the human in this determined task.</p><p>II. METHODOLOGY Long-Term Human Action Anticipation needs to exploit temporal dependencies among observed actions to generate plausible human action sequences in the long-term. Our two-step approach aims at first understanding the observed actions through a Hierarchical MultiTask MLP Mixer (H3M), described in section II-B. Then, an Encoder-Decoder structure inherited by Transformer is proposed for the Intention-Conditioned Variational AutoEncoder (I-CVAE) in section II-C, inspired by <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overview of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Let a past = [a p,1 , ? ? ? , a p,N ] ? R N,2 denote the past N observed actions (composed of verb-noun pair) of the camera wearer from a given untrimmed video, the aim is to anticipate the future sequence of Z actions a f ut = [a f,1 , ? ? ? , a f,Z ] ? R Z,2 by generating K possible sequences.</p><p>Given v t where t = [1, ? ? ? , N ] as the sequence of frames of a given a p,t , this paper makes use of the pre-extracted features provided by the Ego4d benchmark. For every 1 second of frames (30f ps) a Video-Recognition SlowFast 8x8 (R101) Model pre-trained in Kinetics dataset is used that extracts a feature vector f ? R 2304 . By synchronization of videos with timestamp features, zero-padding approach is applied to obtain for every v t a global f t ? R 14,2304 . This strategy is validated as average v t has 13.04 second length with 14 sec as the longest clip for a given action a t .</p><p>Therefore a N -sequence of pre-extracted visual features f past = [f p,1 , ? ? ? .f p,N ] is used as input for the H3M. 2level outputs are obtained that compose the input structure for the I-CVAE. Intention prediction I is denoted as a unique label that classifies the overall goal of the camera wearer for the whole past-future actions. Low-level action pair predictions are defined as a past = [a p,1 , ? ? ? , a p,N ] being a p,t a (verb, noun) pair label. Finally, both I and a past are used for generating K possible future action sequences</p><formula xml:id="formula_0">A f ut = [a f,1 , ? ? ? , a f,K ] ? R K,Y,2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hierarchical Multitask MLP Mixer (H3M)</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the pre-extracted features f past = [f 1 , ? ? ? , f N ] are passed to a Hierarchical Multitask MLP-Mixer (H3M) architecture. The Action Mixer models in parallel each f t to encode the visual information that define a given (verb, noun) action pair. Let f t = [x 1 , ? ? ? , x 14 ] ? R 14,2304 , composed by 14 feature patches x t . Mixer layer is defined in <ref type="bibr" target="#b0">[1]</ref> and is composed of 2 MLP blocks encapsulated between two matrix transposition operations that captor the global context of x t . First MLP block identifies strong temporal dependencies and mixes the data among each tokenized patched, while second MLP block leverages spatial same-patch features. Each MLP block consists on two fully-connected layers with a GELU activation function in-between. Finally, global average pooling is applied in the 14 number of patches dimension to project R N,14,2304 to R N,2304 , encoding the action representation for each N observed actions f act = [f act,1 , ? ? ? , f act,N ]. The action head of the H3M projects and classifies each f act,t as verb and noun through a Fully Connected layer each, obtaining N (verb, noun) pairs a past . Finally, the intention head applies a second MLP Mixer to leverage the global context of the sequence of action representation features f act and classifies the human intention I in the observed video. The overall hierarchical multitask classifier obtains the top-low level information from past observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intention-Conditioned Variational Autoencoder (I-CVAE)</head><p>Conditional Variational Autoencoder architecture is shown in <ref type="figure" target="#fig_2">fig. 3</ref>. Encoder-Decoder Transformer structure is needed to leverage temporal dependencies between past actions to anticipate future sequence of human (verb, noun) pairs. During training I-CVAE learns the conditional probabilistic distribution in the encoder through exploiting both a past and a f ut labels. In testing, only the decoder block is used without any provided information of its future.</p><p>First of all, learnable embedding matrices are used to project independently each verb and noun pair of the observed sequence a past into a d-dimensional embedding vector. Therefore, a past,emb,v = [a p,emb,v,1 , ? ? ? , a p,emb,v,N ] ? R d represents the embedded N observed verbs of the human (same procedure with nouns). For the encoder block, forecasting a f ut,emb,v/n = [a f ut,emb,v/n,1 , ? ? ? , a f ut,emb,v/n,Z ] are also extracted using same embedding weights as past actions. Due to the (verb,noun) pair composition of an action, a past,emb,v and a past,emb,n are concatenated to a past,emb ? R N,2d in the Action Embedder. Intuitively, then, our Transformer Encoder-Decoder aims to reconstruct the a f ut,emb,v/n in the decoder given only the a past,emb,v/n and the intention label I as conditions. Inspired by <ref type="bibr" target="#b1">[2]</ref>, condition label I is projected into extralearnable ? token intention and ? token intention to inherit the action sequence representation after the Transformer Encoder. Together with embedded observed and forecasting actions, this so-called distribution parameter tokens are summed to a sinusoidal Positional Encoder (PE) and fed into the Transformer encoder, as can be seen in the top diagram of <ref type="figure" target="#fig_2">fig. 3</ref>. Output ? and ? are then extracted to perform the define a latent distribution from which we will sample in the decoder (z ? M where M ? R 2d ).</p><p>During the decoder, as illustrated in the bottom diagram of <ref type="figure" target="#fig_2">fig. 3</ref>, only the N observed actions a past and intention label I are used to condition the future generation of next Z actions a f ut . A learnable bias token b token intention is used to shift the latent representation vector z to an intention-dependent space. N observed actions embeddings a past,emb are fed into the Transformer Decoder together with the shift latent space and a O ? vector for each Z actions to predict, all summed with a sinusoidal Positional Encoder (PE), to anticipate a future sequence of actions representations a f ut,emb . Finally, for each Z actions predicted? f ut,emb,t , a Muli-Head Decoder applies 2 fully-connected layer for classifying verb and noun respectively.</p><p>To guide I-CVAE to decode plausible actions representations, an L2 reconstruction loss is applied between the Transformer Encoder and Decoder output:? f ut,emb and a f ut,emb . Finally, to ensure the correct classification of verbnoun pairs, a weighted cross-entropy loss is used between the generated results and the ground-truth action sequence to anticipate a f ut</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>Metrics. Following the proposed evaluation protocols in Ego4D LTA Challenge, we report the Edit Distance (ED) metric computed as the Damerau-Levenshtein distance over sequences of predictions of verbs, nouns and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Results</head><p>We report our submitted results for the challenge in table I based on the test set of the Ego4D LTA dataset. In this experiment, our framework predicted the N = 8 observed actions and the overall intention, to anticipate Z = 20 future actions by generating K = 5. Our framework slightly performs worse when anticipating verbs, but conditioning the generative model through the intention reduces the ED for nouns, thus performing better in the overall action anticipation possibilities.</p><p>Due to our limited computational resources, training was performed independently for each module. Next we will describe the quantitative evaluation first for the H3M module and after for I-CVAE. H3M. Our model is able to classify verb-noun pairs with similar performance than provided baseline model, as reported in II, but in our case only making use of the preextracted features. Due to the lower dimensionality of the features, several techniques were applied to avoid overfitting. We applied gaussian noise injection (N) in the pre-extracted input features to improve robustness of the classifier. By adding multi-task approach and sharing inner layers among the tasks (M) we inherited implicit data augmentation: as there was a need of modelling a representation of three tasks (classifying intention, verb and noun) the model had  to improve pattern extraction in the inner shared layers to optimize results for each task. Finally, focal loss modulation factor (beta=0.99) for the cross-entropy loss was applied to address the class imbalance (I) of verbs, nouns and intentions.</p><p>In table II it is shown that applying M+I+N obtains the best Top1 accuracy results of our framework, but our model slightly decreases its performance regarding Top5 predictions. We claim that by dealing with imbalance of the dataset, most-frequent classes lose importance in the training loss and therefore are worst classified (with impact in accuracy). Finally, our model is limited by the lowdimensionality input data used (visual features pre-extracted) but is still able to approximate to the baseline.</p><p>We have also compared the influence of predicting intention correctly to action classification results, illustrated in table III. Results show a significant direct relationship between nouns and intention, which intuitively indicates that by conditioning the framework through the intention, lower uncertainty can be obtained in nouns representations.</p><p>I-CVAE. We evaluate the performance of our stand-alone model based on the ground-truth action and intention labels provided by Ego4d dataset. We report ED@Z=20 as our evaluation metric for both nouns and verbs.</p><p>Next we will describe the influence of different strategies applied for the training, Action Embedder (AE) and Multi-Head Decoder (MD). The results, detailed in table IV are based on the validation set when N = 8 and Z = 20. In first experiments, embedding matrices from AE were initialized based on pre-trained S-BERT language model to capture the semantic representation of nouns and verbs. Experiments showed that training these layers from scratch resulted in better generalization capacity. Second experiment attempt to unify nouns and verbs inside the Transformer Encoder-Decoder structure. AE concatenates a past,emb,v ? R d and a past,emb,n ? R d , to create embedded representations of the observed actions a past,emb and feed it into the Transformer Decoder. Separated nouns and verbs imply that the the predicted forecasting embedding? f ut,emb ? R 2d is split in   verbs and nouns (inverse operation of AE concatenation), and each part is then projected to a verb/noun classification each through a fully connected layer. We observe that combining verb and noun as action representations allowed the model to avoid 'out of sense' pairs (some verbs-nouns do not have any semantic meaning together). Finally, two training tricks were used that improve results: addressing class imbalance through focal loss, as in H3M classification; and adding random noise to the 'observed actions' to create more robust forecasting even if some errors were found in the initial sequence. This second approached helped to improve the performance in end-to-end evaluation, when I-CVAE used H3M outputs to anticipate the next sequence of actions.</p><p>Results from table IV show the effectiveness of our model when we input N = 8 observed actions. In <ref type="figure" target="#fig_3">fig. 4</ref> we illustrate the influence of reducing the observed number of actions. By only leveraging N = 4 past actions, I-CVAE is able to forecast with even higher performance than with longer past sequences. We claim that taking too many past information causes the model to focus on less-relevant cues, thus collapsing in less plausible action sequences. These behaviour is also seen in <ref type="figure" target="#fig_4">Fig. 5</ref>, where the ED of our model is plot for different number of observed actions N at each timehorizon of Z = 20. We can see the difficulty of anticipating noun changes correctly, which is very influenced by the timehorizon to forecast.</p><p>From <ref type="figure" target="#fig_4">Fig. 5</ref> we can also observe that looking more than N = 4 actions in the past provides worse results in the shortterm and converges in similar results in long-term. Our initial claim was to use intention to provide long-term guidance to the model. From <ref type="figure">Fig. 6</ref> we can determine that conditioning the CVAE with the intention label allows to obtain better results in long-term future horizons, as it guides properly the model through the sequence. One limitation of our approach, however, is that when N = 8, the model performs slightly better without the condition from intention. We claim that the Transformer Encoder-Decoder is able to capture the intention cue and therefore does not need the label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work, we propose two-module framework to efficiently exploit human intention for human action sequence anticipation. Our H3M module leverages multitask approaches to detect with adequate results the observed human actions and intentions, which then condition the future through our I-CVAE. Our framework is able to outperform state-of-the-art baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overall proposed framework. Provided pre-extracted features for N = 8 observed videos are fed to our Hierarchical Multitask MLP Mixer model (H3M) to obtain low-level actions (in black, the predicted correct ones; in blue, errors) and top-level intention (in purple). Results are fed into our Intention-Conditioned Variational AutoEncoder (I-CVAE) that anticipates subsequent Z = 20 low-level actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Detailed structure of H3M architecture. Pre-extracted features are used to classify actions performed by the human and predict the intention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed structure of I-CVAE architecture, illustrating the encoder (top) and decoder (bottom) of our Transformer-based CVAE model given a sequence of N + Z actions and an Intention label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Evaluation of forecasting models trained on different number of observed actions N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Edit Distance (ED) at each time step t depending on the number of observed actions N .Fig. 6. Influence of conditioning future based on intention for N = 2 and for N = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF H3M WITH DIFFERENT TRAINING STRATEGIES, COMPARED TO THE BASELINE. M: MULTITASK SURROGATE LOSS (SHARING WEIGHTS). I: FOCAL LOSS TO SOLVE CLASS IMBALANCE. N:</figDesc><table><row><cell></cell><cell cols="2">NOISE INJECTION</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">VERB</cell><cell cols="2">NOUN</cell></row><row><cell>INTENTION</cell><cell>TOP1</cell><cell>TOP5</cell><cell>TOP1</cell><cell>TOP5</cell></row><row><cell>Correct</cell><cell>20,13</cell><cell>54,32</cell><cell>19,48</cell><cell>41,58</cell></row><row><cell>Error</cell><cell>21,48</cell><cell>57,46</cell><cell>18,79</cell><cell>33,28</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="5">INTENTION INFLUENCE TO ACTION CLASSIFICATION</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV STRATEGY</head><label>IV</label><figDesc>ANALYSIS OF I-CVAE. FIRST STRATEGY REPORTED THE BEST RESULTS IN LTA CHALLENGE.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action-conditioned 3d human motion synthesis with transformer vae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
