<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TREE-SNE: HIERARCHICAL CLUSTERING AND VISUALIZATION USING t-SNE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Robinson</surname></persName>
							<email>isaac.robinson@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<postCode>06511</postCode>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Pierce-Hoffman</surname></persName>
							<email>emma.pierce-hoffman@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<postCode>06511</postCode>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TREE-SNE: HIERARCHICAL CLUSTERING AND VISUALIZATION USING t-SNE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>visualization ? clustering ? hierarchical clustering ? t-SNE ? spectral clustering ? embedding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>t-SNE and hierarchical clustering are popular methods of exploratory data analysis, particularly in biology. Building on recent advances in speeding up t-SNE and obtaining finer-grained structure, we combine the two to create tree-SNE, a hierarchical clustering and visualization algorithm based on stacked one-dimensional t-SNE embeddings. We also introduce alpha-clustering, which recommends the optimal cluster assignment, without foreknowledge of the number of clusters, based off of the cluster stability across multiple scales. We demonstrate the effectiveness of tree-SNE and alphaclustering on images of handwritten digits, mass cytometry (CyTOF) data from blood cells, and single-cell RNA-sequencing (scRNA-seq) data from retinal cells. Furthermore, to demonstrate the validity of the visualization, we use alpha-clustering to obtain unsupervised clustering results competitive with the state of the art on several image data sets. Software is available at https: //github.com/isaacrob/treesne.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>t-SNE (van der Maaten and Hinton 2008) is a widely-used method of visualizing high-dimensional data in low dimensions. It is motivated by minimizing the Kullback-Leibler divergence between the distributions of pairwise affinities among observations in the high-dimensional and low-dimensional spaces. Its predecessor, SNE <ref type="bibr" target="#b9">(Hinton and Roweis 2002)</ref>, uses a Gaussian kernel to transform the low-dimensional distances into affinities, while t-SNE uses a heavier-tailed t-distribution with one degree of freedom. As noted by van der Maaten and <ref type="bibr" target="#b30">Hinton (2008)</ref>, the heavier tails of the t-distribution compared to the Gaussian distribution help to alleviate the "crowding problem" so that distinct blobs appear in the low-dimensional embedding.</p><p>Since t-SNE gives better-separated clusters than SNE,  used the Fourier transform (FFT)-accelerated interpolation-based t-SNE (FIt-SNE) approximation from  to implement a fast version of t-SNE for smaller, fractional degrees of freedom. They used a scaled t-distribution kernel defined as</p><formula xml:id="formula_0">k(d) = 1 (1 + d 2 /?) ?</formula><p>where the degrees of freedom are v = 2? ? 1. Since the degrees of freedom are positive (v &gt; 0), ? is also positive (? &gt; 0). Kobak et al. found that ? &lt; 1 (which corresponds to v &lt; 1) produces t-SNE plots with tighter, smaller blobs, capable of capturing finer-grained structures than standard t-SNE. For instance, standard t-SNE with ? = 1 separates the images of handwritten digits from the MNIST data set by digit, whereas Kobak et al. found that two-dimensional t-SNE with ? = 0.5 results in further separation into blobs representing different handwriting styles of each digit (see <ref type="figure" target="#fig_1">Figure 4</ref>).</p><p>In a lecture at Yale University in fall 2019, Stefan Steinerberger argued that the t-SNE optimization problem can be interpreted as a dynamic systems problem as opposed to a probability distribution matching problem, thereby justifying using 0 &lt; ? &lt; 0.5, for which the resulting kernel is not a valid probability distribution . He argued that this allows for an even more fine-grained look at the high dimensional data. t-SNE has also been shown to cluster well-separated data reliably in any embedding dimension . Therefore, the current theory suggests that for well-clustered data, one-dimensional t-SNE can be leveraged to convey the same information as two-dimensional t-SNE in a more compact manner ).</p><p>Based on those results, and motivated by the popularity of t-SNE and hierarchical clustering individually, we present tree-SNE, a hierarchical clustering method based on one-dimensional t-SNE with decreasing values of ? and perplexity at each level. Tree-SNE allows for visualization and elucidation of high-dimensional hierarchical structures by creating t-SNE embeddings with increasingly heavy tails to reveal increasingly fine-grained structure, and then stacking these embeddings to create a tree-like structure. We then run spectral clustering on each one-dimensional embedding, computationally determining the number of distinct clusters in the embedding. The number of clusters will increase as ? decreases. We define the alpha-clustering of the data to be the cluster assignment that is stable across the largest range of ? values, and then we demonstrate that alpha-clustering is competitive with the state of the art in unsupervised clustering algorithms on several data sets, including <ref type="bibr">MNIST (LeCun et al. 1998</ref>) and COIL-20 <ref type="bibr">(Nene et al. 1996a</ref>). <ref type="figure">Figure 1</ref> shows example visualizations produced by tree-SNE on the USPS <ref type="bibr" target="#b10">(Hull 1994)</ref> handwritten digits data set. <ref type="figure">Figure 1</ref>: Tree-SNE on the USPS handwritten digits data set. Top image: the tree is colored by true digit labels. Bottom image: the tree is colored by alpha-clustering labels. Note that the alpha-clustering labels do not correspond to the digits in each cluster, which accounts for the most of the color differences between the two plots. In all tree-SNE visualizations, the y-axis shows the layer number and the x-axis is the one-dimensional t-SNE embedding coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we provide a description of our algorithm and motivations for various design decisions. Section 2.1 discusses tree-SNE and section 2.2 discusses alpha-clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of tree-SNE</head><p>Tree-SNE reveals hierarchical structures in high dimensional data by creating many t-SNE embeddings with changing perplexity and degrees of freedom of the t-distribution. As both perplexity and degrees of freedom decrease, smaller, more granular clusters are produced in the embedding. In this paper, we restrict ourselves to one-dimensional t-SNE embeddings for computational efficiency and because it makes clustering easier. In the future, two-dimensional embeddings could also be used to create 3-D tree structures that are potentially more informative. However, we will demonstrate that these one-dimensional embeddings are sufficiently informative to provide potentially valuable insight into hierarchical structures of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Generating tree-SNE embeddings</head><p>We begin with a standard one-dimensional t-SNE (? = 1) embedding with high perplexity, by default the square root of the number of data points. A high starting perplexity increases the effective number of neighbors used by t-SNE (van der Maaten and Hinton 2008), which means that larger clusters will tend to form, capturing more global structures in the data (Kobak and Berens 2019). By using a large perplexity at the beginning, tree-SNE can display the entire spectrum of data organization, from global structures at the base of the tree to very fine-grained structures at the top. The default initial perplexity value of ? N was inspired by analysis of perplexity in t-SNE by Oskolkov (2019) and gives favorable results in practice. The exact starting value of perplexity did not appear to be important across many trials, as long as it is high, as t-SNE is fairly robust to small changes in perplexity (van der Maaten and Hinton 2008), and most of the interesting features of the data emerge in further embeddings from the adjustments of perplexity and ?.</p><p>Many one-dimensional t-SNE embeddings (typically in the range of 30 to 100) are stacked on top of each other to create the tree-SNE visualization, with the first layer at the bottom of the plot, and ? and perplexity decreasing in each subsequent layer. To generate each sequential level, the ? used in the previous level is multiplied by a constant factor 0 &lt; r &lt; 1, which can be expressed as ? n+1 = r? n , where ? n refers to the value of ? used in the nth embedding layer. With p n representing the the perplexity on level n, we let p n+1 = p r n . In practice, r is very close to 1. In this way, ? approaches 0 and perplexity approaches 1 as the number of levels n gets large, for a constant r. Perplexity of 1 corresponds to entropy of 0 in the embedding, and ? of 0 means our kernel function k(x, y) = 1 for any points x, y in the low-dimensional embedding. Both of these parameters can be thought of as optimizing for clusters containing only a single data point each, as there is no pressure for points to form clusters under these settings. Accordingly, we observe that clusters become smaller and more numerous moving upward through the layers.</p><p>When creating each layer, the t-SNE embedding is initialized with the embedding from the previous level, rather than a standard random initialization. This is done so that each layer is a refinement of the clustering found on the previous layer, with larger clusters breaking into smaller clusters on subsequent levels of the tree. As a result, the path of a single observation or a group of observations can be traced vertically through the resulting tree visualization.</p><p>While the FIt-SNE paper ) suggests that high initial exaggeration and learning rates improve the embedding quality, we use smaller learning rates and no early exaggeration after the first embedding to ensure relative stability from layer to layer. With smaller initial exaggeration and learning rates, each embedding does not change drastically from its initialization embedding, which is necessary for an interpretable tree structure to emerge. We do, however, use a constant exaggeration factor (default 12), as was done by , to increase cluster separation, based on the finding by Linderman et al. <ref type="bibr">(preprint 2017</ref>) that late stage exaggeration increases differentiation of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Differentiating factors of tree-SNE</head><p>Visualizing data at multiple levels of granularity in this way can be useful both to uncover inherently hierarchical structures within the data and to explore new data, because it can be difficult to determine the optimal level of granularity on which to view or cluster unlabeled data. In standard t-SNE it is possible to attempt to evaluate the data across multiple scales by using different perplexity and/or degrees of freedom settings. However, the shattering of global structure is a common complaint of t-SNE (Kobak and Berens 2019), so it can be difficult to relate a t-SNE plot at one scale to another of a different scale, because the global organization of the data is not necessarily preserved. A hierarchical visualization approach such as this is necessary to enable comparison across scales. Visualizing the data with tree-SNE as opposed to t-SNE allows a researcher (or our alpha-clustering algorithm) to decide which scale is best for their data by showing them a more in-depth view of the hierarchical organization than is possible with standard t-SNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of alpha-clustering</head><p>Alpha-clustering extends and validates the tree-SNE algorithm by assigning cluster labels based on the t-SNE embeddings and then automatically selecting the best level of granularity for clustering the data, the result of which can then be compared to other unsupervised clustering algorithms to show that the structure uncovered by tree-SNE indeed captures meaningful aspects of the data. A researcher does not of course have to use alpha-clustering to use tree-SNE. However, since alpha-clustering runs on top of an expressive visualization tool, it provides a powerful opportunity for explainable unsupervised clustering which is difficult to achieve in other approaches, many of which rely on deep neural networks. It is easy to understand why alpha-clustering makes its decisions, and cluster labels can be examined on multiple levels of granularity so a researcher can choose the level that fits their needs, or the alpha-clustering algorithm will recommend an optimal level. <ref type="figure">Figure 2</ref> provides further insight into the alpha-clustering process. Cluster labels are assigned on each layer of the tree, and in the top image in <ref type="figure">Figure 2</ref>, each layer is colored by the cluster labels for that individual layer. Then, the most stable clustering out of all of the layers is defined as the alpha-clustering. In the second image in <ref type="figure">Figure 2</ref>, all layers of the tree are colored by the alpha-clustering labels. <ref type="figure">Figure 2</ref>: Tree-SNE with alpha-clustering on the MNIST handwritten digits data set. In the first visualization, each layer of the tree is colored by spectral clustering labels for that layer. In the second visualization, the tree is colored by alpha-clustering labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Outline</head><p>Alpha-clustering works by searching for a cluster assignment for the data that is maximally invariant across multiple levels of clustering granularity. As one-dimensional t-SNE tends to generate easily distinguishable clumps of data, we identify and count the clumps on each layer of the tree-SNE tree, then we define the alpha-clustering as the clustering for which the number of clumps does not change across the largest range of ? values.</p><p>In other words, define a clustering algorithm c(n) that accepts a layer numbered n and produces a clustering. If we observe that the clustering algorithm c produces the same clustering on all layers between layers n and m (where m &gt; n) such that c(n) = c(m), then we call clustering c(n) a stable clustering. Each level's embedding is generated by a unique value of ?, so each stable clustering has a nonzero ? range equal to a m ? a n where n and m are the first and last layers that produce the stable clustering. We define the alpha-clustering to be the stable clustering for which the ? range is the largest. Recall that the relationship between the value of ? used to generate embeddings n and n + 1 is ? n+1 = r? n for some 0 &lt; r &lt; 1. This means that ? varies the most in the first few layers, and as we add more layers the difference in magnitude between ? n and ? n+1 rapidly decreases. Therefore, since alpha-clustering determines clustering stability based on the magnitude of the range of ? values that it spans, it will tend to select for clusterings that appear earlier in the tree structure. This helps prevent alpha-clustering from over-fitting to noise in the data, as could happen if we selected a clustering that persists across multiple levels higher up the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Spectral clustering and determination of k</head><p>Alpha-clustering uses spectral clustering on each layer of the tree. To perform spectral clustering, we first generate a graph as described in Section 2.2.3. Then we compute the graph Laplacian, for which the number of zero-valued eigenvalues gives the number of connected components in the graph (von Luxburg 2006). Therefore, as one-dimensional t-SNE embeddings tend to give clearly distinct clusters, if we can build a graph that captures that connectivity, then the graph Laplacian will tell us the number of clusters (k) that are present. This allows us to avoid the need for the user to specify a number of clusters, which can be a drawback of many traditional forms of clustering when the structure of the data is unknown. It is important to note that if we were to implement a version of tree-SNE based on two-dimensional t-SNE embeddings, this approach would not be as successful, as it would be more difficult to build a clearly meaningfully disconnected graph on the more complex shapes that can appear in two-dimensional embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Shared nearest neighbors</head><p>For each one-dimensional embedding, we generate a graph via shared nearest neighbors (SNN) <ref type="bibr" target="#b4">(Ert?z 2002)</ref>. SNN tends to generate more disconnected graphs than k-Nearest Neighbors (k-NN), because in order for two vertices to be connected via an edge, they need to both be nearest neighbors of each other, which reduces the probability of two vertices on the edges of clusters being randomly connected. In order to reduce the probability that two distinct clusters are connected by an edge in the graph, we want to minimize the number of neighbors used; however, the number of neighbors should not be too small, because that could result in a cluster breaking into multiple disconnected components. To balance these requirements, we choose the number of neighbors to be ? log N , where N is the number of observations, and ? = 2 by default. This choice was inspired by the disconnection criterion of random graphs (Erd?s and R?nyi 1960) and was seen to perform well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Subsampling</head><p>To improve the efficiency of spectral clustering on large data sets, since it is run on every layer of the tree, we only perform spectral clustering on a random subsample of 2000 observations. Then a k-nearest neighbors classifier is used to cluster the remaining observations. This greatly speeds up the implementation and does not significantly impact the performance of spectral clustering as long as we sample a large enough number of points. Using a k-nearest neighbors classifier also helps to stop individual points that appear separate from clusters from appearing to form their own clusters, by connecting them to the closest large cluster. For intuition as to why we can subsample in this way without ruining performance, see the Nystr?m method of kernel approximation <ref type="bibr" target="#b34">(Williams and Seeger 2000)</ref>. The Nystr?m method could also be used in potential future implementations of alpha-clustering to circumvent the need for a k-nearest neighbors classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Differentiating factors of alpha-clustering</head><p>An important characteristic of alpha-clustering is that it does not require the user to provide k, the number of clusters, and instead determines the optimal level of clustering granularity from the tree-SNE embedding. In addition to the optimal alpha-clustering labels, the clustering labels at every level of the tree-SNE embedding are also returned, so the user can explore patterns on each level of granularity.</p><p>Unlike most common clustering algorithms, alpha-clustering is easily interpretable because of the accompanying visualization. The user can see the separation between clusters on each individual level, which provides more insight into the data organization than the black box of a neural network and circumvents the need for an external data visualization method to accompany other clustering algorithms like Louvain <ref type="bibr" target="#b1">(Blondel et al. 2008)</ref> or K-Means. Not only that, but the user can see the full depth of organization of the data through the hierarchical visualization, which itself provides more flexibility to color by different features or labels than a typical dendrogram. This enables the user to explore the reasons behind specific splits in the tree and track the progression of points and clusters through the hierarchy. This helps address the increasing need for explainable machine learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Demonstration on MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm choices</head><p>To demonstrate the reasoning behind some of our algorithmic decisions, we run variations on tree-SNE with various aspects missing in <ref type="figure" target="#fig_0">Figure 3</ref>. In the first visualization, we do not vary degrees of freedom as we create more layers. In the second visualization, we do not vary perplexity. The last visualization shows the entire tree-SNE algorithm applied to MNIST. The data points are colored by their true labels in all three plots.</p><p>In the top visualization in <ref type="figure" target="#fig_0">Figure 3</ref>, we run tree-SNE without decreasing the degrees of freedom at each level, meaning we only decrease perplexity. Notice how the data is organized and for the most part captures the main structure of MNIST; the visualization is split into different colors showing the grouping of different MNIST digits, and except for fours and nines they are cleanly divided. However, there is no cluster separation, and the visualization does not divide the structure into any substructures.</p><p>The middle visualization in <ref type="figure" target="#fig_0">Figure 3</ref> does not decrease perplexity, and only decreases the degrees of freedom (?). The clusters become tighter as ? decreases, but they do not split into smaller clusters. We observe in <ref type="figure" target="#fig_1">Figure 4</ref> that in two-dimensional t-SNE, decreasing ? is sufficient to reveal cluster substructure, and decreasing perplexity in tandem is not necessary, as was previously shown in . However, we see in the middle plot of <ref type="figure" target="#fig_0">Figure 3</ref> that decreasing ? alone is not enough to produce finer-grained clusters in one-dimensional t-SNE embeddings. <ref type="figure" target="#fig_0">Figure 3</ref> is our technique, tree-SNE, which decreases ? and perplexity in tandem through the layers. Notice how clusters on earlier levels break into smaller subclusters on later levels, unlike the two other visualizations. We observe that decreasing both ? and perplexity appears to be necessary to reveal cluster substructures in one-dimensional t-SNE, and tree-SNE therefore takes this approach, balancing the spreading and contracting forces of decreasing perplexity and the degrees of freedom to build a tree structure containing meaningful hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The final visualization in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Showing hierarchy</head><p>In order to illustrate how tree-SNE displays hierarchical organization of data, we visualized the MNIST handwritten digits data set, then we examined the averaged digit images for each cluster on 17 of the levels (every fifth level between zero and 80, inclusive). The results are shown in two different formats in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>The top plot in <ref type="figure" target="#fig_2">Figure 5</ref> shows the tree-SNE visualization of MNIST data that was used for this analysis of hierarchical organization. It was run with a different random seed than the visualization in <ref type="figure" target="#fig_0">Figure 3</ref>, but it is highly similar in appearance. This plot is colored by true digit labels, and we observe that the digits separate cleanly for the most part by level 21, the level chosen by alpha-clustering, with two notable exceptions: four is grouped with nine, and three is grouped with five. In both cases, these digits split farther up the tree. It makes sense that these pairs of numbers remain un-separated longer than other digits, considering their similar appearances.</p><p>The middle plot in <ref type="figure" target="#fig_2">Figure 5</ref> shows within-cluster average digits for each cluster identified by spectral clustering on each of 17 levels of the tree-SNE visualization from the top image. Each averaged image is plotted in the location corresponding to the middle of the cluster from which it was generated, so that the images can be viewed in the tree-SNE tree structure. We observe that the clusters are refined moving up the tree, as ? and perplexity decrease. Clusters representing multiple digits separate by digit, and clusters representing a single digit split into smaller clusters representing different handwriting variations for that digit. For instance, ones split into vertical and tilted versions around level 35, and twos split into bubbly and straight sub-groups at level 55. There is a clear hierarchy of digit types, with distinctions becoming finer-grained at the top of the plot. From this visualization, we see that tree-SNE's branching structure conveys meaningful information about the hierarchical organization of the data. Some overlap between images was unavoidable while preserving the tree structure, so the bottom visualization in the same <ref type="figure" target="#fig_2">figure (Figure 5</ref>) is provided for larger views of the within-cluster average digits. Within-cluster average digits are shown for ten of the 17 layers shown in the middle plot, stacked vertically in the same order as they appear in the tree-SNE visualization. The second row from the bottom represents the stable clustering identified by alpha-clustering, in which only fours and nines and threes and fives have yet to separate cleanly. We draw attention to the top row, where we observe that the rightmost cluster of fours that is last to split from nines is triangular in shape, so in fact is more similar in appearance to nines than to the other fours. Based on these visualizations of within-cluster averaged digits, we see that tree-SNE produces meaningful clusters on each embedding, with hierarchical organization from layer to layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Alpha-clustering</head><p>We apply alpha-clustering to MNIST. Recall that alpha-clustering determines the level with the clustering that best fits the overall structure of the data. <ref type="figure" target="#fig_3">Figure 6</ref> shows the tree-SNE embedding of MNIST from <ref type="figure" target="#fig_0">Figure 3</ref> in the top image, and the same embedding with data points on all layers colored by alpha-clustering labels in the bottom plot. As discussed in Section 3.2, alpha-clustering correctly groups six of the ten MNIST classes, and combines the remaining four into two different clusters of two each. It combines four and nine, and three and five. Looking closer at the tree structure in the top plot of <ref type="figure" target="#fig_3">Figure 6</ref>, we see that the split that separates three from five happens at roughly the same level that one splits into two different sub-classes. Since alpha-clustering selects the optimal cluster assignment from a single layer, choosing a clustering level that separates three and five would mean also subdividing the ones into multiple clusters.</p><p>In order to determine the quality of a clustering assignment, we consider the normalized mutual information (NMI) (Strehl and Ghosh 2002) of a clustering. The NMI is defined as</p><formula xml:id="formula_1">I(X; Y ) H(X)H(Y )</formula><p>where X and Y are two different cluster assignments for the data, I is the mutual information, and H is the entropy. Values of NMI fall between 0 and 1, with 1 representing a complete match between X and Y . We can assess the quality of alpha-clustering by computing its NMI relative to the ground truth labels: X = alpha-clustering labels and Y = true labels. On MNIST (10,000 samples), alpha-clustering has an NMI score of 0.84. Importantly, there are is no clustering assignment on any individual layer with a higher NMI. This shows that alpha-clustering is selecting the best possible clustering from all levels of the tree.  <ref type="figure">Figure 1</ref>, note that the alpha-clustering labels do not correspond to the digits in each cluster, which accounts for the most of the color differences between the two plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate tree-SNE on several popular clustering data sets and two biological data sets. The purpose of running tree-SNE on the clustering data sets is to show that it can generate a competitive and interpretable clustering, which means that one-dimensional t-SNE embeddings created in this way are indeed able to express meaningful organization of high-dimensional data. Applying tree-SNE to biological data sets allows us to demonstrate its performance on diverse types of data and compare it to standard visualization techniques in biology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Tree-SNE has three major parameters: the ratio r, the initial perplexity, and the number of layers. For the following experiments, we set the starting perplexity to be the square root of the number of observations in the data set ( ? N ), as described in Section 2.1.1. Tree-SNE embeddings tend to be exhibit highly branched structures corresponding to very local structures in the data for ? &lt; 0.01, so by default we set the lower bound on ? to 0.01. Given this fixed lower bound on ?, we can solve for the ratio r required to decrease ? from 1 to 0.01 in a given number of layers n: r = e ln 0.01 n Using these sensible defaults, r and perplexity are automatically determined for a given data set and a desired number of layers for the tree. For testing alpha-clustering without visualization, we use 30 layers, and for visualization, we use 100 layers. Using fewer layers has minimal effect on the alpha-clustering and is more computationally efficient for repeated experimentation, while a larger number of layers is ideal for visualization because it produces a higher-resolution plot. For example, for 30 layers, r is 0.858, and for 100 layers, r is 0.955. Alpha-clustering has one major parameter, ?, where ? log n is the number of nearest neighbors used to generated the shared nearest neighbors graph for spectral clustering. We report our results with the default of ? = 2, unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of alpha-clustering</head><p>We apply alpha-clustering to four standard clustering data sets and compare the NMI to that of several state of the art clustering methods. The results are shown in <ref type="table">Table 1</ref>, which is based on <ref type="table">Table 4</ref> from <ref type="bibr">Gultepe and Makrehchi (2008)</ref>. Alpha-clustering achieves a competitive NMI on all of the data sets. Note that while most of the other methods featured in <ref type="table">Table 1</ref> require specifying a number of clusters, alpha-clustering determines the best number of clusters from the data. In biomedical applications, the optimal number of clusters is often not known a priori, so algorithms such as tree-SNE that do not require the number of clusters as input may be particularly useful.</p><p>If the number of clusters is known beforehand, the ? parameter can be tuned as needed such that alpha-clustering yields closer to the correct number of clusters. This is particularly useful when the data has a large number of clusters, as some clusters may be artificially merged in the SNN graph, and a lower ? value will decrease the number of neighbors used to construct the graph and reduce this merging behavior. We find that decreasing ? from our default of 2 to 1.3 increases the performance of of alpha-clustering on COIL-100 from 0.899 to 0.926 and increases the number of clusters found from 76 to 101. Note that changing ? to give the correct number of clusters does not always increase the NMI, as the clusters that split apart with lower ? do not necessarily correlate with true label clusters. For example, tuning ? on COIL-20 to give the correct number of clusters results in a decreased NMI of 0.951.</p><p>It should be noted that all of the benchmarking data sets used are image data sets: the two Columbia Object Image Library data sets, COIL-20 <ref type="bibr">(Nene et al. 1996a</ref>) and COIL-100 <ref type="bibr">(Nene et al. 1996b</ref>); as well as two handwritten digit data sets, USPS <ref type="bibr" target="#b10">(Hull 1994)</ref> and <ref type="bibr">MNIST (LeCun et al. 1998</ref>). These data sets were chosen due to readily available comparison metrics from other methods. However, convolutional neural network-based approaches have an advantage on these data sets because they are designed to process image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of tree-SNE</head><p>We demonstrate running tree-SNE on several different data sets, including mass cytometry (CyTOF) data from bone marrow cells <ref type="bibr">(Bendall, et</ref>    bright arm, which subdivides at higher embedding levels. This indicates that tree-SNE separates cytotoxic T cells on lower levels of the embedding, and then splits them into subgroups higher up as ? decreases. Importantly, the CD8 branch splits in two about halfway up the plot. The CD45ra visualization (bottom of <ref type="figure" target="#fig_5">Figure 7</ref>) reveals that the split in the cytotoxic T cell line is mediated by the presence of CD45ra: CD45ra is present in one of the branches but not the other. This indicates that tree-SNE is splitting cytotoxic T cells (CD8 + ) into naive (CD45ra + ) and mature cells (CD45ra ? ). In this way, tree-SNE very clearly reveals the hierarchical organization of the CyTOF data and facilitates identification of the features mediating the branching.</p><p>For comparison with established visualization methods for biological data, we show the same data embedded via t-SNE (van der Maaten and Hinton 2008) in <ref type="figure" target="#fig_8">Figure 8</ref>     <ref type="figure" target="#fig_10">Figure 10</ref> shows the tree-SNE embedding of the data subset.</p><p>Although the tree-SNE hierarchy and the dendrogram created by Shekhar et al. do not match entirely, there are strong similarities between the two. Notice how BC1A and BC1B split apart at a high level of the tree, and how they are more closely related to BC2, BC3A, BC3B, and BC4 than to the BC5 subtypes or BC7. Observe also how tree-SNE show the same organization of the four subtypes of BC5 as Shekhar et al. discovered: BC5A and BC5D are closely related, and BC5B and BC5C are closely related. Tree-SNE also shows further branching of some of the cell types, and these subdivisions may warrant further examination. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Swiss roll</head><p>A common concern about t-SNE is that it does not fully capture some standard artificial data sets, including the famous Swiss roll . In the case of the Swiss roll, t-SNE does not fully unwind the manifold, as seen in <ref type="figure" target="#fig_11">Figure 11</ref>. However, tree-SNE does for the most part unwind the Swiss roll, as seen in the top plot of <ref type="figure" target="#fig_12">Figure  12</ref>. Notice how on the bottom two levels of the embedding, the order of the points is wrong in the same way that it is wrong in the t-SNE embedding, with purple in the middle as opposed to at the end. However, tree-SNE's multi-scale approach fixes this issue and places the purple segment where it should be, correctly capturing the manifold. This phenomenon is highlighted in the bottom plot of <ref type="figure" target="#fig_12">Figure 12</ref>, which shows only the first four levels of the tree-SNE plot.</p><p>Additionally, notice that there is very little cluster separation in the tree-SNE embedding of the Swiss roll in <ref type="figure" target="#fig_12">Figure  12</ref>, and no well-defined branching. This shows that tree-SNE does not artificially introduce clusters or hierarchical structures when applied to data lacking this type of organization. We illustrate this point further in the next section <ref type="bibr">(Section 4.4)</ref>. This suggests that the clusters that emerge in other datasets are unlikely to be artifacts of noise, as otherwise similar clusters would have emerged here.</p><p>Although we demonstrate tree-SNE on the Swiss roll data set because it is a common "toy" data set and the behavior of tree-SNE when applied to the Swiss roll is interesting, tree-SNE is by no means a dimensionality reduction method. It is designed to visualize hierarchical cluster structures and is limited in its ability to represent manifolds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Random noise</head><p>To demonstrate that tree-SNE does not artificially introduce clusters or hierarchical structure into data lacking those structures, we run tree-SNE on 5,000 random samples drawn from a 100-dimensional uniform distribution over the range [0,1). The results are displayed in <ref type="figure" target="#fig_0">Figure 13</ref>. Observe how, unlike the tree-SNE visualizations on meaningfully organized data, the tree does not begin to branch until level 50, and the subdivisions thereafter are long, without further branching. This shows that tree-SNE does not tend to produce clear cluster separation or hierarchical structure by mere artifact. Further note that there are many points seemingly randomly scattered in the embedding, which does not tend to appear in well-organized data. In <ref type="figure" target="#fig_1">Figure 14</ref>, we generate 5,000 samples of uniform noise in a two-dimensional plane in 100-dimensional space, then embed the samples with both tree-SNE and t-SNE. The tree-SNE plot does not have as long of an initial stem as it does in <ref type="figure" target="#fig_0">Figure 13</ref>, because the data lies along a plane rather than completely randomly throughout the 100 dimensions. The tree-SNE plot still has relatively uniformly distributed branches without a clear hierarchy, which does not tend to happen with organized data. The standard two-dimensional t-SNE embedding is provided for comparison. The t-SNE embedding gives the impression of clumping or clustering, despite the fact that the data is completely random, whereas tree-SNE does not show any clusters.</p><p>Together, these experiments on random noise illustrate that tree-SNE does not introduce artificial structure into unstructured data, and the tree-SNE plots of noise have a characteristic profile that is distinguishable from the plots of well-organized data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present tree-SNE, a novel hierarchical visualization method for high-dimensional data, and a companion clustering method, alpha-clustering. Tree-SNE provides a way to explore the organization of data at multiple levels of granularity, which is not available in many other visualization techniques. Alpha-clustering is built on top of the tree-SNE embedding and provides quantitative validation that tree-SNE reveals meaningful structures in the data. It also achieves NMI scores on various data sets that are competitive with state-of-the-art clustering approaches, without ever needing to specify the number of clusters. Finally, we demonstrate the viability of this approach on several diverse real-world data sets. Our analysis suggests that tree-SNE offers promise in terms of yielding insights not currently revealed by other visualization approaches.</p><p>In the future, we intend to modify the tree-SNE algorithm to be able to leverage neural network embeddings, with the hope that this would help us be more competitive with neural network-based approaches, particularly on image or sequential data. We hope to create a means for out-of-sample extension of alpha-clustering such that the clustering approach can be used for more than exploratory data analysis, perhaps leveraging previous work on parametric t-SNE (van der Maaten 2009). We also want to explore the idea of a variation of alpha-clustering that varies scale (?) differently for different branches, acknowledging that a global scale may not be optimal for all data sets. In addition, we want to investigate using alternative clustering approaches instead of subsampled SNN-based spectral clustering, perhaps leveraging the Nystr?m method. And finally, we want to extend tree-SNE to support stacked two-dimensional t-SNE plots.</p><p>helpful discussions of visualizations and alpha-clustering; Smita Krishnaswamy for encouragement and feedback; and Ariel Jaffe for discussing the Nystr?m method and its relationship to subsampled spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head><p>7.1 Appendix A: Methods for subsetting scRNA-seq data with alpha-clustering</p><p>In Section 4.3.2, we applied tree-SNE to a subset of a scRNA-seq data set from retinal cells <ref type="bibr" target="#b25">(Shekhar et al. 2016</ref>). Now, we demonstrate using alpha-clustering to partition the full data set from Shekhar et al. to obtain the subset of interest used in Section 4.3.2. Prior to running tree-SNE, we use PCA to reduce the dimensions of the data set from 24,576 features to 100 dimensions to increase the efficiency of the algorithm. We start with the tree-SNE embedding of the full scRNA-seq data set, in <ref type="figure" target="#fig_2">Figure 15</ref>, colored by the labels from Shekhar et al. Notice that the Muller Glia (teal, leftmost branch) and Rod BC (dark green, rightmost branch) cell types take up a lot of space in the visualization, crowding the main group of cone bipolar cell (BC) subtypes of interest. We would like to remove the M?ller glia and rod BCs to reduce this crowding, and we will use alpha-clustering labels to partition the data to do so.  <ref type="figure" target="#fig_3">Figure 16</ref> shows the same tree-SNE embedding of the full scRNA-seq data set, this time colored by alpha-clustering labels. Alpha-clustering automatically determined that the optimal number of clusters was three, and these three clusters correspond to (left to right) M?ller glia, cone BC subtypes and photoreceptor (PR) cells, and rod BCs. Since alpha-clustering has already partitioned the data, it is simple to remove clusters 0 and 2 and re-apply tree-SNE to only the observations within cluster 1, allowing for a visualization that has more space to preserve the hierarchical organization of the cells of interest. This new embedding is shown in <ref type="figure" target="#fig_5">Figure 17</ref>. Notice that on the far right, there is still a small group of rod PR cells present in this plot, which are not a cell type of interest.</p><p>Running alpha-clustering on the tree-SNE embedding from <ref type="figure" target="#fig_5">Figure 17</ref> yields an optimal cluster assignment of just two clusters; again, the number of clusters was automatically detected by the algorithm. <ref type="figure" target="#fig_8">Figure 18</ref> is colored by the alpha-clustering labels, showing that the two clusters discovered from this subset of the data correspond to the majority of the tree (left) and the rod PR cells (right). The alpha-clustering labels make it trivial to remove cluster 1 and re-embed the remaining data from cluster 0. For this final round of tree-SNE, we also reduce the number of PCA components used to 37 to more closely match the methods from <ref type="bibr" target="#b25">Shekhar et al. (2016)</ref> and enable more direct comparison. The final tree-SNE embedding is shown in <ref type="figure" target="#fig_9">Figure 19</ref> (which is the same as <ref type="figure" target="#fig_10">Figure 10</ref> shown previously), and is discussed in more detail in Section 4.3.2. <ref type="figure" target="#fig_9">Figure 19</ref>: Tree-SNE embedding of only the cone BC subtypes in the scRNA-seq data (with a few rogue cone photoreceptor cells on the right side), colored by labels from <ref type="bibr" target="#b25">Shekhar et al. (2016)</ref>.</p><p>The ability to automatically detect, partition, and remove particular significantly different subsets of the data is, to the best of our knowledge, unique to tree-SNE and alpha-clustering. This allows a researcher to very easily select and hone in on subsets of interest in their data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>MNIST tree-SNE (10,000 images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Two-dimensional t-SNE on 10,000 MNIST digits. The top row has ? = 1 and the bottom row has ? = 0.5. The left column has perplexity = 30, while the right column has perplexity = 8. The plots are colored by truth labels for the digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>MNIST (10,000 samples) tree-SNE with averaged images of digits within each cluster. The top plot is a tree-SNE visualization of MNIST colored by the true digit labels. The middle plot shows the within-cluster average digits superimposed on the tree structure. The bottom visualization shows within-cluster average digits for a sample of layers. Clusters determined via alpha-clustering on each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>MNIST tree-SNE embeddings. The top image is colored by true digit labels, and the bottom image is colored by alpha-clustering labels. As with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>al. 2011) and single-cell RNA-sequencing (scRNA-seq) data from retinal cells (Shekhar et al. 2016). We compare tree-SNE with t-SNE (van der Maaten and Hinton 2008) and PHATE (Moon et al. 2019), standard approaches in biomedical data visualization.4.3.1 CyTOF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>CyTOF tree-SNE embeddings. The top embedding is labeled by the level of CD8 in the sample, and the bottom is labeled by CD45ra expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>shows a tree-SNE embedding of bone marrow single-cell mass cytometry (CyTOF) data<ref type="bibr" target="#b0">(Bendall et al. 2011)</ref>, colored by the expression levels of specific markers in each cell. The top visualization is colored by intensity of CD8, which is a marker for cytotoxic T cells, and the bottom is colored by intensity of CD45ra, which is a marker for naive T cells<ref type="bibr" target="#b6">(Golubovskaya and Wu 2016)</ref>. Notice how in the top visualization most of the cytotoxic T cells are in the same</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and via PHATE (Moon et al. 2019) in Figure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>CyTOF t-SNE embeddings. The left is colored by CD8, and the right is colored by CD45ra.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>CyTOF PHATE embeddings. The left is colored by CD8, and the right is colored by CD45ra.4.3.2 scRNA-seqWe demonstrate tree-SNE on single-cell RNA-sequencing (scRNA-seq) data from mouse retinal cells and compare the results with the findings from the original paper by<ref type="bibr" target="#b25">Shekhar et al. (2016)</ref>. Shekhar et al. used Louvain community detection<ref type="bibr" target="#b1">(Blondel et al. 2008</ref>) to detect clusters, two-dimensional t-SNE to visualize the clusters, and hierarchical clustering with Euclidean distances and average linkage to produce a hierarchical depiction of the relationships between different mouse retinal bipolar cells (BCs). We run tree-SNE on the same data and label it according to the labels assigned by Shekhar et al. from clustering and morphological analysis. Some pre-processing, mediated by alphaclustering, was performed to roughly isolate the cone bipolar cells, which encompass the cell subtypes evaluated by hierarchical clustering by Shekhar et al. The full process is described in Appendix A (Section 7.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>scRNA-seq tree-SNE embedding colored by the labels from Shekhar et al. (2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Swiss roll data set in full 3-D (left) and its t-SNE embedding (right), both colored by labels corresponding to the position of points along the major axis of the manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Swiss roll tree-SNE embedding with 100 layers (top) and just the bottom four layers (bottom), again colored by labels corresponding to the position of points along the major axis of the manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Uniform noise tree-SNE embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Random samples from a plane in 100-dimensional space visualized by tree-SNE (top) and t-SNE (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Tree-SNE embedding of the whole scRNA-seq data set, colored by labels from Shekhar et al (2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Tree-SNE embedding of the whole data set of scRNA-seq data, colored by alpha-clustering labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Tree-SNE embedding of a subset of the scRNA-seq data, excluding cells that are not of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Tree-SNE embedding of a subset of the scRNA-seq data, colored by alpha-clustering labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :Table 1 .</head><label>11</label><figDesc>The NMI scores of various clustering algorithms on different data sets, with top three in bold. For DBSCAN, ? was tuned until the correct number of clusters was obtained. We performed DBSCAN, K-Means, and tree-SNE benchmarking; the rest of the values are reported from the original papers. * means the clustering algorithm requires knowing the number of clusters beforehand. ** this was run with ? = 1.3, whereas the rest were run with the default ? = 2. The methods in the table are: Joint Unsupervised Learning (JULE, Yang et al. 2016), Discriminatively Boosted Clustering (DBC, Li et al. 2018), Infinite Ensemble Clustering (IEC, Liu et al. 2016), Autoencoder-based Clustering (AEC, Song et al. 2013), NMF with Deep learning model (NMF-D, Trigeorgis et al. 2014), Task-specific Deep Architecture for Clustering (TSC-D, Wang et al. 2016), Deep Convolutional Embedded Clustering (DCEC, Guo et al. 2017), SpectralNet</figDesc><table><row><cell cols="5">Performance of different clustering algorithms</cell></row><row><cell>Method</cell><cell cols="4">COIL-20 COIL-100 USPS MNIST</cell></row><row><cell>Deep Learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JULE* (2016)</cell><cell>1.000</cell><cell>0.985</cell><cell cols="2">0.913 0.913</cell></row><row><cell>DBC* (2017)</cell><cell>0.895</cell><cell>0.905</cell><cell cols="2">0.724 0.917</cell></row><row><cell>IEC* (2016)</cell><cell>-</cell><cell>0.787</cell><cell cols="2">0.641 0.542</cell></row><row><cell>DEPICT* (2017)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.927 0.917</cell></row><row><cell>AEC* (2013)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.651 0.669</cell></row><row><cell>NMF-D* (2014)</cell><cell>0.692</cell><cell>0.719</cell><cell cols="2">0.287 0.152</cell></row><row><cell>TSC-D* (2016)</cell><cell>0.928</cell><cell>-</cell><cell>-</cell><cell>0.651</cell></row><row><cell>DCEC* (2017)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.826 0.885</cell></row><row><cell cols="2">SpectralNet* (2018) -</cell><cell>-</cell><cell>-</cell><cell>0.924</cell></row><row><cell>Non-deep learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICA BSS* (2018)</cell><cell>0.965</cell><cell>0.962</cell><cell cols="2">0.868 0.824</cell></row><row><cell>AC-PIC* (2013)</cell><cell>0.855</cell><cell>0.840</cell><cell cols="2">0.840 0.017</cell></row><row><cell>K-means*</cell><cell>0.774</cell><cell>0.775</cell><cell cols="2">0.613 0.490</cell></row><row><cell>DBSCAN</cell><cell>0.892</cell><cell>0.705</cell><cell cols="2">0.299 -</cell></row><row><cell>Ours</cell><cell>0.958</cell><cell>0.926**</cell><cell cols="2">0.885 0.864</cell></row></table><note>(Shaham et al. 2018), Independent Component Analysis Blind Source Separation (ICA BSS, Gultepe and Makrehchi 2018), Agglomerative Clustering via Path Integral (AC-PIC, Zhang et al. 2013), K-Means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN, Ester et al. 1996). Benchmarking data sets: COIL-20 (Nene et al. 1996a), COIL-100 (Nene et al. 1996b), USPS (Hull 1994), MNIST 60,000 samples (LeCun et al. 1998).</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>The authors thank Stefan Steinerberger for inspiration, support, and advice; George Linderman for enabling onedimensional t-SNE with degrees of freedom &lt; 1 in the FIt-SNE package; Scott Gigante for data pre-processing and</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single-cell mass cytometry of differential immune and drug responses across a human hematopoietic continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Bendall</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1198704</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">332</biblScope>
			<biblScope unit="page" from="687" to="696" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-5468/2008/10/P10008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Dizaji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06327</idno>
		<title level="m">Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erd?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>R?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magayr Tud. Akad. Mat. Kutato Int. Kozl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="61" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new shared nearest neighbor clustering algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ert?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Clustering High Dimensional Data and its Applications. Second SIAM International Conference on Data Mining</title>
		<meeting><address><addrLine>Arlington, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 2nd International Conference on Knowledge Discovery and Data Mining<address><addrLine>Portland, OR, in print</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Different Subsets of T Cells, Memory, Effector Functions, and CAR-T Immunotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golubovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.3390/cancers8030036</idno>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving clustering performance using independent component analysis and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gultepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makrehchi</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-018-0148-3</idno>
	</analytic>
	<monogr>
		<title level="j">Hum. Cent. Comput. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page">382</biblScope>
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Neural Information Processing Systems (NIPS&apos;02)</title>
		<meeting>the 15th International Conference on Neural Information Processing Systems (NIPS&apos;02)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hull</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.291440</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The art of using t-SNE for single-cell transcriptomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kobak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-13056-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5416</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heavy-tailed kernels reveal a finer cluster structure in t-SNE visualisations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kobak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05804</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, in print</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, in print</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminatively boosted image clustering with fully convolutional auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.05.019</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient algorithms for t-distributed stochastic neighborhood embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rachh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Hoskins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09005</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rachh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Hoskins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-018-0308-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">243</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering with t-SNE, provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<idno type="DOI">10.1137/18M1216134</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Math. Data Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="313" to="332" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Infinite ensemble for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in print. ACM 1745-1754</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing structure and transitions in high-dimensional biological data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-019-0336-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1482" to="1492" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Columbia Object Image Library (COIL-20)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>No. CUCS-005-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Columbia Object Image Library (COIL-100)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>No. CUCS-006-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to tune hyperparameters of tSNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oskolkov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>blog post</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="https://towardsdatascience.com/how-to-tune-hyperparameters-of-tsne-7c0596a18868" />
		<title level="m">Towards Data Science</title>
		<imprint>
			<date type="published" when="2019-07-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01587</idno>
		<title level="m">spectral clustering using deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comprehensive classification of retinal bipolar neurons by single-cell transcriptomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shekhar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2016.07.054</idno>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="1308" to="1323" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoder based data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-41822-8_15</idno>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">124</biblScope>
		</imprint>
	</monogr>
	<note>print</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Math 421a: The Mathematics of Data Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecture</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cluster Ensembles -A Knowledge Reuse Framework for Combining Multiple Partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1162/153244303321897735</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Deep Semi-NMF Model for Learning Hidden Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a Parametric Embedding by Preserving Local Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, in print. PMLR</title>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics, in print. PMLR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-007-9033-z</idno>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a task-specific deep architecture for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974348.42</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using the Nystr?m method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Neural Information Processing Systems (NIPS&apos;00), in print</title>
		<meeting>the 13th International Conference on Neural Information Processing Systems (NIPS&apos;00), in print</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="661" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint Unsupervised Learning of Deep Representations and Image Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.556</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Agglomerative clustering via maximum incremental path integral</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2013.04.013</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="3056" to="3065" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
