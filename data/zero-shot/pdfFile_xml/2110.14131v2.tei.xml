<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEMPORAL KNOWLEDGE DISTILLATION FOR ON-DEVICE AUDIO CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Morton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Hyperconnect</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">TEMPORAL KNOWLEDGE DISTILLATION FOR ON-DEVICE AUDIO CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-On-device</term>
					<term>Audio Classification</term>
					<term>Knowl- edge Distillation</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the ubiquity of real time communication, on-device audio understanding has received great attention. On-device models have achieved comparable performance to much larger models on several tasks such as keyword spotting (KWS) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Nevertheless, compared to large models, ondevice models still struggle with more complex tasks (e.g., audio event detection (AED) <ref type="bibr" target="#b2">[3]</ref>). Improving the performance of on-device models is challenging due to the restricted memory and computing resources in the mobile environment.</p><p>Several studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> utilize knowledge distillation (KD) <ref type="bibr" target="#b5">[6]</ref> to tackle the problem described above, applying the knowledge of large models (teacher) to on-device models (student) without incurring any computational overhead at inference time. Many on-device models commonly focus on the knowledge embedded in logits produced by the classification ? Equal contribution. * Corresponding author.</p><p>layer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, mainly because it can be easily applied even when the teacher and the student have dissimilar architectures. However, temporal information, which is known to be beneficial in audio tasks <ref type="bibr" target="#b7">[8]</ref>, cannot be easily distilled when it is compressed into classifier logits <ref type="bibr" target="#b8">[9]</ref>. With the success of the transformer <ref type="bibr" target="#b9">[10]</ref>, recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have focused on distilling the knowledge from self-attention maps, preserving the temporal information. However, their methods are limited to transferring the knowledge between the same transformer-based architectures only, where even the smallest transformer variants remain computationally expensive for many mobile devices. Also, it is not straightforward to transfer the knowledge of self-attention maps from the large transformer-based model to other architectures such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs).</p><p>In this paper, we introduce a simple yet effective method that can distill the temporal knowledge from attention weights of large transformer-based models to on-device models of various architectures. We first employ XLSR-wav2vec 2.0 <ref type="bibr" target="#b12">[13]</ref> as a teacher model and extract attention weights from its selfattention maps. We design the attention distillation loss for the on-device (student) models by attaching a simple attention layer only at training time to align the teacher and the student attention weights. To evaluate the effectiveness of our proposed method, we conduct experiments on a real-world AED dataset (FSD50K <ref type="bibr" target="#b2">[3]</ref>) and a noisy KWS dataset. The noisy KWS dataset is constructed by injecting the existing KWS dataset samples (Google Speech Commands v2 <ref type="bibr" target="#b0">[1]</ref>) into different speech-like noise audios <ref type="bibr" target="#b13">[14]</ref>, making the temporal information more important for classifying each specific target keyword. Experimental results demonstrate that applying our method improves the predictive performance of various ondevice models without any architectural changes on inference by distilling temporal information during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>In this section, we first describe a large-scale transformerbased model that is used as a teacher model ( ?2.1) and ondevice models employed as student models ( ?2.2). We then introduce our method that transfers the temporal knowledge from self-attention of the teacher model to student models  Our proposed method is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Teacher: Large Transformer-based Model</head><p>We employ the XLSR-wav2vec 2.0 <ref type="bibr" target="#b12">[13]</ref> as our teacher model F T , which is a large-scale transformer-based ASR model with state-of-the-art performance on multilingual ASR. The teacher model translates a raw audio x to latent representations for n time-steps using a convolutional feature encoder. The latent representations are passed through m consecutive transformer layers to output context representations c i=1,??? ,n , where c i ? R d is the d-dimensional vector.</p><p>To perform audio classification, we attach a fully-connected layer to the output c of F T . Similar to the fine-tuning of language models <ref type="bibr" target="#b14">[15]</ref>, we feed only the first output c 1 to the fully-connected layer. F T and the fully-connected layer are trained end to end on audio classification datasets (Details described in Sec. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Student: Lightweight On-device Models</head><p>In this paper, we consider the following on-device audio classification models adopted by <ref type="bibr" target="#b1">[2]</ref> as our student models F S : a simple RNN-based model (LSTM-P) <ref type="bibr" target="#b15">[16]</ref>, a CNN-based model (TC-ResNet) <ref type="bibr" target="#b16">[17]</ref>, a model that uses both CNN and RNN (CRNN) <ref type="bibr" target="#b17">[18]</ref>, a model including an attention mechanism (Att-RNN) <ref type="bibr" target="#b18">[19]</ref>, and a multi-head variant of Att-RNN (MHAtt-RNN) <ref type="bibr" target="#b1">[2]</ref>. For the student models, we pass the raw audio to the MFCC-based feature encoder. The student models extract the context representations c i=1,??? ,n for n time steps, where c i ? R d is the d -dimensional vector. Note that the student models have different sizes n and d depending on their architecture.</p><p>We integrate the attention mechanism that extracts attention weights a S ? R n from every student model except Att-RNN and MHAtt-RNN architectures which already include the attention mechanism. The extracted student attention weights act as recipients for transferring high-level knowledge from the teacher model F T . The attention weights are computed by applying the softmax function on the innerproduct of the context representations c and a query q ? R d as follows:</p><formula xml:id="formula_0">a S = softmax(c i ? q) n i=1 .</formula><p>There are many strategies for designing the query q, e.g., random initialization <ref type="bibr" target="#b19">[20]</ref> or projection of the medium context representation <ref type="bibr" target="#b18">[19]</ref>, where we chose the latter. As the training progresses, a S learns to capture the importance of each context representation c i . For Att-RNN, we directly use its attention weights as a S . For MHAtt-RNN, which adopts a multi-head attention mechanism, we are motivated by <ref type="bibr" target="#b9">[10]</ref> to choose one of the heads to yield attention weights. While this approach is not universally applicable for arbitrary architectures, we emphasize that the only architectural requirement for the student model to satisfy is to output an intermediate features which preserve the temporal information. There are already many prominent architectures that satisfy this requirement, e.g. CNN feature map before the global average pooling layer or sequence of outputs of RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal Knowledge Distillation</head><p>To extract the temporal knowledge from the teacher model, we leverage m self-attention maps A i=1,??? ,m T ? R n?n from m transformer layers of the teacher model F T . The attention rollout technique <ref type="bibr" target="#b20">[21]</ref> is applied to the self-attention maps to result in a single unified attention map A T ? R n?n . We utilize the first vector a 1 ? R 1?n in A T as attention weights of the teacher model since the teacher is trained by performing the audio classification task based on the context representation c 1 of the first time step.</p><p>To transfer the temporal knowledge from the teacher model to the student model, we align a 1 and a S using the L KL loss. We define L KL as a Kullback-Leibler (KL) divergence between the two attention weights so that minimizing the loss will penalize the misalignment. However, the loss term L KL cannot necessarily be directly computed since the dimensions of the two weights a 1 and a S might not match (n = n ). Therefore, we employ a simple linear interpolation method to resize the attention weights to match the dimension of a 1 with a S while preserving the temporal knowledge. After applying the linear interpolation on the attention weights a 1 , we obtain the final attention weights a T of the teacher model F T . Using the attention weights of the teacher and student models, the loss term L KL is computed as follows:</p><formula xml:id="formula_1">L KL = D KL (a S |a T ),<label>(1)</label></formula><p>where D KL is the KL divergence. The final student loss L is defined as follows:</p><formula xml:id="formula_2">L = ?L KL + (1 ? ?)L CLS ,<label>(2)</label></formula><p>where the L CLS is a cross-entropy-based classification loss of the student model. ? is a hyperparameter that controls the influence of each loss term. Note that the on-device models (student models) except Att-RNN and MHAtt-RNN leverage the attention weights only during training to receive the knowledge from the teacher model, and they do not use the attention weights during inference. In other words, there is no architectural change in the model during inference, hence showing zero computational overhead for the inference of on-device models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>Datasets. We verify the effectiveness of our proposed KD method with experiments on a real-world AED dataset (FSD50K) and a noisy KWS dataset (called noisy speech commands v2).</p><p>FSD50K <ref type="bibr" target="#b2">[3]</ref>: The FSD50K dataset <ref type="bibr" target="#b2">[3]</ref> is a multi-label audio event detection dataset, which represent real-world audios. The dataset is composed of 51,197 human-labeled audio events with 200 classes with lengths ranging from 0.3 to 30 seconds. The audio inputs are zero-padded when the inputs are shorter than 30 seconds.</p><p>Noisy Speech Commands v2: To clearly demonstrate the effectiveness of our method in distilling the high-level knowledge of temporal information, we construct a noisy KWS dataset by inserting the existing KWS dataset, Speech Commands v2 <ref type="bibr" target="#b0">[1]</ref>, to the background speech noise. The Speech Commands v2 dataset <ref type="bibr" target="#b0">[1]</ref> contains 105,829 one-second utterances of 35 words stored as 16-bit mono PCM WAVE files with a 16KHz sample rate. Following the settings from [2, 1], we use their training splits and the 12 class labels, which include the "silence" label with no speech and "unknown" label with an additional 20 keywords. We generate the synthetic audios by injecting the one-second speech command audios into the background speech noise obtained from the "Hubbub", "speech noise", "speech babble" classes of the AudioSet dataset <ref type="bibr" target="#b13">[14]</ref>. The speech noise is randomly cropped to a predefined duration. The locations of the speech command audios are uniformly sampled within the speech noise audio. We define four Noisy Speech Commands datasets, each using a fixed duration of 2, 4, 6, and 8 seconds noise. The noisy datasets are split using the same training and validation splits as the Speech Commands v2 dataset. Audio mixing is done by weighted sum of both the one-second Speech Commands v2 and AudioSet noise PCM signals, with respective weights of 0.75 and 0.25. Baselines. We employ the five on-device models described in Sec. 2.2 as baseline models. By varying the hyperparameter ? ? {0.0, 0.1, 0.25, 0.5}, we observe the change of predictive performance to show the effect of the hyperparameter that controls the influence of our proposed method. When ?=0.0, the loss becomes equal to the vanilla cross-entropy loss without knowledge distillation. Metrics. Following <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b1">[2]</ref>, we use mean average precision (mAP) and accuracy to evaluate the performance on FSD50K and Noisy speech commands v2, which are the multi-and single-label classification datasets, respectively. The higher score in both metrics indicates higher performance. Implementation Details. The teacher model (XLSR-wav2vec 2.0) is pretrained on a multilingual speech dataset <ref type="bibr" target="#b12">[13]</ref>. We fine-tune the teacher model with a batch size of 16 for 50 epochs. For FSD50K and Noisy speech commands v2, we set the learning rate as 2e-5 and 5e-4, respectively, with 1K warmup steps. We apply SpecAugment <ref type="bibr" target="#b21">[22]</ref> with probability 0.75 which consists of two 10% frequency mask while training.</p><p>The on-device models are trained for 20K iterations with a batch size of 100. Audios are re-sampled to a sample rate of 16kHz. The student models take the MFCC representations of the audio as an input. We employed a best keeping strategy to keep the weights with the best performance on the validation set using the evaluation metrics. We leverage an existing code <ref type="bibr" target="#b1">[2]</ref> for our experiments.  <ref type="table" target="#tab_0">Table 1</ref> demonstrates the effectiveness of our method on FSD50K dataset. Our method shows clear improvement on all the representative architectures we have chosen, where the mAP scores of LSTM-P, TC-ResNet, CRNN, Att-RNN, and MHAtt-RNN increased by 13.9%, 7.6%, 9.5%, 21.5%, and 25.3%, respectively. Especially, improvement on attentionbased methods is substantial. <ref type="table" target="#tab_1">Table 2</ref> summarizes the evaluation results on the Noisy Speech Commands v2. Our method shows superior performance across all datasets on all the student architectures. Furthermore, attention-based student models that applied attention distillation often exceed the teacher model performance, whereas vanilla student models are always inferior to the teacher model. We also observed that the accuracy disparity between models trained with vanilla and attention distillation losses tends to grow for non-attention-based models as the audio length increases. This implies that those models benefit from the temporal knowledge extracted from attention weights without any architectural changes. Attention-based student models Att-RNN MHAtt-RNN <ref type="figure">Fig. 2</ref>. Visualization of attention weights extracted from multiple models. We input an arbitrary sample from the Noisy Speech Commands v2 dataset with 8 seconds noise. We plot the location of the one second keyword to all the plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Results on Real-word AED dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Results on Noisy KWS dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Further Analysis</head><p>In <ref type="figure">Figure 2</ref>, we visualize attentions extracted from the teacher model and all the student models applied on 8 second audio sample from Noisy Speech Commands v2. Student architectures are selected based on the best categorical accuracy from <ref type="table" target="#tab_1">Table 2</ref>. The gray region represents the position of the keyword location within the audio. We observe that even though the teacher model is trained only with the classification label, attention weights successfully focuses on the keyword location. We can also see that all the on-device models attend at similar positions inside the keyword location, indicating that the teacher and the student attention weights are accurately aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a novel attention distillation method that transfers the temporal knowledge from large teacher models to on-device student audio classification models. We extract the attention weights from both the teacher and the student models, and align them via KL divergence. Our method can be applied to various architectures with no architectural change during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of our proposed method. without any architectural changes during inference ( ?2.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the FSD50K dataset. Test mAP of the best model found by the validation is reported, where the validation mAP is obtained every 400 steps.</figDesc><table><row><cell>Model</cell><cell>Vanilla ? = 0.0</cell><cell cols="3">Attention Distillation ? = 0.1 ? = 0.25 ? = 0.5</cell></row><row><cell>wav2vec 2.0</cell><cell>0.5498</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell>LSTM-P</cell><cell>0.1141</cell><cell>0.1274</cell><cell>0.1300</cell><cell>0.1043</cell></row><row><cell>TC-ResNet</cell><cell>0.1814</cell><cell>0.1841</cell><cell>0.1951</cell><cell>0.1509</cell></row><row><cell>CRNN</cell><cell>0.2789</cell><cell>0.2670</cell><cell>0.2835</cell><cell>0.3053</cell></row><row><cell>Att-RNN</cell><cell>0.2856</cell><cell>0.3471</cell><cell>0.2885</cell><cell>0.2891</cell></row><row><cell>MHAtt-RNN</cell><cell>0.2647</cell><cell>0.1694</cell><cell>0.3182</cell><cell>0.3317</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on Noisy Speech Commands v2 dataset. Test accuracy (%) of the best model found by the validation accuracy is reported, where the validation accuracy is obtained every 400 steps. Best accuracies are in bold, and the performance of the student models that outperform the teacher model is underlined.</figDesc><table><row><cell>Audio Length</cell><cell>Model</cell><cell>Vanilla ? = 0.0</cell><cell cols="3">Attention Distillation ? = 0.1 ? = 0.25 ? = 0.5</cell></row><row><cell></cell><cell>wav2vec 2.0</cell><cell>90.59</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>LSTM-P</cell><cell>88.73</cell><cell>88.98</cell><cell>89.31</cell><cell>88.92</cell></row><row><cell>2s</cell><cell>TC-ResNet</cell><cell>87.77</cell><cell>88.08</cell><cell>86.21</cell><cell>86.27</cell></row><row><cell></cell><cell>CRNN</cell><cell>89.96</cell><cell>90.06</cell><cell>90.00</cell><cell>89.46</cell></row><row><cell></cell><cell>Att-RNN</cell><cell>89.88</cell><cell>91.31</cell><cell>91.67</cell><cell>90.94</cell></row><row><cell></cell><cell>MHAtt-RNN</cell><cell>89.75</cell><cell>91.25</cell><cell>91.67</cell><cell>91.75</cell></row><row><cell></cell><cell>wav2vec 2.0</cell><cell>91.22</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>LSTM-P</cell><cell>85.19</cell><cell>88.23</cell><cell>88.52</cell><cell>89.08</cell></row><row><cell>4s</cell><cell>TC-ResNet</cell><cell>87.60</cell><cell>88.33</cell><cell>87.21</cell><cell>84.62</cell></row><row><cell></cell><cell>CRNN</cell><cell>89.69</cell><cell>89.44</cell><cell>89.46</cell><cell>90.21</cell></row><row><cell></cell><cell>Att-RNN</cell><cell>90.65</cell><cell>91.98</cell><cell>91.79</cell><cell>91.58</cell></row><row><cell></cell><cell>MHAtt-RNN</cell><cell>91.19</cell><cell>91.58</cell><cell>92.12</cell><cell>91.73</cell></row><row><cell></cell><cell>wav2vec 2.0</cell><cell>90.93</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>LSTM-P</cell><cell>45.27</cell><cell>69.21</cell><cell>85.58</cell><cell>85.10</cell></row><row><cell>6s</cell><cell>TC-ResNet</cell><cell>86.00</cell><cell>86.85</cell><cell>84.23</cell><cell>82.10</cell></row><row><cell></cell><cell>CRNN</cell><cell>88.58</cell><cell>89.88</cell><cell>89.29</cell><cell>89.46</cell></row><row><cell></cell><cell>Att-RNN</cell><cell>90.88</cell><cell>90.77</cell><cell>90.73</cell><cell>91.19</cell></row><row><cell></cell><cell>MHAtt-RNN</cell><cell>90.58</cell><cell>90.96</cell><cell>91.67</cell><cell>91.10</cell></row><row><cell></cell><cell>wav2vec 2.0</cell><cell>90.95</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>LSTM-P</cell><cell>78.44</cell><cell>82.19</cell><cell>34.58</cell><cell>66.25</cell></row><row><cell>8s</cell><cell>TC-ResNet</cell><cell>77.81</cell><cell>85.79</cell><cell>85.71</cell><cell>80.15</cell></row><row><cell></cell><cell>CRNN</cell><cell>88.94</cell><cell>89.02</cell><cell>89.79</cell><cell>89.77</cell></row><row><cell></cell><cell>Att-RNN</cell><cell>88.81</cell><cell>90.44</cell><cell>90.98</cell><cell>90.75</cell></row><row><cell></cell><cell>MHAtt-RNN</cell><cell>88.33</cell><cell>91.50</cell><cell>91.79</cell><cell>91.35</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Streaming keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirk?</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">FSD50K: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distilling the knowledge of BERT for sequence-tosequence ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Futami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sei</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge distillation for small-footprint highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4820" to="4824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel Tairum</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4249" to="4253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation for model compression of attention-based sequence-tosequence speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raden</forename><surname>Mu&amp;apos;az Mun&amp;apos;im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakamasa</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6151" to="6155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Codert: Distilling encoder representations with colearning for transducer-based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupak</forename><surname>Vignesh Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Mouchtaris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07734</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intra-utterance similarity preserving knowledge distillation for audio tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Chi</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Abdelrahman Mohamed, and Michael Auli</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal convolution for realtime keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan?mer</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coimbra De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loesener Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
