<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bias Loss for Mobile Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusine</forename><surname>Abrahamyan</surname></persName>
							<email>lusine.abrahamyan@vub.be</email>
							<affiliation key="aff0">
								<orgName type="department">ETRO Department</orgName>
								<orgName type="institution">Vrije Universiteit Brussel (VUB)</orgName>
								<address>
									<addrLine>Pleinlaan 2</addrLine>
									<postCode>B-1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">imec</orgName>
								<address>
									<addrLine>Kapeldreef 75</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Ziatchin</surname></persName>
							<email>valentin.ziatchin@picsart.com</email>
							<affiliation key="aff1">
								<orgName type="institution">PicsArt Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ETRO Department</orgName>
								<orgName type="institution">Vrije Universiteit Brussel (VUB)</orgName>
								<address>
									<addrLine>Pleinlaan 2</addrLine>
									<postCode>B-1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">imec</orgName>
								<address>
									<addrLine>Kapeldreef 75</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Deligiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ETRO Department</orgName>
								<orgName type="institution">Vrije Universiteit Brussel (VUB)</orgName>
								<address>
									<addrLine>Pleinlaan 2</addrLine>
									<postCode>B-1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">imec</orgName>
								<address>
									<addrLine>Kapeldreef 75</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndeligia@etrovub</forename><surname>Be</surname></persName>
						</author>
						<title level="a" type="main">Bias Loss for Mobile Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in recent years. However, they still fail to provide the same predictive power as CNNs with a large number of parameters. The diverse and even abundant features captured by the layers is an important characteristic of these successful CNNs. However, differences in this characteristic between large CNNs and their compact counterparts have rarely been investigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic. Diverse features present in the activation maps derived from a data point during model inference may indicate the presence of a set of unique descriptors necessary to distinguish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose architectures are brought to boost the number of unique descriptors in the last layers. Experiments conducted on benchmark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, our SkipblockNet-M can achieve 1% higher classification accuracy than MobileNetV3 Large with similar computational 50 100 150 200 250 300 350 400 FLOPs (millions) 68 70 72 74 76 ImageNet Top-1 Accuracy(%) SkipNet (ours) MobileNetV3 Large MobileNetV2 MuxNet ShuffleNetV2 MnasNet FBNet Figure 1. Accuracy v.s. FLOPs on ImageNet. Our SkipblockNet model trained with the proposed bias loss outperforms previous well-performing compact neural networks trained with the crossentropy loss.</p><p>cost on the ImageNet ILSVRC-2012 classification dataset. The code is available on the linkhttps://github. com/lusinlu/biasloss_skipblocknet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in recent years. However, they still fail to provide the same predictive power as CNNs with a large number of parameters. The diverse and even abundant features captured by the layers is an important characteristic of these successful CNNs. However, differences in this characteristic between large CNNs and their compact counterparts have rarely been investigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic. Diverse features present in the activation maps derived from a data point during model inference may indicate the presence of a set of unique descriptors necessary to distinguish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose architectures are brought to boost the number of unique descriptors in the last layers. Experiments conducted on benchmark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, our SkipblockNet-M can achieve 1% higher classification accuracy than MobileNetV3 Large with similar computational </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep CNNs have shown superior performance on numerous computer vision tasks, such as classification, semantic segmentation, and object detection. Typically, models with high predictive power contain a large number of parameters and require a substantial amount of floating point operations (FLOPs); for example, Inception-v3 <ref type="bibr" target="#b45">[45]</ref> has approximately 24M parameters and requires 6GFLOPs to process an image with a spatial size of 299 ? 299 pixels. With the advent of AI applications in mobile devices, sev-eral studies have focused on developing high-performance CNNs for resource-constrained settings. Several studies have focused on compressing existing high-performance pretrained models. The compression of the models can be achieved by performing quantization <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b55">55]</ref>, pruning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">16]</ref>, or knowledge distillation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b3">4]</ref>. Typically, the downside of these methods is an inevitable degradation of performance.</p><p>Another research line has focused on designing compact neural networks and architectural units <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">48]</ref>. For example, Xception <ref type="bibr" target="#b5">[6]</ref> introduced a cost-efficient replacement for the conventional convolution, namely, depthwise separable convolution. ShuffleNet <ref type="bibr" target="#b53">[53]</ref> replaced convolutional layers with a combination of pointwise group convolution with channel shuffle operation. The authors of EfficientNet <ref type="bibr" target="#b48">[48]</ref> proposed a scaling method that uniformly scales a model's width, depth, and resolution using a set of fixed scaling coefficients. However, a significant performance improvement in these methods is mostly connected with an increase in the number of parameters <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b11">12]</ref>. The solution to this problem can be the design of a task-specific objective function. The advantage of designing an objective function over the creation of a new architecture is that the former approach can improve the accuracy of a model without increasing the number of parameters. In general, the preferred loss function for classification is the cross-entropy; however, there exist studies indicating that other objectives can outperform the standard cross-entropy loss <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b32">32]</ref>. The authors of <ref type="bibr" target="#b46">[46]</ref> proposed to compute cross-entropy with the weighted mixture of targets from the uniform distribution. In scenarios where the class imbalance problem exists, <ref type="bibr" target="#b32">[32]</ref> proposed to down-weight the loss assigned to well-classified examples. In <ref type="bibr" target="#b39">[39]</ref>, the authors proposed a meta-learning reweighting algorithm in order to tackle the problem of label noise in the dataset. Although these objectives achieve great performance boost, they target specific problems related mostly to the dataset and do not consider differences between the optimization of compact neural networks and their large counterparts. Diverse and even abundant information in the feature maps of high-performance CNNs often guarantees a comprehensive understanding of the input data. In compact CNN, due to the small numbers of parameters, the amount of extracted features will be smaller, and may not be sufficient to describe the object to be classified. For certain data points, these features may lack unique descriptors required to distinguish between the objects of different classes. As a result, in the absence of a sufficient amount of unique descriptors, the model cannot produce a valid prediction. We refer to these as random predictions that contribute no useful learning signal to the optimization process.</p><p>To address this problem, we design Bias Loss, a new loss that weights each data point's contribution in proportion to the diversity of features it provides. As a simple measure of diversity, we take the signal's variance, which describes how far the feature maps' values are spread from the average. Based on the variance, we design a nonlinear function, whose values serve as weights for the cross-entropy. This way, we let data points with diverse features have a higher impact on the optimization process and reduce a mislead caused by random predictions.</p><p>To further realize bias loss's full potential, we propose the SkipblockNet architecture to address the problem of a lack of extracted features in the last layer. Specifically, we design lightweight intermediate blocks to straightforwardly transfer the low-level features from the first layers to the lasts using skip connections. The usage of the proposed blocks will increase the number of data points with a large number of unique descriptors. Experimental results showed that the proposed Bias Loss is able to boost the performance of the existing mobile models, such as Mo-bileNetV3 Large <ref type="bibr" target="#b18">[18]</ref> (+0.5%), ShuffleNetV2 0.5? <ref type="bibr" target="#b35">[35]</ref> (+0.6%), SqueezeNet <ref type="bibr" target="#b23">[23]</ref> (+1%). Moreover, Skipblock-Net can surpass state-of-the-art compact neural networks such as MobileNetV3, on numerous tasks with fast inference on mobile devices.</p><p>To summarize, our contributions are three-fold: (1) we design a loss function to reduce the mislead in the optimization caused by random predictions in compact CNNs;</p><p>(2) we propose an efficient neural architecture to increase the number of data points with a large number of unique descriptive features; (3) our model achieves state-of-the-art performance on the ImageNet classification task under resource constrained settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many strategies have been proposed for designing compact, computationally efficient, and high-performance CNNs. Bellow, we present two major categories of solutions: the design of mobile architectures and task-oriented objective functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mobile Architectures</head><p>Several CNN architectures have been developed for resource constraint settings <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">34]</ref>. Among them, the MobileNet <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b18">18]</ref> and ShuffleNet <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b35">35]</ref> families stand out due to their high performance achieved with fewer FLOPs. MobileNetV2 <ref type="bibr" target="#b42">[42]</ref> introduced inverted residual blocks to improve the performance over MobileNetV1 <ref type="bibr" target="#b19">[19]</ref>. Furthermore, MobileNetV3 <ref type="bibr" target="#b18">[18]</ref> utilized NAS (Neural Architecture Search) technology <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b37">37]</ref> resulting in achieving higher performance with fewer FLOPs. ShuffleNet <ref type="bibr" target="#b53">[53]</ref> introduced the channel shuffle operation to boost the flow of the information within channel groups. ShuffleNetV2 <ref type="bibr" target="#b35">[35]</ref> further improved the actual speed on hardware. Despite the high performance achieved with very few FLOPs, the importance of maintaining unique descriptive features in the last layers of the network has never been well exploited. To that end, we propose SkipblockNet, an architecture that is designed to increase the number of unique descriptive features in the last layers and reduce the number of random predictions. SkipblockNet shares many similarities with the previous high-performance CNNs, in particular, the inverted residual blocks used in MobileNetV3 <ref type="bibr" target="#b18">[18]</ref> and the concept of skip connections utilized in U-Net <ref type="bibr" target="#b40">[40]</ref>. We emphasize that our simple modifications achieve superior results not due to innovation in design but due to the combination of the network with our novel loss. In this way, we can benefit from the developed loss the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Objective Functions</head><p>The most common choice for the objective function in many tasks is the cross-entropy. However, various studies have indicated that the design of the loss function, aimed to tackle a specific problem, can have significant benefits <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b22">22]</ref>. Lin et al. <ref type="bibr" target="#b32">[32]</ref> proposed to reshape the standard cross-entropy to address the problem of foreground-background class imbalance encountered during the training of an object detector. The mechanism of label smoothing <ref type="bibr" target="#b46">[46]</ref> suggests using "soft" targets in the cross-entropy calculation. These "soft" targets are a weighted mixture of original targets with the uniform distribution over labels. This technique helps preventing the network from becoming over-confident in numerous tasks like image classification, language translation, and speech recognition. Various studies have attempted to address the obstacle caused by noisy labels <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b54">54]</ref>. In <ref type="bibr" target="#b39">[39]</ref> the authors introduce a variation of the weighted cross-entropy, where weights are being learned by the multi-layer perceptron. The focus of these works has primarily been to optimize the performance of models with a large number of parameters. On the contrary, our loss is designed to tackle the problem arising because of the lack of parameters in compact models, namely the problem of possible mislead in an optimization process caused by random predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bias Loss</head><p>We design the Bias Loss to address the resourceconstrained classification scenario in which there can be a mislead during the optimization process of deep convolutional neural networks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b15">15]</ref> caused by random predictions. We advocate that, in compact neural networks, data points failing to provide a sufficient amount of unique features that can describe the object force the model to produce random predictions, that is, predictions made in the absence of feature diversity.</p><p>As a simple metric of diversity in all of our experiments, we adopt the signal variance, which can indicate how far the  <ref type="formula" target="#formula_6">(6)</ref>, where v is the scaled variance. The function comprises two hyperparameters ?, and ?. An increase of ? reduces the impact of low variance data points on the cumulative loss. ? controls the influence of the high variance data points. </p><formula xml:id="formula_0">v i = n j=1 (t j ? ?) 2 n ? 1 ,<label>(1)</label></formula><p>where ? = n j=1 t j n .</p><p>(</p><p>In addition, the variance is scaled to the range [0, 1] for further use in the loss function, that is,</p><formula xml:id="formula_2">v i = (v i ? min) (max ? min) ,<label>(3)</label></formula><p>where, at each iteration, max and min is the maximum and minimum values of the activations in the batch of feature maps. This is performed to ensure that outliers in the variance values will not lead to large changes in the loss and will not make the model unstable. Futhermore, we propose to inject this knowledge about the absence of the unique descriptive features into the optimization process, and to this end, we present the new loss function, namely the Bias Loss. The bias loss is a dynamically scaled cross-entropy loss, where the scale decays as the variance of data point decreases.</p><p>Let X ? R c?h?w be the feature space, where c is a number of input channels and h, w are the height and width of the input data, and Y = {1, ..., k} be the label space, where k is the number of classes. In a standard scenario, we are given a dataset D = (</p><formula xml:id="formula_3">x i , y i ) N i=1 , where each (x i , y i ) ? X ? Y , and a neural network f (x; ?),</formula><p>where ? denotes the model parameters. Conventionally, the training aims at learning a model by minimizing the expected loss for the training set. In general, the cross-entropy loss for a classification problem is</p><formula xml:id="formula_4">L ce = ? 1 N N i=1 k j=1 y ij log f j (x i ; ?),<label>(4)</label></formula><p>where we consider that the output layer of the neural network is a softmax. In order to calibrate the contribution of each data point into the cumulative loss, we propose to add a nonlinear scaling function, which aims at creating a bias between the data points with low and high variance. The bias loss is defined as</p><formula xml:id="formula_5">L bias = ? 1 N N i=1 k j=1 z(v i )y ij log f j (x i ; ?),<label>(5)</label></formula><formula xml:id="formula_6">z(v i ) = exp(v i * ?) ? ?,<label>(6)</label></formula><p>where ? and ? are tunable contribution parameters and v is the scaled variance of the output of the convolutional layer. The bias function is visualized for several values of ? and ? in <ref type="figure" target="#fig_2">Figure 3</ref>. We notice two properties of the bias function: (i) when the variance is low, the function values reach their minimum, (1 ? ?), and the impact of these data points is down-weighted. As the variance increases, the z(v)'s values, together with the influence of the data point, exponentially increase. (ii) The parameter ? smoothly adjusts the rate of the impact of high variance examples. With the increase of ?, the impact of high variance data points also increases. In addition, <ref type="figure" target="#fig_1">Figure 2</ref> presents the values of the bias loss depending on the variance and the prediction score. The loss is down-weighted mainly for low confidence and low variance data points for both correct and incorrect predictions. Furthermore, it is up-weighted for the high confidence and high variance incorrect predictions, as learning from this kind of data points with a large number of unique features can have a positive impact on the optimization process. Our empirical results suggest that selecting ? = 0.3, ? = 0.3 leads to the best performance. Intuitively, the proposed function helps focusing the learning on examples that can provide a large number of unique features and reducing the possible mislead in the optimization process caused by random predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SkipblockNet Mobile Architectures</head><p>We also introduce a new computational block and a new CNN architecture to further increase the gain in performance obtained via the bias loss. The presented block can be easily integrated into existing architectures and boost the information flow toward the last layers, without additional effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Skip Block</head><p>The idea of the skip block is to deliver the low-level features directly from the first layers to the last ones. The block's design is motivated by the U-Net <ref type="bibr" target="#b40">[40]</ref> architecture, where, in an autoencoder style architecture, the outputs of layers with the same spatial dimensions in the encoder and decoder are connected via skip connections. Generally, in classification networks, the layers' spatial sizes gradually decrease, making it impossible to use skip connections straightforwardly. To address this limitation, we propose an intermediate block, which is brought to connect layers with different spatial sizes and enrich the last layers with the low-level features extracted from the first layers. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the skip block consists of a pooling operation combined with convolutions. First, in order to keep the key features and reduce the spatial sizes, we apply an adaptive average pooling, followed by three convolutional layers. Batch normalization (BN) <ref type="bibr" target="#b24">[24]</ref> and ReLU nonlinearity <ref type="bibr" target="#b0">[1]</ref> are applied after each convolutional layer except for the last one where ReLU is not used. The choice of the adaptive average pooling is motivated by the fact that it takes all features into account, making it possible for the skip block to process all input values. Concerning the convolutional layers' parameters, the setup proposed in Mo-bileNetV3 for the inverted residual blocks is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SkipblockNet</head><p>Since our primary goal is to boost the number of unique descriptive features in compact neural networks, while mitigating computational complexity, we propose a Skipblock-Net architecture that deploys skip blocks. Due to its superior performance as a design baseline, we follow the architecture of MobileNetV3 <ref type="bibr" target="#b18">[18]</ref>. SkipblockNet <ref type="figure" target="#fig_3">(Figure 4</ref>) consists of the stack of inverted residual and the classification blocks of MobileNetV3, and includes our novel skip block. The first layer is a convolution with 16 filters followed by 15 inverted residual blocks. Two skip blocks are inserted after the first inverted residual block <ref type="figure" target="#fig_3">(Figure 4)</ref> with the purpose of transferring the information to the sixth and tenth inverted residual blocks. After the skip and inverted residual blocks, a convolutional layer and global average pooling are applied before the final classification block, which consist of dropout and fully connected layers. Similar to MobileNetV3, we use hard-swish nonlinear functions due to their efficiency. As can be seen in <ref type="table">Table 2</ref>, the latency of SkipblockNet on mobile devices is on par with that of MobileNetV3. Although the described architecture can already guarantee high performance and low latency, there can be situations where a faster model or higher accuracy may be required. In order to provide a fully customizable network, we integrate the width multiplier, presented in the inverted residual block, into the skip block so as to to control the number of the channels in each layer. By manipulating the width multiplier, the width of the entire network can be changed. That will lead to changes in the model size and computational cost, as well as changes in performance. In general, increase of the multiplier will lead to increase in the performance and latency, and vice versa. The presented architectures give a basic design for reference, and for further improvement, AutoML methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27]</ref> can be used to tune the skip blocks and boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present empirical results to demonstrate the efficiency of the novel bias loss and the proposed family of SkipblockNet models. We report results on three tasks: image classification, object detection, and transfer learning. All experiments were performed on a single machine with 2 GeForce RTX 2080 Ti GPUs. Further, during trainings, activation maps with outliers produce very high variances.</p><p>In turn, these high variances will lead to high values of the bias function and make the training unstable. To avoid this effect, in all experiments, we clamp the output of the bias function to the range of [0.5, 1.5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet Classification</head><p>We set experiments on ImageNet <ref type="bibr" target="#b41">[41]</ref> and compare the achieved accuracies versus various measures of resource usage such as FLOPs and latency.</p><p>Training Setup: ImageNet is a large-scale image classification dataset with over 1.2M training and 50K validation images belonging to 1000 classes. For experiments on ImageNet, we follow most of the settings used in Effi-cientNet <ref type="bibr" target="#b48">[48]</ref>: the RMSProp optimizer with a decay of 0.9 and a momentum of 0.9; a batch norm momentum of 0.99; a weight decay of 1e ? 5; and an initial learning rate of 1e ? 6 that increased to 0.032 in the initial 3 epochs <ref type="bibr" target="#b10">[11]</ref> and then decays by 0.97 every 2.4 epochs. Furthermore, we adopt Inception preprocessing with an image size of 224 ? 224 pixels <ref type="bibr" target="#b44">[44]</ref>, a batch size of 512, and complement the training with an exponential moving average with a decay rate of 0.99995. The reported results are singlecrop performance evaluations on the ImageNet validation set. The aforementioned setting is adopted in order to perform a fair comparison, as most of the state-of-the-art architectures <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b18">18]</ref> that we are comparing with are using the same setup.</p><p>Results: <ref type="table" target="#tab_0">Table 1</ref> shows the performance of the Skip-blockNet family models in relation to several modern resource-constraint network architectures. The networks are grouped into four levels of computational complexity: 50 ? 100, 100 ? 200, 200 ? 300, and 300 ? 400 million FLOPs. We compare them in terms of accuracy, number of parameters and computational complex- ity (FLOPs). Overall, our family of SkipblockNet models (SkipblockNet-XS, SkipblockNet-S, SkipblockNet-M, SkipblockNet-L) trained with the bias loss outperforms other competitors at the different computational complexity levels. Specifically, SkipblockNet-M archieves 76.2% accuracy with 246 MFLOPs, which is higher by 1% compared with the MobileNetv3 Large <ref type="bibr" target="#b18">[18]</ref> and by 0.4% compared with MixNet-S <ref type="bibr" target="#b49">[49]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> visualize the trade-off obtained by SkipblockNet and previous compact neural networks. Inference Speed: We measure the inference speed of the SkipblockNet-M on Google Pixel phones using the Py-Torch V1.6 Mobile framework <ref type="bibr" target="#b36">[36]</ref>. We use a single core in all our measurements. <ref type="table">Table 2</ref> reports the latencies of the SkipblockNet along with the other state-of-the-art compact neural networks. The results suggest that Skipblock- Net can achieve 1% higher accuracy than the MobileNetV3 with computational overhead higher only by 1ms on Google Pixel 4. Impact of Different Components on the Performance: To investigate the importance of the different techniques used in SkipblockNet, we conduct a series of experiments on the ImageNet dataset, shown in <ref type="table" target="#tab_1">Table 3</ref>. We first consider the MobileNetV3, a baseline architecture for our SkipblockNet, and trained it with the Bias Loss. As shown in <ref type="table" target="#tab_1">Table 3</ref>, the bias loss can increase the accuracy of MobileNetV3 by 0.5%, compared with the training with cross-entropy. To evaluate the impact of skip blocks, we examined the performance of the baseline MobileNetV3 with the SkipblockNet-M (which is the MobileNetV3 architecture plus skip blocks), both trained with the cross-entropy.</p><p>The results indicate that a gain of 0.3% can be obtained only by using the skip blocks. Moreover, by enriching the last layers with the low-level information of the first layers, we  can increase the number of data points with high variance and make the boost in the performance related to the usage of the bias loss even higher (i.e., an increase of 0.5% in the case of MobileNetV3 and 0.7% for SkipblockNet-M). Moreover, in order to show the skip blocks' advantage over a simple increase of the depth multiplier, we trained MobileNetV3 1.05? with the 247M FLOPs and 5.9M parameters and compare it with SkipblockNet-M (246M FLOPs, 5.5M parameters). When trained with the crossentropy, SkipblockNet-M achieves 75.5% accuracy, while MobileNetV3 1.05? achieves 75.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Classification with Bias Loss</head><p>To verify the effectiveness of the proposed bias loss, we apply it on several resource constraint neural networks and conduct experiments using the CIFAR-100 <ref type="bibr" target="#b31">[31]</ref> classification dataset. The CIFAR-100 dataset <ref type="bibr" target="#b31">[31]</ref> consists of 60, 000 images from 100 classes. The dataset is divided into 50, 000 training and 10, 000 testing images. For training on CIFAR-100, we use an SGD optimizer with a momentum equal to 0.9 and a weight decay of 5e ? 4. The initial learning rate is set to 1e ? 1 and then decays at the epochs 60, 120, 160 at a rate of 0.2. For data augmentation, images are randomly flipped horizontally and rotated between the angles [? <ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b15">15]</ref>. <ref type="table" target="#tab_2">Table 4</ref> reports the accuracy of neural networks trained with cross-entropy, focal loss <ref type="bibr" target="#b32">[32]</ref> and bias loss. The results shows that models trained with bias loss systematically outperform models trained with crossentropy and focal loss by about 1% and 0.5%, respectively. The results indicate that our loss can boost the performance regardless of the architecture. In particular, when compared with the cross-entropy, for ShuffleNetV2 <ref type="bibr" target="#b35">[35]</ref> 0.5?, the accuracy is increased by 1.5%, for SqueezeNet <ref type="bibr" target="#b23">[23]</ref> by 1%, and for MobileNetV2 0.75? <ref type="bibr" target="#b42">[42]</ref> by 0.6%.  <ref type="figure">Figure 6</ref>. Transfer learning performance. Trade-off between top-1 accuracy and number of FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Transfer Learning</head><p>We have also evaluated our SkipblockNet on the transfer learning task using the Food101 <ref type="bibr" target="#b2">[3]</ref> dataset. Food-101 consists of 75, 750 training and 25, 250 testing images from 101 different classes. <ref type="figure">Figure 6</ref> compares the accuracy against FLOPs for our models and the list of other neural networks. Each SkipblockNet model is first trained from scratch on ImageNet and all weights are fine-tuned on the Food101 dataset using a setup similar to <ref type="bibr" target="#b28">[28]</ref>. The accuracy and FLOPs results for the rest of the models are taken from <ref type="bibr" target="#b28">[28]</ref>. The results show that our SkipblockNets significantly outperform previous compact neural networks and have accuracy on par with the models with a large number of parameters. Specifically, SkipblockNet-M achieves 0.95% higher accuracy, than MobileNetV2 <ref type="bibr" target="#b42">[42]</ref>, with 1.2? higher efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Object Detection</head><p>To evaluate the generalization ability of SkipblockNet, we conduct object detection experiments on the PASCAL VOC detection benchmark <ref type="bibr" target="#b7">[8]</ref>. We use the PASCAL VOC 2012 trainval split as training data and report the mean Average Precision (mAP) on the test split. Our experiments use the Single Shot Detector (SSD) <ref type="bibr" target="#b33">[33]</ref> as a detection frame- work and SkipblockNet as the feature extraction backbone.</p><p>To set up additional layers, we follow the procedure described in MobileNetV2 <ref type="bibr" target="#b42">[42]</ref>. We train all the models with the SGD optimizer for 200 epochs, with a batch size of 42, an input image size of 300 ? 300 ? 3, and an initial learning rate of 0.01 with cosine annealing. <ref type="table">Table 6</ref> reports the mAP achieved with the SkipblockNet compared with other models. Under similar resource usage, SkipblockNet-S + SSD achieves 0.5% higher mAP than MobileNetV2 <ref type="bibr" target="#b42">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis of the Variance in the Neural Networks</head><p>To show the role of the variance in the CNNs and the impact that the bias loss and the skip blocks can have on it, we conduct experiments on a range of well-known architectures. We examine the distribution of the values in convolutional layers in the networks with a large number of parameters like Inception V3 <ref type="bibr" target="#b45">[45]</ref> and DensNet169 <ref type="bibr" target="#b21">[21]</ref> and in compact ones. The purpose of the experiment is to compare the variance in the large and compact models and quantify the boost in the variance that the bias loss and skip blocks can provide. We took models pre-trained on ImageNet and examined the average, maximum and minimum values of the variance within the different layers. The results presented in <ref type="table" target="#tab_0">Table 1</ref> indicate that: (1) the variance in large models is significantly higher than that in compact models. Hence, large models can extract a decent amount of descriptive features for almost all samples, and the proposed strategy of reweighting will not boost their performance. (2) The bias loss can increase the variance throughout the model.  <ref type="figure">0?(CE)</ref>). The increase in the variance leads to the boost in the number of up-weighted data points meaning that, in the case of training with the bias loss, the optimizer will benefit from learning from more useful data points. Hence, the combination of skip blocks with the bias loss can bring a higher gain in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed the Bias Loss, a novel loss function designed to improve the performance of the compact CNNs by reducing a mislead during the optimization process caused by the data points with poor learning features. Furthermore, we presented a family of Skipblock-Net models whose architectures are brought to reduce the number of data points with poor learning features. Our extensive experiments, conducted on benchmark datasets and models, illustrate that the proposed loss is able to boost the performance of existing compact CNNs. Moreover, our SkipblockNet-M achieves significantly better accuracy and efficiency than all the latest compact CNNs on the Ima-geNet classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Accuracy v.s. FLOPs on ImageNet. Our SkipblockNet model trained with the proposed bias loss outperforms previous well-performing compact neural networks trained with the crossentropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed nonlinear function z(v) given in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>LceLbias (variance = 0.2)Lbias (variance = 0.6) Lbias (variance = 1.0) Loss v.s. prediction probability (the output of the softmax) for (a) correct predictions and (b) incorrect predictions. Lce denotes the cross-entropy and the ?, ? hyperparameters in the L bias loss are equal to 0.3. feature maps' values are spread out from the average. The intuition behind this choice is that the higher the variance, the higher the chances of obtaining a large number of unique features. For the variance calculations, the feature maps of the last convolutional layer (before the pooling and dropout operations) are used. This helps avoiding distortions in the results and estimating better the learning signal that a data point provides. Let T ? R b?c?h?w be the output of the convolutional layer, where b is a batch size, c is a number of input channels, and h and w are the height and width of the tensor. Prior to the variance calculations, T is unfolded into a two-dimensional array t ? R b?n , where n = c ? h ? w. The variance of the feature maps of the ith data point in the batch is equal to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Overview of the SkipblockNet architecture. On top of the inverted residuals<ref type="bibr" target="#b18">[18]</ref>, SkipblockNet uses skip blocks to transfer high-level features of the first block to the last layers. The network design is deliberately simple, which allows concentrating on the contribution of the novel bias loss, which boosts the performance by focusing the training on a set of data points with a rich learning signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Top-1 classification accuracy v.s. number of parameters on ImageNet for various compact CNNs. Our SkipblockNet is trained with the proposed bias loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>Skip blocks enrich later layers of a model with low-level features, thereby increasing the variance (SkipblockNet-M (CE) vs MobileNetV3 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of state-of-the-art resource constraint neural networks over accuracy, FLOPs, and number of parameters. The results are grouped into sections by FLOPs for better visualization.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FLOPs Parameters Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell cols="2">MobileNetV2 0.5? [42]</cell><cell></cell><cell></cell><cell>97M</cell><cell>2.0M</cell><cell>65.4</cell><cell>86.4</cell></row><row><cell cols="2">MUXNet-xs [34]</cell><cell></cell><cell></cell><cell>66M</cell><cell>1.8M</cell><cell>66.7</cell><cell>86.8</cell></row><row><cell cols="3">MobileNetV3 Small 1.0? [18]</cell><cell></cell><cell>66M</cell><cell>2.9M</cell><cell>67.4</cell><cell>-</cell></row><row><cell cols="5">SkipblockNet-XS (with bias loss) 81M</cell><cell>2.3M</cell><cell>69.9</cell><cell>88.9</cell></row><row><cell cols="2">ShuffleNetV2 1.0?</cell><cell></cell><cell></cell><cell>146M</cell><cell>2.3M</cell><cell>69.4</cell><cell>88.9</cell></row><row><cell cols="2">MUXNet-s [34]</cell><cell></cell><cell></cell><cell>117M</cell><cell>2.4M</cell><cell>71.6</cell><cell>90.3</cell></row><row><cell cols="2">ChamNet-C [7]</cell><cell></cell><cell></cell><cell>212M</cell><cell>3.4M</cell><cell>71.6</cell><cell>-</cell></row><row><cell cols="3">MobileNetV3 large 0.75?</cell><cell></cell><cell>155M</cell><cell>4.0M</cell><cell>73.3</cell><cell>-</cell></row><row><cell cols="3">SkipblockNet-S (with bias loss)</cell><cell></cell><cell>152M</cell><cell>3.6M</cell><cell>73.8</cell><cell>91.4</cell></row><row><cell cols="2">FBNet-A [50]</cell><cell></cell><cell></cell><cell>249M</cell><cell>4.3M</cell><cell>73.0</cell><cell>-</cell></row><row><cell cols="3">MobileNetV3 Large 1.0?</cell><cell></cell><cell>219M</cell><cell>5.4M</cell><cell>75.2</cell><cell>-</cell></row><row><cell cols="2">MUXNet-m</cell><cell></cell><cell></cell><cell>218M</cell><cell>3.4M</cell><cell>75.3</cell><cell>92.5</cell></row><row><cell cols="2">GhostNet 1.3?</cell><cell></cell><cell></cell><cell>226M</cell><cell>7.3M</cell><cell>75.7</cell><cell>92.7</cell></row><row><cell cols="2">MixNet-S [49]</cell><cell></cell><cell></cell><cell>256M</cell><cell>4.1M</cell><cell>75.8</cell><cell>92.8</cell></row><row><cell cols="4">SkipblockNet-M (with bias loss)</cell><cell>246M</cell><cell>5.5M</cell><cell>76.2</cell><cell>92.8</cell></row><row><cell cols="2">ProxylessNAS [5]</cell><cell></cell><cell></cell><cell>320M</cell><cell>4.1</cell><cell>74.6</cell><cell>92.2</cell></row><row><cell cols="2">MnasNet-A2 [47]</cell><cell></cell><cell></cell><cell>340M</cell><cell>4.8M</cell><cell>75.6</cell><cell>92.7</cell></row><row><cell cols="2">EfficientNet-B0 [48]</cell><cell></cell><cell></cell><cell>390M</cell><cell>5.3</cell><cell>76.3</cell><cell>93.2</cell></row><row><cell cols="2">MUXNet-l</cell><cell></cell><cell></cell><cell>318M</cell><cell>4.0M</cell><cell>76.6</cell><cell>93.2</cell></row><row><cell cols="3">MobileNetV3 large 1.25?</cell><cell></cell><cell>356M</cell><cell>7.5M</cell><cell>76.6</cell><cell>-</cell></row><row><cell cols="3">SkipblockNet-L (with bias loss)</cell><cell></cell><cell>364M</cell><cell>7.1M</cell><cell>77.1</cell><cell>93.4</cell></row><row><cell cols="5">Table 2. Top-1 accuracy v.s. latency on Google Pixel family</cell></row><row><cell cols="5">phones (Pixel-n denotes a Google Pixel-n phone). All latencies</cell></row><row><cell cols="5">are in ms and are measured using a single core with a batch size of</cell></row><row><cell cols="3">one. The top-1 accuracy is calculated on ImageNet.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Top-1 (%) Pixel 4 Pixel 3</cell><cell></cell></row><row><cell>SkipblockNet-M</cell><cell>76.2</cell><cell>27</cell><cell>42</cell><cell></cell></row><row><cell>GhostNet 1.3?</cell><cell>75.7</cell><cell>27</cell><cell>41</cell><cell></cell></row><row><cell>MnasNet-A2</cell><cell>75.6</cell><cell>21</cell><cell>39</cell><cell></cell></row><row><cell>MobileNetV3 1.0?</cell><cell>75.2</cell><cell>26</cell><cell>38</cell><cell></cell></row><row><cell>MobileNetV2 1.0?</cell><cell>71.8</cell><cell>27</cell><cell>38</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for different techniques. The baseline is MobileNetV3 1.0? and the combination of the baseline with the skip blocks is SkipblockNet-M.</figDesc><table><row><cell cols="4">Top-1 (%) baseline skip block bias loss</cell></row><row><cell>75.2</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>75.7</cell><cell>?</cell><cell></cell><cell>?</cell></row><row><cell>75.5</cell><cell>?</cell><cell>?</cell><cell></cell></row><row><cell>76.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of compact CNNs accuracies trained on CIFAR-100 with the bias loss and cross-entropy.</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell cols="3">Params Top-1 (%) Top-1 (%) Top-1(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CE loss</cell><cell>Focal loss Bias loss</cell></row><row><cell cols="3">ShuffleNetV2 0.5?</cell><cell>1.4M</cell><cell>69.5</cell><cell>69.8</cell><cell>71</cell></row><row><cell cols="3">MobileNetV2 0.75?</cell><cell>2.6M</cell><cell>68</cell><cell>68.2</cell><cell>68.6</cell></row><row><cell cols="3">NASNet-A (N = 4)</cell><cell>5.3M</cell><cell>77.2</cell><cell>77.5</cell><cell>78</cell></row><row><cell cols="2">SqueezeNet</cell><cell></cell><cell>1.25M</cell><cell>69.4</cell><cell>69.8</cell><cell>70.4</cell></row><row><cell cols="3">DenseNet (k = 12)</cell><cell>7M</cell><cell>78.9</cell><cell>79.5</cell><cell>79.9</cell></row><row><cell></cell><cell>87.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy(%)</cell><cell>85.5 86.0 86.5 87.0</cell><cell></cell><cell></cell><cell></cell><cell>SkipNet(ours) MobileNetV2 MobileNetV2 (1.4) InceptionV1 ResNet-50 DenseNet-121 NASNet-A MobileNetV1</cell></row><row><cell></cell><cell>0</cell><cell cols="4">500 1000 1500 2000 2500 3000 3500 4000 FLOPs (millions)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Average/Max/Min variances of the output of the n-th and last convolutional layers for different models, where BL and CE indicates trainings with the bias loss and cross-entropy, respectively. The performance for PASCAL VOC2007 Detection.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>5th layer</cell><cell></cell><cell></cell><cell>20th layer</cell><cell></cell><cell></cell><cell>last layer</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>max</cell><cell>min</cell><cell>avg.</cell><cell>max</cell><cell>min</cell><cell>avg.</cell><cell>max</cell><cell>min</cell></row><row><cell cols="2">SkipblockNet-M (CE)</cell><cell>1.7</cell><cell>2.4</cell><cell>1.6</cell><cell>0.6</cell><cell>1.2</cell><cell>0.1</cell><cell>0.09</cell><cell>0.2</cell><cell>0.04</cell></row><row><cell cols="2">SkipblockNet-M (BL)</cell><cell>2.</cell><cell>2.5</cell><cell>1.7</cell><cell>1.</cell><cell>1.6</cell><cell>0.2</cell><cell>0.15</cell><cell>0.2</cell><cell>0.09</cell></row><row><cell>ShuffleNet (CE)</cell><cell></cell><cell>1.2</cell><cell>1.6</cell><cell>0.9</cell><cell>0.3</cell><cell>0.5</cell><cell>0.02</cell><cell>0.02</cell><cell>0.07</cell><cell>0.01</cell></row><row><cell>ShuffleNet (BL)</cell><cell></cell><cell>1.4</cell><cell>1.7</cell><cell>0.9</cell><cell>0.4</cell><cell>0.7</cell><cell>0.03</cell><cell>0.04</cell><cell>0.1</cell><cell>0.02</cell></row><row><cell cols="2">MobileNetV3 1.0? (CE)</cell><cell>1.7</cell><cell>2.3</cell><cell>1.6</cell><cell>0.4</cell><cell>1.</cell><cell>0.06</cell><cell>0.05</cell><cell>0.09</cell><cell>0.01</cell></row><row><cell cols="2">MobileNetV3 1.0? (BL)</cell><cell>1.9</cell><cell>2.4</cell><cell>1.9</cell><cell>0.7</cell><cell>1.5</cell><cell>0.1</cell><cell>0.09</cell><cell>0.1</cell><cell>0.03</cell></row><row><cell cols="2">Inception V3 (CE)</cell><cell>3.3</cell><cell>5.9</cell><cell>1.9</cell><cell>5.2</cell><cell>9.3</cell><cell>2.4</cell><cell>0.7</cell><cell>3.6</cell><cell>0.2</cell></row><row><cell>DenseNet (CE)</cell><cell></cell><cell>3</cell><cell>6.1</cell><cell>1.9</cell><cell>4.1</cell><cell>7.2</cell><cell>1.4</cell><cell>0.7</cell><cell>2.4</cell><cell>0.2</cell></row><row><cell>Model</cell><cell cols="3">Parameters FLOPs mAP (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG + SSD</cell><cell>26.2M</cell><cell>31B</cell><cell>77.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNet + SSD</cell><cell>9.4M</cell><cell>1.6B</cell><cell>67.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV2 + SSD</cell><cell>8.9M</cell><cell>1.4B</cell><cell>73.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SkipblockNet-S + SSD</cell><cell>9.4M</cell><cell>1.4B</cell><cell>73.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11398" to="11407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient and robust automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2752" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<title level="m">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Muxconv: Information multiplexing in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4753" to="4762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="568" to="583" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
