<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization using Pretrained Models without Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
							<email>kan.ren@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
							<email>xinyangjiang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Zhang</surname></persName>
							<email>zhanghp@shanghaitech.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Generalization using Pretrained Models without Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-tuning pretrained models is a common practice in domain generalization (DG) tasks. However, fine-tuning is usually computationally expensive due to the ever-growing size of pretrained models. More importantly, it may cause over-fitting on source domain and compromise their generalization ability as shown in recent works. Generally, pretrained models possess some level of generalization ability and can achieve decent performance regarding specific domains and samples. However, the generalization performance of pretrained models could vary significantly over different test domains even samples, which raises challenges for us to best leverage pretrained models in DG tasks. In this paper, we propose a novel domain generalization paradigm to better leverage various pretrained models, named specialized ensemble learning for domain generalization (SEDGE). It first trains a linear label space adapter upon fixed pretrained models, which transforms the outputs of the pretrained model to the label space of the target domain. Then, an ensemble network aware of model specialty is proposed to dynamically dispatch proper pretrained models to predict each test sample. Experimental studies on several benchmarks show that SEDGE achieves significant performance improvements comparing to strong baselines including state-of-the-art method in DG tasks and reduces the trainable parameters by ? 99% and the training time by ? 99.5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distribution shift is a common problem caused by physical or psychological factors of the real-world applications, which breaks the independent and identically distributional (i.i.d.) assumption of machine learning algorithms. Thus, generalization becomes increasingly important when training and applying machine learning models in practice.</p><p>The task of domain generalization and the corresponding benchmark <ref type="bibr" target="#b14">[Gulrajani and Lopez-Paz, 2020]</ref> have been proposed for studying and improving model generalization by training on source domains and test on target domains. These methods focus on generalizable model training following a fine-tuning paradigm which often leverages pretrained models like ResNet <ref type="bibr" target="#b16">[He et al., 2016]</ref> as an initialization and fine-tunes that with some elaborate training algorithms on the source domains. Then, the trained models would be evaluated on the unseen target domains. One common assumption behind this commonly used paradigm is that fine-tuning brings better performance. However, finetuning is usually computationally expensive due to the ever-growing size of pretrained model, and proven to possibly compromise the generalization ability of pretrained models and under-perform in out-of-distribution scenarios <ref type="bibr" target="#b23">[Kumar et al., 2021</ref>.</p><p>Therefore, instead of using pretrained model as an initialization like most existing methods on domain generalization, this paper seeks a better way to leverage the vast amount of the existing pretrained models <ref type="bibr" target="#b16">[He et al., 2016</ref><ref type="bibr" target="#b22">, Krizhevsky et al., 2012</ref><ref type="bibr" target="#b19">, Iandola et al., 2014</ref><ref type="bibr" target="#b59">, Zoph et al., 2018</ref><ref type="bibr" target="#b17">, He et al., 2021</ref><ref type="bibr" target="#b37">, Radford et al., 2021</ref>. Generally, the existing pretrained models have already possessed certain generalization ability over out-of-distribution scenarios. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (B), one simple way to exploit pretrained models' generalization ability is to train a linear label space adapter over a fixed weight pretrained model, which directly transforms the outputs of the pretrained model to the target label space. Our experiments show this minor adjustment bring enhancement over the fine-tuned one on certain target domains (detailed results in section 5.3).</p><p>However, fixed pretrained models do not constantly generalize on all domains, and the generalization performances of different pretrained models vary significantly over different target domains, label classes or even samples, as shown in <ref type="figure">Figure 3</ref>. This is caused by various aspects of the pretraining procedure such as model hypothesis, training algorithms and pretraining datasets.</p><p>Due to the significant variance of pretrained models' generalization ability, it is essential to identify the samples a pretrained model generalize to (i.e. model specialty). Here we propose a novel learning paradigm that dispatches proper pretrained models to each sample based on their generalization ability, named specialized ensemble learning for domain generalization (SEDGE). As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (C), specifically, in addition to the label adapter that projects the pretrained domain to the target domain upon the model with the fixed parameters, we further incorporate a model specialty aware ensemble network that selects a set of proper pretrained models and aggregate together to conduct predictions for each specific sample.</p><p>The advantages of our proposed learning paradigm lie in three aspects. First, it shows a significant improvement over the existing state-of-the-art (SOTA) result using the model pool pretrained only on ImageNet <ref type="bibr" target="#b22">[Krizhevsky et al., 2012]</ref> dataset and gains even larger using the relatively larger model pool pretrained with additional datasets. Second, it exhibits significantly higher training efficiency. The only parameters trained on the source domains contain (1) a linear adapter transforming model outputs to the target label space and (2) a lightweight ensemble network that has largely reduced the training cost on the source domains comparing with that fine-tuning the pretrained models. We visualize the comparison of the performance w.r.t. to training parameter size and cost of training time in <ref type="figure">Figure 2</ref>. Last, this method illustrates a flexible way to utilize pretrained models, making it easier to exploit the abundant resource of pretrained models.  <ref type="bibr">ERM (25.6M,</ref><ref type="bibr">63.8%,</ref><ref type="bibr">15h)</ref> Figure 2: The comparison of the average performance (x-axis, the higher the better) of different algorithms, their training time (y-axis, the smaller the better), and the number of their training parameters (the size of the marker). We also list the corresponding information (number of training parameters, test accuracy, training time) of each algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Domain Generalization</head><p>Mainstream domain generalization research can be divided into following categories. (1) Domain alignment. In order to find the invariant representation across various domains, <ref type="bibr" target="#b12">Ganin et al. [2016]</ref> adversarially train a generator and discriminator to reach the equilibrium of optimal invariant features across domains, hence the classifier trained on multiple source domains would generalize well to target unseen domains. <ref type="bibr" target="#b13">Gong et al. [2019]</ref> consider reducing domain discrepancy in a manifold space. Some works resort to explicit feature distribution alignment on maximum mean discrepancy (MMD) <ref type="bibr" target="#b33">[Pan et al., 2010</ref><ref type="bibr" target="#b48">, Tzeng et al., 2014</ref>, second order correlation <ref type="bibr" target="#b34">, Peng and Saenko, 2018</ref>, moment matching <ref type="bibr" target="#b35">[Peng et al., 2019]</ref> and Wasserstein distance <ref type="bibr" target="#b56">[Zhou et al., 2020</ref><ref type="bibr" target="#b30">, Lyu et al., 2021</ref>, etc. Besides learning invariant representation, <ref type="bibr" target="#b0">Arjovsky et al. [2019]</ref> consider to learn an optimal invariant classifier on top of the representation space, and enforce the learned classifier predicts according with causal mechanism.</p><p>(2) Data manipulation. <ref type="bibr" target="#b45">Tobin et al. [2017]</ref> first introduce this idea, which aims to create diverse training data to simulate unseen target domain. Besides,  and <ref type="bibr" target="#b47">Tremblay et al. [2018]</ref> strengthen the generalization capability of the models via domain randomization, while other works consider using self-supervised learning <ref type="bibr" target="#b4">[Carlucci et al., 2019</ref> to match representation of an image with various augmentations. (3) Meta-learning. Inspired by <ref type="bibr" target="#b10">Finn et al. [2017]</ref> and with the expectation to capture the most transferable representations across domains, MLDG <ref type="bibr" target="#b27">[Li et al., 2018a]</ref> split the multiple source domains data into meta-train and meta-test set to simulate domain shifts to learn more generalizable representations. <ref type="bibr" target="#b7">Dou et al. [2019]</ref> introduce additional losses to explicitly pertain to the semantic structure in representations. <ref type="bibr" target="#b2">Balaji et al. [2018]</ref> consider learning a regularization function on classifier to avoid biasing to domain-specific information, while <ref type="bibr" target="#b8">Du et al. [2020]</ref> resort to regularize Kullback-Leibler (KL) divergence between distributions of latent representations within samples from different domains.</p><p>While above categories more focus on algorithmic improvements, our proposed method SEDGE emphasizes the innovation of a learning paradigm based on a specialized pretrained model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensemble Learning</head><p>Ensemble learning methods <ref type="bibr" target="#b15">[Hansen and</ref><ref type="bibr">Salamon, 1990, Zhou et al., 2018]</ref> exploit multiple models to produce prediction results and combine the results with various techniques, e.g., boosting <ref type="bibr" target="#b38">[Schapire, 1990</ref><ref type="bibr" target="#b11">, Freund, 1995</ref><ref type="bibr" target="#b32">, Moghimi et al., 2016</ref> or mean aggregation <ref type="bibr" target="#b58">[Zhou et al., 2018</ref><ref type="bibr" target="#b55">, Zhang et al., 2020</ref>, etc., to achieve better performance than individual model alone. These methods combine base model learning and ensemble as a whole and focus more on training diverse base models <ref type="bibr" target="#b58">[Zhou et al., 2018]</ref>.</p><p>In DG, specifically, ensemble methods are used to exploit the relationship between source domains and the overall prediction results are composed of the superposition of the multiple networks on each domain. <ref type="bibr" target="#b31">Mancini et al. [2018]</ref> proposed to aggregate different predictions from specific trained source models. <ref type="bibr" target="#b39">Segu et al. [2020]</ref> proposed domain specific batch-norm statistics for each source domain.  proposed one shared CNN feature extractor with domain specific classifiers and each classifier is an expert to its own domain but non-expert to other domains. MulDEns <ref type="bibr" target="#b44">[Thopalli et al., 2021]</ref> relaxes the requirement for domain-specific models and uses a model-domain relevance matrix to define the relations between models and domains. Besides aggregating different domain expert models, other works consider combining model weights in different runs. SWAD <ref type="bibr">[Cha et al., 2021]</ref> avoids overfitting models to local sharp minima by averaging model weights below a validation loss threshold. EoA <ref type="bibr" target="#b1">[Arpit et al., 2021]</ref> further lessens the frequent computations on validation set by averaging model weights simply from start to the end.</p><p>These ensemble learning methods rely on training or fine-tuning from a pretrained model, share the same limitation of training cost and initialization model selection. They did not consider the model specialty in different domains, classes or even samples. We start from a novel perspective that incorporates various pretrained models without fine-tuning and builds a lightweight specialty-aware ensemble network, which illustrates better generalization performance and largely reduces training costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Domain generalization aims to tackle the shift of data distribution among different domains by zeroshot transferring knowledge from seen to unseen domains. Specifically, unlike domain adaptation, samples from unseen target domain(s) are inaccessible in domain generalization. For a domain, its input and label space can be denoted as X ? R d and Y ? R C , and its samples are observed constructing a dataset D = {(x i , y i )} N i=1 with N sample points. Consider that we have S source domains D s = {D 1 , . . . , D S } and T target domains D t = {D 1 , . . . , D T } with different distributions on X ? Y and sharing the label space. Given instances drawn from source domains, the task is to learn a predictor parameterized by ? as f ? ? M: R d ?? R C , where d is the dimension of input and C is the number of classes in Y. We can define a population loss as</p><formula xml:id="formula_0">E D (?) = 1 |D| |D| j=1 E xi?Dj [l(f ? (x i ), y i )]</formula><p>over the given domain D. The objective is to minimize the task-specific loss l (e.g., cross-entropy loss for classification) over both source domains D s and target domains D t by only minimizing the empirical risk? Ds (?) w.r.t. model parameter ?. The performance on the target domains, then, measures the generalization ability of the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary Analysis</head><p>In this section, we want to investigate the generalization ability of various pretrained models, to gain some insights to motivate our method. As suggested by preliminary work <ref type="bibr" target="#b23">[Kumar et al., 2021</ref>], a pretrained model with a linear probing layer (i.e., replacing the last layer of the pretrained model and retraining that) may achieve better accuracy in out-of-distribution scenarios than fine-tuning the whole model. However, linear probing is not feasible due to different pretrained models having different penultimate layer output feature dimensions. Instead, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (B), we only train a label space adapter which learns the mapping function parameterized with ? as h ? ? A: R Co ?? R C , where C o is the dimension of the label space of the original pretraining dataset (pretraining domain). Thus, all the pretrained models on the same pretraining dataset (e.g., ImageNet) share the same label adapter, through which it largely reduces the adaptation cost of the pretrained models on the new domains.</p><p>Given the pretrained model pool {f k } K k=1 with K pretrained models each of which is parameterized as ? k , we further parameterize the adapted model h ? (f k (?)) as ? k = [?; ? k ]. Then we train this shared adapter h ? with the empirical loss? Ds (?) without fine-tuning the pretrained model parameters {? k }. With the trained adapter, we use the likelihood of the ground truth label p(y i | x i ; ? k ) on the i-th sample produced by each adapted model, which also indicates the confidence of the ground truth label y i of the model and y?Y p(y | x i ; ? k ) = 1. We utilize this likelihood as the evaluation metric of its sample-level model specialty.</p><p>First, we analyze the specialty distribution of each pretrained model from an aggregation view (i. .e, domains and classes, respectively), and we verify if there exists a dominant pretrained model that generalizes best across different unseen domains. We calculate domain-level model specialty as summation of logarithms of the sample-level specialty over all domain samples as (xi,yi)?D log p(y i | x i ; ? k ), on TerraIncognita <ref type="bibr" target="#b3">[Beery et al., 2018]</ref> with four domains. To reflect the relative model performance, we perform min-max normalization for model specialty values in the same domain. These results are shown in <ref type="figure">Figure 3</ref> (a). As can be seen, pretrained models vary greatly in performance on different domains, with no single model being dominant in all domains. It suggests that finding a specific powerful pretrained model is non-trivial and not straight-forward for domain generalization.</p><p>Then, based on the previous finding, we further examine whether performance divergence also exists at a finer-grained level, such as class-level. Similar as that at domain-level, <ref type="figure">Figure 3</ref> (b) presents the relative model performance on 10 classes in TerraIncognita-L100. Model performance variances between classes are also noticeable. To clearly compare model specialty differences at the two levels, we present heatmaps of specialty differences (measured by Kullback-Leibler divergence) for domain and class pairs, respectively in <ref type="figure">Figure 3</ref>. The heatmaps exhibit a more pronounced divergence in model specialty at the finer class level. It supports the necessity to utilize pretrained models on top of taking their fine-grained specialty, in finer-grained level even on each sample, into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEDGE: A New Paradigm for Domain Generalization</head><p>In this section, we introduce our proposed learning paradigm, namely specialized ensemble learning for domain generalization (SEDGE), with the motivation and specific details of the whole method. We first present the whole framework in Section 4.1 and then discuss the gathered pretrained models in Section 4.2. After that, we introduce the model dispatcher with ensemble learning in Section 4.3 and the corresponding learning algorithm in Section 4.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework: Pretrained Model without Fine-tuning</head><p>Recall that the focus of the paper is on leveraging pretrained models without fine-tuning to cope with domain generalization. As motivated in Section 3.2, each model has its own specialty and each sample may require to choose a specific set of models to give a good prediction. As a result, we learn the matching of pretrained models and samples from the source domains' training data.</p><p>Based on this idea, as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, we propose a novel specialty-aware domain generalization framework to dispatch an ensemble of specialized models for each sample. Specifically, a label space adapter described in Section 3.2 is trained to transform the prediction of the pretrained models. And then, an ensemble network is learned to dispatch the models in a model pool to each sample according to their estimated specialty at sample level, and aggregate their outputs as an ensemble to output the final prediction for each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretrained Model Pool</head><p>This section presents the pretrained models used in SEDGE. With more and more pretrained models being published, it is straightforward to build a pretrained model pool consisting of several public pretrained models for direct adapting to novel domains. On the other hand, several DG algorithms also use models pretrained on other datasets, such as 1G-1B <ref type="bibr" target="#b1">[Arpit et al., 2021]</ref> and ILSVRC12 <ref type="bibr" target="#b43">[Thomas et al., 2021]</ref>. Therefore, we build Model Pool-B which contains two more CLIP models <ref type="bibr" target="#b37">[Radford et al., 2021]</ref>, ViT-B/16 and ViT-B/32, which are trained on a subset of the YFCC100M dataset of roughly the same size as ImageNet. We denote SEDGE using Model Pool-B as SEDGE + to distinguish it from the one using Model Pool-A.</p><p>Note that, the models pretrained on the same dataset, i.e., ImageNet, will share the same label adapter transforming the vanilla model outputs to the target label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ensemble Network</head><p>As demonstrated by the findings in Section 3.2, for generalizing to unseen domains, we need to take advantage of each of the pretrained models with consideration of their specialties. Additionally, rather than using one model to predict each sample, we propose to use an ensemble of multiple pretrained models, which is known to bring less generalization error <ref type="bibr" target="#b49">[Ueda and Nakano, 1996]</ref>. Moreover, our method incorporates sample-level model specialty into consideration and conducts fine-grained specialty-aware ensemble learning, which is novel comparing to the existing ensemble learning methods as discussed in Section 2. This section will elaborate on the process of obtaining the most specialized pretrained models for a given sample and aggregating the outputs of selected models based on their specialty. The process can be divided into three steps as shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>First, we embed the input sample and the available models to a hidden space. For an image sample x i , we use a fixed pretrained model (i.e., ResNet-34 in our implementation) to embed x i to e i ? R dq . Meanwhile, we introduce a learnable latent variable E m ? R K?dm as model embedding dictionary corresponding to the K models {f k } K k=1 , which is randomly initialized and optimized during training. Furthermore, we map e i and E m to a joint latent space as</p><formula xml:id="formula_1">c i = ?(e i W i ), C m = ?(E m W m ),<label>(1)</label></formula><p>where W i ? R dq?dv , W m ? R dm?dv , and ?(?) = max{?, 0}. We then perform matrix multiplication of c i and C m to calculate the matching score s = c i C T m ? R K between the sample and each model. To dispatch each model output to the final prediction on the sample, we use one layer multi-layer perceptron and perform softmax operation to output the ensemble weights w = [w 1 , . . . , w K ] ? R K with w k equals to</p><formula xml:id="formula_2">w k = e (?(W(s))) k K j=1 e (?(W(s)))j ,<label>(2)</label></formula><p>where W ? R K?K and ?(?) = log(1 + exp(?)). Finally, the prediction for x i based on an ensemble of K model outputs is written a?</p><formula xml:id="formula_3">y i = K k=1 w k h ? (f k (x i )), s.t. K k=1 w k = 1.</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning Algorithm</head><p>As discussed above, the ensemble network acts as a model dispatcher through generating ensemble weights to aggregate multiple model predictions for each sample. Section 3.2 shows that model performance varies significantly over samples. Thus, we expect to assign more weights to the models with higher sample-level specialty to achieve the best utilization of the pretrained models. That is, we try to minimize the estimation risk of the estimated model specialty on the ground truth, i.e., w k and p(y i | x i ; ? k ), as</p><formula xml:id="formula_4">L c = ? K k=1 [p (y i | x i ; ? k ) ? ln(w k ) + (1 ? p (y i | x i ; ? k )) ? ln(1 ? w k )] .<label>(4)</label></formula><p>L c is used to optimize the ensemble network to be a specialty-aware model dispatcher.</p><p>To train the general label space adapter h ? for all pretrained models, we incorporate the classification losses of adapted predictions of pretrained models</p><formula xml:id="formula_5">L b = K k=1 w k ? l (h ? (f k (x i )), y i ) ,<label>(5)</label></formula><p>to update the shared adapter. Additionally, we use the classification loss</p><formula xml:id="formula_6">L e = l (? i , y i )<label>(6)</label></formula><p>to optimize the likelihood of final ensemble output. L e is used to update both ensemble network and adapter.</p><p>It is worth noting that the only parameters to update is the label space adapter and ensemble network, each of which is lightweight compared to the pretrained models which remain fixed in our method yet have been fine-tuned in the previous works.</p><p>Relation to weight ensemble. Previous methods, such as SWAD <ref type="bibr">[Cha et al., 2021]</ref> and EoA <ref type="bibr" target="#b1">[Arpit et al., 2021]</ref>, show that averaging model weights during training can avoid overfitting and achieve better generalization performance. Their experimental results show superior performance compared with methods without weight averaging. While in SEDGE, all pretrained model weights are not involved in training. Accordingly, we perform weight averaging for adapter and ensemble network starting from a certain iteration, which is served as a hyper-parameter.</p><p>Top-k model selection in inference. To save the inference time, we further select models with the highest k ensemble weights and perform softmax on their ensemble weights for aggregation. In this paper, we set k as 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Protocol</head><p>We conduct experiments on DomainBed suite <ref type="bibr" target="#b14">[Gulrajani and Lopez-Paz, 2020]</ref>, which provides like-for-like comparisons between algorithms and has a standard evaluation protocol to follow.</p><p>Datasets. We experiment on 5 real-world benchmark datasets including PACS (4 domains, 9,991 samples, 7 classes) , VLCS (4 domains, 10,729 samples, 5 classes) <ref type="bibr" target="#b9">[Fang et al., 2013]</ref>, OfficeHome (4 domains, 15,588 samples, 65 classes) <ref type="bibr" target="#b51">[Venkateswara et al., 2017]</ref>, TerraIncognita (4 domains, 24,778 samples, 10 classes) <ref type="bibr" target="#b3">[Beery et al., 2018]</ref>, and DomainNet (6 domains, 586,575 samples, 345 classes) <ref type="bibr" target="#b35">[Peng et al., 2019]</ref>.</p><p>For fair comparison, we follow the training and evaluation protocol of DomainBed <ref type="bibr" target="#b14">[Gulrajani and Lopez-Paz, 2020]</ref>. We use the training-domain validation set protocol for model selection. Specifically, one domain in a dataset is selected as the target domain and the rest as source domains, from which 20% of samples are used as the validation set. All runs are repeated 3 times using different random seeds, thus, with different train-validation splits. The out-of-domain test performance averaged over all domains will be reported for each dataset. In addition, we use the standard number of iterations of 5,000 for all datasets, with early-stop based on validated accuracy to reduce unnecessary computational costs.</p><p>Baselines. We compare SEDGE with some strong DG baselines including state-of-the-art. As discussed in Section 2, some of the compared methods incorporate elaborate learning algorithms including ERM <ref type="bibr" target="#b50">[Vapnick, 1998]</ref>, CORAL , MLDG <ref type="bibr" target="#b27">[Li et al., 2018a]</ref>, MMD <ref type="bibr" target="#b28">[Li et al., 2018b]</ref>, DANN <ref type="bibr" target="#b12">[Ganin et al., 2016]</ref>, C-DANN <ref type="bibr" target="#b29">[Li et al., 2018c]</ref>, and Fish <ref type="bibr" target="#b40">[Shi et al., 2021]</ref>.</p><p>Some other works compared in our evaluation incorporate ensemble learning as listed as below.</p><p>? Stochastic Weight Averaging Densely (SWAD) <ref type="bibr">[Cha et al., 2021]</ref>: SWAD performs weight ensemble during model training. ? Ensemble of Average (EoA) <ref type="bibr" target="#b1">[Arpit et al., 2021]</ref>: EoA combines both model ensemble and weight ensemble by taking an ensemble of moving average models from 6 runs. They experiment with two different pretrained models as initialization. One is pretrained on ImageNet with ResNet-50 and the other is pretrained on both ImageNet and a much larger additional dataset, IG-1B, with a more advanced backbone, ResNeXt-50 <ref type="bibr" target="#b53">[Xie et al., 2017]</ref>.</p><p>We denote the latter one as EoA + to indicate it uses the additional dataset. ? Random ensemble: In contrast to SEDGE of learning to select models for ensemble, we also compare it with average ensemble of k models chosen randomly for each sample.</p><p>In addition, LP-FT <ref type="bibr" target="#b23">[Kumar et al., 2021]</ref> reveals the generalization of pretrained models and proposes an elaborated fine-tuning strategy. However, it does not follow the protocol of DomainBed and does not provide implementation details for replication. Our runs for LP-FT show it performs worse than  <ref type="bibr" target="#b28">[Li et al., 2018b]</ref> 85.0 ?0.2 76.7 ?0.9 67.7 ?0.1 49.3 ?1.4 39.4 ?0.8 63.6 C-DANN (ECCV'18) <ref type="bibr" target="#b29">[Li et al., 2018c]</ref> 82.8 ?1.5 78.2 ?0.4 65.6 ?0.5 47.6 ?0.8 38.9 ?0.1 62.6 ERM (ICLR'21) <ref type="bibr" target="#b14">[Gulrajani and Lopez-Paz, 2020]</ref>   <ref type="bibr" target="#b40">[Shi et al., 2021]</ref> 85.5 ?0.3 77.8 ?0.3 68.6 ?0.4 45.1 ?1.3 42.7 ?0.2 63.9 Ensemble algorithms SWAD (NIPS'21) <ref type="bibr">[Cha et al., 2021]</ref> 88. ERM, whose results are shown in Appendix. Note that, all the compared methods mentioned above incorporate a fine-tuning paradigm upon a pretrained model, which is essentially different to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DomainBed Benchmarking</head><p>This section presents experimental results on the DomainBed suite, with performance comparison shown in <ref type="table" target="#tab_2">Table 1</ref> and training/inference time comparison in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Comparison with fine-tuning paradigm. The main difference between previous algorithms and SEDGE lies in fine-tuning or no fine-tuning on top of pretrained models. To verify whether the dispatcher of fixed pretrained models can outperform fine-tuning paradigm, we conduct a comparison of the algorithms that use models only pretrained on ImageNet, e.g., SEDGE using Model Pool-A. As shown in <ref type="table" target="#tab_2">Table 1</ref>, SEDGE achieves an average performance of 69.4%, exceeding SWAD by 2.5%. Results show evidence that our novel paradigm is more effective than the traditional paradigm.</p><p>Performance benefits from adding more pretrained models. SEDGE provides a feasible way to incorporate the ever-emerging publicly available pretrained models. Although Model Pool-A pretrained on ImageNet is in common use, <ref type="bibr" target="#b23">Kumar et al. [2021]</ref> finds that model pretrained on ImageNet may not be good for datasets such as DomainNet. By using Model Pool-B that includes models that have been pretrained on the CLIP dataset <ref type="bibr" target="#b37">[Radford et al., 2021]</ref>, SEDGE + further improves the average performance by 4.7% over SEDGE and ranks first on all datasets. This confirms that SEDGE paradigm is expected to generalize better on unseen domains by including models pretrained on more diverse datasets in the model pool.</p><p>Training cost comparison. SEDGE only utilizes fixed pretrained models and learns to dispatch them through a lightweight ensemble network with the help of a linear label space adapter. Therefore, the number of learnable parameters of SEDGE (up to 0.6M) is minor compared with the normal image backbone network (25.6M for ResNet-50). For fair training cost comparison, we run experiments of ERM, SWAD, SEDGE on a single Nvidia Tesla V100 and compare their overall back-propagation time from the start of training to the end (or early-stop). As shown in <ref type="table" target="#tab_5">Table 2</ref>, training SEDGE paradigm uses noticeably less time. SEDGE + takes only 0.6% of the time of ERM on DomainNet. The significant training time advantage of the method and its surpassing performance suggest that SEDGE is an effective and efficient paradigm for domain generalization.</p><p>Inference cost comparison. As shown in <ref type="table" target="#tab_5">Table 2</ref>, although ensemble methods like EoA and SEDGE achieve better generalization performance at the cost of higher inference FLOPs, SEDGE still manages to save a large amount of inference cost compared to the previous best ensemble model (half of the inference FLOPs compared to EoA). This is because SEDGE only selects models with the highest k(&lt; K) ensemble weights. Therefore, only k of K models are activated for inference per sample, which reduces the inference cost to a large extent.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We want to verify the effectiveness of SEDGE design by answering two research questions: (Q1) Is grafting a label space adapter on top of model outputs sufficient, for utilizing pretrained models to generalize to novel domains? (Q2) Is specialty-aware ensemble necessary, compared to an average ensemble method?</p><p>To verify whether a single model with an adapter can perform well, we train an individual adapter on source domains for each pretrained model in the model pool and compare their performance on target domain with state-of-the-art DG algorithm. To show the "cheating" upper bound of performance under this ablation study, we report the best single model performance on test set as best single model + adapter. As shown in <ref type="table" target="#tab_6">Table 3</ref>, the best single model + adapter among Model Pool-A can outperform SWAD only on OfficeHome. It first indicates that the generalization ability of the fixed pretrained models may be more promising than model with fine-tuning on specific domains. However, in other four datasets, it lags behind SWAD by a large margin. This demonstrates a single pretrained model with a label space adapter is not sufficient to generalize to unseen domains, which motivates the main contribution of our method of introducing ensemble learning.</p><p>DG algorithms that combine ensemble learning, such as SWAD and EoA, demonstrate promising performance. A natural question is whether using an ensemble of pretrained models rather than a single model can improve performance. Following the ensemble approaches <ref type="bibr" target="#b24">[Lakshminarayanan et al., 2017]</ref> using mean average, we experiment a random ensemble over the fixed pretrained models, i.e., randomly sampling k models for each sample and averaging their outputs for final prediction.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 3</ref>. As can be seen, random ensemble results in worse performance than the single model, while SEDGE with specialty-aware ensemble boosts the final performance significantly, albeit with strong or weak individual model performance, which verifies the necessity to select and ensemble the pretrained models based on their specialty over samples as mentioned in Section 3.2 (Q2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis</head><p>As shown in <ref type="figure">Figure 3</ref>, model performance varies across domains, while SEDGE is designed to dispatch specialized models for samples. To analyze whether SEDGE is handling as expected, we present its domain-level model assignment on different domains of TerraIncognita. Specifically, we calculate the sum of ensemble weights assigned to a model as an evaluation of its importance. In <ref type="figure">Figure 5</ref>, we show the rankings of model importance on different domains. By comparing ranking between different domains, it can be seen that SEDGE dispatches models quite differently over unseen target domains. For example, while CLIP-ViT-B/32 model is used frequently on L38/43/46  <ref type="figure">Figure 5</ref>: Ranking models using the sum of ensemble weights on the sample in four domains of TerraIncognita. Each color block corresponds to a model. The higher rank indicates that this model is given a higher weight in predicting the samples in this domain.</p><p>datasets, it lags behind other models on L100. It suggests that SEDGE is making rational model selection as <ref type="figure">Figure 3</ref> shows CLIP-ViT-B/32 is not a powerful model on this domain. Since the target domain is not known prior to making predictions, SEDGE learns to find suitable models for each sample by learning on source domains only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Domain generalization algorithms use the pretrained model as initialization for fine-tuning, while a few works have found that fine-tuning may lead to out-of-distribution performance degradation. Different from the previous fine-tuning paradigm, this paper proposes a novel paradigm for domain generalization, specialized ensemble learning which learns to dispatch an ensemble of fixed pretrained models for each sample based on the model specialty on it. Experiments on five benchmark datasets show that our proposed method has achieved state-of-the-art performance with significant training cost reduction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different training paradigms in domain generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Domain-level performance and performance divergence of pretrained models Figure 3: Performance distribution of the pretrained models over the samples within (a) different domains and (b) different classes. Each column of the left panel displays the relative performance distribution of the pretrained models; the right panel shows the Kullback-Leibler divergence between the performance distribution of different (a) domains and (b) classes. The comparison of the domainlevel and class-level specialty shows that the performance of the pretrained models differs more significantly at the finer level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>SEDGE framework. Based on a pool of several fixed pretrained models, an ensemble network learns the matching of models and samples for model dispatching with the help of a label space adapter for prediction transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Overall training time on DomainBed Average performance on DomainBed (%)</head><label></label><figDesc></figDesc><table><row><cell>60 (h)</cell><cell>Fish (25.6M,63.9%,15h)</cell><cell cols="2">CORAL (25.6M,64.1%,15h)</cell><cell></cell><cell cols="2">Pretrained on ImageNet Pretrained on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SWAD</cell><cell cols="2">additional dataset</cell></row><row><cell>10 (h)</cell><cell></cell><cell></cell><cell cols="2">(25.6M,66.9%,15h)</cell><cell></cell><cell></cell></row><row><cell>900 (s)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SEDGE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(0.2M,69.4%,864s)</cell><cell></cell></row><row><cell>700 (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">C-DANN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(25.6M,62.6%,15h)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>500 (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SEDGE</cell><cell>+</cell></row><row><cell>300 (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(0.2M,74.1%,329s)</cell></row><row><cell>62</cell><cell>64</cell><cell>66</cell><cell>68</cell><cell>70</cell><cell>72</cell><cell>74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>All baseline results are taken from their papers. Our experiments are repeated 3 times using different random seeds.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="4">OfficeHome TerraIncognita DomainNet avg.</cell></row><row><cell></cell><cell cols="2">Model Pool-A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Non-ensemble algorithms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DANN (JMLR'16) [Ganin et al., 2016]</cell><cell>84.6 ?1.1</cell><cell>78.7 ?0.3</cell><cell>65.4 ?0.6</cell><cell>48.4 ?0.5</cell><cell>38.4 ?0.0</cell><cell>63.1</cell></row><row><cell>CORAL (ECCV'16) [Sun and Saenko, 2016]</cell><cell>86.0 ?0.2</cell><cell>77.7 ?0.5</cell><cell>68.6 ?0.4</cell><cell>46.4 ?0.8</cell><cell>41.8 ?0.2</cell><cell>64.1</cell></row><row><cell>MLDG (AAAI'18) [Li et al., 2018a]</cell><cell>84.8 ?0.6</cell><cell>77.1 ?0.4</cell><cell>68.2 ?0.1</cell><cell>46.1 ?0.8</cell><cell>41.8 ?0.4</cell><cell>63.6</cell></row><row><cell>MMD (CVPR'18)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The comparison of training and inference cost. The run for SWAD on DomainNet failed due to out-of-memory. Here, "# parameters" means the number of learnable parameters.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">ERM (our runs) SWAD (our runs) EoA (estimated) SEDGE (Pool-A) SEDGE (Pool-B)</cell></row><row><cell></cell><cell>PACS</cell><cell>3.4h</cell><cell>2.1h</cell><cell>20.4h</cell><cell>19.4s</cell><cell>6.6s</cell></row><row><cell></cell><cell>VLCS</cell><cell>3.6h</cell><cell>2.4h</cell><cell>21.6h</cell><cell>49.4s</cell><cell>8.3s</cell></row><row><cell>Training</cell><cell>OfficeHome</cell><cell>3.3h</cell><cell>2.1h</cell><cell>19.8h</cell><cell>76.6s</cell><cell>15.9s</cell></row><row><cell></cell><cell cols="2">TerraIncognita 3.4h</cell><cell>2.1h</cell><cell>20.4h</cell><cell>46.4s</cell><cell>52.1s</cell></row><row><cell></cell><cell>DomainNet</cell><cell>9.8h</cell><cell>/</cell><cell>58.8h</cell><cell>11.2m</cell><cell>4.1m</cell></row><row><cell></cell><cell># parameters</cell><cell>25.6M</cell><cell>25.6M</cell><cell>153.4M</cell><cell>0.2 ? 0.6M</cell><cell>0.3 ? 0.6M</cell></row><row><cell cols="2">Inference GFLOPs</cell><cell>3.9</cell><cell>3.9</cell><cell>23.5</cell><cell>12.0</cell><cell>10.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of applying label space adapter only and random ensemble.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="4">OfficeHome TerraIncognita DomainNet avg.</cell></row><row><cell></cell><cell></cell><cell cols="2">Model Pool-A</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SWAD (NIPS'21) [Cha et al., 2021] 88.1 ?0.1</cell><cell>79.1 ?0.1</cell><cell>70.6 ?0.2</cell><cell>50.0 ?0.3</cell><cell>46.5 ?0.1</cell><cell>66.9</cell></row><row><cell>best single model + adapter</cell><cell>79.7</cell><cell>73.6</cell><cell>78.3</cell><cell>49.2</cell><cell>32.5</cell><cell>62.7</cell></row><row><cell>random ensemble</cell><cell cols="3">58.1 ?0.13 58.5 ?1.26 59.6 ?0.38</cell><cell>31.5 ?0.40</cell><cell>15.8 ?1.40</cell><cell>44.5</cell></row><row><cell>SEDGE</cell><cell cols="2">84.1 ?0.45 79.8 ?0.0</cell><cell>79.9 ?0.12</cell><cell>56.8 ?0.21</cell><cell>46.3 ?0.39</cell><cell>69.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Model Pool-B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>best single model + adapter</cell><cell>95.4</cell><cell>82.0</cell><cell>78.3</cell><cell>49.2</cell><cell>52.6</cell><cell>71.5</cell></row><row><cell>random ensemble</cell><cell cols="3">59.5 ?0.95 61.1 ?0.12 59.5 ?0.07</cell><cell>30.8 ?0.37</cell><cell>18.7 ?0.62</cell><cell>46.0</cell></row><row><cell>SEDGE +</cell><cell cols="3">96.1 ?0.04 82.2 ?0.03 80.7 ?0.21</cell><cell>56.8 ?0.29</cell><cell>54.7 ?0.10</cell><cell>74.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Cadene/pretrained-models.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ensemble of averages: Improving model selection and boosting performance in domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10832</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dual path networks. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to learn with variational information bottleneck for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting a weak learning algorithm by majority. Information and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="256" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selfreg: Self-supervised contrastive regularization for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekoo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9619" to="9628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-tuning distorts pretrained features and underperforms out-of-distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbie</forename><forename type="middle">Matthew</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01902</idno>
		<title level="m">Matthias Scheutz, and Shuchin Aeron. Barycentricalignment and invertibility for domain generalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Best sources forward: domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1353" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Boosted convolutional neural networks. In BMVC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Synthetic to real adaptation with generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1982" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3803" to="3810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Batch normalization embeddings for deep domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Segu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12672</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Seely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siddharth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09937</idno>
		<title level="m">Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive methods for aggregated domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04766</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-domain ensembles for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kowshik</forename><surname>Thopalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameeksha</forename><surname>Katoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<title level="m">IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Training deep networks with synthetic data: Bridging the reality gap by domain randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Cameracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaad</forename><surname>Boochoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="969" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalization error of ensemble estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Networks (ICNN&apos;96)</title>
		<meeting>International Conference on Neural Networks (ICNN&apos;96)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">An empirical study of pre-trained models on out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The diversified ensemble neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Boyu Wang, and Brahim Chaib-draa. Domain generalization with optimal transport and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjian</forename><surname>Shui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10573</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Domain adaptive ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8008" to="8018" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Diverse ensemble evolution: Curriculum data-model marriage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
