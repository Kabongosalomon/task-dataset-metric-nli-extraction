<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
							<email>zhuqiannan@ruc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanqun</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
							<email>fangfang0703@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
							<email>liuyi@cert.org.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">National Computer Network Emergency Response Technical Team</orgName>
								<address>
									<country>Coordination Center of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
							<email>qian.li@uts.edu.au</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? For the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods. For the problem 2, to make better use of the three levels of SE, we propose a novel GNNbased KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. Finally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) like Freebase <ref type="bibr" target="#b6">(Bollacker et al. 2008)</ref> and WordNet <ref type="bibr" target="#b13">(Miller 1995)</ref> are significant resources to support numerous artificial intelligence applications, such as recommendation system <ref type="bibr" target="#b25">(Wang et al. 2018)</ref>, question answering <ref type="bibr" target="#b30">(Yasunaga et al. 2021</ref>) and text generation , etc. KGs store graph-structured knowledge in triple form <ref type="bibr">(h, r, t)</ref>. To integrate symbolic knowledge into numerical down-stream applications, Knowledge Graph Embedding (KGE) technique that attempts to encode the relations and entities into low-dimensional embeddings, has attracted increasing attention. The core idea of KGE is to design triple modeling function f (h, r, t), that can predict correct tail entity t from (h, r, ?), or head entity h from (?, r, t), by scoring high for positive triple (h, r, t), and low for negative triples (h , r, t) and (h, r, t ) 1 .</p><p>Many KGE models have been proposed and can be categorized into three families <ref type="bibr" target="#b26">(Wang et al. 2017;</ref><ref type="bibr" target="#b0">Arora 2020)</ref>, which are Translational Distance Models like TransE <ref type="bibr" target="#b7">(Bordes et al. 2013)</ref>, RotatE <ref type="bibr" target="#b19">(Sun et al. 2019)</ref>; Semantic Matching Models like DistMult <ref type="bibr" target="#b29">(Yang et al. 2015)</ref>, ComplEx <ref type="bibr" target="#b22">(Trouillon et al. 2016)</ref>, ConvE <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref>; and GNN-based Models like R-GCN <ref type="bibr" target="#b16">(Schlichtkrull et al. 2018</ref>), CompGCN <ref type="bibr" target="#b24">(Vashishth et al. 2020b</ref>). Most of these KGE models have gained great success, especially on extrapolation scenarios, which is that given an unseen triple (h, r, t), a well trained model can still correctly predict t from (h, r, ?) or h from (?, r, t), such ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, but explains little about why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to, from a data relevant view, study KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability?</p><p>For the problem 1, for an unseen triple (h, r, t), we treat the prediction from (h, r, ?) to t with a semantic matching idea. For a good extrapolative matching, (h, r, ?) and t must have obtained some semantic relatedness during training, and we consider the relatedness may come from three lev- <ref type="figure">Figure 1</ref>: The demonstration of three levels of Semantic Evidence. els: the individual r part with t (relation level), the individual h part with t (entity level), and the combination (h, r, ?) part with t (triple level). We name such three factors as Semantic Evidence (SE), to indicate the supporting semantic information they provide for extrapolation. Then, we quantify the SEs with three corresponding metrics respectively. For relation level, it is measured by the co-occurrence of r and t in train set; for entity level, it is the path connections from h to t in train set; for triple level, it is the similarity between existed ground truth entities of (h, r, ?) and t. The demonstration of three levels of SE can be seen in figure 1. Furthermore, we verify the effectiveness of SEs through extensive experiments on several typical KGE methods, and demonstrate that SEs serve as an important role for understanding the extrapolation ability of KGE.</p><p>For the problem 2, based on the conclusion of problem 1, Semantic Evidences are important for designing KGE models with powerful extrapolation ability. However, current works capture the SE information mainly through an implicit and insufficient way, which limits their extrapolation performance. Hence in this work, to make better use of the three levels of SE, we propose a novel GNNbased KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation mechanism of GNNs, which contributes to obtaining more extrapolative knowledge representation. The model architecture is demonstrated in <ref type="figure" target="#fig_1">figure 3</ref>.</p><p>In summary, our main contributions are as follows: ? We are the first to explore KGE extrapolation problem, from a data relevant and model independent view, and further introduce three levels of Semantic Evidence to understand the extrapolation ability of KGE. We also conduct extensive experiments on various typical KGE models to verify our assumption. ? We dive into the way of designing the KGE model with better extrapolation ability, through explicitly and sufficiently modeling the Semantic Evidences into knowledge embedding. We propose a novel GNN-based KGE method called SE-GNN, which helps the learned knowledge representation achieve more improved extrapolation performance. ? Through extensive experiments on FB15k-237 and WN18RR datasets of Knowledge Graph Completion task, we demonstrate the validity of our introduced Semantic Evidence concept and SE-GNN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge Graph Embedding Knowledge Graph Embedding is an active research area. Based on the scoring function and whether global graph structure is utilized, literature works can be divided into three families <ref type="bibr" target="#b26">(Wang et al. 2017;</ref><ref type="bibr" target="#b0">Arora 2020</ref> Extrapolation Ability Study In Machine Learning Theory field, there are many works that attempt to study the generalization and extrapolation ability of Neural Networks or Multilayer perceptrons (MLPs) <ref type="bibr" target="#b11">(Haley and Soloway 1992;</ref><ref type="bibr" target="#b4">Barnard and Wessels 1992;</ref><ref type="bibr" target="#b5">Bietti and Mairal 2019;</ref><ref type="bibr" target="#b1">Ba et al. 2020;</ref><ref type="bibr" target="#b28">Xu et al. 2021)</ref>. Like in <ref type="bibr" target="#b28">(Xu et al. 2021)</ref>, it is proved that ReLU MLPs can not extrapolate most nonlinear functions, but can extrapolate linear function when the training distribution is sufficiently diverse. And for Graph Neural Networks, it is showed that they can encode non-linearity in architecture and features to help extrapolation. However, the conclusions of above works cannot directly apply to KGE field. Because the analysis of Neural Networks mostly concentrates on classification or regression task, with only one single object or distribution. For Graph Neural Networks, the study is also mainly about node classification or graph classification task. While for KGE task, there are three targets (h, r, t) mutually influencing and serving as a matching task between (h, r, ?) and t, which makes the extrapolation analysis of KGE differs from the correspondence in ML field. In addition, in Knowledge Graphs there are abundant data pattern and fact interdependency that can be mined, which is very important to understand the extrapolation performance of KGE. Therefore, in this work we focus on a data relevant and model independent view to study the KGE extrapolation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge Graph Embedding Extrapolation</head><p>In this section, we firstly give the definition of KGE extrapolation problem. Then we introduce three levels of Semantic Evidence to explain the extrapolation ability of KGE models. Finally we conduct experiments on various typical KGE models to verify our assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>A knowledge graph is denoted as G = (E, R, F), where E and R represent the set of entities and relations, and</p><formula xml:id="formula_0">F = {(h, r, t)} ? E ? R ? E is the set of triple facts.</formula><p>For the KGE learning process, firstly F will be partitioned into train, valid and test set, denoted as F tr , F va , F te respectively. The model will be trained on F tr and the best parameters will be selected according to F va , then the extrapolation performance will be evaluated on unseen dataset F te . KGE task aims to predict t given (h, r, ?), or h given (?, r, t). Here we treat the prediction task with the idea of semantic matching between query and answer, and without loss of generality, we denote both directions as query(h, r) ? t. Under such denotation, the extrapolation problem we want to study is that why the KGE model is only trained for high scoring of query(h, r) ? t, (h, r, t) ? F tr , but can still perform well for unseen data query(h, r) ? t, (h, r, t) ? F te 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extrapolate with Semantic Evidences</head><p>For a good extrapolative matching query(h, r) ? t, query(h, r) and t must have obtained some semantic relatedness during training. We consider the relatedness may come from three levels: the individual r part with t (relation level), the individual h part with t (entity level) and the combination query(h, r) part with t (triple level), demonstrated as follows:</p><p>? Relation level relatedness between r and t: In train set if t frequently occur with queries containing r, i.e. there are many query(h i , r) ? t in F tr , the r will contain information to predict t. From intuition this can be regarded as entity type information. Instantly, for query(h i , born in), the probability of predicting location F lorida should be higher than predicting movie Iron M an, no matter what the specific h i is.</p><p>? Entity level relatedness between h and t: In train set if there are observed queries or indirect queries from h to t, this will close their semantic relevancy and provide evidences for other queries between h and t. For example, query(h, is mother) ? e 1 and query(e 1 , is f ather) ? t will bring confidence for predicting query(h, is grandmother) ? t. Under the graph view, this can be regarded as the path from h to t.</p><p>? Triple level relatedness between query(h, r) and t: For query(h, r), it may exist other ground truth entities t in train set. If the model has been trained for query(h, r) ? t , meanwhile t and t share much similarity, it will be natural for the model to extrapolate to query(h, r) ? t. For example, if we have known query(James Cameron, profession) ? film director and screen writer, it is not difficult to predict query(James Cameron, profession) ? film producer. All above relatednesses are from train set and can be observed, so for a KGE model, though it does not train for the unseen data query(h, r) ? t, it has gained enough information from observed triples to make the prediction. We name such relatedness as Semantic Evidence (SE), to indicate the supporting semantic information they provide for extrapolation. We demonstrate the three levels of SE in figure 1. In addition, we also do the extensive case study for the three levels of SE, to provide an intuitive demonstration about how the Semantic Evidence helps extrapolate. The case study content is placed in appendix A because of the space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Verification</head><p>In this section, we attempt to verify the effectiveness of the proposed SE concept through experiments. Firstly, for an unseen prediction query(h, r) ? t, we propose three corresponding metrics to quantify the evidence strength of each SE as follows:</p><p>? S rel for relation level SE: It is the number of triples in train set that satisfy (h i , r, t), which can be formulated as:</p><formula xml:id="formula_1">S rel = |{(h i , r, t)|(h i , r, t) ? F tr }|</formula><p>where |set| denotes the element number of a set. ? S ent for entity level SE: It is the number of path from h to t in train set, indicating the semantic relevancy of h and t. For simplification, we limit the path length ? 2. S ent is formulated as:</p><formula xml:id="formula_2">S ent =|{(h, r i , t)|(h, r i , t) ? F tr }|+ |{(h, r i , e k , r j , t)|(h, r i , e k )</formula><p>, (e k , r j , t) ? F tr }| ? S tri for triple level SE: It is the similarity measurement between t and query(h, r)'s ground truth entity t in train set:</p><formula xml:id="formula_3">S tri = t Sim(t, t ), (h, r, t ) ? F tr</formula><p>For similarity function Sim(t, t ), though there have been proposed many entity similarity computing methods for KGs <ref type="bibr" target="#b9">(Choudhury et al. 2015;</ref><ref type="bibr" target="#b32">Zhu and Iglesias 2017;</ref><ref type="bibr" target="#b18">Sun et al. 2018</ref>), most of them need external information like entity category, description text, etc. Here we hope for a method that only relates to KG itself, so we take the idea of Distributional Semantic Hypothesis: words that are used and occur in the same contexts tend to purport similar meanings <ref type="bibr" target="#b12">(Harris 1954)</ref>, and measure the entity similarity according to its neighbor structure (context). This can be formulated as the number of common neighbor entity-relation pairs that t and t share:</p><formula xml:id="formula_4">Sim(t, t ) = |{(h i , r i )|(h i , r i , t) ? F tr } ? {(h i , r i )|(h i , r i , t ) ? F tr }|</formula><p>Then we reproduce several typical KGE models and analyze their extrapolation performance under different SE configurations. Specifically, we use FB15k-237 dataset <ref type="bibr" target="#b21">(Toutanova and Chen 2015)</ref>, a frequently used public KG dataset, and compute the above three SE metrics for each data query(h, r) ? t in test set. For each SE, we divide the data evenly into three ranges with ascending order of metric value. Hence the three ranges represent the low, medium and high evidence strength respectively. One exception is for entity level SE, that because the range [0, 1) cannot be divided further, the proportion of three ranges is about 6:2:2. Then we select six typical KGE models of different types, which are TransE, RotatE (Translational Distance Models), DistMult, ComplEx, ConvE (Semantic Matching Models), CompGCN (GNN-based Models), and evaluate their prediction results on each SE range. The results are demonstrated in figure 2.</p><p>We can see that for all models it exists a consistent better prediction result with evidence strength increasing. When there is abundant SE, all the KGE models can perform a good extrapolation result, like the rightmost range of each SE in figure 2. And if the SE is lacked, the models' extrapolation ability will also be limited, like the leftmost range. In addition, we also conduct the similar experiment verification on WN18RR dataset <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref>, and the results are placed in figure 6 of appendix B. It can be seen that there is the same phenomenon on WN18RR dataset. This further verifies the strong correlation between SE and extrapolation performance. That is to say, regardless of the specific method selected, the models always extrapolate well to data with high SE evidence, which verifies that the proposed SE is a reasonable data view explanation to understand the impressive extrapolation ability of KGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Evidence aware GNN</head><p>In this section, to make better use of the Semantic Evidence information for more extrapolative knowledge representation, we propose a novel GNN-based KGE model called Semantic Evidence aware Graph Neural Network (SE-GNN), which is designed to model the three levels of SE explicitly and sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling SEs with Neighbor Pattern</head><p>Knowing from previous section, Semantic Evidences are important to design KGE models with powerful extrapolation ability. However, for most current KGE works, there is no awareness of such extrapolation factors and they capture the SE information mainly through an implicit and insufficient way, which limits their extrapolation performance. Hence in this work, we explicitly treat each SE as different neighbor pattern and model them sufficiently through multi-layer aggregation mechanism of GNNs, for obtaining more extrapolative knowledge representations.</p><p>Specifically, for relation level SE, it describes the overall relation-entity interactions, which can be captured through neighbor relation pattern of an entity. By aggregating all the connected relations, we can get the representation as:</p><formula xml:id="formula_5">s rel i = ? ? ? (ej ,rj )?Ni ? rel ij W rel r j ? ?<label>(1)</label></formula><p>s rel i ? R n denotes the relation level SE representation of entity e i , where n is hidden dimension. r j ? R n is the embedding of relation r j . N i = {(e j , r j )|(e j , r j , e i ) ? F tr } denotes e i 's neighbor entities, associated with connecting relation in train set. W rel ? R n?n is linear transformation matrix and ? is non-linear activation function. ? rel ij is aggregation attention, which is computed as:</p><formula xml:id="formula_6">? rel ij = exp r T j e i (e k ,r k )?Ni exp r T k e i<label>(2)</label></formula><p>e i ? R n is the embedding of entity e i . We use dot product to dynamically compute the attention importance of neighbor relation r j to center entity e i . For entity level SE, it describes the path connection information between entities, and can be captured from neighbor entity pattern. With aggregating neighbor entities once, we can capture all the 1 length paths, and we can access to longer paths by iterative multi-layer aggregation. Here we only introduce a single layer formulation and we will introduce the whole model architecture in section 4.2:</p><formula xml:id="formula_7">s ent i = ? ? ? (ej ,rj )?Ni ? ent ij W ent e j ? ?<label>(3)</label></formula><p>s ent i ? R n is the entity level SE representation of e i . ? ent ij is aggregated attention and computed as follows:</p><formula xml:id="formula_8">? ent ij = exp e T j e i (e k ,r k )?Ni exp e T k e i<label>(4)</label></formula><p>For triple level SE, it describes the triple similarity characteristics from the neighbor structure view, where both neighbor entities and relations should be considered. We design the aggregation function as:</p><formula xml:id="formula_9">s tri i = ? ? ? (ej ,rj )?Ni ? tri ij W tri ?(e j , r j ) ? ?<label>(5)</label></formula><p>where ?(e, r) is the composition function to fuse the entity and relation information. The selection includes addition function: ?(e, r) = e+r; multiplication function: ?(e, r) = e * r; Multilayer Perceptron: ?(e, r) = MLP([e||r]), where || is vector concatenation operation. The attention weights ? tri ij is computed similarly as:</p><formula xml:id="formula_10">? tri ij = exp ?(e j , r j ) T e i (e k ,r k )?Ni exp (?(e k , r k ) T e i )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Architecture</head><p>In section 4.1, we introduce the neighbor aggregation method to model each SE and obtain the corresponding representation of s rel i , s ent i and s tri i . The three embeddings provide important evidences to help the model extrapolate. We merge them with original knowledge embedding as:</p><formula xml:id="formula_11">e i = e i + s rel i + s ent i + s tri i<label>(7)</label></formula><p>This can be seen as one single aggregation layer of GNN, which only captures the SE information in 1-hop neighborhood. To acquire the multi-hop neighbor information and model the deeper interaction of SE components, we introduce a multi-layer version for SE aggregation, which is demonstrated in <ref type="figure" target="#fig_1">figure 3</ref>. We take the output e i as the next layer's input, and aggregate iteratively as:</p><formula xml:id="formula_12">e l+1 i = e l i + (s rel i ) l + (s ent i ) l + (s tri i ) l (8) e l+1</formula><p>i denotes the embedding of e i in (l + 1)-th layer. (s tri i ) l is the triple level SE embedding in l-th layer, which is computed from e l i and r l i :</p><formula xml:id="formula_13">(s tri i ) l = ? ? ? (ej ,rj )?Ni (? tri ij ) l (W tri ) l ?(e l j , r l j ) ? ? (9)</formula><p>where (? tri ij ) l is computed in the same way as equation 6. The layer-wise embedding of (s rel i ) l and (s ent i ) l can be obtained in the similar way. In first layer, e 1 is the initialized embedding, and after K layers' aggregation, we take e K as the output entity embedding.</p><p>With regard to relation embedding, we initialize different r l for each layer, in the consideration that relations may play a different role in different layer. For output relation embedding, we concat all the r l used and merge them together by a transform matrix W out . Hence the output of SE-GNN is formulated as: e out = e K r out = W out Concat({r l |l = 1, ..., K})</p><p>Then we utilize the output embedding to perform the prediction from (h, r, ?) to t or from (?, r, t) to h. To align with the terminology in previous work, here we also denote this process as Knowledge Graph Completion (KGC) task. We choose ConvE <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref> as our decoder, which uses 2D convolutional neural network to match query(h, r) and answer t. We refer readers to original paper for more details, and here we directly denote the model function as:</p><formula xml:id="formula_15">q = ConvE(h, r)<label>(11)</label></formula><p>q ? R n is the computed query embedding. h and r are taken from e out and r out . Note that in fact any KGC decoder can be considered here, while this is not the focus of this paper. We leave the more explorations to the future work. Then we use binary cross entropy loss to measure the matching between q and potential answer entities t:</p><formula xml:id="formula_16">L = ? 1 N t 1(t) ? log (m(q, t)) + (1 ? 1(t)) ? log(1 ? m(q, t))<label>(12)</label></formula><p>where N is the total number of candidate entities, and m(q, t) ? [0, 1] is the matching function of query q and entity t. We use dot product in this work:</p><formula xml:id="formula_17">m(q, t) = Sigmoid(q T t)<label>(13)</label></formula><p>1(t) ? {0, 1} is the denotation function that outputs 1 for positive entity and 0 for negative entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>We conduct experiments of Knowledge Graph Completion task on two commonly used public datasets: FB15k-237 <ref type="bibr" target="#b21">(Toutanova and Chen 2015)</ref> and WN18RR <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref>. The detailed introduction of two dataset are provided in appendix C. We measure the model performance by five frequently used metrics: MRR (the Mean Reciprocal Rank of correct entities), MR (the Mean Rank of correct entities), Hits@1, Hits@3, Hits@10 (the accuracy of correct entities ranking in top 1/3/10). We follow the filtered setting protocol <ref type="bibr" target="#b7">(Bordes et al. 2013)</ref> for evaluation, i.e. all the other true entities appearing in train, valid and test set are excluded when ranking. In addition, based on the observation of <ref type="bibr" target="#b20">(Sun et al. 2020)</ref>, to eliminate the influence of abnormal score distribution, if prediction targets have the same score with multiple other entities, we take the average of upper bound and lower bound rank as the result. Additional experimental details are provided in the appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of Knowledge Graph Completion task</head><p>Our baselines are selected from three categories which are Translational Distance Models: TransE <ref type="bibr" target="#b7">(Bordes et al. 2013)</ref>, RotatE <ref type="bibr" target="#b19">(Sun et al. 2019)</ref>, PaiRE <ref type="bibr" target="#b8">(Chao et al. 2021)</ref>; Semantic Matching Models: DistMult <ref type="bibr" target="#b29">(Yang et al. 2015)</ref>, ComplEx <ref type="bibr" target="#b22">(Trouillon et al. 2016)</ref>, TuckER (Balazevic and Allen 2019), ConvE <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref>, Inter-actE <ref type="bibr" target="#b23">(Vashishth et al. 2020a</ref>), PROCRUSTES <ref type="bibr" target="#b15">(Peng et al. 2021)</ref>; GNN-based Models: R-GCN <ref type="bibr" target="#b16">(Schlichtkrull et al. 2018)</ref>, <ref type="bibr">KBGAT (Nathani et al. 2019)</ref>, SACN <ref type="bibr" target="#b17">(Shang et al. 2019)</ref>, A2N <ref type="bibr" target="#b3">(Bansal et al. 2019)</ref>, CompGCN <ref type="bibr" target="#b24">(Vashishth et al. 2020b</ref>). The results are demonstrated in table 1, from which we can know that:</p><p>? In view of the five metrics on two datasets, SE-GNN achieves 9 of 10 SOTAs, which is an overall best performance compared to baselines. And for the exception of MR report on WN18RR, SE-GNN still gets the competitive result with regard to the most baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? SE-GNN obtains obvious improvement compared to</head><p>CompGCN, which is a typical GNN-based KGE model. This shows that the aggregation function in SE-GNN for modeling three levels of SE information is a more sufficient way and performs a better extrapolation ability. ? In addition, we can see that the improvement of SE-GNN is more evident on FB15k-237 dataset. We think this is because in FB15k-237 there are more than 200 types of relation (table 3 in appendix C) and the data interactions are very complex, which makes the extrapolation scenario more challenging. In this case, the explicit modeling of SEs will play a more important role to help extrapolation.</p><p>In the overall consideration across metrics on two datasets, SE-GNN obtains the best extrapolation performance on unseen test data, which indicates the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effective Modeling of Semantic Evidences</head><p>In this section, we tend to verify that SE-GNN is capable of effectively modeling the Semantic Evidences. Like in section 3.3, we evaluate the extrapolation performance of SE-GNN in different SE ranges. To control the variables, we compare the results with ConvE, which is our selected decoder in SE-GNN. So the only difference here is that in SE-GNN, explicit modeling of three levels of Semantic Evidence is introduced before decoder (equation 10), while in ConvE, entity and relation embedding are directly fed into the decoder, with implicit modeling of SE information. The results are demonstrated in <ref type="figure" target="#fig_2">figure 4</ref>. We can see that SE-GNN performs better for all levels of SE across all ranges, which shows SE-GNN can capture the SE information more effectively and possess better extrapolation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study of each Semantic Evidence</head><p>To evaluate the effect of each SE part, we do the ablation study of only removing one SE modeling part and simultaneously removing two of them. The results are demonstrated in table 2. We can observe that the performance degrades for  <ref type="bibr" target="#b20">(Sun et al. 2020</ref>) because original results suffer from same score evaluation problem, which is discussed in section 5.1. Other results are from the published paper. all six variants of SE-GNN, which shows the effectiveness of each SE modeling part.</p><p>In addition, we consider that for most GNN-based KGE works like R-GCN <ref type="bibr" target="#b16">(Schlichtkrull et al. 2018)</ref>, CompGCN <ref type="bibr" target="#b24">(Vashishth et al. 2020b)</ref>, the core idea is to merge the relation and entity together when neighbor aggregating. This can be regarded as the w/o relation &amp; entity SE variant of SE-GNN, which only models the triple SE part. While both our SE extrapolation analysis and the ablation experiments show that it is insufficient, and separately modeling relation and entity information are also beneficial for KGE task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>FB15k <ref type="formula">-</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we make the attempt to study the KGE extrapolation problem from a data relevant and model independent view. We show that there are three levels of Semantic Evidence that play an important role when predicting unseen data, which are the co-occurrence between relations and entities, the path connection between entities, and the similarity between observed entities and predicted entities. Then we verify the effectiveness of SEs through extensive quantitative experiments and qualitative case study. Based on such observation, we design a novel SE-GNN model to obtain more extrapolative knowledge representation and achieve consistent improvement on different datasets. Some future directions include exploiting more extrapolative evidences and designing more elaborated SE modeling method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Case study of Semantic Evidence</head><p>In this section, we tend to provide an intuitive demonstration about how the Semantic Evidence helps extrapolate to unseen data and hence this information is important for KGE task. For each Semantic Evidence, we select multiple example cases from FB15k-237 test set, and list their corresponding evidences and prediction results of different KGE models. The cases are demonstrated in <ref type="figure" target="#fig_3">figure 5</ref>.</p><p>For relation SE, the idea is that the co-occurrence between relation and tail entity can help extrapolate. For example, for the extrapolative prediction (San Diego, travel month, ?) ? December, if the model has observed large amounts of co-occurrences of travel month and December in train set, it will be aware of the month type of December and also know that December is a popular time for traveling, which is beneficial for the model to perform a new travel month prediction on December entity. Note that there are three levels of SE information that can help extrapolate in the meantime, and in this case relation evidence just servers as one part.</p><p>For entity SE, it is that the connection or path between entities can help extrapolation. Like the case (Robert Downey Jr, live in, ?) ? New York City, in the train set we know that Downey was born in New York City, his wife lives in New York City, his friend lives in New York City, etc. These connections between Robert Downey Jr and New York City will enhance their semantic relevancy and help the model to predict "some relation" between them, such as live in.</p><p>For triple SE, it follows the idea that if prediction holds for one entity, it should also hold for a similar one. For example, if we have known that Freshman Program contains the major of Mathematics, Electrical Engineering, Chemistry Science, it is natural to infer that it also contains Computer Science, which is a similar major of the observed ones.</p><p>Above three levels of SE are important to help the model do extrapolating. For a further illustration, we reproduce several typical KGE models and list their prediction in rightmost column. We can see that for these unseen cases with abundant SE information, all the models can perform a good extrapolative prediction, which verifies the effectiveness of the proposed Semantic Evidence concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Correlation between SE and Extrapolation Performance on WN18RR Dataset</head><p>In this section, we compute the extrapolation performance of various KGE models on different SE ranges on WN18RR dataset. The results are demonstrated in figure 6. Because there are much low evidence data in WN18RR dataset, it is hard to evenly divide the test data into three ranges like figure 2. Therefore we divide the data into [0, 1) and [1, Max] two ranges instead, which can respectively repre- sent the data with evidence and without evidence (Max denotes the max value of corresponding evidence metric).</p><p>From the results, we can see that the correlation between evidence strength and extrapolation performance also holds on WN18RR dataset, for all models and across all evidences, which further verifies the effectiveness of SE. Note that because of the various data characteristics, different dataset may reveal a different focus on three levels of SE when extrapolating. Like in WN18RR dataset, there are only 11 types of relation (demonstrated in table 3). Such simple pattern makes relation level SE a less important role compared to FB15k-237 dataset, which has an abundant relation set of 237 types. Hence in <ref type="figure" target="#fig_4">figure 6</ref>, there is a weaker downward trend for relation level SE, and the entity and triple level SE play a more important role when extrapolating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Statistics</head><p>In this section we provide the information of FB15k-237 and WN18RR dataset used in our experiment.</p><p>? FB15k-237 (Toutanova and Chen 2015) contains entities and relations from Freebase, which is a large commonsense knowledge base. FB15k-237 is a pruned version of FB15k <ref type="bibr" target="#b7">(Bordes et al. 2013</ref>) dataset, with duplicate and inverse relations being removed to prevent direct prediction. Furthermore, FB15k-237 also ensures that every triple (h, r, t) in valid and test set does not have any direct connection (h, r , t) in train set, to make the prdiction more challenging. ? WN18RR <ref type="bibr" target="#b10">(Dettmers et al. 2018</ref>) is derived from Word-Net, a lexical database of semantic relations between words. Similar to FB15k-237, WN18RR is pruned from WN18 <ref type="bibr" target="#b7">(Bordes et al. 2013</ref>) dataset by removing the duplicate and inverse relations, while there is no direct connection restriction in WN18RR. Statistics of two datasets are summarized in table 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>FB15k  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>In this section we discuss some more details of the experiment implementation. Following CompGCN <ref type="bibr" target="#b24">(Vashishth et al. 2020b</ref>), we transform the knowledge graph to undirected graph, by introducing an inverse edge (t, r ?1 , h) for each edge (h, r, t), which aims to pass the information bidirectionally and enhance graph connectivity. In addition, like ConvE <ref type="bibr" target="#b10">(Dettmers et al. 2018)</ref>, we also introduce an inverse version for each relation when predicting. For the two directions (h, r, ?) ? t and (?, r, t) ? h of a triple prediction, we transform them as (h, r, ?) ? t and (t, r ?1 , ?) ? h, which can unify the prediction format and improve computational efficiency. Furthermore, during the aggregation process of SE-GNN, for each training batch, we randomly remove a proportion of corresponding edges in the knowledge graph. This can prevent the information leakage problem, i.e. the model has seen the prediction edges when aggregating. This can also guide the model to learn the interactions between existed edges and prediction missing edges when aggregating, which is a closer scenario of extrapolation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>KGE extrapolation performance on different SE ranges. The test data of FB15k-237 is divided into three ranges based on evidence metric. Bottom x-axis denotes the metric value, top x-axis denotes the data portion of each range and y-axis denotes the Mean Rank of model prediction result (low value indicates good performance, and 1 is the best).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The model architecture of SE-GNN. The blue, green and orange graph represents relation, entity and triple level SE aggregation process respectively. The yellow node is the example center node of neighbor aggregation. By layer-wise iteration, SE-GNN can access a wide range of graph structure and model the deep interaction of SEs. Finally, the output entity and relation embedding are fed into a Knowledge Graph Completion decoder to perform the extrapolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The performance comparation of SE-GNN and ConvE under the same Semantic Evidence range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Case study of Semantic Evidence on FB15k-237 dataset. The symbol ? means that two entities are semantically similar. The rank result is the prediction rank of correct entity in all entities. The best results are marked as bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Extrapolation performance of KGE models on different SE ranges on WN18RR dataset. The bottom x-axis denotes the range value, the top x-axis denotes the data portion of each range and y-axis denotes the Mean Rank of model prediction result (low value indicates good performance, and 1 is the best).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model reports on FB15k-237 and WN18RR test set. The best results are in bold. ? denotes that we reproduce the results using the code 3 . ? means that the results of KBGAT are from</figDesc><table><row><cell>Models</cell><cell cols="5">FB15k-237 MRR MR H@1 H@3 H@10</cell><cell cols="5">WN18RR MRR MR H@1 H@3 H@10</cell></row><row><cell>Translational Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransE (Bordes et al. 2013)  ?</cell><cell cols="3">.330 173 .231</cell><cell>.369</cell><cell>.528</cell><cell cols="3">.223 3380 .014</cell><cell>.401</cell><cell>.529</cell></row><row><cell>RotatE (Sun et al. 2019)</cell><cell cols="3">.338 177 .241</cell><cell>.375</cell><cell>.533</cell><cell cols="3">.476 3340 .428</cell><cell>.492</cell><cell>.571</cell></row><row><cell>PaiRE (Chao et al. 2021)</cell><cell cols="3">.351 160 .256</cell><cell>.387</cell><cell>.544</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Semantic Matching</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DistMult (Yang et al. 2015)  ?</cell><cell cols="3">.308 173 .219</cell><cell>.336</cell><cell>.485</cell><cell cols="3">.439 4723 .394</cell><cell>.452</cell><cell>.533</cell></row><row><cell>ComplEx (Trouillon et al. 2016)  ?</cell><cell cols="3">.323 165 .229</cell><cell>.353</cell><cell>.513</cell><cell cols="3">.468 5542 .427</cell><cell>.485</cell><cell>.554</cell></row><row><cell cols="2">TuckER(Balazevic and Allen 2019) .358</cell><cell>-</cell><cell>.266</cell><cell>.394</cell><cell>.544</cell><cell>.470</cell><cell>-</cell><cell>.443</cell><cell>.482</cell><cell>.526</cell></row><row><cell>ConvE (Dettmers et al. 2018)</cell><cell cols="3">.325 244 .237</cell><cell>.356</cell><cell>.501</cell><cell cols="3">.430 4187 .400</cell><cell>.440</cell><cell>.520</cell></row><row><cell>InteractE (Vashishth et al. 2020a)</cell><cell cols="3">.354 172 .263</cell><cell>-</cell><cell>.535</cell><cell cols="3">.463 5202 .430</cell><cell>-</cell><cell>.528</cell></row><row><cell>PROCRUSTES (Peng et al. 2021)</cell><cell>.345</cell><cell>-</cell><cell>.249</cell><cell>.379</cell><cell>.541</cell><cell>.474</cell><cell>-</cell><cell>.421</cell><cell>.502</cell><cell>.569</cell></row><row><cell>GNN-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-GCN (Schlichtkrull et al. 2018)</cell><cell>.248</cell><cell>-</cell><cell>.151</cell><cell>-</cell><cell>.417</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KBGAT (Nathani et al. 2019)  ?</cell><cell cols="2">.157 270</cell><cell>-</cell><cell>-</cell><cell>.331</cell><cell cols="2">.412 1921</cell><cell>-</cell><cell>-</cell><cell>.554</cell></row><row><cell>SACN (Shang et al. 2019)</cell><cell>.350</cell><cell>-</cell><cell>.260</cell><cell>.390</cell><cell>.540</cell><cell>.470</cell><cell>-</cell><cell>.430</cell><cell>.480</cell><cell>.540</cell></row><row><cell>A2N (Bansal et al. 2019)</cell><cell>.317</cell><cell>-</cell><cell>.232</cell><cell>.348</cell><cell>.486</cell><cell>.450</cell><cell>-</cell><cell>.420</cell><cell>.460</cell><cell>.510</cell></row><row><cell>CompGCN(Vashishth et al. 2020b)</cell><cell cols="3">.355 197 .264</cell><cell>.390</cell><cell>.535</cell><cell cols="3">.479 3533 .443</cell><cell>.494</cell><cell>.546</cell></row><row><cell>SE-GNN (ours)</cell><cell cols="3">.365 157 .271</cell><cell>.399</cell><cell>.549</cell><cell cols="3">.484 3211 .446</cell><cell>.509</cell><cell>.572</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of three SEs, where w/o means removing the corresponding modeling part in SE-GNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Dataset statistics</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This prediction process is also called Knowledge Graph Completion task, which shares many common concepts with Knowledge Graph Embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The unseen data does not mean the new entity or relation, but the new triple combination. In fact all the entities and relations in Fte should occur in Ftr, in order to learn their embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/DeepGraphLearning/ KnowledgeGraphEmbedding, commit ID: 2e440e0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the the Youth Innovation Promotion Association CAS (No.2018192), the National Natural Science Foundation of China (No. 62102421) , and Intelligent Social Governance Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Survey on Graph Neural Networks for Knowledge Graph Completion. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A2N: Attending to Neighbors for Knowledge Graph Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4387" to="4392" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extrapolation and interpolation in neural network classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="50" to="53" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Inductive Bias of Neural Tangent Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="12873" to="12884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06-10" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PairRE: Knowledge Graph Embeddings via Paired Relation Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021-08-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SimCat: an entity similarity measure for heterogeneous knowledge graph with categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chelliah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM IKDD Conference on Data Sciences</title>
		<meeting>the Second ACM IKDD Conference on Data Sciences<address><addrLine>Bangalore, CoDS; India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-03-18" />
			<biblScope unit="page" from="112" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extrapolation limitations of multilayer feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Haley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1992] IJCNN International Joint Conference on Neural Networks</title>
		<meeting>1992] IJCNN International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06" />
			<biblScope unit="page" from="2364" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference, ESWC 2018, Heraklion</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-End Structure-Aware Convolutional Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topic Model Based Knowledge Graph for Entity Similarity Measuring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on e-Business Engineering</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-10-12" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Re-evaluation of Knowledge Graph Completion Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
		<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">InteractE: Improving Convolution-Based Knowledge Graph Embeddings by Increasing Feature Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-10-22" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Qu?bec City, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014-07-27" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06" />
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2031" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Computing Semantic Similarity of Concepts in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Iglesias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="85" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
