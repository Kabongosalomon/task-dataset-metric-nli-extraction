<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Not All Memories are Created Equal: Learning to Forget by Expiring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Poff</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
						</author>
						<title level="a" type="main">Not All Memories are Created Equal: Learning to Forget by Expiring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanisms have shown promising results in sequence modeling tasks that require longterm memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories . However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer architectures <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref> have demonstrated strong performance across a variety of tasks <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b33">Roller et al., 2020;</ref><ref type="bibr" target="#b5">Brown et al., 2020)</ref>, including those that require learning long term relationships <ref type="bibr">(Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Fan et al., 2019a;</ref><ref type="bibr" target="#b20">Izacard &amp; Grave, 2020)</ref>. Recent work has focused on scaling attention mechanisms efficiently to longer memory sizes, enabling large improvements on long context tasks <ref type="bibr" target="#b9">(Dai et al., 2019;</ref><ref type="bibr" target="#b39">Sukhbaatar et al., 2019a)</ref>. However, a critical component of human memory is not just the ability to remember, but also forgetting irrelevant information to focus on the salient, relevant bits. Most studies of long-term memory in humans indicate that not everything is remembered <ref type="bibr" target="#b28">(Murre &amp; Dros, 2015;</ref><ref type="bibr" target="#b2">Bahrick et al., 2008)</ref> -instead, only vivid, remarkable memories are retained from the far past <ref type="bibr" target="#b46">(Wixted, 2004)</ref>.</p><p>Standard Transformer architectures lack the ability to search over extremely large memories, as the self-attention mechanism is computationally intensive and the storage cost of preserving the large memory grows quickly. Recent work <ref type="bibr" target="#b6">(Child et al., 2019;</ref><ref type="bibr" target="#b31">Rae et al., 2020)</ref> has proposed learning how to extend to greater context through sparse mechanisms or through compression, to more compactly represent the past. However, there exists a fundamental problem with large memories beyond strict computational concerns: as the amount of information stored increases, deciding which information is relevant becomes more challenging. Other work <ref type="bibr" target="#b23">(Lample et al., 2019)</ref> approaches this by considering how to efficiently search large memories. We focus on an efficient way to learn what to forget, thereby reducing the computational burden of the model and easing the challenges of the search problem.</p><p>We propose EXPIRE-SPAN, a straightforward extension to attention mechanisms that learns when to expire unneeded memories. By expiring memories that are no longer useful, EXPIRE-SPAN enables scaling to tens of thousands of timesteps into the past. This learnable mechanism allows the model to adjust the span size as needed, selecting which information is critical to retain and forgetting the rest. More concretely, we augment the self-attention with a simple predictor that outputs an expiration value for each hidden state that determines how long a memory should be retained and accessible to the model. After the EXPIRE-SPAN runs out, the memory will be forgotten, but in a gradually differentiable way to retain end-to-end training with backpropagation. This process is done independently for each layer, allowing different layers to specialize at different time-scales. As EXPIRE-SPAN can flexibly adjust its span based on context, it is more efficient in terms of memory and training time compared to existing long memory approaches. arXiv:2105.06548v2 <ref type="bibr">[cs.</ref>LG] 13 Jun 2021</p><p>We demonstrate that EXPIRE-SPAN can distinguish between critical and irrelevant information on several illustrative tasks in natural language processing and reinforcement learning that are specifically designed to test this ability. We then show we can achieve state-of-the-art results on long-context language modeling benchmarks, and EXPIRE-SPAN can scale to memories in the tens of thousands on a frame-by-frame colliding objects task -by expiring irrelevant information, capacity is freed to have even larger memory. Then, we compare the efficiency of our method to competitive baselines and show EXPIRE-SPAN is faster and has a smaller memory footprint. Finally, we analyze the information retained and expired by EXPIRE-SPAN models, to understand the importance of long context memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Memory is crucial for many tasks and has been studied in recurrent networks <ref type="bibr" target="#b11">(Elman, 1990;</ref><ref type="bibr" target="#b19">Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b27">Mikolov et al., 2010)</ref> for a long time. The development of memory augmented networks <ref type="bibr" target="#b18">(Graves et al., 2014;</ref><ref type="bibr" target="#b38">Sukhbaatar et al., 2015b)</ref> made it possible to store large quantities of information and selectively access them using attention <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>. The Transformer <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref> took full advantage of this approach. Processing long sequences with Transformers is an active area with applications in language understanding <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>, reinforcement learning <ref type="bibr" target="#b29">(Parisotto et al., 2020)</ref>, video processing <ref type="bibr" target="#b47">(Wu et al., 2019)</ref>, and protein folding <ref type="bibr" target="#b32">(Rives et al., 2019;</ref><ref type="bibr" target="#b7">Choromanski et al., 2020)</ref>. However, extending the memory span is computationally expensive due to the quadratic time and space complexity of self-attention. Other work focuses on benchmarking long memories <ref type="bibr" target="#b42">(Tay et al., 2021)</ref>, but focuses on encoder-only tasks, whereas we focus on decoder-only Transformers.</p><p>Various work has focused on reducing this complexity and increasing memory capacity <ref type="bibr" target="#b35">(Schlag et al., 2021)</ref>. Dynamic attention spans, such as Adaptive-Span <ref type="bibr" target="#b39">(Sukhbaatar et al., 2019a)</ref> and Adaptively Sparse Transformer <ref type="bibr" target="#b8">(Correia et al., 2019)</ref>, focus on learning which attention heads can have shorter spans, but can only extend to spans of a few thousand. Other work sparsifies attention by computing fewer tokens <ref type="bibr" target="#b13">(Fan et al., 2019b)</ref>, often by using fixed attention masks <ref type="bibr" target="#b6">(Child et al., 2019)</ref> or sliding windows and dilation <ref type="bibr" target="#b4">(Beltagy et al., 2020)</ref>. The BP Transformer <ref type="bibr">(Ye et al., 2019)</ref> structures tokens as a tree, so some tokens have coarse attention. These works focus on learning what to attend to, but searching larger and larger memories is very difficult. In contrast, we focus on learning to expire what is irrelevant. Compressive Transformer  reduces the number of memories by replacing every few memories with a single compressed one. A disadvantage of this is that all memories have the same compression ratio, so relevant  <ref type="figure">Figure 1</ref>. Expire-Span. For every memory hi, we compute an EXPIRE-SPAN ei that determines how long it should stay in memory. Here, memories h2, h5 are already expired at time t, so the query qt can only access {h1, h3, h4} in self-attention. memories are equally compressed.</p><p>Another line of work investigates linear-time attention mechanisms. <ref type="bibr">Wu et al. (2018)</ref> replace self-attention with convolutions that run in linear time, but the scalability to long context tasks remains limited. <ref type="bibr" target="#b45">Wang et al. (2020)</ref> propose linear time attention by decomposing attention into multiple smaller attentions, that recombine to form a low-rank factorization of the original attention. <ref type="bibr" target="#b21">Katharopoulos et al. (2020)</ref> propose linear attention by expressing self-attention as instead a linear dot-product of feature maps. <ref type="bibr" target="#b30">Peng et al. (2021)</ref> propose Random Feature Attention, used to approximate the softmax. Those methods, however, focus on making attention more efficient without reducing the number of memories. Further, as our goal is to reduce the number of memories that feed to self-attention by learning to expire, EXPIRE-SPAN can be easily combined with these efficiency improvements. For a review of further recent Transformer variants, see <ref type="bibr" target="#b41">Tay et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Transformer architectures have been widely used as decoderonly auto-regressive models for sequential tasks. A Transformer decoder is made of a stack of identical layers, composed of a multi-head self-attention sublayer followed by a feedforward sublayer. The output of each timestep is the hidden state h l t ? R d at layer l, which is then projected to key k, value v, and query q vectors:</p><formula xml:id="formula_0">q l t = W l q h l t , k l t = W l k h l t , v l t = W l v h l t .<label>(1)</label></formula><p>Going forward, we focus on a single layer and omit the layer index l for brevity. Information from previous timesteps is accessed through attention a ti to create output o t :</p><formula xml:id="formula_1">a ti = Softmax i?Ct q t k i , o t = W o i?Ct a t,i v i .<label>(2)</label></formula><p>The set C t indicates which memories can be accessed at time t, which is the focus of this work. The space and time complexity of self-attention is linearly correlated to the size of this set |C t |, making it an important metric of efficiency. For the rest of the paper, we will refer to |C t | as the memory size.</p><p>Including all previous timesteps in self-attention by setting C t = {1, . . . , t?1} results in a quadratic complexity O(T 2 ) to compute the full attention over a sequence of length T . Fixed-spans <ref type="bibr" target="#b9">(Dai et al., 2019</ref>) take a more scalable approach such that C t = {t?L, . . . , t?1} so the attention is restricted to previous L steps. The total complexity in this case is O(T L), where L is the attention span.</p><p>Adaptive-Span <ref type="bibr" target="#b39">(Sukhbaatar et al., 2019a)</ref> further improves upon upon this by learning an optimal span L per attention head from data, which results in small L values for many heads. Compression approaches  reduce memory size by compressing multiple timesteps into a single memory, with complexity O(T L/c), where c is the compression rate. However, in all these approaches, all memories are treated equally without regards to their importance to the task. In this work, we focus on distinguishing between relevant and irrelevant memories by learning to expire unneeded information -by expiring, the remaining attention on relevant information can scale beyond existing long context memory approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expire-Span</head><p>We describe EXPIRE-SPAN and how to integrate it into Transformers to focus on relevant information and expire the rest, meaning memories can be permanently deleted. We describe how to scale EXPIRE-SPAN and practically train with drastically longer memory spans.  <ref type="figure">Figure 1</ref>, allows models to selectively forget memories that are no longer relevant. We describe it in the context of a single Transformer layer and omit the layer index l for brevity. Our goal is to reduce the size of C t defined in Section 3 for more efficiency without performance degradation. For each memory h i ? R d , we will compute a scalar EXPIRE-SPAN e i ? [0, L]:</p><formula xml:id="formula_2">e i = L?(w h i + b).<label>(3)</label></formula><p>1 The full implementation can be found at https://github.com/facebookresearch/ transformer-sequential.</p><p>Here w ? R d and b ? R represent trainable parameters, ? is the sigmoid function, and L is the maximum span. This expire-span e i determines how long h i should be kept and included in C t . At time t, the remaining span of h i is r ti = e i ? (t ? i). When r ti becomes negative, it indicates the memory h i is expired and can be removed from C t . This can be implemented by updating attention weights a ti with a binary masking function m ti = 1 rti&gt;0 :</p><formula xml:id="formula_3">a ti = m ti a ti j m tj a tj , o t = i a ti v i .<label>(4)</label></formula><p>However, with such discrete masking, the Expire-Span e i will not receive any gradient for training. Instead, we use a soft masking function from <ref type="bibr" target="#b39">Sukhbaatar et al. (2019a)</ref> that smoothly transitions from 0 to 1 (see <ref type="figure">Figure 2</ref>):</p><formula xml:id="formula_4">m ti = max(0, min(1, 1 + r ti /R)),<label>(5)</label></formula><p>where R is a hyperparameter that determines the length of a ramp that is bounded between 0 to 1. This function has nonzero gradient for values in [?R, 0] to train e i , but also can take a value of 0, which is necessary for expiring memories.</p><formula xml:id="formula_5">Thus C t = {i | m ti &gt; 0}.</formula><p>Since m ti is a monotonically decreasing function of t, once a memory is expired, it can be permanently deleted.</p><p>Our goal is to reduce the average memory size, which is directly related with the average EXPIRE-SPAN:</p><formula xml:id="formula_6">1 T t |C t | = 1 T t i&lt;t 1 mti&gt;0 = 1 T i R + t&gt;i 1 rti&gt;0 = 1 T i R + t&gt;i 1 ei&gt;t?i = R ? 1 + 1 T i e i<label>(6)</label></formula><p>Therefore, we add an auxiliary term to the loss function to penalize the L1-norm of EXPIRE-SPAN:</p><formula xml:id="formula_7">L total = L task + ? i e i /T,<label>(7)</label></formula><p>where ? &gt; 0 is a hyperparameter. This term decreases the span of memories that contribute less to the main task, resulting in a small memory that focuses only on relevant information. Note the new parameters, w and b, and the computations of EXPIRE-SPANS are negligible in size compared to the total number of parameters and computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adding Expire-Span to Transformers</head><p>We describe how EXPIRE-SPAN can be utilized within Transformer self-attention layers to decrease the memory size and focus on salient information. This section describes each modification clearly, to facilitate easier reproduction. We discuss practical training concerns, such as efficiency and regularization. Additional details can be found in the appendix.</p><p>Modifications to Multi-Head Attention Self-attention consists of multiple heads that have different keys, values, and queries. However, they all share one underlying memory, so a memory cannot be removed if it is used by any of the heads. Thus, we compute an EXPIRE-SPAN at each layer that is shared amongst the heads.</p><p>Block Parallel This modification allows memories to be permanently deleted in EXPIRE-SPAN. We use the caching mechanism <ref type="bibr" target="#b9">(Dai et al., 2019)</ref>, where a block of timesteps B = [t, . . . , t + K ? 1] is processed in parallel for efficiency -once a block is computed, its hidden states [h t , . . . , h t+K?1 ] are cached so that future blocks can attend to them. This means a memory can be deleted only if it is not used by any of the queries in B. Concretely, h i will be deleted when m ti = 0 where t is the first token of B. However, this is not a concern for very long-term memories where L K.</p><p>Loss Computation The L1-norm loss for EXPIRE-SPAN must be computed for every memory h i . A straightforward way is to compute it for the current block B. This empirically results in poor performance -a possible explanation is that the time between positive and negative gradients on e i may become too distant. Negative gradients that increase e i only come from the main loss L task through the masking function m ti , which has non-zero gradients only when memory h i is about to expire with 0 &lt; m ti &lt; 1 for t ? B. For a large L K, h i may have been computed many blocks before and since then the model weights would have changed. In contrast, the positive gradients that decrease e i are computed on the current block i ? B. To remove this discrepancy, we compute the auxiliary loss on e i at the same time as negative gradients when 0 &lt; m ti &lt; 1 for t ? B.</p><p>Regularization A potential challenge in exceptionally long memory is greater capacity to overfit. As EXPIRE-SPAN can scale to memories in the tens of thousands, it can overfit to learning specific span sizes on the training set that do not generalize. As a form of regularization, we propose to randomly shorten the memory during training. For each batch, we sample l ? U(0, L) and set a ti = 0 for all t ? i &gt; l only during training. This way, the model cannot assume the memory will always contain specific information, as the memory is randomly shortened.</p><p>Stable Training with Extremely Large Spans Multiplier L in Eq. 3 is the maximum span, so it can take very large values, exceeding tens of thousands. This is a potential problem because small changes in h i or w will be amplified in EXPIRE-SPAN e i , and subsequently have dramatic effects on the model behaviour. As a straightforward remedy, for very large L values, we replace Eq. 3 with</p><formula xml:id="formula_8">e i = L? (w h i + b)/R .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>We show that EXPIRE-SPAN focuses on salient information on various constructed and real-world tasks that necessitate expiration. First, we describe baselines and efficiency metrics for comparing various models. Second, we illustrate the importance of expiration on various constructed tasks. Then, we highlight the scalability of EXPIRE-SPAN when operating on extremely large memories. Additional experiments and details are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head><p>We compare our method against several baselines from Section 3 that take different approaches to access information in the past. We compare the performance of these methods, along with two efficiency metrics: GPU memory and training speed for a fixed model size and batch size. First, we compare to Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019)</ref>, which corresponds to the fixed-span approach where simply the last L memories are kept. Our Transformer-XL implementation also serves as a base model for all the other baselines to guarantee that the only difference among them is how memories are handled. The other baselines are Adaptive-Span <ref type="bibr" target="#b39">(Sukhbaatar et al., 2019a)</ref> and Compressive Transformer , two popular approaches for long memory tasks. For Compressive Transformer, we implemented the mean-pooling version, which was shown to have strong performance despite its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Importance of Expiration: Illustrative Tasks</head><p>Remembering One Key Piece of Information To illustrate a case where proper expiration of unnecessary memories is critical, we begin with an RL gridworld task: walking down a corridor. In this Corridor task, depicted in <ref type="figure" target="#fig_2">Figure 3</ref> (left), the agent is placed at one end of a very long corridor, next to an object that is either red or blue. The agent must walk down the corridor and go to the door that corresponds to the color of the object that it saw at the beginning to receive +1 reward. The requirement on the memory is very low: the agent only needs to remember the object color so it can walk through the correct door.  Accuracy (%)</p><p>Trans-XL Expire-span <ref type="figure">Figure 4</ref>. We plot performance as a function of memory size for three tasks. Training scores are shown. Ideal models can achieve strong performance with small memories by identifying which information is important to remember. Corridor Task (left) -We train 10 baseline models with different memory sizes, and five EXPIRE-SPAN models with different seeds. Portal Task (middle)-We train models with different memory sizes and random seeds. Instruction Task (right) -We train 6 baseline models with different memory sizes, and five EXPIRE-SPAN models with different seeds.</p><p>EXPIRE-SPAN models can take advantage of this fact and keep the memory size small regardless of the corridor length, which can vary between 3 and 200. This is confirmed in the results shown in <ref type="figure">Figure 4</ref> (left) where the EXPIRE-SPAN models achieve high performance on this task with very small memories. Without the ability to forget, the Transformer-XL models require large memory for storing all navigation steps that grow with the corridor length.</p><p>Remembering Long Sequences of Information Next, we analyze EXPIRE-SPAN on another reinforcement learning task, but this time testing memorization of sequences: Portal through Multiple Rooms. An agent in a gridworld must navigate through multiple rooms separated by different doors, depicted in <ref type="figure" target="#fig_2">Figure 3</ref> (middle). Each room has two exit doors with different colors -one door portals to the adjacent room, while the other portals back to the start. However, which door works in which room is random for each episode. Thus, the only way to visit more rooms is by trial-and-error, where agents need to remember the sequence of correct doors to successfully navigate to the end. The environment is partially observable and randomized at each episode.</p><p>We display results in <ref type="figure">Figure 4 (middle)</ref>. The Transformer-XL models need longer memory to perform better and visit more rooms, because each new room requires many navigation steps to reach. However, those navigation steps are actually irrelevant because the agent only needs to memorize the colors of the correct doors. Usually, the agent needs to pass through the same room multiple times to solve the remaining rooms, but it only needs to remember the door color from the first pass, while all subsequent passes can be expired. Since EXPIRE-SPAN models can discard irrelevant memories and focus its memory on memorizing the exact sequence of door colors, they achieve strong performance with much smaller memory compared to the Transformer-XL baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remembering Long Sequences with Severe Distractors</head><p>To illustrate a more difficult task where a model must learn to expire, we use a dialogue-based story generation task from the LIGHT <ref type="bibr" target="#b43">(Urbanek et al., 2019)</ref>  multiple instructions can be in queue for execution.</p><p>We experiment with a dataset where the average distance between receiving and executing instructions is around 950 distractor words. Models are trained as language models, but evaluated only on their success in executing the instruction. Task details and model architecture are provided in the appendix. We illustrate in <ref type="figure">Figure 4</ref> (right) that EXPIRE-SPAN is much more successful at this task than Transformer-XL and Adaptive-Span (see the appendix), as it can focus on the specific instruction lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Scalability of Expire-Span</head><p>We analyze the scalability of EXPIRE-SPAN. On a copy task, we train models with spans up to 128k timesteps. Then, we show the utility of EXPIRE-SPAN on character-level language modeling -Enwik8 and PG-19 -and a moving objects task that is processed frame by frame. For these tasks, we also analyze the efficiency of EXPIRE-SPAN compared to existing methods, and demonstrate that our method has a smaller memory footprint and faster processing speed. We quantify efficiency with two metrics: (1) peak GPU memory usage and (2) training time per batch (comparing fixed batch size for similar size models).</p><p>Extremely Long Copy To illustrate the scalability of EXPIRE-SPAN, we construct a copy task where the model sees a sequence of A very far in the past. The rest of the characters are B. The model must copy the correct quantity of A. We design the task such that a long span (up to 128k) can be required, as the A tokens are very far into the past. In <ref type="table">Table 1</ref>, we show that only by scaling the maximum span to 128k it is possible to achieve improved performance. We compare to a Transformer-XL baseline with 2k attention span and a EXPIRE-SPAN model with smaller span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Level Language Modeling: Enwik8</head><p>We subsequently experiment on Enwik8 for character level language modeling <ref type="bibr" target="#b25">(Mahoney, 2011</ref> Trans-XL 24L <ref type="bibr" target="#b9">(Dai et al., 2019)</ref> 277M 0.99 Sparse Trans. <ref type="bibr" target="#b6">(Child et al., 2019)</ref> 95M 0.99 Adapt-Span 24L <ref type="bibr" target="#b39">(Sukhbaatar et al., 2019a)</ref> 209M 0.98 All-Attention <ref type="bibr" target="#b40">(Sukhbaatar et al., 2019b)</ref> 114M 0.98 Compressive Trans.  277M 0.97 Routing Trans. <ref type="bibr" target="#b34">(Roy et al., 2020)</ref> -0.99 Feedback Trans. <ref type="bibr" target="#b15">(Fan et al., 2020b)</ref> 77M 0.96 EXPIRE-SPAN 24L 208M 0.95 small quantity of salient information for good performance.</p><p>Next, we compare EXPIRE-SPAN to existing work in <ref type="table" target="#tab_3">Table 2</ref>. A small EXPIRE-SPAN model with the maximum span L = 16k outperforms similarly sized baselines by a large margin. We also trained a larger EXPIRE-SPAN model with L = 32k and LayerDrop <ref type="bibr" target="#b14">(Fan et al., 2020a)</ref>, which outperforms the Compressive Transformer and sets a new state of the art on this task. This indicates that models can learn to expire relevant information and encode long context effectively, even on very competitive language modeling benchmarks.</p><p>Finally, we compare the efficiency of EXPIRE-SPAN with the Transformer-XL, Adaptive-Span and Compressive Transformer baselines. We find that EXPIRE-SPAN models achieve much better performance, as shown in <ref type="table">Table 4</ref>   <ref type="figure">Figure 6</ref>. Performance on Character-level PG-19. We report bitper-character on test. 3506. We train several baselines: Transformer-XL with maximum spans of 1k and 2k, and Adaptive-Span and Compressive Transformers with 16k span. We train EXPIRE-SPAN with maximum spans of 8k and 16k. We present results in <ref type="figure">Figure 6</ref>, where we show that EXPIRE-SPAN is substantially better than Transformer-XL, and matches the performance of Adaptive-Span and Compressive Transformer.</p><p>However, EXPIRE-SPAN uses its available memory very effectively. The 16k maximum span EXPIRE-SPAN model has an average memory size of 860. In comparison, the Adaptive-Span model has an average memory size of 2440, almost 3x that of the 16k EXPIRE-SPAN model. This indicates that EXPIRE-SPAN enables models to identify the critical bits of information and expire the rest, reaching the same performance with a much smaller memory.</p><p>Finally, comparing efficiency <ref type="table">(Table 4)</ref>, EXPIRE-SPAN trains at double the speed of Compressive Transformer. EXPIRE-SPAN is faster than Adaptive-Span, though uses slightly more memory. The memory usage of EXPIRE-SPAN is usually lower, around 12GB, but spikes for some sentences. Lastly, while the average span size of EXPIRE-SPAN is lower than Adaptive-Span, the computation requires additional tensors allocated in memory, which can potentially be addressed by an optimized implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Maximum Span Test Error (%) Frame-by-Frame Processing: Object Collision An important setting where learning which long context may be important is in video understanding, a field with increasing focus as model architectures provide the capability to process long sequences. Despite video data being memory intensive, salient events might be localized in space and time. We test our model on a task where two objects move around and collide, and the goal is to reason about the location of specified-color collisions. Objects have a color that can randomly change. We divide the grid into four quadrants and the model is asked to recall the quadrants of the last collision of a specific color pair. Because the collisions are rare, and collisions of specific colors are even rarer, the model must process a large quantity of frames.</p><p>We illustrate the task in <ref type="figure" target="#fig_3">Figure 7</ref> and results in <ref type="table" target="#tab_5">Table 3</ref>. The task requires many frames, so long context is very beneficial -as the EXPIRE-SPAN maximal span increases, performance steadily rises. Our largest span, 64k, matches the size of the largest attention limit reported to date <ref type="bibr" target="#b22">(Kitaev et al., 2019)</ref> and has the strongest performance. This model is trained with the random drop regularization method described in Section 4.2. Compared to Compressive Transformer and Adaptive-Span baselines, our EXPIRE-SPAN model has the strongest performance.</p><p>Comparing efficiency, EXPIRE-SPAN trains almost 3x faster than both baselines (see <ref type="table">Table 4</ref>) while having much stronger performance. Further, expiration is critical to this performance -a Adaptive-Span model with L = 32k runs out of memory in the same setting where we trained our EXPIRE-SPAN model with L = 64k. Through expiration, our model can keep the GPU memory usage reasonable and train with the longer spans necessary for strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis and Discussion</head><p>EXPIRE-SPAN creates the phenomena of selective forgetting: it allows memories to be permanently deleted if the model learns they are not useful for the final task. In this section, we analyze the information retained and expired by EXPIRE-SPAN models to better understand how models use  <ref type="table">Table 4</ref>. Efficiency of EXPIRE-SPAN. We report peak GPU memory usage and per-batch training time, fixing the batch size. <ref type="figure">Figure 8</ref>. Expiration in EXPIRE-SPAN on Enwik8. In (a), the model strongly memorizes two areas, "Egypt" and "Alexander". In (b), if we replace "Egypt" with "somewhere", then it's forgotten fast. In (c), we insert "Humpty Dumpty" and the model retains these rare words in memory. <ref type="figure">Figure 9</ref>. Accuracy Needs Memory. As the maximum span is artificially decreased at inference time from 16k to only 1k, the prediction is less accurate.</p><p>the ability to forget. Additional analyses are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retaining Salient Information</head><p>We analyze what is retained by an EXPIRE-SPAN model on Enwik8 to understand how models utilize the ability to forget. In <ref type="figure">Figure 8 (a)</ref>, we show that the model retains information about named entities such as Egypt and Alexander the Great by giving them longer spans (darker color). Next, we analyze how expire-spans changes when we artificially edit the past text.</p><p>In <ref type="figure">Figure 8 (b)</ref>, we replace the entity Egypt with the generic text somewhere, and this generic word is quickly expired. In <ref type="figure">Figure 8</ref> (c), we edit Egypt to Humpty Dumpty, which is a very rare entity, and the model retains it in memory without expiring. In addition to entities, EXPIRE-SPAN memorizes spaces, newlines, and section titles, all of which retain information about words, sentences, or sections. The model's expiration choices vary by layer, indicating that EXPIRE-SPAN models use the memory at each layer to remember different information.</p><p>Importance of Long Term Memory Next, we analyze which predictions benefit the most from memory capacity. We take an EXPIRE-SPAN model trained on Enwik8 and decrease the maximum span size to 1024 at inference time, even though the model was trained with a maximum span of 16k. We then compare which predictions decreased in accuracy. In <ref type="figure">Figure 9</ref>, we see that models have a much higher loss when predicting the named entity Guinea coast compared to having the full 16k maximal span. Guinea coast was mentioned 3584 tokens earlier, which indicates that long attention is often necessary to predict words mentioned in far away context. In general, we found that rare tokens and structural information about documents, such as section headings or document titles, required longer attention span to accurately predict.</p><p>Efficiency Advantages of Expire-Span Finally, we end with a brief discussion about why EXPIRE-SPAN is more efficient compared to existing architectures that focus on long context. First, Transformer-XL cannot adapt to the data at all, so it becomes slow and inefficient quite quickly as the span size increases. Adaptive-Span can adapt to the data and adjust its memory, but this memory size is fixed after training and does not have the dynamic adjustment of Expire-Span (where memory depends on local context even at inference time). Finally, the Compressive Transformer compresses past memories, but it compresses always at a fixed rate. The compression rate is an adjustable parameter, but aggressive compression potentially hurts performance. In contrast, EXPIRE-SPAN can expire irrelevant content, which both improves performance by focusing on salient information, and reduces the load on GPU memory and allows for faster processing per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present EXPIRE-SPAN, an operation that can be added to any attention mechanism to enable models to learn what to forget. By expiring irrelevant information, models can scale attention to tens of thousands of past memories. We highlight the strong performance of EXPIRE-SPAN in language modeling, reinforcement learning, object collision, and algorithmic tasks, and use it to attend over tens of thousands of past memories. The scalability and much greater efficiency of our proposed EXPIRE-SPAN method has strong potential for allowing models to be applied to more challenging, human-like tasks that would require expiration.  <ref type="bibr" target="#b36">(Shaw et al., 2018)</ref> make it possible to condition on the order of inputs by modifying the attention to a ti = Softmax(q t k i + q t p t?i ). However, because this second term is computed for the whole block in parallel for efficiency, it can become expensive for a large L even when the average memory size |C t | is small. Our solution is to remove position embeddings from older memories i &lt; t ? K (where K is the block size), which empirically does not affect performance. The computational complexity of the position embeddings is then O(K), thus allowing us to increase the maximum span L. This modification makes training EXPIRE-SPAN more efficient, but does not improve accuracy.</p><p>Training with Small Initial Spans EXPIRE-SPAN scales to long attention spans as it quickly learns to expire irrelevant content. However, at the beginning of training, the long span can use large quantities of GPU memory. To circumvent this, we initialize the bias term b with a negative value. This prevents large memory usage at the beginning of training, after which the model quickly learns to expire and the memory usage is no longer problematic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Experimental Results</head><p>Efficiency for Instruction Task We include a comparison of EXPIRE-SPAN to Adaptive-Span and Compressive Transformer in <ref type="table">Table 5</ref> and show that EXPIRE-SPAN has stronger performance, is faster, and saves GPU memory.</p><p>Wikitext-103 Language Modeling The Wikitext-103 word-level language modeling benchmark <ref type="bibr" target="#b26">(Merity et al., 2016)</ref> consists of a collection of Wikipedia articles and a fixed vocabulary size of 270K. We set the max attention span for EXPIRE-SPAN to 8K. We compare EXPIRE-SPAN to existing work in <ref type="table">Table 6</ref> and show that even fairly small models trained with EXPIRE-SPAN achieve competitive results. Next, we analyze the performance of EXPIRE-SPAN on Wikitext-103 as the memory size increases. We compare to a Transformer-XL model in <ref type="figure" target="#fig_4">Figure 10</ref> -even with far smaller memory, EXPIRE-SPAN performs much better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expire-span Performance and Analysis on Enwik8</head><p>In <ref type="figure">Figure 11</ref>, we analyze multiple layers of a trained model and show that different layers memorize different types of information. Several layers retain summarizing information about sentences or sections by increasing the expire-spans of spaces, new lines, and section titles.</p><p>Additionally, we did an ablation by running our large Expire-Span model without LayerDrop. Its validation performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev. (perplexity)</head><p>Trans-XL Expire-span dropped from 0.98bpb to 1.00bpb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of Structured Dropout for Regularization</head><p>We analyze the importance of structured dropout to regularize the large memory capacity provided by EXPIRE-SPAN.</p><p>In an experiment on enwiki8, <ref type="figure" target="#fig_5">Figure 12</ref> shows that loss on a portion of validation data was incredibly large. This part corresponds to a 66K token long Colliding Objects, An Easy Version We experiment with an easier version of the Colliding Objects task where objects do not have colors. The model has to predict either the last collision, or a mapping of the last 3 collisions. In contrast to the harder task, there are no color switches and any collision prediction is valid. As this version is less memory intensive, the EXPIRE-SPAN model almost solves it with a shorter maximum span, as shown in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1. REINFORCEMENT LEARNING TASKS</head><p>We used MazeBase <ref type="bibr" target="#b37">(Sukhbaatar et al., 2015a)</ref> to construct tasks in grid world. Agents can observe its surrounding 3 ? 3 area and move in the four cardinal directions. Every objects and their properties are described by words such as "agent","block", "blue", etc. Thus, the input to the model is a binary tensor of size 3 ? 3 ? vocabulary-size.</p><p>We train 2-layer Transformers with 64 hidden units using actor-Critic algorithm. We used a BPTT length of 100, and an entropy cost of 0.0005.  <ref type="table">Table 5</ref>. Efficiency of EXPIRE-SPAN. We report peak GPU memory usage and per-batch training time, fixing the batch size. We evaluate the mean pooling version of the Compressive Transformer. <ref type="figure">Figure 11</ref>. Per-Layer EXPIRE-SPAN values on Enwik8. We visualize the expire-spans of different layers: layer 6 gives long span to spaces, layer 9 memorizes special tokens like newlines and section titles, and layer 10 retains named entities in memory. maximum span L to 200, the loss coefficient ? to 5e-6, and the ramp length R to 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corridor Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Multi-Room Portal In this task, there are 50 rooms sequentially connected together. Each room is 5 ? 5 in size, and have two doors with different colors. If agent go to the correct door, it will be teleported to the next room, but if it is the wrong door, the agent will be teleported back to the first room and have to start over. Which of the two doors is correct in each room is randomly decided and fixed throughout the episode. This information is not visible to the agent, thus can only be discovered by trial and error within each episode. The current room number is visible to the agent.</p><p>When the agent successfully transitions from the k-th room to the next, it receives a reward of 0.1k. The episode ends if the agent makes two mistakes in the same room, reaches the last room, or when the number of steps reach 1000. A reward discount of 0.98 is used. All models are trained with Adam optimizer with a learning rate of 5e-4, and a batch size of 1024, with gradients are clipped at 0.1. We set L = 100, R = 16 and ? =1e-6 for the expire-span models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2. INSTRUCTION TASK IN LIGHT</head><p>We train 6-layer models with a hidden size of 512 and 8 attention heads. To train, we use the Adam optimizer with a learning rate of 7e-4 and 8000 warmup updates. We set the expire-span ramp R to 64 and the expire-span loss ? to 2e-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3. COLLISION TASK</head><p>At the start of the simulation, each particle samples a Gaussian Normal velocity and position uniform inside a 16 ? 16 grid. At each time step the particles' position is updated by adding its velocity (unless it would go off the grid, in which case its velocity is re-sampled). There are 5 different colors, and a particle can change its color randomly at each step with 0.05 probability. A collision happens when the two particles have the same rasterized locations, but it does not affect the movement.</p><p>Given a question specifying two colors, the task is to report in which of the four quadrants of the grid the last collision of the specified-colors occurred. To make the task easier to learn, 40% of the queries will have the matching colors as the last collision.</p><p>The model is given an input sequence of tokens that has 8 entries per timestep. The first 4 are the rounded and rasterized (x, y) locations of the two particles, and next 2 Learning to Forget by Expiring are tokens representing the colors of the particles. The last 2 entries are "question" tokens that specify the colors of the collision. The output sequence has a token for each quadrant. We generate 50M steps for training, which equals to 400M entries.</p><p>Easy Version: The particles have no color in this version. There are two types of questions, in which the task is to report either: (i) in which of the four quadrants of the grid the last collision occurred, or (ii) the label mapping of the last 3 collisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4. LANGUAGE MODELING DETAILS</head><p>Enwik8 Our small model has 12 layers with a hidden size of 512 and 8 attention heads. To train, we use Adam optimizer with a learning rate of 7e-4, a batch size of 512, a block size of 512 and 8000 warmup updates. All models are trained for 100k updates. The model in <ref type="table" target="#tab_3">Table 2</ref> is further fine-tuned for another 10k updates with a 10x smaller LR. The baseline models used for comparison are the same size model following the same training protocol.</p><p>The large EXPIRE-SPAN model <ref type="table" target="#tab_3">Table 2</ref> is a 24-layer model with a hidden size of 768 and 4096 feedforward units. It is trained with a learning rate of 4e-4 and 16k warmup steps. In addition to 0.5 dropout, we also add 0.2 layer-drop. The EXPIRE-SPAN parameters are L = 32k, ? =3e-7, and R = 128. We used the version of Eq. 6 due to the very long maximum span.</p><p>Character-level PG-19 Besides the maximum span, all model parameters and training parameters were held constant. Each model had 12 layers, a hidden size of 512, a feedforward size of 2048, 8 attention heads, and processed a block of 512 characters at a time. We initialized the weights using a uniform distribution as described by <ref type="bibr" target="#b16">(Glorot &amp; Bengio, 2010)</ref>, used dropout of 0.2, clipped the gradients at 0.3, warmed up the learning rate linearly for 8000 steps, and used cosine annealing to decay the learning rate after warmup <ref type="bibr" target="#b24">(Loshchilov &amp; Hutter, 2016)</ref>. For the EXPIRE-SPAN models, we used a ramp of R = 128 and an expiration loss coefficient of ? =1e-6 (3e-7) for L = 8k (16k).</p><p>Wikitext-103 All models have 8 layers and 1024 hidden units (4096 in feedforward layers). In addition to the dropout of 0.3 applied to attention and ReLU activation, outputs from the embedding layer and the last layer had a dropout of 0.2. We used the adaptive input <ref type="bibr" target="#b0">(Baevski &amp; Auli, 2019)</ref> and the adaptive softmax <ref type="bibr" target="#b17">(Grave et al., 2017)</ref> for reducing the number of parameters within word embeddings. The models are trained for 300k updates with a block size of 256, and gradients are clipped at 0.1. The other hyperparameters are the same as the small Enwik8 experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2. Soft Mask EXPIRE-SPAN, depicted in Figure 1, allows models to selectively forget memories that are no longer relevant. We describe it in the context of a single Transformer layer and omit the layer index l for brevity. Our goal is to reduce the size of C t defined in Section 3 for more efficiency without performance degradation. For each memory h i ? R d , we will compute a scalar EXPIRE-SPAN e i ? [0, L]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Corridor Task (left)-Agents must memorize the color of an object and walk through the door of the corresponding color at the end of a long corridor. Portal Task (middle)-An agent must trial-and-error to memorize the sequence of doors. Instruction Task (right)-A model must recognize instructions, memorize them, and execute when at the correct location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Object Collision task tests if models can remember the location of specified colored collisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Performance as a function of Memory Size on Wikitext-103</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 .</head><label>12</label><figDesc>Extreme Overfitting on part of validation occurs without proper regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>text world game environment. The model visits various locations and receives instructions of the form can you tell the [butler] that the [town official] wants to see them?. When the model is in a Table 1. Copy Task. We report accuracy on the test set.</figDesc><table><row><cell>Model</cell><cell cols="2">Maximum span Accuracy (%)</cell></row><row><cell>Transformer-XL</cell><cell>2k</cell><cell>26.7</cell></row><row><cell>EXPIRE-SPAN</cell><cell>16k</cell><cell>29.4</cell></row><row><cell>EXPIRE-SPAN</cell><cell>128k</cell><cell>52.1</cell></row></table><note>location where the butler is present, they must execute the instruction by generating You tell the butler "town official wants to see you!". Between receiving and executing, thou- sands of words of distractor text exist as shown Figure 3 (right). The model must learn to expire the distractors. Note</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Enwik8 Results. We report bit-per-byte (bpb) on test and the number of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Results on Object Collision. We report the error on the test set comparing to various baselines.</figDesc><table><row><cell>Transformer-XL</cell><cell>1k</cell><cell>73.3</cell></row><row><cell>Compressive</cell><cell>8k</cell><cell>63.8</cell></row><row><cell>Adaptive-Span</cell><cell>16k</cell><cell>59.8</cell></row><row><cell></cell><cell>16k</cell><cell>52.2</cell></row><row><cell>EXPIRE-SPAN</cell><cell>32k</cell><cell>36.7</cell></row><row><cell></cell><cell>64k</cell><cell>26.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Wu, F., Fan, A., Baevski, A., Dauphin, Y., and Auli, M. Pay less attention with lightweight and dynamic convolutions.In International Conference on Learning Representations, 2018.Ye, Z., Guo, Q., Gan, Q., Qiu, X., and Zhang, Z.</figDesc><table><row><cell cols="2">A. Appendix</cell><cell></cell><cell></cell></row><row><cell cols="2">A.1. Additional Method Details</cell><cell></cell><cell></cell></row><row><cell>Position</cell><cell>Embedding Relative</cell><cell>position</cell><cell>embed-</cell></row><row><cell>dings</cell><cell></cell><cell></cell><cell>Bp-</cell></row><row><cell cols="4">transformer: Modelling long-range context via binary</cell></row><row><cell cols="4">partitioning. arXiv preprint arXiv:1911.04070, 2019.</cell></row><row><cell cols="4">Zhang, J., Luan, H., Sun, M., Zhai, F., Xu, J., Zhang, M.,</cell></row><row><cell cols="4">and Liu, Y. Improving the transformer translation model</cell></row><row><cell cols="4">with document-level context. In Proceedings of the 2018</cell></row><row><cell cols="4">Conference on Empirical Methods in Natural Language</cell></row><row><cell cols="2">Processing, pp. 533-542, 2018.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>table. We hypothesize that the model likely never encountered such a table during training. During validation, this caused all non-table tokens to expire. Without regularizing the model memory size during training, the model can easily overfit.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Wikitext-103 Results. We report perplexity on test. Colliding Objects Results. We report test error.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Params Test</cell></row><row><cell cols="2">DEQ-Trans. (Bai et al., 2019)</cell><cell cols="2">110M 23.3</cell></row><row><cell cols="2">Trans-XL (Dai et al., 2019)</cell><cell cols="2">257M 18.3</cell></row><row><cell cols="2">Feedback Trans. (Fan et al., 2020b)</cell><cell>77M</cell><cell>18.3</cell></row><row><cell cols="2">Trans.+LayerDrop (Fan et al., 2020a)</cell><cell cols="2">423M 17.7</cell></row><row><cell cols="4">Compressive Trans. (Rae et al., 2020) 277M 17.1</cell></row><row><cell cols="2">Routing Trans. (Roy et al., 2020)</cell><cell>-</cell><cell>15.8</cell></row><row><cell>EXPIRE-SPAN</cell><cell></cell><cell cols="2">140M 19.6</cell></row><row><cell>Model</cell><cell cols="3">Maximum Span Test Error (%)</cell></row><row><cell>Transformer-XL</cell><cell>1k</cell><cell>39.1</cell></row><row><cell></cell><cell>1k</cell><cell>19.5</cell></row><row><cell>EXPIRE-SPAN</cell><cell>2k</cell><cell>9.1</cell></row><row><cell></cell><cell>4k</cell><cell>3.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Facebook AI Research 2 LORIA. Correspondence to: Sainbayar Sukhbaatar &lt;sainbar@fb.com&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fifty years of memory of college grades: Accuracy and distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Bahrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="688" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Masked language modeling for proteins via linearly scalable long-context transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptively sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using local knowledge graph construction to scale seq2seq models to multi-document inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategies for structuring story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2650" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Addressing some limitations of transformers with feedback memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09402</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large memory layers with product keys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8548" to="8559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://www.mattmahoney.net/text/text.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Replication and analysis of ebbinghaus&apos; forgetting curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stabilizing transformers for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02143</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Random feature attention</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compressive transformers for longrange sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>bioRxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13637</idno>
		<title level="m">Recipes for building an open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Linear transformers are secretly fast weight memory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11174</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (2)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mazebase: A sandbox for learning from games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>abs/1511.07401</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to speak and act in a fantasy text adventure game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="673" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The psychology and neuroscience of forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wixted</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="235" to="269" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
