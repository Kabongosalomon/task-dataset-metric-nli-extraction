<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The end-to-end Human Mesh Recovery (HMR) approach <ref type="bibr" target="#b11">(Kanazawa et al. 2018)</ref> has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular benchmarks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Motivation. The widely-used HMR <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref>) reconstructs human body by directly regressing mesh parameters, while lacking explicit guidance from 3D pose. As a result, it often fails to capture complex pose variations, e.g., the right arm of this actor. To tackle such problem, we propose pose calibration for human mesh recovery (PC-HMR), which can effectively integrate advanced 3D estimator and HMR in a novel manner to correct mesh. been gradually used as the mainstream architecture for endto-end 3D reconstruction. However, HMR reconstructs body configuration by directly regressing 3D mesh parameters of SMPL <ref type="bibr" target="#b17">(Loper et al. 2015)</ref>, while lacking explicit and geometrical knowledge of 3D human pose. As a result, the generated mesh often fails to correctly capture human pose variations in complex activities, e.g., HMR achieves an unsatisfactory mesh on the right arm of the actor in <ref type="figure">Fig. 1</ref>.</p><p>Hence, our basic idea is to use 3D human pose as guidance to reduce structural ambiguity in the 3D mesh. However, the ground truth 3D pose is usually unavailable in the testing phase. Fortunately, 3D pose estimation has recently made remarkable progress <ref type="bibr" target="#b18">(Martinez et al. 2017;</ref><ref type="bibr" target="#b2">Cai et al. 2019;</ref><ref type="bibr" target="#b25">Pavllo et al. 2019)</ref>, based on the fast development of deep learning. Inspired by this observation, we propose to integrate 3D pose estimators and HMR approaches in a unified fashion, so that we can take advantage of their complementary combination to calibrate human mesh.</p><p>Specifically, we introduce three contributions in this work. First, we develop two novel frameworks for Pose Calibration (PC), i.e., Serial and Parallel PC-HMR. Serial PC-HMR is a mesh-pose AutoEncoding framework. It can adaptively refine 3D pose to correct HMR mesh via multilevel encoding-decoding architecture. Parallel PC-HMR is a mesh-pose TwoStream framework. It can directly take advantages of off-the-shelf 3D pose estimators to deform mesh in HMR. Based on such serial or parallel manner, these two frameworks can effectively take tradeoffs between reconstruction error and computation cost into account. Second, we design a concise pose calibration module for these two frameworks. By leveraging non-rigid pose transformation as guidance, this module can effectively reduce misplacement caused by bone length variations, and thus generate more natural calibrated mesh. Third, our frameworks are the generic integration of pose estimators and mesh generators. On one hand, it can flexibly extend human mesh recovery for both images and videos. On the other hand, such plug-and-play design significantly releases training difficulties, and does not need extra 3D pose annotations for inference. We evaluate our PC-HMR frameworks on three popular benchmarks, i.e., Human3.6M, 3DPW, and SURREAL, where we achieve the SOTA results on mesh reconstruction.</p><p>2 Related Works 3D Mesh Reconstruction. SMPL <ref type="bibr" target="#b17">(Loper et al. 2015)</ref> has been widely used for 3D human mesh reconstruction. To boost its power in practice, a number of deep learning frameworks have been proposed by using SMPL as a mesh generation module <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref><ref type="bibr" target="#b13">Kocabas, Athanasiou, and Black 2020;</ref><ref type="bibr" target="#b30">Sun et al. 2019b;</ref><ref type="bibr" target="#b22">Nikos Kolotouros 2019;</ref><ref type="bibr" target="#b1">Bogo et al. 2016)</ref>. In particular, HMR <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref>) is one mainstream framework which regresses SMPL parameters directly from input images by end-toend training. Following this research direction, several extensions have been introduced to improve 3D mesh reconstruction in images <ref type="bibr" target="#b12">Kanazawa et al. 2019;</ref><ref type="bibr" target="#b30">Sun et al. 2019b;</ref><ref type="bibr" target="#b7">Guler and Kokkinos 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020)</ref> or model temporal relations for recovering 3D mesh in videos <ref type="bibr" target="#b13">Kocabas, Athanasiou, and Black 2020)</ref>. Additionally, several non-parametric frameworks have been proposed via voxel-based methods <ref type="bibr" target="#b27">(Saito et al. 2019;</ref><ref type="bibr" target="#b9">Huang et al. 2020</ref>). However, most HMRbased approaches lack explicit guidance of 3D pose. As a result, the body parts of generated mesh are often located at unsatisfactory positions. To alleviate such problem, we leverage advanced 3D pose estimators to calibrate mesh in two general PC-HMR frameworks.</p><p>3D Pose Estimation. Compared to 3D mesh reconstruction, 3D pose estimation has achieved more successes via deep learning. Basically, most current models can be categorized into two frameworks. The first is to directly estimate 3D pose from images, based on volumetric representation <ref type="bibr" target="#b24">(Pavlakos et al. 2017;</ref><ref type="bibr" target="#b29">Sun et al. 2018;</ref><ref type="bibr" target="#b23">Pavlakos, Zhou, and Daniilidis 2018;</ref><ref type="bibr" target="#b36">Wang, Shin, and Fowlkes 2020;</ref><ref type="bibr" target="#b35">Wang et al. 2019)</ref>. But these approaches may involve in complex post-processing steps. Based on the explosive improvement in 2D pose estimation <ref type="bibr" target="#b21">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b3">Chen et al. 2018)</ref>, another framework is to estimate 2D pose from images and then lift 2D pose to 3D pose <ref type="bibr" target="#b18">(Martinez et al. 2017;</ref><ref type="bibr" target="#b41">Zhao et al. 2019;</ref><ref type="bibr" target="#b5">Ci et al. 2019;</ref><ref type="bibr" target="#b2">Cai et al. 2019;</ref><ref type="bibr" target="#b25">Pavllo et al. 2019)</ref>. Since these approaches take 2D joint locations as input, 3D human pose estimation simply focuses on learning depth of each joint. This releases learning difficulty and leads to better 3D pose. In this work, we use advanced 3D pose estimators as guidance to calibrate human mesh in the HMR-based reconstruction approaches. This would disentangle human mesh recovery respectively into shape and pose modeling, which effectively alleviates pose ambiguity in the generated mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce HMR and explain how to build up our PC-HMR frameworks. Then, we design a pose calibration module in our frameworks, which uses pose transformation as guidance to correct HMR mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Human Mesh Recovery</head><p>HMR <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref>) is a widely-used deep learning framework to generate 3D human mesh from images. First, it uses CNN to estimate 3D mesh parameters from an input image of human, i.e., ? = (?, ?, R, t, s) = CNN(Img), where ? refers to human body shape, ? refers to relative 3D rotation of K joints, and (R, t, s) represent camera parameters. Second, it feeds (?, ?) into the well-known SMPL <ref type="bibr" target="#b17">(Loper et al. 2015)</ref> to generate a triangulated mesh M hmr with N vertices,</p><formula xml:id="formula_0">M hmr = [V hmr 1 , V hmr 2 , ..., V hmr N ].<label>(1)</label></formula><p>Third, it uses a linear mesh-to-pose projector to produce 3D pose J hmr ,</p><formula xml:id="formula_1">J hmr = UM hmr ,<label>(2)</label></formula><p>where U is a linear projection matrix. Finally, it applies (R, t, s) within a weak-perspective camera model, which is a 3D-to-2D pose projector to obtain 2D pose Z hmr ,</p><formula xml:id="formula_2">Z hmr = s?(RJ hmr ) + t,<label>(3)</label></formula><p>where ? represents the orthographic projection. By adding supervision on (?, M hmr , J hmr , Z hmr ), HMR can be efficiently trained in an end-to-end manner. One can refer <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref>) for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our PC-HMR Frameworks</head><p>Directly regressing mesh parameters ? would introduce interference of shape modeling when learning 3D pose. Hence, HMR-based approaches are often limited when capturing complex pose variations. To tackle this problem, we design two pose calibration frameworks, i.e., serial and parallel PC-HMR. With guidance of target (or reference) 3D pose J target from pose estimators, serial / parallel PC-HMR can effectively calibrate HMR mesh M hmr by mesh-pose autoencoding / twostream. Serial PC-HMR. As shown in <ref type="figure">Fig. 2</ref> (a), we first adapt HMR to be a multi-level autoencoding framework for calibration. (I) 3D Pose AutoEncoding. We use it to generate target 3D pose J target . Specifically, we notice that successful 3D pose estimators often consist of 2D pose estimator and 2D-to-3D lifter in the literature <ref type="bibr" target="#b18">(Martinez et al. 2017;</ref><ref type="bibr" target="#b25">Pavllo et al. 2019)</ref>. Inspired by this fact, we propose to use the HMR framework as a 2D pose estimator, and introduce 2D-to-3D pose lifter on top of 3D-to-2D pose projector in Eq. (3). In this case, pose projector becomes a pose encoder that encodes HMR 3D pose as HMR 2D pose (J hmr ? <ref type="figure">Figure 2</ref>: PC-HMR Frameworks. Our serial and parallel frameworks provide two generic manners to calibrate HMR mesh with explicit guidance of 3D pose. More details can be found in Section 3.2.</p><p>Z hmr ), and pose lifter becomes a pose decoder that decodes HMR 2D pose as target 3D pose (Z hmr ? J target ),</p><formula xml:id="formula_3">J target = P oseLif ter(Z hmr ).<label>(4)</label></formula><p>It is worth mentioning that, target 3D pose J target tends to be better than HMR 3D pose J hmr via our autoencoding manner. The main reason is that, J hmr is generated from a simple linear projection of HMR mesh M hmr (Eq. 2), which has limitation to capture pose variations as mentioned before. On the contrary, J target is generated from an advanced 2D-to-3D pose lifter, which can effectively leverage datadriven deep learning to adjust 2D locations of Z hmr and estimate depth of each joint. In our experiments, we investigate several well-known pose lifters to show its effectiveness. (II) 3D Mesh AutoEncoding. After obtaining target 3D pose J target , we design a pose calibration module to correct HMR mesh M hmr as calibrated mesh M target . In particular, we add this calibration module on top of 2D-to-3D pose lifter, leading to a 3D mesh autoencoder. First, we use Mesh-to-Pose projector (Eq. 2) as a mesh encoder, which encodes HMR 3D mesh into HMR 3D pose (M hmr ? J hmr ). Then, we use 3D pose autoencoder to map HMR 3D pose into target 3D pose (J hmr ? J target ). Finally, we use the calibration module as a mesh decoder, which deforms HMR mesh as calibrated mesh (M hmr ? M target ),</p><formula xml:id="formula_4">M target = Calibration(M hmr |J hmr , J target ).<label>(5)</label></formula><p>Note that, our calibration function uses (J hmr , J target ) as condition. This is mainly because, this function leverages non-rigid pose transformation between J hmr and J target as effective guidance, to deform HMR mesh as calibrated mesh. We will further explain this module in Section 3.3.</p><p>(III) Training Serial PC-HMR. The output of each encoder and decoder in our serial PC-HMR has its physical meaning such as 3D mesh, 3D pose or 2D pose. This makes our training procedure become convenient and flexible, i.e., we can train each module of our serial PC-HMR separately, and then fine-tune the entire framework. In our experiments, we first pretrain HMR (including mesh-to-pose and 3D-to-2D projectors) and 2D-to-3D pose lifter separately. Then, we fine-tune the entire framework, by adding 3D pose supervision on pose lifter and 3D mesh supervision on pose calibration module. As a result, our serial PC-HMR can effectively calibrate HMR mesh by 3D pose refinement.</p><p>Parallel PC-HMR. To obtain better target 3D pose for calibration, we adapt HMR as a twostream framework in <ref type="figure">Fig. 2 (b)</ref>. (I) 3D Pose Stream. Instead of pose refinement in serial PC-HMR, we directly use advanced 3D pose estimator as extra stream to generate target 3D pose J target , e.g., we first apply 2D pose estimator to obtain 2D pose from the input image (Img ? Z target ), and then use 2Dto-3D pose lifter to estimate target 3D pose from 2D pose (Z target ? J target ). Note that, 2D pose Z target is better than Z hmr in serial PC-HMR. The main reason is that, Z hmr is indirectly generated through HMR, Mesh-to-Pose projector, 3D-to-2D projector with complex interference of shape modeling. On the contrary, Z target is directly generated from advanced 2D pose estimator, which is able to predict 2D joint locations accurately from an input image. Subsequently, by using a better 2D pose Z target , target 3D pose J target from this extra stream tends to be more reliable than the one (Eq. 4) from serial PC-HMR. (II) 3D Mesh Stream. After obtaining target 3D pose, we introduce a 3D mesh stream, where we first obtain human mesh from HMR, and then correct it via pose calibration module (Eq. 5). (III) Training Parallel PC-HMR. Similar to serial PC-HMR, on the corresponding bone of target 3D pose. Note that, the bone length of target 3D pose can be different from that of HMR 3D pose. Our non-rigid transformation can effectively tackle this problem by bone extension or shortening (i.e., W?).</p><p>parallel PC-HMR can be trained in a flexible manner. In our experiments, we first pretrain mesh stream (HMR+Mesh-to-Pose Projector) and pose stream separately, and then finetune the entire framework by adding 3D pose supervision on pose lifter and 3D mesh supervision on calibration module.</p><p>Discussions. We discuss the proposed PC-HMR frameworks from the following aspects. (I) Extension on Video-Based Mesh Recovery. Our PC-HMR frameworks provide two generic manners to reconstruct human mesh with explicit guidance of 3D pose. Hence, they can be straightforwardly extended for video-based mesh reconstruction, by integrating video-based mesh generators and pose estimators in our plug-and-play fashion. In our experiments, we will show flexibility and robustness of our PC-HMR frameworks for both image/video-based mesh recovery. (II) Accuracy-Efficiency Tradeoffs. Our PC-HMR frameworks take tradeoffs between reconstruction error and computation cost into account, i.e., parallel framework achieves a smaller reconstruction error with extra 3D pose estimation stream, while serial framework maintains a lighter computation cost by 3D pose refinement with 2D-to-3D pose lifter. In practice, both can effectively calibrate HMR mesh. We can choose either of them, depending on accuracy-efficiency balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Calibration Module</head><p>In this section, we further explain pose calibration module (Eq. 5) in PC-HMR frameworks. To leverage target 3D pose J target as guidance, we propose to establish transformation between J target and HMR 3D pose J hmr . Then, we use this pose transformation as reference to deform HMR mesh M hmr into our calibrated mesh M target . Note that, since J hmr and J target may not share the same bone lengths, our transformation is designed to be non-rigid to alleviate misplacement in the calibrated mesh. Moreover, our module is based on geometrical transformation with learnable parameters. Hence, it can take advantages of both physical and data-driven learning to calibrate HMR mesh effectively.</p><p>Non-Rigid Pose Transformation. Without loss of generality, we mainly describe how to make non-rigid transformation on one bone. Specifically, for one bone in the HMR 3D pose J hmr , we denote (J hmr p , J hmr c , J hmr b ) respectively as the parent joint of this bone, the child joint of this bone, and an arbitrary point on this bone between parent and child joints. For the corresponding bone in the target 3D pose J target , we use the similar notations such as</p><formula xml:id="formula_5">(J target p , J target c , J target b</formula><p>). The parent and child joints in the bone are defined according to human skeleton, i.e., the top joint in the bone is parent, and the bottom joint in the bone is child. Note that, the parent and child joints are given in both HMR and target 3D poses. Hence, our goal is to build up non-rigid transformation of any point between ). The whole transformation process is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. (I) Rotation ?. We first compute the rotation matrix ?, in order to rotate the bone in HMR pose along the direction of the corresponding bone in target pose. Specifically, based on Lie algebra, we can compute the rotation vector ? between bone directions b hmr and b target ,</p><formula xml:id="formula_6">? = arccos b hmr b target b hmr b target b hmr ? b target b hmr ? b target , (6) where b hmr = J hmr p ? J hmr c , b target = J target p ? J target c</formula><p>and ? is cross product. Then, we use Rodrigues rotation formula <ref type="bibr" target="#b14">(Koks 2006)</ref> to transform rotation vector ? into rotation matrix ?,</p><formula xml:id="formula_7">? = cos ? I + (1 ? cos ? )?? T + sin ? ? ,<label>(7)</label></formula><p>where ? = ? ? is the unit vector of ?, ? T is transpose of</p><formula xml:id="formula_8">?, and ? = 0 ?? z ? y ? z 0 ?? x ?? y ? x 0</formula><p>is the cross product matrix of ?.</p><p>(II) Translation T. After joint rotation, we need to align the rotated joint of HMR bone with the corresponding joint of target bone. Specifically, we use parent joint as reference, and compute the translation vector T for alignment,</p><formula xml:id="formula_9">T = J target p ? ?J hmr p .<label>(8)</label></formula><p>(III) Non-Rigid Term ?. Given rotation and translation, we next transform the child joint of HMR bone J hmr c into the space of target bone,</p><formula xml:id="formula_10">J target c = ?J hmr c + T.<label>(9)</label></formula><p>However, there may be gap betweenJ target c and the given child joint of target pose J target c , due to bone length variations between HMR and target poses. To fill up this gap, we propose a non-rigid term on the child joint,</p><formula xml:id="formula_11">? = J target c ?J target c .<label>(10)</label></formula><p>As a result, for an arbitrary point on the bone of HMR pose J hmr b , we can find its corresponding point on the target pose J target b , according to a non-rigid transformation as follows,</p><formula xml:id="formula_12">J target b = ?J hmr b + T + W?.<label>(11)</label></formula><p>As shown in Eq. <ref type="formula" target="#formula_0">(11)</ref>  by proportionally adjusting non-rigid term W? with a learnable parameter matrix W. In this case, our non-rigid transformation can extend or shortenJ target b along the target bone, in order to make correct alignment between HMR and target 3D poses.</p><p>Mesh Calibration. After obtaining pose transformation in Eq. (11), we use it as an distinct guidance to calibrate HMR mesh. Specifically, we assume that each vertex on the mesh follows the similar transformation in Eq. (11). In this case, we can transform the j-th vertex V hmr j in the HMR mesh M hmr as</p><formula xml:id="formula_13">V target j,i = ? (i) V hmr j + T (i) + W j,i ? (i) ,<label>(12)</label></formula><p>where V target j,i is the prediction of the corresponding vertex in the target mesh, according to pose transformation of the i-th bone (? (i) , T (i) , ? (i) ) . Subsequently, we summarize the contribution of all the bones by a learnable weight matrix A, and produce the final prediction of the j-th vertex in the target mesh M target ,</p><formula xml:id="formula_14">V target j = i A j,i V target j,i .<label>(13)</label></formula><p>We can see in Eq. (12)-(13) that, V target j is generated by non-rigid pose transformation with learnable parameters W j,i and A j,i . Hence, our calibration module can integrate geometrical modeling and data-driven learning together to effectively calibrate HMR mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets and Implementation Details. To evaluate our PC-HMR frameworks, we investigate extensive experiments on three popular benchmarks, i.e., Human3.6M <ref type="bibr" target="#b10">(Ionescu et al. 2014</ref><ref type="bibr">), 3DPW (von Marcard et al. 2018</ref>) and SURREAL <ref type="bibr" target="#b33">(Varol et al. 2017)</ref>. Specifically, Human3.6M is a popular motion capture dataset. We use 5 subjects (S1, S5, S6, S7 and S8) for training and 2 subjects (S9 and S11) for testing. 3DPW is an in-the-wild dataset with multiple actors occurred in the same image. We use its official data split for Human3.6M MPJPE? PA-MPJPE? MPVE? Self-mocap <ref type="bibr" target="#b31">(Tung et al. 2017)</ref> 98.4 -145.8 HMR <ref type="bibr" target="#b11">(Kanazawa et al. 2018)</ref> 88.0 56.8 96.1 HMMR  85.2 56.7 94.2 TemporalContext <ref type="bibr" target="#b0">(Arnab, Doersch, and Zisserman 2019)</ref> 77.8 54.3 -GraphCMR  -50.1 -VIBE <ref type="bibr" target="#b13">(Kocabas, Athanasiou, and Black 2020)</ref> 65.9 41.5 -Pose2Mesh <ref type="bibr" target="#b4">(Choi, Moon, and Lee 2020)</ref> 64.9 47.0 -HoloPose <ref type="bibr" target="#b7">(Guler and Kokkinos 2019)</ref> 60.3 46.5 -HKMR <ref type="bibr" target="#b6">(Georgakis et al. 2020)</ref> 59.6 --DSD-SATN <ref type="bibr" target="#b30">(Sun et al. 2019b)</ref> 59.1 42.4 -I2L-MeshNet  55.7 41.7 -DaNet <ref type="bibr" target="#b38">(Zhang et al. 2019a)</ref> 54.6 42.9 66.5 Occluded <ref type="bibr" target="#b40">(Zhang, Huang, and Wang 2020</ref>   <ref type="bibr" target="#b11">(Kanazawa et al. 2018;</ref><ref type="bibr" target="#b30">Sun et al. 2019b;</ref><ref type="bibr" target="#b26">Rong et al. 2019;</ref><ref type="bibr" target="#b42">Zimmermann and Brox 2017)</ref>, we mainly use three protocols to measure accuracy of our generated mesh, i.e., Mean Per Joint Position Error (MPJPE), Procrustes Aligned MPJPE (PA-MPJPE), Mean Per Vertex Error (MPVE). We implement our PC-HMR frameworks as follows. First, we choose HMR <ref type="bibr" target="#b11">(Kanazawa et al. 2018</ref>) as our basic architecture. Second, for Human3.6m and SURREAL, we use CPN <ref type="bibr" target="#b8">(Hong et al. 2018</ref>) as 2D pose estimator, and VideoPose3D <ref type="bibr" target="#b25">(Pavllo et al. 2019)</ref> as 2D-to-3D pose lifter in our framework. For 3DPW, we use PoseNet (Moon, Chang, and Lee 2019) as 3D pose estimator in our framework. Finally, we train all the modules separately, according to their official codes with default hyperparameter and supervision settings. Then, we fine-tune the entire framework using 3D mesh as supervision, where we set 90/50 training epochs with 1024/128 mini-batch size for Human3.6M and 3DPW. We set the learning rate as 0.001 for our calibration module and 1 ? 10 ?5 for other modules. We implement our frameworks by PyTorch. All the codes and models will be released afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SOTA Comparison</head><p>In   <ref type="table">Table 3</ref>: Detailed designs of our PC-HMR (Human3.6M). <ref type="bibr" target="#b30">(Sun et al. 2019b;</ref><ref type="bibr" target="#b4">Choi, Moon, and Lee 2020)</ref> that also leverage pose estimators for reconstruction. This shows our PC-HMR framework is a more effective manner to boost mesh recovery by human pose. Additionally, for most metrics and benchmarks, our PC-HMR outperforms SPIN ) that plugs iterative SMPLify optimizer into HMR for further calibrating mesh parameters. It shows our pose calibration is a more preferable calibration design than traditional optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Generalization Capacity. We mainly use Human3.6M to further investigate properties of our PC-HMR frameworks.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we examine generalization capacity of our frameworks, via changing different mesh generators, 2D-to-3D pose lifters and 2D pose estimators. First, both serial and parallel frameworks outperform baselines, no matter which types of mesh generators, pose lifters and estimators we use. It shows the effectiveness of our pose calibration. Second, our proposed frameworks take tradeoffs between recovery error and computation cost, i.e., Serial PC-HMR is lighter while Parallel PC-HMR is more accurate. This provides more reasonable choices in practice, depending on which factor is important in the deployment. Third, both frameworks can be straightforwardly used for video-based mesh reconstruction without any difficulties. For example, we use video-based HMMR ) as mesh generator, or use video-based VideoPose3D <ref type="bibr" target="#b25">(Pavllo et al. 2019)</ref> as 2D-to-3D pose lifter in our frameworks. We can see that, video-based frameworks outperform image-based frameworks, no matter which styles we use (serial or parallel). It shows the importance of using temporal information. Moreover, VideoPose3D <ref type="bibr" target="#b25">(Pavllo et al. 2019</ref>) is more critical than HMMR  for temporal modeling, e.g., when we choose VideoPose3D as pose lifter, the performance would be comparable no matter which mesh generators we use (HMR or HMMR). It indicates that, it is more effective to introduce temporal modeling for 3D human pose, without complex interference of learning mesh shape. Finally, we use 2D pose in the official code of Video-Pose3D <ref type="bibr" target="#b25">(Pavllo et al. 2019)</ref>, where 2D pose estimator refers to CPN <ref type="bibr" target="#b8">(Hong et al. 2018</ref>). Hence, CPN <ref type="bibr" target="#b8">(Hong et al. 2018)</ref> and VideoPose3D <ref type="bibr" target="#b25">(Pavllo et al. 2019</ref>) are more compatible to be a 3D pose estimator, which leads to the best accuracy in our parallel framework. For other parallel cases, the better 2D pose estimator (e.g. HRNet) achieves a better performance of mesh recovery as expected. Detailed Designs. Since MPVE directly reflects recovery error of mesh surface, we use it to evaluate the detailed designs. Additionally, we choose the parallel framework for this study, due to its higher accuracy. (I) w/o Non-Rigid. For comparison, we delete the non-rigid term W j,i ? (i) of Eq. (12) in our pose calibration module. As shown in <ref type="table">Table 3</ref>, the non-rigid one achieves a smaller mesh error, showing its effectiveness. (II) w/o Fine-Tune. In our design, we fine-tune the entire framework after training each module separately. We delete this fine-tuning procedure for comparison. As expected, the setting of fine-tuning is better, since our pose calibration module becomes compatible for mesh recovery via end-to-end learning. Additionally, our pose calibration module can still boost HMR, even without fine-tuning. It mainly thanks to geometry modeling in this module. (III) PC-Template. In our parallel framework, we replace the HMR mesh stream by a template mesh, and operate pose calibration module to correct this template mesh to be target mesh of the input image. In <ref type="table">Table 3</ref>, our PC-HMR outperforms PC-Template. It shows that HMR provides more preferable shape in the mesh, and thus it is reasonable to integrate our pose calibration module in HMR. (IV) GT 3D Pose. We use GT 3D pose for comparison. As expected, GT 3D pose can achieve a better performance. But still, our PC-HMR with estimated 3D pose significantly outperforms HMR. (V) Extra Self-Rotation. There may exist a relative rotation around the bone itself. Hence, we use a two-layer  MLP to estimate it, where we take mesh parameters ? in HMR and ? in our pose calibration module as input. Then we multiply the self rotation matrix with Eq. <ref type="formula" target="#formula_7">(7)</ref> as final rotation matrix in our calibration module. In <ref type="table">Table 3</ref>, the results are comparable between our default and extra self-rotation setting. For simplicity, we use our default setting in the experiments. (VI) w/o Shape Compensation. To reduce detailed shape error, we use a two-layer MLP to estimate 3D rotation of keypoints in the calibrated mesh, with input of mesh parameters ? in HMR and ? in our pose calibration module. Then we feed the estimated 3D rotation into pose blend shape function of SMPL as post-processing compensation in our framework. In <ref type="table">Table 3</ref>, the w/o shape compensation setting is slightly worse. Hence, we use our default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>Mesh Visualization. We visualize HMR (baseline) and our PC-HMR (parallel) in <ref type="figure">Fig. 5</ref>. As expected, our PC-HMR can generate 3D mesh with more reliable pose, even for occluded (e.g., Human3.6M) or in-the-wild (e.g., 3DPW) sce-narios. It indicates the effectiveness of our model. Weight Visualization. We visualize the trained weights of our pose calibration module, i.e., A in Eq. (13), W in Eq. (12) and A ? W. The parallel framework is used as illustration in <ref type="figure">Fig. 6</ref>. First, A(:, i) controls the importance of bone i when obtaining each final vertex in the calibrated mesh. Hence, bone i has more contribution on the vertices around it. Second, according to Eq. (12)-(13), A(:, i) ? W(:, i) controls the proportion of non-rigid term ? (i) in bone i. Via further learning W(:, i), we can see that A(:, i) ? W(:, i) is gradually highlighted around the child joint of each bone, in order to proportionally adjust bone length in the calibrated mesh. Both facts allow us to calibrate HMR mesh smoothly and reasonably with non-rigid transformation.</p><p>Calibration Module. We further show non-rigid transformation in our pose calibration module. We use parallel framework as illustration in <ref type="figure">Fig. 7</ref>. As expected, the generated mesh with our non-rigid transformation is much more natural, without significant misplacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we design two generic plug-and-play PC-HMR frameworks to calibrate human mesh with explicit guidance of 3D pose. They leverage a non-rigid pose calibration module to couple HMR mesh generators and 3D pose estimators in the serial or parallel manner, so that they can be flexibly applied for image/video-based mesh recovery, and have no requirement of 3D pose annotations in the testing. The extensive experiments on popular benchmarks show our frameworks significantly boost recovery performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Non-Rigid Pose Transformation. For an arbitrary point J hmr b on a bone of HMR 3D pose, we design a nonrigid transformation to find its corresponding point J target b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we first transform J hmr b into the space of target pose, i.e.,J target b = ?J hmr b + T. Then, we further</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Mesh calibration with guidance of non-rigid 3D pose transformation. For the j-th vertex V hmr j in the HMR mesh, we first transform it as V target j,i in Eq. (12), according to pose transformation of the i-th bone (? (i) , T (i) , ? (i) ). Then, we summarize the contribution of all the bones by a learnable weight matrix A, and produce the final prediction of the j-th vertex in the target mesh by Eq. (13). improveJ target b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Human Mesh Visualization. The left/right columns are respectively from Human3.6M/3DPW. Weight Visualization. For bone i, we visualize the trained A(:, i), W(:, i), and A(:, i) ? W(:, i) on the mesh, where the grey color indicates weights are close to zero, while the bright color means weights are close to one. Non-Rigid vs. Rigid Transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: SOTA comparison for human mesh reconstruction</cell></row><row><cell>on Human3.6M, 3DPW and SURREAL datasets. For most</cell></row><row><cell>metrics and benchmarks, our parallel PC-HMR framework</cell></row><row><cell>achieves the SOTA performance, e.g., for Human3.6M, it</cell></row><row><cell>outperforms (Sun et al. 2019b; Choi, Moon, and Lee 2020)</cell></row><row><cell>that also leverage pose estimators for mesh reconstruction.</cell></row><row><cell>This shows our PC-HMR framework is a more effective</cell></row><row><cell>manner to boost mesh recovery by human pose.</cell></row><row><cell>training and testing. SURREAL is a large-scale synthetic</cell></row><row><cell>dataset with SMPL body annotations. We directly evaluate</cell></row><row><cell>its test set by our model pretrained on Human3.6M to show</cell></row><row><cell>generalization capacity. Moreover, as suggested in the liter-</cell></row><row><cell>ature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>, our parallel PC-HMR framework achieves the SOTA performance, e.g., for Human3.6M, it outperforms<ref type="bibr" target="#b11">Kanazawa et al. 2018</ref>))<ref type="bibr" target="#b41">(Zhao et al. 2019</ref>)<ref type="bibr" target="#b25">(Pavllo et al. 2019</ref>)<ref type="bibr" target="#b8">(Hong et al. 2018</ref>) </figDesc><table><row><cell></cell><cell>Mesh Generator</cell><cell></cell><cell cols="2">2D-to-3D Pose Lifter</cell><cell cols="2">2D Pose Estimator</cell><cell></cell><cell></cell></row><row><cell>Designs</cell><cell>HMR</cell><cell>HMMR</cell><cell>SemanticGCN</cell><cell>VideoPose3D</cell><cell>CPN</cell><cell>HRNet</cell><cell cols="2">MPVE? GFLOPs?</cell></row><row><cell>(Baseline</cell><cell>-</cell><cell>-</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>96.1 94.2</cell><cell>3.5 3.6</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.9</cell><cell>3.5</cell></row><row><cell>Serial</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>--</cell><cell>--</cell><cell>82.7 90.3</cell><cell>3.5 3.6</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>82.7</cell><cell>3.6</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>79.0</cell><cell>16.1</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>77.3</cell><cell>19.5</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>61.1</cell><cell>16.1</cell></row><row><cell>Parallel</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.6 77.1</cell><cell>19.5 16.2</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>76.2</cell><cell>19.6</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>61.1</cell><cell>16.2</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>68.4</cell><cell>19.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="9">: Generalization capacity (Human3.6M). Our frameworks can be straightforwardly used for video-based mesh recon-</cell></row><row><cell cols="9">struction without any difficulties, e.g., we apply video-based HMMR (Kanazawa et al. 2019) as mesh generator, or apply</cell></row><row><cell cols="9">video-based VideoPose3D (Pavllo et al. 2019) as 2D-to-3D pose lifter. More explanations can be found in Section 4.2.</cell></row><row><cell cols="9">Method HMR Our PC-HMR w/o Non-Rigid w/o Fine-Tune PC-Template GT 3D Pose Extra Self-Rotation w/o Shape Compensation</cell></row><row><cell>MPVE</cell><cell>96.1</cell><cell>61.1</cell><cell>69.3</cell><cell>85.5</cell><cell>125.72</cell><cell>29.9</cell><cell>61.0</cell><cell>61.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing Network Structure for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical Kinematic Human Mesh Recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cascaded Pyramid Network for 3D Human Pose Estimation Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01616</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ARCH: Animatable Reconstruction of Clothed Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE T-PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end Recovery of Human Shape and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning 3D Human Dynamics from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VIBE: Video Inference for Human Body Pose and Shape Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explorations in mathematical physics: the concepts behind an elegant language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Modelfitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SMPL: A Skinned Multi-Person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ordinal Depth Supervision for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Integral Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human Mesh Recovery from Monocular Images via a Skeletondisentangled Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised Learning of Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric Inference of 3D Human Body Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from Synthetic Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric Pose Affordance: 3D Human Pose with Scene Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arxiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3D Human Mesh Regression with Dense Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DaNet: Decompose-and-aggregate Network for 3D Human Shape and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting 3D Human Dynamics from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object-Occluded Human Shape and Pose Estimation From a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Semantic Graph Convolutional Networks for 3D Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Hand Pose from Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
