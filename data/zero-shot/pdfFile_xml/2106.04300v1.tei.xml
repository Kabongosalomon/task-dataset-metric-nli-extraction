<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Generative Framework for Aspect-Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Generative Framework for Aspect-Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an endto-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of seven ABSA subtasks.</p><p>? Aspect Term Extraction(AE): Extracting all the aspect terms from a sentence.</p><p>? Opinion Term Extraction (OE): Extracting all the opinion terms from a sentence.</p><p>? Aspect-level Sentiment Classification (ALSC): Predicting the sentiment polarities for every given aspect terms in a sentence.</p><p>? Aspect-oriented Opinion Extraction (AOE): Extracting the paired opinion terms for every given aspect terms in a sentence.</p><p>? Aspect Term Extraction and Sentiment Classification (AESC): Extracting the aspect terms as well as the corresponding sentiment polarities simultaneously.</p><p>? Pair Extraction (Pair): Extracting the aspect terms as well as the corresponding opinion terms simultaneously.</p><p>? Triplet Extraction (Triplet): Extracting all aspects terms with their corresponding opinion terms and sentiment polarity simultaneously.</p><p>Although these ABSA subtasks are strongly related, most of the existing work only focus 1?3 subtasks individually. The following divergences make it difficult to solve all subtasks in a unified framework.</p><p>and Triplet) only take the text sentence as input, while the remained subtasks ( ALSC and AOE) take the text and a given aspect term as input. 2. Output: Some tasks (AE, OE, ALSC, <ref type="bibr">AOE)</ref> only output a certain type from a, s or o, while the remained tasks (AESC, Pair and Triplet) return compound output as the combination of a, s and o. 3. Task Type: There are two kinds of tasks: extraction task (extracting aspect and opinion) and classification task (predicting sentiment).</p><p>Because of the above divergences, a myriad of previous works only focus on the subset of these subtasks. However, the importance of solving the whole ABSA subtasks in a unified framework remains significant. Recently, several works make attempts on this track. Some methods <ref type="bibr">(Peng et al., 2020;</ref><ref type="bibr">Mao et al., 2021</ref>) apply the pipeline model to output the a, s, o from the inside sub-models separately. However, the pipeline process is not end-to-end. Another line follows the sequence tagging method by extending the tagging schema . However, the compositionality of candidate labels hinders the performance. In conclusion, the existing methods can hardly solve all the subtasks by a unified framework without relying on the sub-models or changing the model structure to adapt to all ABSA subtasks.</p><p>Motivated by the above observations, we propose a unified generative framework to address all the ABSA subtasks. We first formulate all these subtasks as a generative task, which could handle the obstacles on the input, output, and task type sides and adapt to all the subtasks without any model structure changes. Specifically, we model the extraction and classification tasks as the pointer indexes and class indexes generation, respectively. Based on the unified task formulation, we use the sequence-to-sequence pre-trained model <ref type="bibr">BART (Lewis et al., 2020)</ref> as our backbone to generate the target sequence in an end-to-end process. To validate the effectiveness of our method, we conduct extensive experiments on public datasets. The comparison results demonstrate that our proposed framework outperforms most state-of-the-art (SOTA) models in every subtask.</p><p>In summary, our main contributions are as follows:</p><p>? We formulate both the extraction task and classification task of ABSA into a unified index gen-eration problem. Unlike previous unified models, our method needs not to design specific decoders for different output types.</p><p>? With our re-formulation, all ABSA subtasks can be solved in sequence-to-sequence framework, which is easy-to-implement and can be built on the pre-trained models, such as BART.</p><p>? We conduct extensive experiments on four public datasets, and each dataset contains a subset of all ABSA subtasks. To the best of our knowledge, it is the first work to evaluate a model on all ABSA tasks.</p><p>? The experimental results show that our proposed framework significantly outperforms recent SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ABSA Subtasks</head><p>In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Single Output Subtasks</head><p>Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o.</p><p>AE Most studies treat AE subtask as a sequence tagging problem <ref type="bibr">(Li and Lam, 2017;</ref><ref type="bibr" target="#b29">Xu et al., 2018;</ref><ref type="bibr">Li et al., 2018b)</ref>. Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models <ref type="bibr">(Ma et al., 2019;</ref>.</p><p>OE Most studies treat OE subtask as an auxiliary task <ref type="bibr" target="#b25">(Wang et al., 2016a</ref><ref type="bibr" target="#b26">(Wang et al., , 2017</ref><ref type="bibr" target="#b24">Wang and Pan, 2018;</ref><ref type="bibr" target="#b3">Chen and Qian, 2020;</ref><ref type="bibr" target="#b8">He et al., 2019)</ref>. Most works can only extract the unpaired aspect and opinion terms 2 . In this case, opinion terms are independent of aspect terms. <ref type="bibr">ALSC Tang et al. (2016a)</ref> use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. <ref type="bibr" target="#b27">Wang et al. (2016b)</ref>; Liu and <ref type="bibr">Zhang (2017);</ref><ref type="bibr">Ma et al. (2017)</ref>; <ref type="bibr" target="#b21">Tay et al. (2018)</ref> incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) <ref type="bibr">(Li et al., 2018a;</ref><ref type="bibr" target="#b31">Xue and Li, 2018)</ref>, gated neural network <ref type="bibr" target="#b33">(Zhang et al., 2016;</ref><ref type="bibr" target="#b31">Xue and Li, 2018)</ref>, memory neural network <ref type="bibr" target="#b20">(Tang et al., 2016b;</ref><ref type="bibr" target="#b2">Chen et al., 2017)</ref> have also been applied.</p><p>AOE This subtask is first introduced by <ref type="bibr" target="#b7">Fan et al. (2019)</ref> and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask <ref type="bibr" target="#b28">(Wu et al., 2020;</ref><ref type="bibr" target="#b13">Pouran Ben Veyseh et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Compound Output Subtasks</head><p>Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows:</p><p>AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema <ref type="bibr">(Mitchell et al., 2013;</ref><ref type="bibr" target="#b32">Zhang et al., 2015;</ref><ref type="bibr" target="#b9">Li et al., 2019)</ref> or multi-task learning <ref type="bibr" target="#b8">(He et al., 2019;</ref><ref type="bibr" target="#b3">Chen and Qian, 2020)</ref> to avoid the error-propagation problem <ref type="bibr">(Ma et al., 2018)</ref>. Spanbased AESC works are also proposed recently <ref type="bibr" target="#b9">(Hu et al., 2019)</ref>, which can tackle the sentiment inconsistency problem in the unified tagging schema.</p><p>Pairs  propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask.</p><p>Triplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently.  design the position-aware tagging schema and apply model based on <ref type="bibr">CRF (Lafferty et al., 2001)</ref> and Semi-Markov CRF <ref type="bibr" target="#b16">(Sarawagi and Cohen, 2004)</ref>. However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. <ref type="bibr">Mao et al. (2021)</ref> formulate Triplet as a two-step MRC problem, which applies the pipeline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequence-to-Sequence Models</head><p>The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b23">Vinyals et al., 2015;</ref><ref type="bibr">Luong et al., 2015)</ref>. Inspired by the success of PTMs (pre-trained models) <ref type="bibr" target="#b14">(Qiu et al., 2020;</ref><ref type="bibr">Peters et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>, ; <ref type="bibr" target="#b15">Raffel et al. (2020)</ref>; <ref type="bibr">Lewis et al. (2020)</ref> try to pre-train sequence-tosequence models. Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism <ref type="bibr" target="#b23">(Vinyals et al., 2015)</ref>, such as MASS .</p><p>BART is a strong sequence-to-sequence pretrained model for Natural Language Generation (NLG). BART is a denoising autoencoder composed of several transformer <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> encoder and decoder layers. It is worth noting that the BART-Base model contains a 6-layer encoder and 6-layer decoder, which makes it similar number of parameters 3 with the BERT-Base model. BART is pretrained on denoising tasks where the input sentence is noised by some methods, such as masking and permutation. The encoder takes the noised sentence as input, and the decoder will restore the original sentence in an autoregressive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Although there are two types of tasks among the seven ABSA subtasks, they can be formulated under a generative framework. In this part, we first introduce our sequential representation for each ABSA subtask. Then we detail our method, which utilizes BART to generate these sequential representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>As depicted in <ref type="figure">Figure 1</ref>, there are two types of tasks, namely the extraction and classification, whose target can be represented as a sequence of pointer indexes and class indexes, respectively. Therefore, we can formulate these two types of tasks in a unified generative framework. We use a, s, o, to represent the aspect term, sentiment polarity,and opinion term, respectively. Moreover, we use the superscript s and e to denote the start index and end index of a term. For example, o s , a e represent the start index of an opinion term o and the end index of an aspect term a. We use the s p to denote the index of sentiment polarity class. The target sequence for each subtask is as follows:</p><formula xml:id="formula_0">? AE : Y = [a s 1 , a e 1 , ..., a s i , a e i , ...], ? OE : Y = [o s 1 , o e 1 , ..., o s i , o e i , ...], ? AESC : Y = [a s 1 , a e 1 , s p 1 , ..., a s i , a e i , s p i , ...], ? Pair: Y = [a s 1 , a e 1 , o s 1 , o e 1 , ..., a s i , a e i , o s i , o e i ,...], ? Triplet : Y = [a s 1 , a e 1 , o s 1 , o e 1 , s p 1 , ..., a s i , a e i , o s i , o e i , s p i , ...],</formula><p>The above subtasks only rely on the input sentence, while for the ALSC and AOE subtasks, they also depend on a specific aspect term a. Instead of putting the aspect term on the input side, we put where the source is "&lt;s&gt;the battery life is good &lt;/s&gt;" and the target is "2 3 5 5 8 6"(Only partial decoder sequence is shown where the 6 (&lt;/s&gt;) should be the next generation index). The "Index2Token Conversion" converts the index to tokens. Specifically, the pointer index will be converted to its corresponding token in the source text, and the class index will be converted to corresponding class tokens. Embedding vectors in ll boxes are retrieved from same embedding matrix. We use different position embeddings in the source and target for better generation performance.</p><p>The wine list is interesting and has good values , but the service is dreadful Positive Positive  them on the target side so that the target sequences are as follows:</p><formula xml:id="formula_1">? ALSC : Y = [a s , a e , s p ], ? AOE : Y = [a s , a e , o s 1 , o e 1 , ..., o s i , o e i , .</formula><p>..], where the underlined tokens are given during inference. Detailed target sequence examples for each subtask are presented in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Model</head><p>As our discussion in the last section, all subtasks can be formulated as taking the X = [x 1 , ..., x n ] as input and outputting a target sequence Y = [y 1 , ..., y m ], where y 0 is the start-of-the-sentence token. Therefore, different ABSA subtasks can be formulated as:</p><formula xml:id="formula_2">P (Y |X) = m t=1 P (y t |X, Y &lt;t ).</formula><p>(1)</p><p>To get the index probability distribution P t = P (y t |X, Y &lt;t ) for each step, we use a model composed of two components: (1) Encoder; (2) Decoder.</p><p>Encoder The encoder part is to encode X into vectors H e . We use the BART model, therefore, the start of sentence (&lt;s&gt;) and the end of sentence (&lt;/s&gt;) tokens will be added to the start and end of X, respectively. We ignore the &lt;s&gt; token in our equations for simplicity. The encoder part is as follows:</p><formula xml:id="formula_3">H e = BARTEncoder([x 1 , ..., x n ]),<label>(2)</label></formula><p>where H e ? R n?d , and d is the hidden dimension.</p><p>Decoder The decoder part takes the encoder outputs H e and previous decoder outputs Y &lt;t as inputs to get P t . However, the Y &lt;t is an index sequence. Therefore, for each y t in Y &lt;t , we first need to use the following Index2Token module to conduct a  <ref type="table">Table 1</ref>: The statistics of four datasets, where the #s, #a, #o, #p denote the numbers of sentences, aspect terms, opinion terms, and the &lt;a, o&gt; pairs, respectively. We use "-" to denote the missing data statistics of some datasets. The "Subtasks" column refers to the ABSA subtasks that can be applied on the corresponding dataset.</p><p>conversion</p><formula xml:id="formula_4">y t = X yt , if y t is a pointer index, C yt?n , if y t is a class index,<label>(3)</label></formula><p>where C = [c 1 , ..., c l ] is the class token list 4 . After that, we use the BART decoder to get the last hidden state</p><formula xml:id="formula_5">h d t = BARTDecoder(H e ;? &lt;t ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">h d t ? R d . With h d t ,</formula><p>we predict the token probability distribution P t as follows:</p><formula xml:id="formula_7">E e = BARTTokenEmbed(X),<label>(5)</label></formula><formula xml:id="formula_8">H e = MLP(H e ),<label>(6)</label></formula><formula xml:id="formula_9">H e = ?? e + (1 ? ?)E e ,<label>(7)</label></formula><formula xml:id="formula_10">C d = BARTTokenEmbed(C),<label>(8)</label></formula><formula xml:id="formula_11">P t = Softmax([H e ; C d ]h d t ),<label>(9)</label></formula><p>where E e , H e ,? e ,H e ? R n?d ; C d ? R l?d ; and P t ? R (n+l) is the final distribution on all indexes. During the training phase, we use the teacher forcing to train our model and the negative loglikelihood to optimize the model. Moreover, during the inference, we use the beam search to get the target sequence Y in an autoregressive manner. After that, we need to use the decoding algorithm to convert this sequence into the term spans and sentiment polarity. We use the Triplet task as an example and present the decoding algorithm in Algorithm 1, the decoding algorithm for other tasks are much depicted in the Supplementary Material. </p><formula xml:id="formula_12">L = {(a s 1 , a e 1 , o s 1 , o e 1 , s 1 ), ..., (a s i , a e i , o s i , o e i , s i ), ...} 1: L = {}, e = [], i = 1 2: while i &lt;= m do 3: y i = Y [i] 4:</formula><p>if y i &gt; n then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method on four ABSA datasets. All of them are originated from the Semeval Challenges <ref type="bibr">(Pontiki et al., 2014a,b,c)</ref>, where only the aspect terms and their sentiment polarities are labeled.</p><p>The first dataset(D 17 5 ) is annotated by <ref type="bibr" target="#b26">Wang et al. (2017)</ref>, where the unpaire opinion terms are labeled. The second dataset <ref type="bibr">(D 19</ref> ) is annotated by <ref type="bibr" target="#b7">Fan et al. (2019)</ref>, where they pair opinion terms with Baselines E2E Task Formulation Backbone Datasets AE OE ALSC AOE AESC Pair Triplet  <ref type="bibr">(2020)</ref>, where the missing triplets with overlapping opinions are corrected. We present the statistics for these four datasets in <ref type="table">Table 1</ref>.</p><formula xml:id="formula_13">SPAN-BERT - Span.Extraction BERT D17 - - - - IMN-BERT Seq.Tagging BERT D17 - - - RACL-BERT - Seq.Tagging BERT D17 - - - IOG Seq.Tagging LSTM D19 - - - - - - LOTN Seq.Tagging LSTM D19 - - - - - - ONG Seq.Tagging BERT D19 - - - - - - RINANTE+ - Seq.Tagging LSTM+CRF D20a,D 20b - CMLA+ - Seq.Tagging Attention D20a,D 20b - Li-unified+ - Seq.Tagging LSTM D20a,D 20b - Peng-two-stage - Seq.Tagging LSTM+GCN D20a,D 20b - JET-BERT Seq.Tagging BERT D20a,D 20b - Dual-MRC - Span.MRC BERT D17,D19,D20a,D 20b - Ours Span.Generation BART D17,D19,D20a,D 20b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To have a fair comparison, we summarize topperforming baselines of all ABSA subtasks. Given different ABSA subtasks, datasets, and experimental setups, existing baselines can be separated into three groups roughly as shown in <ref type="table" target="#tab_4">Table 2</ref>. The baselines in the first group are conducted on D 17 dataset, covering the AE, OE, ALSC, and AESC subtasks. Span-based method SPAN-BERT <ref type="bibr" target="#b9">(Hu et al., 2019)</ref> and sequence tagging method, IMN-BERT <ref type="bibr" target="#b8">(He et al., 2019)</ref> and RACL-BERT (Chen and Qian, 2020), are selected. Specifically, the IMN-BERT model is reproduced by <ref type="bibr" target="#b3">Chen and Qian (2020)</ref>. All these baselines are implemented on BERT-Large.</p><p>The baselines of the second group are conducted on D 19 dataset, mainly focusing on AOE subtask. Interestingly, we find that sequence tagging method is the main solution for this subtask <ref type="bibr" target="#b7">(Fan et al., 2019;</ref><ref type="bibr" target="#b28">Wu et al., 2020;</ref><ref type="bibr" target="#b13">Pouran Ben Veyseh et al., 2020)</ref>.</p><p>The baselines of the third group are mainly conducted on D 20a and D 20b datasets, which could cover almost all the ABSA subtasks except for one certain subtask depending on the baseline structures. For the following baselines: RINANTE <ref type="bibr" target="#b5">(Dai and Song, 2019)</ref>, CMLA <ref type="bibr" target="#b26">(Wang et al., 2017)</ref>, <ref type="bibr">Liunified (Li et al., 2019)</ref>, the suffix "+" in <ref type="table" target="#tab_4">Table 2</ref> denotes the corresponding model variant modified by <ref type="bibr">Peng et al. (2020)</ref> for being capable of AESC, Pair and Triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implement Details</head><p>Following previous studies, we use different metrics according to different subtasks and datasets. Specifically, for the single output subtasks AE, OE, and AOE, the prediction span would be considered as correct only if it exactly matches the start and the end boundaries. For the ALSC subtask, we require the generated sentiment polarity of the given aspect should be the same as the ground truth. As for compound output subtasks, AESC, Pair and Triplet, a prediction result is correct only when all the span boundaries and the generated sentiment polarity are accurately identified. We report the precision (P), recall (R), and F1 scores for all experiments 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>On D 17 dataset <ref type="bibr" target="#b26">(Wang et al., 2017)</ref>, we compare our method for AE, OE, ALSC, and AESC. The comparison results are shown in <ref type="table" target="#tab_6">Table 3</ref>. Most of our results achieve better or comparable results to      . Baselines are from . We highlight the best results in bold.</p><p>baselines. However, these baselines yield competitive results based on the BERT-Large pre-trained models. While our results are achieved on the BART-Base model with almost half parameters. This shows that our framework is more suitable for these ABSA subtasks.</p><p>On D 19 dataset <ref type="bibr" target="#b7">(Fan et al., 2019)</ref>, we compare our method for AOE. The comparison results are shown in <ref type="table" target="#tab_7">Table 4</ref>. We can observe that our method achieves significant P/R/F1 improvements on 14res, 15res, and 16res. Additionally, we notice that our F1 score on 14lap is close to the previous SOTA result. This is probably caused by the dataset domain difference as the 14lap is the laptop comments while the others are restaurant comments.</p><p>On D 20a dataset (Peng et al., 2020), we compare our method for AESC, Pair, and Triplet. The comparison results are shown in <ref type="table" target="#tab_8">Table 5</ref>. We can observe that our proposed method is able to outperform other baselines on all datasets. Specifically, we achieve the better results for Triplet, which demonstrates the effectiveness of our method on capturing interactions among aspect terms, opinion terms, and sentiment polarities. We also observe that the Span-based methods show superior performance to sequence tagging methods. This may be caused by the higher compositionality of candidate labels in sequence tagging methods <ref type="bibr" target="#b9">(Hu et al., 2019)</ref>. As the previous SOTA method, the Dual-MRC shows competitive performance by utilizing the span-based extraction method and the MRC mechanism. However, their inference process is not an end-to-end process.</p><p>On D 20b dataset , we compare our method for Triplet. The comparison results can be found in <ref type="table" target="#tab_9">Table 6</ref>. Our method achieves the best results with nearly 7 F1 points improvements on 14res, 15res, and 16res. Our method achieves nearly 13, 9, 7, 12 points improvements on each dataset for the recall scores compared with other baselines. This also explains the drop performance of the precision score. Since D 20b is refined from D 20a , we specifically compare the Triplet results of the corresponding dataset in D 20a and D 20b . Interestingly, we discover that all baselines have a much bigger performance change on 15res. We conjecture the distribution differences may be the cause reason. In conclusion, all the experiment results confirm that our proposed method, which unifies the training and the inference to an end-to-end generative framework, provides a new SOTA solution for the whole ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Framework Analysis</head><p>To better understand our proposed framework, we conduct analysis experiments on the D 20b dataset .</p><p>To validate whether our proposed framework could adapt to the generative ABSA task, we metric the invalid predictions for the Triplet. Specifically, since the Triplet requires the prediction for-mat like [a s , a e , o s , o e , s p ], it is mandatory that one valid triplet prediction should be in length 5, noted as "5-len", and obviously all end index should be larger than the corresponding start index, noted as "ordered prediction". We calculate number of non?5?len total prediction , referred to as the "Invalid size", and the number of non?ordered prediction total 5?len prediction , referred to as the "Invalid order". The "Invalid token" means the a s is not the start of a token, instead, it is the index of an inside subword. From <ref type="table" target="#tab_10">Table  7</ref>, we can observe that BART could learn this task form easily as the low rate for all the three metrics, which demonstrate that the generative framework for ABSA is not only a theoretically unified task form but also a realizable framework in practical. We remove these invalid predictions in our implementation of experiments.</p><p>As shown in <ref type="table" target="#tab_7">Table 4</ref>, we give some analysis on the impact of the beam size, as we are a generation method. However, the beam size seems to have little impact on the F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Errors 14res 14lap 15res 16res</head><p>Invalid size 0.48% 0.77% 1.41% 1.40%</p><p>Invalid order 1.75% 3.70% 3.26% 3.26%</p><p>Invalid token 0.48% 0.78% 1.02% 1.02%  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper summarizes the seven ABSA subtasks and previous studies, which shows that there exist divergences on all the input, output, and task type sides. Previous studies have limitations on handling all these divergences in a unified framework. We propose to convert all the ABSA subtasks to a unified generative task. We implement the BART to generate the target sequence in an end-to-end process based on the unified task formulation. We conduct massive experiments on public datasets for seven ABSA subtasks and achieve significant improvements on most datasets. The experimental results demonstrate the effectiveness of our method. Our work leads to several promising directions, such as sequence-to-sequence framework on other tasks, and data augmentation. We use the triangular learning rate warmup. All experiments are conducted in the Nvidia Ge-Force RTX-3090 Graphical Card with 24G graphical memory. The averages running time for experiments on each dataset is less than 15 minutes. The number of parameters is as follows:</p><p>? BART-Base model: 12 layers, 768 hidden dimensions and 16 heads with the total number of parameters, 139M;</p><p>? BERT-Base model: 12 layers, 768 hidden dimensions and 12 heads with the total number of parameters, 110M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Decoding Algorithm for Different Datasets</head><p>In this part, we introduce the decoding algorithm we used to convert the predicted target sequence Y into the target span set L. These algorithm can be found in Algorithm 2, 3, 4.</p><p>Algorithm 2 Decoding Algorithm for the AOE subtask Input: Number of tokens in the input sentence n, target sequence Y = [y 1 , ..., y m ] and y i ? [1, n + |C|], L T is a given length for different tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on each dataset</head><p>As the different subtasks are conducted on different datasets, specifically, we conduct the following experiments on each dataset:</p><p>? On the D 17 dataset, we conduct the AESC and the OE in multi-task learning method. To that end, we feed the pre-defined task tags "&lt;AESC&gt;" and "&lt;OE&gt;" to the decoder first. For example, for the input "The drinks are always :::: well ::::: made and wine selection is ::::: fairly :::::: priced" from D 17 dataset, we e.append(y i ) 9: i+ = 1 10: end while 11: return L define the AESC sequence and the OE target sequence as "&lt;AESC&gt;, 1, 1, POS, 7, 8, POS, &lt;/s&gt;" and "&lt;OE&gt;, 4, 5, 10, 11, &lt;/s&gt;".</p><p>? On the D 19 dataset, we conduct the AOE. As the AOE subtask requires to detect the opinion terms given aspect terms in advance, the aspect terms need to be fed to our decoder first. For the aforementioned example sentence from D 19 dataset, we define the AOE target sequence as " 1, 1, 4, 5, &lt;/s&gt;" and the " 7, 8, 10, 11, &lt;/s&gt;".</p><p>? On the D 20a and D 20b datasets, we conduct the Triplet Extraction. For the aforementioned example sentence from D 20a and D 20b dataset, we define the Triplet target sequence as "1, 1, 4, 5, POS, 7, 8, 10, 11, POS, &lt;/s&gt;".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific Subtask Metrics</head><p>? On the D 17 dataset, we get the AESC and OE results directly. Following previous work, we only calculate the metrics for AESC and ALSC from those true positive AE predictions. Specifically, the F1</p><p>? On the D 19 dataset, we get the AOE results directly. The metrics for AOE are standard Precision, Recall and the F1 score.</p><p>? On the D 20a and D 20b datasets, we get the Triplet results directly. We preserve the &lt;AT,OT&gt; for Pair metric and &lt;AT, SP&gt; for AESC metric. The metrics for them are standard Precision, Recall and the F1 score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of the framework. This shows an example generation process for the Triplet subtask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Target sequences for different subtasks. The underlined indexes are given in advance. We convert the sentiment class index to the corresponding class token for better understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The F1 change curve with the increment of beam size on the dev set of D 20b . The beam size seems to have little effect on the F1 scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Output: Target span set L = {(o s 1 , o e 1 , ..., o s i , o e i )} 1: L = {}, e = [], i = 3 2: while i &lt;= m do 3:y i = Y [i]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3</head><label>3</label><figDesc>Decoding Algorithm for the AESC Subtask Input: Number of tokens in the input sentence n, target sequence Y = [y 1 , ..., y m ] and y i ? [1, n + |C|] 1 ), ..., (a s i , a e i , s i )} 1: L = {}, e = [], i = 1 2: while i &lt;= m do while 12: return L Algorithm 4 Decoding Algorithm for the AE/OE/Pair subtasks Input: Number of tokens in the input sentence n, target sequence Y = [y 1 , ..., y m ] and y i ? [1, n + |C|], L T is a given length for different tasks. Output: Target span set L = {x 1 , ..., x i }(x i is (a s i , a e i ), (o s i , o e i ) and (a s i , a e i , o s i , o e i ) for AE/OE/Pair, respectively) 1: L = {}, e = [], i = 1 2: while i &lt;= m do 3:y i = Y [i] 4:if len(e) == L T then 5:L.add((e, C y i ?n ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The drinks are always well made and wine selection is fairly priced .</figDesc><table><row><cell>S: Positive s 1</cell><cell></cell><cell></cell><cell>s 2</cell><cell>Positive</cell></row><row><cell>a 1</cell><cell>o 1</cell><cell></cell><cell>a 2</cell><cell>o 2</cell></row><row><cell>Subtask</cell><cell></cell><cell>Input</cell><cell cols="2">Output</cell><cell>Task Type</cell></row><row><cell>Aspect Term Extraction(AE)</cell><cell></cell><cell>S</cell><cell>a 1 , a 2</cell><cell>Extraction</cell></row><row><cell cols="2">Opinion Term Extraction(OE)</cell><cell>S</cell><cell>o 1 , o 2</cell><cell>Extraction</cell></row><row><cell cols="2">Aspect-level Sentiment Classification(ALSC)</cell><cell>S + a 1 S + a 2</cell><cell>s 1 s 2</cell><cell>Classification</cell></row><row><cell>Aspect-oriented Opinion Extraction(AOE)</cell><cell></cell><cell>S + a 1 S + a 2</cell><cell>o 1 o 2</cell><cell>Extraction</cell></row><row><cell cols="2">Aspect Term Extraction and Sentiment Classification(AESC)</cell><cell>S</cell><cell cols="2">(a 1 , s 1 ), (a 2 , s 2 )</cell><cell>Extraction &amp; Classification</cell></row><row><cell>Pair Extraction(Pair)</cell><cell></cell><cell>S</cell><cell cols="2">(a 1 , o 1 ), (a 2 , o 2 )</cell><cell>Extraction</cell></row><row><cell>Triplet Extraction(Triplet)</cell><cell></cell><cell>S</cell><cell cols="2">(a 1 , o 1 , s 1 ), (a 2 , o 2 , s 2 )</cell><cell>Extraction &amp; Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 Decoding Algorithm for the Triplet Subtask Input: Number of tokens in the input sentence</figDesc><table /><note>n, target sequence Y = [y 1 , ..., y m ] and y i ? [1, n + |C|] Output: Target span set</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The baselines of our experiments. To further demonstrate that our proposed method is a real unified end- to-end ABSA framework, we present our work in the last row. "E2E" is short for End-to-End, which means the model should output all the subtasks' results synchronously rather than requiring any preconditions, e.g., pipeline methods. The "Datasets" column refers to the datasets that this baseline is conducted.corresponding aspects. The third dataset(D 20a ) is from Peng et al. (2020). They refine the data in &lt;a, o, s&gt; triplet form. The fourth dataset(D 20b ) from Xu et al. (2020) is the revised variant of Peng et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.29 75.56 73.56 83.52 77.86 76.76 67.37 75.48 76.49 73.91 66.61</figDesc><table><row><cell>Model</cell><cell>AE</cell><cell>OE</cell><cell>14res ALSC AESC</cell><cell>AE</cell><cell>OE</cell><cell cols="3">14lap ALSC AESC</cell><cell>AE</cell><cell>OE</cell><cell>15res ALSC AESC</cell></row><row><cell>SPAN-BERT</cell><cell>86.71</cell><cell>-</cell><cell cols="2">71.75 73.68 82.34</cell><cell>-</cell><cell></cell><cell>62.5</cell><cell cols="2">61.25 74.63</cell><cell>-</cell><cell>50.28 62.29</cell></row><row><cell>IMN-BERT</cell><cell cols="4">84.06 85.10 75.67 70.72 77.55</cell><cell cols="2">81.0</cell><cell cols="4">75.56 61.73 69.90 73.29 70.10 60.22</cell></row><row><cell>RACL-BERT</cell><cell cols="9">86.38 87.18 81.61 75.42 81.79 79.72 73.91 63.40 73.99</cell><cell>76.0</cell><cell>74.91 66.05</cell></row><row><cell>Dual-MRC</cell><cell>86.60</cell><cell>-</cell><cell cols="2">82.04 75.95 82.51</cell><cell>-</cell><cell></cell><cell cols="3">75.97 65.94 75.08</cell><cell>-</cell><cell>73.59 65.08</cell></row><row><cell>Ours</cell><cell cols="2">87.07 87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison F1 scores for AE, OE, SC, and AESC on the D 17 dataset<ref type="bibr" target="#b26">(Wang et al., 2017)</ref>. The baseline results are retrieved fromMao et al. (2021). We highlight the best results in bold. It is worth noting that all the baseline results are obtained via BERT-Large, while our results are obtained via BART-Base.78.43 83.73 78.21 81.66 79.90 77.19 71.98 74.50 86.07 80.77 83.33 Ours 86.01 84.76 85.38 83.11 78.13 80.55 80.12 80.93 80.52 89.22 86.67 87.92</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>14res R</cell><cell>F1</cell><cell>P</cell><cell>14lap R</cell><cell>F1</cell><cell>P</cell><cell>15res R</cell><cell>F1</cell><cell>P</cell><cell>16res R</cell><cell>F1</cell></row><row><cell>IOG</cell><cell cols="12">82.38 78.25 80.23 73.43 68.74 70.99 72.19 71.76 71.91 84.36 79.08 81.60</cell></row><row><cell>LOTN</cell><cell>84.0</cell><cell cols="11">80.52 82.21 77.08 67.62 72.02 76.61 70.29 73.29 86.57 80.89 83.62</cell></row><row><cell>ONG</cell><cell cols="12">83.23 81.46 82.33 73.87 77.78 75.77 76.63 81.14 78.81 87.72 84.38 86.01</cell></row><row><cell>Dual-MRC</cell><cell>89.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison results for AOE on the D 19 dataset<ref type="bibr" target="#b7">(Fan et al., 2019)</ref>. Baselines are from the original papers. We highlight the best results in bold. Triple. AESC Pair Triple. AESC Pair Triple. AESC Pair Triple.</figDesc><table><row><cell cols="13">14res AESC Pair CMLA+  ? Model 70.62 48.95 43.12 56.90 44.10 32.90 53.60 44.60 35.90 61.20 50.00 41.60 14lap 15res 16res</cell></row><row><cell>RINANTE+  ?</cell><cell cols="5">48.15 46.29 34.03 36.70 29.70</cell><cell>20.0</cell><cell cols="2">41.30 35.40</cell><cell>28.0</cell><cell cols="3">42.10 30.70 23.30</cell></row><row><cell>Li-unified+  ?</cell><cell cols="12">73.79 55.34 51.68 63.38 52.56 42.47 64.95 56.85 46.69 70.20 53.75 44.51</cell></row><row><cell>Peng-two-stage  ?</cell><cell cols="12">74.19 56.10 51.89 62.34 53.85 43.50 65.79 56.23 46.79 71.73 60.04 53.62</cell></row><row><cell>JET-BERT</cell><cell>-</cell><cell>-</cell><cell>63.92</cell><cell>-</cell><cell>-</cell><cell>50.0</cell><cell>-</cell><cell>-</cell><cell>54.67</cell><cell>-</cell><cell>-</cell><cell>62.98</cell></row><row><cell>Dual-MRC ?</cell><cell cols="12">76.57 74.93 70.32 64.59 63.37 55.58 65.14 64.97 57.21 70.84 75.71 67.40</cell></row><row><cell>Ours</cell><cell cols="12">78.47 77.68 72.46 68.17 66.11 57.59 69.95 67.98 60.11 75.69 77.38 69.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison F1 scores for AESC, Pair and Triplet on the D 20a dataset (Peng et al., 2020). The baseline results with " ?" are retrieved from Mao et al. (2021), and result with " " is from. We highlight the best results in bold.63.66 51.46 37.38 50.38 42.87 48.07 57.51 52.32 46.96 64.24 54.21 JET-BERT 70.56 55.94 62.40 55.39 47.33 51.04 64.45 51.96 57.53 70.42 58.37 63.83 Ours 65.52 64.99 65.25 61.41 56.19 58.69 59.14 59.38 59.26</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>14res R</cell><cell>F1</cell><cell>P</cell><cell>14lap R</cell><cell>F1</cell><cell>P</cell><cell>15res R</cell><cell>F1</cell><cell>P</cell><cell>16res R</cell><cell>F1</cell></row><row><cell>CMLA+</cell><cell cols="10">39.18 47.13 42.79 30.09 36.92 33.16 34.56 39.84 37.01 41.34</cell><cell>42.1</cell><cell>41.72</cell></row><row><cell>RINANTE+</cell><cell cols="10">31.42 39.38 34.95 21.71 18.66 20.07 29.88 30.06 29.97 25.68</cell><cell>22.3</cell><cell>23.87</cell></row><row><cell>Li-unified+</cell><cell cols="2">41.04 67.35</cell><cell>51.0</cell><cell cols="9">40.56 44.28 42.34 44.72 51.39 47.82 37.33 54.51 44.31</cell></row><row><cell>Peng-two-stage</cell><cell cols="10">43.24 66.6</cell><cell cols="2">68.68 67.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison results for Triplet on the D 20b dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The errors for Triplet on the test set of the D 20b .</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Pair Extraction F1 scores</cell><cell></cell><cell></cell><cell cols="2">Triplet Extraction F1 scores</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell>69</cell><cell></cell><cell></cell></row><row><cell>F1 score</cell><cell>69 72</cell><cell></cell><cell></cell><cell>F1 score</cell><cell>63 66</cell><cell></cell><cell></cell><cell>14lap 14res</cell></row><row><cell></cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell>15res 16res</cell></row><row><cell></cell><cell>63</cell><cell></cell><cell></cell><cell></cell><cell>57</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>Beam Size</cell><cell></cell><cell></cell><cell></cell><cell>Beam Size</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is also referred to as the AE-OE co-Extraction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Because of the cross-attention between encoder and decoder, the number of parameters of BART is about 10% larger than its counterpart ofBERT (Lewis et al., 2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our implement, yt ? [1, n + l]. The x1 has the pointer index 1.5  Each dataset only contains a subset of all ABSA subtasks. We use the published year of the dataset to distinguish them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Due to the limited space, we would present detailed experiments for each dataset in the Supplementary Material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their insightful comments. The discussion with colleagues in AWS Shanghai AI Lab was quite fruitful. We also thank the developers of fastNLP 7 and fitlog 8 . This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106700) and National Natural Science Foundation of China (No. 62022027).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>For the consideration of ethical concerns, we would make detailed description as follows:</p><p>(1) All the experiments are conducted on existing datasets, which are derived from public scientific papers.</p><p>(2) We describe the characteristics of the datasets in a specific section. Our analysis is consistent with the results.</p><p>(3) Our work does not contain identity characteristics. It does not harm anyone.</p><p>(4) Our experiments do not need a lot of computer resources compared to pre-trained models.</p><p>( <ref type="formula">5)</ref> We will open source all our code.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">FastNLP is a natural language processing python package</title>
		<ptr target="https://github.com/fastnlp/fitlog.Fit-logisanexperimenttrackingpackage" />
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relation-aware collaborative learning for unified aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3685" to="3694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural aspect and opinion term extraction with mined rules as weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1520</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5268" to="5277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Target-oriented opinion words extraction with target-fused neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2509" to="2518" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An interactive multi-task learning network for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="504" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open-domain targeted sentiment analysis via span-based extraction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1051</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-2227-2237</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguis-2227-2237<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introducing syntactic structures into target opinion word extraction with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Amir Pouran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien Huu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.719</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8947" to="8956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11431-020-1647-3</idno>
	</analytic>
	<monogr>
		<title level="j">SCIENCE CHINA Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems</title>
		<meeting><address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2004-12-13" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MASS: masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="5956" to="5963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive neural structural correspondence network for crossdomain aspect and opinion co-extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2171" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent opinions transfer network for target-oriented opinion words extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9298" to="9305" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Double embeddings and CNN-based sequence labeling for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2094</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="592" to="598" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Position-aware tagging for aspect sentiment triplet extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.183</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2339" to="2349" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="3087" to="3093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SpanMlt: A span-based multi-task learning framework for pair-wise aspect and opinion terms extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longtao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3239" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
