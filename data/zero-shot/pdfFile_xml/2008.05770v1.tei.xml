<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LI ET AL.: WEAKLY SUPERVISED GENERATIVE NETWORK FOR 3D POSE HYPOTHESES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D human pose estimation from a single image is an inverse problem due to the inherent ambiguity of the missing depth. Several previous works addressed the inverse problem by generating multiple hypotheses. However, these works are strongly supervised and require ground truth 2D-to-3D correspondences which can be difficult to obtain. In this paper, we propose a weakly supervised deep generative network to address the inverse problem and circumvent the need for ground truth 2D-to-3D correspondences. To this end, we design our network to model a proposal distribution which we use to approximate the unknown multi-modal target posterior distribution. We achieve the approximation by minimizing the KL divergence between the proposal and target distributions, and this leads to a 2D reprojection error and a prior loss term that can be weakly supervised. Furthermore, we determine the most probable solution as the conditional mode of the samples using the mean-shift algorithm. We evaluate our method on three benchmark datasets -Human3.6M, MPII and MPI-INF-3DHP. Experimental results show that our approach is capable of generating multiple feasible hypotheses and achieves state-ofthe-art results compared to existing weakly supervised approaches. Our source code is available at: https://github.com/chaneyddtt/weakly-supervised-3d-pose-generator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose estimation from a monocular image refers to the task of recovering 3D human pose from a 2D image of the person. This task is extensively studied in the computer vision community due to its potentially useful applications in surveillance, healthcare, movie productions, robotics, etc. Most existing works for the task of 3D human pose estimation from a monocular image assume a uni-modal posterior distribution where only a single solution can exist. On the contrary, following the arguments by <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>, we reason that 3D human pose estimation from a monocular image is actually an inverse problem with the possibility of multiple feasible solutions due to the inherent ambiguity of the missing depth. Enforcing a uni-modal posterior distribution on the models can lead to overfitting that gives undesirable performance.</p><p>To the best of our knowledge, the only existing works that addressed the inverse problem of 3D human pose estimation from a monocular image are Jahangiri and Yullie <ref type="bibr" target="#b14">[14]</ref>, and Li and Lee <ref type="bibr" target="#b18">[18]</ref>. More specifically, <ref type="bibr" target="#b14">[14]</ref> uses optimization based method that generates H2 GT multiple hypotheses for the inverse problem. Despite the ability to generate multiple hypotheses, the method shows unsatisfactory performance compared to existing deep learning approaches that produce only a single solution. <ref type="bibr" target="#b18">[18]</ref> is the first and currently the only deep learning approach that generates multiple hypotheses for the inverse problem of 3D human pose estimation. It uses a mixture density network to model the posterior with a multi-modal mixture-of-Gaussian distribution. Although this approach outperforms other state-of-the-art deep learning single solution approaches, it is supervised that requires a huge amount of ground truth 2D-to-3D correspondences that are often difficult to obtain. To circumvent the need for ground truth data, an increasing number of weakly supervised <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> and unsupervised <ref type="bibr" target="#b26">[26]</ref> deep learning approaches are proposed in the recent years. However, these approaches are still based on a uni-modal posterior assumption that gives a single solution to the inverse problem of 3D human pose estimation from a monocular image.</p><p>In this paper, we propose a weakly supervised deep generative network to address the inverse problem of 3D human pose estimation. To this end, we design a deep generative network to model a proposal distribution which we use to approximate the unknown multimodal posterior distribution. <ref type="figure" target="#fig_0">Figure 1</ref> shows an illustration of our approach. We achieve the approximation by minimizing the KL divergence between our proposal distribution and the target posterior distribution. This leads to a loss function that minimizes the expectation of a 2D reprojection error and a prior term over the samples drawn from the proposal distribution. The 2D reprojection error ensures that samples of the 3D human pose drawn from our deep generative network reproject closely to the 2D pose observed in the image. We use a discriminator based on the maximum mean discrepancy (MMD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19]</ref> as the prior term to encourage the generated 3D human pose to be "human-like". Furthermore, we prevent the mode collapse problem of our generative network by introducing two additional losses <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b36">36]</ref> into the prior term to encourage diversity in the generated 3D human poses.</p><p>Given an input 2D human pose during inference, we draw samples from the posterior distribution by generating multiple 3D human poses from our generative network. We determine the most probable solution as the conditional mode of the samples using the mean-shift algorithm. We further propose a time-efficient variant to approximate the conditional mode. Specifically, we approximate the conditional mode as the output of our generative network from an all-zero latent code input. Experimental results show that our approach can achieve superior performance compared to state-of-the-art weakly supervised approaches on the Hu-man3.6M dataset <ref type="bibr" target="#b13">[13]</ref>. We also test on the MPII <ref type="bibr" target="#b0">[1]</ref> and the MPI-INF-3DHP datasets <ref type="bibr" target="#b21">[21]</ref> to show the generalization capacity. Our contributions are summarized as: <ref type="bibr" target="#b0">(1)</ref> We propose a weakly supervised deep generative network to generate multiple hypotheses for the inverse problem of 3D human pose estimation. (2) We prevent mode collapse of our network by introducing additional losses to encourage diversity of the generated hypotheses. (3) We achieve state-of-the-art results compared to other weakly supervised approaches.</p><p>Existing 3D pose estimation approaches can be divided into three categories: Fully and weakly supervised approaches based on a uni-modal posterior, and fully supervised approaches based on a mixture-of-Gaussians distribution.</p><p>Most existing works are fully supervised, which train their models either in an end-toend <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35]</ref> or a two-stage manner <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b32">32]</ref>. Pavlakos et al. <ref type="bibr" target="#b24">[24]</ref> use a volumetric representation for the 3D space and train a deep network to estimate the probability that a joint is located at each voxel. Because of the high dimension of the output, a coarse-to-fine strategy is adopted to finetune the estimation iteratively. To improve the generalization capacity, Zhou et al. <ref type="bibr" target="#b35">[35]</ref> proposes a transfer learning approach such that the network can be trained with both outdoor and indoor images. For the two-stage approaches, Martines et al. <ref type="bibr" target="#b20">[20]</ref> use a simple deep neural network to estimate 3D pose from 2D joint detections. Despite the impressive results, these approaches require ground truth 2D-to-3D labels, which are tedious to collect especially for outdoor environments.</p><p>More recently, several works begin to focus on weakly supervised <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, unsupervised <ref type="bibr" target="#b26">[26]</ref> and self-supervised learning <ref type="bibr" target="#b7">[7]</ref>. Wandt et al. <ref type="bibr" target="#b30">[30]</ref> weakly supervise their network with only 2D ground truth labels by projecting the estimated 3D pose into 2D space. A critic network is then used to enforce the estimated poses to be realistic. Chen et al. <ref type="bibr" target="#b7">[7]</ref> propose a self-supervised learning framework that lifts the 2D input to 3D pose, projects the 3D pose after a random transform, lifts the projection to 3D, undo the random transform and then projects back onto the 2D image. A self-consistency constraint and a 2D pose discriminator is applied on the original and final 2D poses to enable the lifting network to estimate valid 3D poses. The discriminators applied in both approaches play a key role to enforce valid estimations.</p><p>All of the above mentioned approaches assume a uni-modal posterior distribution, and only estimate one 3D pose for each 2D input. Two recent works <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref> explore a new line of research in generating multiple hypotheses for 3D human pose estimation. They argue that 3D pose estimation from 2D observations is an inverse problem where multiple solutions exist. To generate the multiple solutions, Jahangiri and Yullie <ref type="bibr" target="#b14">[14]</ref> learn an occupancy matrix to represent the plausible angular regions for each joint, and then generate multiple hypotheses by sampling from the occupancy matrix. Li and Lee <ref type="bibr" target="#b18">[18]</ref> use a mixture density network (MDN) to learn the multi-modal posterior distribution and take the conditional mean values of the mixture-of-Gaussian distribution as the hypotheses. Although <ref type="bibr" target="#b18">[18]</ref> achieves promising results, the method is strongly supervised and require ground truth 2Dto-3D correspondences for training. In contrast to <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>, we propose a weakly supervised generative model to generate multiple 3D pose hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>We propose a weakly supervised approach to generate multiple hypotheses from a given 2D human pose input. Let x ? R 2C denotes the 2D joint detection, where C is number of joints in a skeleton. We generate multiple 3D pose hypotheses y ? R 3C for each 2D pose input, where all the hypotheses reproject close to the 2D pose input. The true posterior P(y | x) is a multi-modal distribution because of the depth ambiguity and occluded joints. We design a deep generative network as a proposal distribution Q(y | x) to approximate the unknown target posterior distribution P(y | x). <ref type="figure" target="#fig_1">Figure 2</ref> shows the deep generative network that we designed as the proposal distribution Q(y | x). It consists of four main components: <ref type="formula" target="#formula_0">(1)</ref>   (2) a camera network that estimates the camera matrix M ? R 2?3 to project the generated 3D pose hypotheses into the 2D space; (3) a discriminator as the prior P(y) of the generated 3D pose; and (4) an encoder as a second prior to prevent the model collapse of our generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Pose Generator</head><p>Our goal is to train the model Q(y | x) to generate samples of 3D pose hypotheses from the unknown target posterior distribution P(y | x). To this end, we minimize the KL divergence between the proposal Q(y | x) and the target posterior P(y | x) distributions:</p><formula xml:id="formula_0">L = KL[Q(y | x) P(y | x)] + H(Q(y | x)).<label>(1)</label></formula><p>Following <ref type="bibr" target="#b8">[8]</ref>, we also minimize the entropy of the proposal distribution so that the output 3D pose y learns enough information from the 2D input x. According to the definition of KL divergence and entropy, the objective function is evaluated as:</p><formula xml:id="formula_1">L = ? ? y Q(y | x) log P(y | x) Q(y | x) ? ? y Q(y | x) log Q(y | x) = ? ? y {Q(y | x) log P(y | x)}. (2)</formula><p>We get our final objective function by applying the Bayes rule on P(y | x) = P(x|y)P(y) P(x) :</p><formula xml:id="formula_2">L = ? E y?Q(y|x) {log P(x | y) + log P(y)}.<label>(3)</label></formula><p>The objective function consists of a likelihood term P(x | y) and a prior P(y) term after we drop the constant term log P(x). We represent the likelihood term P(x | y) by a Laplace distribution:</p><formula xml:id="formula_3">P(x | y) = 1 2b exp ? |?(y) ? x| b ,<label>(4)</label></formula><p>where b is the scale parameter, ?(y) and x are the 2D reprojection of the generated 3D pose and the input 2D pose, respectively. Note that the Gaussian or the Laplacian distribution can be used for the likelihood term, and we chose the Laplacian distribution due to its robustness to noisy and outlier 2D joint inputs. Inspired by <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b30">30]</ref>, we estimate a camera matrix M ? R 2?3 from the 2D observation by using a camera network. The generated 3D pose y and camera matrix M are fed into a reprojection module to get the 2D reprojection. Under a weak perspective camera assumption, the 2D reprojection of the generated pose y is given by ?(y) = My. Maximizing the log-likelihood term is equivalent to minimizing the reprojection error, which results in our 2D loss: L 2D = |My ? x|.</p><p>The prior term P(y) represents the prior knowledge of real 3D poses, e.g. bone length, joint angle limit and symmetric information, and we use the discriminator from the MMD GAN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19]</ref> to learn the prior knowledge from a set of 3D poses. Note that it is not necessary for this set of 3D poses to be the ground truth labels of the respective input 2D poses. The input to the discriminator is a concatenation of the 3D pose and the corresponding KCS matrix <ref type="bibr" target="#b30">[30]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our pose generator has similar structure to a conditional GAN. The generator generates pose hypotheses from latent code z ? N (0, I) conditioned on the input 2D pose x, while the discriminator try to distinguish the generated poses from real poses. Consequently, a sample y is drawn from the proposal distribution in Equation <ref type="formula" target="#formula_2">(3)</ref> as:</p><formula xml:id="formula_4">y ? Q(y | x, z ? N (0, I)).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diverse Pose Hypotheses</head><p>Minimization of the KL divergence between the proposal distribution and the target posterior distribution may result in the generator learning only a subset of the target posterior distribution. This phenomenon known as the mode collapse problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">27]</ref> is widely discussed in the GAN literature. This problem manifests itself in the generator generating the same poses for different input latent codes conditioned on the same input 2D pose. To circumvent this problem, we add a second prior with a regularizer <ref type="bibr" target="#b31">[31]</ref> to explicitly encourage diversity and an encoder to reconstruct the input noise <ref type="bibr" target="#b36">[36]</ref>. Let G(x, z 1 ) and G(x, z 2 ) denote the output of the generator given 2D observation x, latent codes z 1 and z 2 sampling from N (0, I). We encourage the generator to generate diverse hypotheses by maximizing the objective:</p><formula xml:id="formula_5">L reg = E z 1 ,z 2 [min( |G(x, z 1 ) ? G(x, z 2 )| |z 1 ? z 2 | , ?)],<label>(6)</label></formula><p>where ? is a constant to ensure numerical stability. The regularizer forces the generator to generate diverse poses depending on the distance between the input latent codes.</p><p>To further prevent the mode collapse, we also introduce another encoder E to reconstruct the input latent code <ref type="bibr" target="#b36">[36]</ref>:</p><formula xml:id="formula_6">L rec = E z?N (0,1) |z ? E(G(x, z))|.<label>(7)</label></formula><p>The reconstruction loss encourages the connection between the output 3D pose and input latent code to be invertible, such that it helps prevent the many-to-one mapping problem in mode collapse. Intuitively, if G(x, z 1 ) and G(x, z 2 ) are the same when z 1 = z 2 , we can never recover z 1 or z 2 because the inputs to the encoder E are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>Inspired by the MMD GAN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19]</ref>, we use the kernel maximum mean discrepancy to distinguish the generated and real data distributions. The unbiased estimator of the squared MMD is given by:</p><formula xml:id="formula_7">MMD 2 u (P, Q) = 1 m(m ? 1) m ? i = j k(y i , y j ) + 1 n(n ? 1) m ? i = j k(? i ,? j ) ? 2 mn m ? i=1 n ? j=1 k(y i ,? j ). (8)</formula><p>where y ? P(y) and? ? Q(y | x) represent samples from the real and generated distributions respectively. We adopt a mixed kernel consisting of the rational quadratic (RQ) kernel and the dot kernel: k rq * = k rq + k dot following <ref type="bibr" target="#b3">[4]</ref>, where</p><formula xml:id="formula_8">k rq ? (x 1 , x 2 ) = (1 + x 1 ? x 2 2 2? ) ?? , k dot (x 1 , x 2 ) = x 1 , x 2 .<label>(9)</label></formula><p>The pose generator tries to fool the discriminator by generating realistic poses, hence it minimizes a adversarial loss given by: L adv = MMD 2 u (P, Q). At the same time, the pose generated from the same 2D input should be diverse and also keep consistent with the 2D input. Finally, the full objective function of the generator is expressed as:</p><formula xml:id="formula_9">L G = L adv + ? 2D L 2D ? ? reg L reg + ? rec L rec ,<label>(10)</label></formula><p>where ? 2D , ? reg and ? rec represent the weights of the corresponding losses. On the other hand, the discriminator tries to distinguish the real and fake distributions by minimizing L D = ?L adv + ? gp L gp . The gradient penalty L gp term <ref type="bibr" target="#b2">[3]</ref> is added to enforce the Lipschitz constraint. The camera estimation network also optimizes a camera loss <ref type="bibr" target="#b30">[30]</ref> such that it fulfils the weak perspective camera constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Best Pose Selection</head><p>After training, the generator can generate 3D pose hypotheses for the same 2D input by sampling latent code z from N (0, I). In practice, we also want to find the most probable 3D pose from the multiple hypotheses, i.e., the best conditional mode of the posterior distribution. Let H = {h 1 , h 2 , h 3 , ..., h N } be the pose hypotheses generated from Z = {z 1 , z 2 , z 3 , ..., z N } conditioned on x, where N is the number of samples. To find the pose with the highest probability, we employ a local mode-finding approach based on mean-shift <ref type="bibr" target="#b9">[9]</ref> with a Gaussian kernel. The kernel density estimator is given by:</p><formula xml:id="formula_10">f (h) = 1 Nw d N ? i=1 K( h ? h i w ),<label>(11)</label></formula><p>where w, d and K represent the bandwidth, feature dimension and kernel function, respectively. However, the mean-shift algorithm is computationally expensive especially when the number of samples is large and might be unsuitable for scenario where efficiency is the priority. We propose an alternative method to improve the efficiency. We directly feed an all-zero code z into the generator and obtain the final pose. This is similar to the 'zero code' used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">33]</ref> to obtain a most likely single view depth. Intuitively, an all-zero code is the most likely code because we sample z from N (0, I) during training. We will show in the experiments that the zero code can achieve comparable results with the mean-shift algorithm.</p><p>Implementation Details. We train our model with ADAM optimizer with an initial learning rate of 0.0001 and decay every epoch with a decay rate of 0.94. The weights for different losses ? gp , ? 2D , ? reg and ? rec are set to 0.1, 10.0, 7.5 and 10.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>We evaluate our approach on three 3D human pose estimation benchmarks: Hu-man3.6M <ref type="bibr" target="#b13">[13]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b21">[21]</ref> and MPII datasets <ref type="bibr" target="#b0">[1]</ref>. The human3.6M dataset is the largest and most commonly used dataset for 3D human pose estimation. There are 15 daily activities in total performed by 7 professional actors under 4 camera views. The MPI-INF-3DHP is a recently proposed dataset which includes both indoor and outdoor scenes. The MPII dataset a challenging benchmark for 2D human pose estimation because of the complex background and severe occlusion. We train our model on the Human3.6M dataset and show results on all three datasets Data Preprocessing. Following previous work <ref type="bibr" target="#b30">[30]</ref>, we align every 3D pose in the Hu-man3.6M dataset to a template by applying a transformation to the 3D pose. The transformation, which includes a scale, rotation and translation, is obtained from procrustes analysis on the hip and shoulder joints. Both 2D and 3D poses are centered at the root joint, and each 2D pose is further normalized by dividing its standard deviation. Following previous work <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">20]</ref>, we use the stacked hourglass network <ref type="bibr" target="#b23">[23]</ref> trained on both MPII and Human3.6M datasets to estimate 2D poses from images.</p><p>Evaluation Protocols. Following the standard protocol for Human3.6M dataset <ref type="bibr" target="#b16">[16]</ref>, we use subjects 1, 5, 6, 7 and 8 for training, and evaluation is done on every 64 th frame of subjects 9 and 11. The evaluation metric is the Mean Per Joint Position Error (MPJPE) measured in millimeters. The 3D Percentage of Correct Keypoints (3DPCK) under 150mm radius <ref type="bibr" target="#b21">[21]</ref> is adopted as the metric for the MPI-INF-3DHP dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Results on Human3.6M Dataset</head><p>The poses generated by the generator are in the template frame as described in the Data Preprocessing. Consequently, we evaluate the effectiveness of the pose generator by showing the MPJPE under protocol #2, where a rigid alignment is applied to the estimated pose before comparison with the ground truth. <ref type="table" target="#tab_2">Table 1</ref> shows the results of our approach and other state-of-the-art fully and weakly supervised approaches. 'MH' represents approaches that generate multiple hypotheses and 'WS' represents weakly supervised approaches. We evaluate our approach under both mean-shift (MS) and zero code (ZC) settings, where we  generate a single pose with the highest probability w.r.t. the proposal distribution. Following previous works <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref> that generate multiple hypotheses, we also evaluate our approach under the best hypothesis (BH) setting, where we select the best of ten hypotheses according to the ground truth. As can be seen from <ref type="table" target="#tab_2">Table 1</ref>, our approach achieves comparable performance with our supervised counterpart <ref type="bibr" target="#b18">[18]</ref>, which also generates multiple hypotheses, and superior results compared to other weakly supervised approaches <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>. The similar performance achieved by MS and ZC demonstrates that the pose with highest probability can be approximated from the zero code. This significantly improves the efficiency because sampling is not needed in ZC setting. Moreover, the performance under ZC (or MS) is close to BH. This shows that the pose hypothesis with the highest probability is close to the ground truth pose among all hypotheses generated by the generator. 'GT' represents results when using ground truth 2D joints as input, which indicates that our performance can be further improved when 2D detections are more accurate. We evaluate our model under protocol #1, where the generated poses are transformed into the camera frame with a rotation matrix R computed from the camera network output M ? R 2?3 . As shown in <ref type="table" target="#tab_4">Table 2</ref>, our approach outperforms state-of-the-art weakly supervised approach <ref type="bibr" target="#b30">[30]</ref>. Note that our approach performs worse than our supervised counterpart <ref type="bibr" target="#b18">[18]</ref> under this setting, which can be attributed two reasons: (1) we do not use the 2D-to-3D correspondences where the 3D poses are already in the camera frame, and (2) we add constraint to the camera estimation network based on a weak perspective camera assumption, which is not true for the Human3.6M dataset.</p><p>Following <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>, we also evaluate the robustness of the pose generator by testing on scenarios with missing joints. This is common in realistic scenarios when some joints are severely occluded and cannot be detected. During training, one or two missing joints are randomly selected from the the limb joints including l/r wrist, l/r knee, l/r elbow and l/r ankle. We use the ground truth 2D joints as input and set 2D coordinate of missing joints to zeros. The weights for different losses ? gp , ? 2D , ? reg and ? rec are set to 0.1, 20.0, 7.5 and 10.0, respectively. We set the weights for missing joints in the 2D loss L 2D to zeros because missing joints do not provide any information for the training. The results are shown in <ref type="table">Table 3</ref> where the numbers of <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b30">30]</ref> are based on the public available implementation or checkpoints. We can see that our approach outperforms state-of-the-art weakly supervised approach <ref type="bibr" target="#b30">[30]</ref>, and achieve comparable results with our supervised counterpart <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Do L reg and L rec prevent model collapse? We compare our model with and without L reg and L rec to verify their effectiveness on diversity. We do the evaluation on two metrics: (1)  <ref type="table">Table 3</ref>: Results with one (the first five rows) or two (the last five rows) missing joints. randomly sample 10 pose hypotheses for the same 2D input and calculate the standard deviation (STD) of each joint coordinate w.r.t. the root joint; (2) use the farthest point sampling (FPS) <ref type="bibr" target="#b11">[11]</ref> to sample 5 diverse hypotheses from 100 random samples and compute the standard deviation (STD-FPS). <ref type="table" target="#tab_7">Table 4a</ref> shows the MPJEP under best hypothesis(BH) and zero code(MS) settings, STD and STD-FPS of our model with and without L reg and L rec . We can see that the pose hypotheses generated by our full model is much more diverse than the model without L reg and L rec . Moreover, the full model achieves lower error shows the advantage of generating diverse hypotheses. We also show the five hypotheses sampled by FPS in <ref type="figure" target="#fig_3">Figure 3</ref>. We can see that the generated poses have different degree of diversity depending on the input 2D poses. The 2D reprojections of all 3D pose hypotheses (last column) overlap with each other shows that there are multiple solutions for each 2D input.   How does ? reg affect the diversity and accuracy? We add the L reg to explicitly encourage the diversity of the generated 3D poses, here we analyze the impact of changing the corresponding weight ? reg . <ref type="table" target="#tab_7">Table 4b</ref> shows the estimation error under BH setting and diversity (STD) when ? reg is set to 7.0, 7.5, 8.0, 9.0, 10.0, 11.0, 12.0 with weights for other losses fixed. We can see that the STD increases when ? reg gets larger, which verifies that the L reg helps to increase the diversity. At the same time, the error also becomes large where we impose overly strong constraint on diversity with high ? reg . Consequently, the value of ? reg should be a trade-off between accuracy and diversity.  We test the generalization capacity of our approach on the MPI-INF-3DHP and MPII datasets. The MPI-INF-3DHP dataset includes images under three different scenes: indoor images with (GS) and without green screen background (no GS), outdoor images (Outdoor), and the MPII dataset only includes outdoor images. <ref type="table" target="#tab_9">Table 5</ref> shows the quantitative results of our approach under ZC and BH settings for the MPI-INF-3DHP dataset. Our results is slightly worse than <ref type="bibr" target="#b30">[30]</ref> under ZC setting but outperforms other approaches  under BH setting. We only show qualitative results for the MPII dataset because the 3D ground truth is not available. As can been seen from <ref type="figure" target="#fig_4">Figure 4</ref>, our approach generalizes well to outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on MPI-INF-3DHP and MPII datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a weakly supervised generative network for 3D human pose estimation. Our network is designed to model a proposal distribution and learned by minimizing the KL divergence with the true posterior distribution. Experiments show that our network is able to generate feasible 3D pose hypotheses consistent with 2D reprojections and also achieves better results compared to existing weakly supervised approaches. Moreover, results on the MPII and MPI-INF-3DHP datasets verify the generalization capacity of our network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our deep generative network is conditioned on a input 2D pose. Latent codes are drawn from a normal distribution to generate samples of 3D pose hypotheses that correspond to the target multi-modal posterior distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our deep generative network to generate multiple 3D human pose hypotheses. pose generator network that generates a 3D pose hypothesis y from on an input 2D pose x and latent code z ? N (0, I);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of five hypotheses sampled by FPS (third to seventh columns). The first and second columns represent the input 2D pose and the corresponding 3D ground truth. The last column shows the 2D reprojections of the five hypotheses (the corresponding 2D reprojection and 3D pose are drawn in the same color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on the MPII dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of MPJPE on the Human3.6M dataset under protocol #2. The best results for weakly supervised methods are marked in bold. (Our results under ZC setting is used for fair comparison.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>42.4 36.6 42.1 38.2 39.8 34.7 40.2 45.6 60.8 39.0 42.6 42.0 29.8 31.7 39.9 Li [18](BH) 43.8 48.6 49.1 49.8 57.6 61.5 45.9 48.3 62.0 73.4 54.8 50.6 56.0 43.4 45.5 52.7 Wandt [30] 77.5 85.2 82.7 93.8 93.9 101.0 82.9 102.6 100.5 125.8 88.0 84.8 72.6 78.8 79.0 89.9 Ours (ZC) 67.9 75.5 71.8 81.8 81.4 93.7 75.2 81.3 88.8 114.1 75.9 79.1 83.3 74.3 79.0 81.1 Ours (MS) 66.0 74.7 71.1 80.6 81.1 93.0 73.2 83.7 90.0 117.4 75.8 79.3 82.1 74.4 77.8 80.9 Ours (BH) 62.0 69.7 64.3 73.6 75.1 84.8 68.7 75.0 81.2 104.3 70.2 72.0 75.0 67.0 69.0 73.9 Ours (GT+BH) 54.8 61.9 48.6 63.6 55.8 73.7 59.0 61.3 62.2 85.7 52.8 60.2 57.5 51.3 56.8 60.0</figDesc><table><row><cell>Protocol #1</cell><cell>MH WS Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Martinez [20]</cell><cell>51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9</cell></row><row><cell>Sun [28]</cell><cell>47.5 47.7 49.5 50.2 51.4 55.8 43.8 46.4 58.9 65.7 49.4 47.8 49.0 38.9 43.8 49.6</cell></row><row><cell>Zhou [34]</cell><cell>34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results of MPJPE on the Human3.6M under protocol #1.The best results for weakly supervised methods are marked in bold. (Our results under ZC setting is used for fair comparison.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Algorithm MH WS Direct. Discuss Eating Greet Phone Smoke Pose Purch. Sitting SitD. Smoke Wait WalkD. Walk WalkT. Avg. 38.5 37.1 37.8 40.2 49.0 37.1 35.1 47.8 56.7 40.7 39.5 40.9 31.2 34.7 39.8 Wandt [30] 36.9 42.2 36.5 43.7 41.4 46.7 40.4 42.0 48.7 57.3 42.0 43.4 42.9 38.4 38.4 42.7 Ours 35.4 41.3 33.7 42.3 39.1 47.1 36.2 46.9 46.4 57.7 38.6 43.0 42.0 34.8 37.0 41.2 Martinez [20] 41.9 48.4 47.8 49.9 51.8 63.5 49.6 44.4 64.7 70.5 52.6 53.4 52.2 46.7 50.1 52.5 Jahangiri [14] 125.0 121.8 115.1 124.1 116.9 123.8 116.4 119.6 130.8 120.6 118.4 127.1 125.9 121.6 127.6 122.3 Li [18] 36.7 42.4 41.6 43.6 46.6 57.0 42.7 39.9 57.0 65.8 46.8 45.4 46.5 36.3 41.0 46.0 Wandt [30] 52.2 62.2 48.4 59.5 56.7 70.6 53.9 57.8 61.5 83.5 57.7 58.6 73.9 58.2 62.8 60.8 Ours 50.9 53.9 49.8 54.8 54.7 65.1 49.4 49.3 63.5 76.1 54.5 54.3 59.8 54.8 56.1 56.4</figDesc><table><row><cell>Martinez [20]</cell><cell>36.4 42.4 41.2 43.3 44.2 54.2 43.6 39.2 55.0 58.7 45.2 45.6 46.1 38.2 42.1 45.0</cell></row><row><cell>Jahangiri [14]</cell><cell>108.6 105.9 105.6 109.0 105.5 109.9 102.0 111.3 119.6 107.8 107.1 111.3 108.4 107.0 110.3 108.6</cell></row><row><cell>Li [18]</cell><cell>31.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>(a):Our model with and without L reg and L rec . (b): The impact of changing the weights ? reg on the diversity and accuracy</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on the MPI-INF-3DHP dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2008.05770v1 [cs.CV] 13 Aug 2020 2 LI ET AL.: WEAKLY SUPERVISED GENERATIVE NETWORK FOR 3D POSE HYPOTHESES</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">LI ET AL.: WEAKLY SUPERVISED GENERATIVE NETWORK FOR 3D POSE HYPOTHESES fc 1024</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">LI ET AL.: WEAKLY SUPERVISED GENERATIVE NETWORK FOR 3D POSE HYPOTHESES</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?l&amp;apos;on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<title level="m">Demystifying mmd gans</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Codeslam???learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2560" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The farthest point strategy for progressive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehoshua Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1315" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02330</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09024</idno>
		<title level="m">Diversity-sensitive conditional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scenecode: Monocular dense semantic reconstruction using learned encoded scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaifeng</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11776" to="11785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
