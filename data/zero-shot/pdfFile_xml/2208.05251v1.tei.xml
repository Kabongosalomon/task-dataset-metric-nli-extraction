<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consistency-based Self-supervised Learning for Temporal Anomaly Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniello</forename><surname>Panariello</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Consistency-based Self-supervised Learning for Temporal Anomaly Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Anomaly Detection</term>
					<term>Temporal Action Localization</term>
					<term>Weakly Supervised</term>
					<term>Self Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work tackles Weakly Supervised Anomaly detection, in which a predictor is allowed to learn not only from normal examples but also from a few labeled anomalies made available during training. In particular, we deal with the localization of anomalous activities within the video stream: this is a very challenging scenario, as training examples come only with video-level annotations (and not frame-level). Several recent works have proposed various regularization terms to address it i.e. by enforcing sparsity and smoothness constraints over the weaklylearned frame-level anomaly scores. In this work, we get inspired by recent advances within the field of self-supervised learning and ask the model to yield the same scores for different augmentations of the same video sequence. We show that enforcing such an alignment improves the performance of the model on XD-Violence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Video Anomaly Detection is to detect and localize anomalous events occurring in a video stream. Usually, these events involve human actions such as abuse, falling, fighting, theft, etc. In the last decades, such a task has gained relevance thanks to its potential for a video surveillance pipeline. In light of the widespread presence of CCTV cameras, employing enough personnel to examine all the footage is indeed infeasible and, therefore, automatic tools must be exploited.</p><p>Traditional approaches leverage low level features [16] to deal with this task, computed either on visual cues <ref type="bibr">[3,</ref><ref type="bibr">22]</ref> or object trajectories <ref type="bibr">[8,</ref><ref type="bibr">35]</ref>. These techniques -which could suffer and be unreliable in some contexts [20, 32] -have been surpassed by reconstruction-based approaches <ref type="bibr">[19]</ref>: these methods require a set of normal data to learn a model of regularity; afterward, an example can be assessed as "anomalous" if it substantially deviates from the learned model. In this regard, the architectural design often resorts to deep unsupervised autoencoders <ref type="bibr">[45,</ref><ref type="bibr" target="#b4">55]</ref>, which may include several strategies for regularizing the latent space.</p><p>The major recent trend <ref type="bibr">[42]</ref> regards the exploitation of weak supervision, which provides the learning phase also with examples from the anomalous classes. However, the annotations come in a video-level format: the learner does not know in which time steps the anomaly will show, but only if it does at least one time. For such a reason, the proposed techniques often fall into the framework of multiple instance learning <ref type="bibr">[17,</ref><ref type="bibr">43,</ref><ref type="bibr">47]</ref>, devising additional optimization constraints that mitigate the lack of frame-level annotations.</p><p>In this work, we attempt to complement the existing regularization techniques with an idea from the fields of self-supervised learning and consistency regularization <ref type="bibr">[6,</ref><ref type="bibr">15,</ref><ref type="bibr">41,</ref><ref type="bibr">48]</ref>. In this context, data augmentation is exploited to generate positive pairs, consisting of a couple of slightly different versions of the same example. Afterward, the network is trained to output very similar representations, in a self-supervised manner (no class labels are required). Similarly, we devise a data augmentation strategy tailored to sequences and encourage the network to assign the same frame-level anomaly scores for the elements of each positive pair. Such a strategy resembles the smoothness constraint often imposed in recent works; however, while those approaches focus on adjacent time steps, our approach can randomly span over longer temporal windows.</p><p>To evaluate the merits of our proposal, we conduct experiments on the XD-Violence [47] dataset. We found that the presence of our regularization term yields remarkable improvements, although being not yet enough to reach stateof-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Video Anomaly Detection Traditionally researchers and practitioners exploit object trajectories <ref type="bibr">[8,</ref><ref type="bibr">34,</ref><ref type="bibr">35]</ref>, low-level handcrafted features [3, 22, 33] (such as Histogram of Gradients and Histogram of Flows), and connected component analysis cues <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref>. These traditional approaches have proven to be effective on benchmark datasets, but turn out to be still ineffective when used on a real domain. This means that these methods do not adapt well to anomalies that have never been seen before. For this reason, the task of anomaly detection has recently been approached with deep neural networks.</p><p>Most recent works lean towards unsupervised methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">19]</ref> usually based on deep autoencoders. These models leverage the reconstruction error to measure how much an incoming example conforms to the training set distribution. Remarkably, the authors of [29] resorted to a variation of this common paradigm: namely, they proposed an approach that learns the distribution of normal data by guessing the appearance of future frames. In this respect, the idea was to compare the actual frame and the predicted one: if their difference is high, then it is possible to assume that an anomalous event occurred.</p><p>While it is easy to label whole videos (e.g., anomaly present or not), the availability of fine-grained labeled anomalous videos is often scarce. For this reason, weakly supervised methods have seen a great advance. Among these works, Sultani et al.</p><p>[42] introduced a new pattern for video anomaly detection termed Multiple Instance Learning (MIL), upon which most of the subsequent weakly supervised methods developed. In this scenario, a positive video (i.e. containing an anomaly) and a negative one are taken into account at each iteration. These videos are usually split into segments; the model assigns them an anomaly score through a sequence of convolutional layers followed by an MLP. The scores are collected into a positive bag and a negative bag: as in the former, there is at least an anomaly (while in the latter there are no anomalies) the minimum score from the positive bag has to be as far as possible from the maximum score of the negative bag. To impose this constraint, the objective function contains a MIL ranking loss as well as smoothness and sparsity constraints. Zhou et al. <ref type="bibr" target="#b4">[55]</ref> introduced the attention mechanism combined with the MIL approach, to improve the localization of the anomalies.</p><p>Temporal Action Localization The majority of the works dealing with Temporal Action Localization are either fully supervised or weakly supervised. The former ones can be broadly categorized into one stage <ref type="bibr">[27,</ref><ref type="bibr">30]</ref> and two stages methods <ref type="bibr">[26,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b2">53]</ref>. For the first type, in works such as <ref type="bibr">[27]</ref>, action boundaries and labels are predicted simultaneously; in [30] this concept is developed by exploiting Gaussian kernels to dynamically optimize the temporal scale of each action proposal. On the other hand, two-stage methods initially generate temporal action proposals and then classify them. A manner to produce proposals is by exploiting the anchor mechanism [13,18,50], sliding window <ref type="bibr">[40]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In the following, we present the proposed model and its training objective. The third part is dedicated to explaining the process of proposal generation, which consists of a post-processing step grouping adjacent similar scores into contiguous discrete intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>In our set-up, each video is split into segments of 16 frames, with no overlap between consecutive segments. To extract video-level features, some works <ref type="bibr">[12,</ref><ref type="bibr" target="#b1">52]</ref> opted for combining 2D-CNNs and Recurrent Neural Networks; instead, we give each segment to a pre-trained I3D network <ref type="bibr">[10]</ref>. Indeed, the authors of <ref type="bibr">[24]</ref> have shown that the I3D features can prove to be effective for video-anomaly detection, even when a shallow classifier as XGBoost [9,14] is employed for later classification. In our case, the I3D network is pre-trained on the Kinetics [21] dataset and not fine-tuned on our target data. This way, each example is represented by a variable-length sequence of T feature vectors X = (x 1 , x 2 , . . . , x T ). During training, examples come with a label y indicating whether an anomalous event appears in that sequence at least one time. Hence, given a training set of examples {(X j , y j )} N j=1 forged in the above-mentioned manner, we seek to train a neural network f (?; ?) that solves such a task with the lowest empirical error:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input sequence</head><formula xml:id="formula_0">X 3 X 2 X 1 X 6 X 4 X 5 X 12 X 11 X 10 X 8 X 9 X 7 time X A X B X 7 X 11 X 5 X 2 X 12 X 11 X 6 X 3 MODEL X A X B ATTENTION WEIGHTS Alignment Loss || ? A -? B || 2</formula><formula xml:id="formula_1">min ? 1 N N i=1 BCE(f (X i ; ?), y i )<label>(1)</label></formula><p>where BCE(?, ?) stands for the binary cross entropy loss function. For the architectural design of f (?; ?), we took inspiration from <ref type="bibr">[36]</ref>. It consists of two main parts, discussed in the following paragraphs: namely, the computation of attention coefficients and the creation of an aggregate video-level representation.</p><p>Attention coefficients The aim of this module is to assign a weight ? t ? [0, 1] to each element of the input sequence. As explained in the following, these weights will identify the most salient segments of the video i.e., the likelihood of having observed an abnormal event within each segment. The module initially performs a masked temporal 1D convolution [25] to allow each feature vector to encode information from the past. Such a transformation -which does not alter the number of input feature vectors -is followed by two fully connected layers activated by ReLU functions, except for the last layer where a sigmoid function is employed.</p><p>Video-level representation Once we have guessed the attention values, we exploit them to aggregate the input feature vectors. Such an operation -which resembles a temporal weighted average pooling presented in [37] -produces a single feature vector with unchanged dimensionality; formally:</p><formula xml:id="formula_2">x = T t=1 ? t x t .<label>(2)</label></formula><p>We finally feed it to a classifier g(?), composed of two fully connected layers. The final output represents the guess of the network for the value of y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>As mentioned before, we train our network in a weakly supervised fashion i.e., only video level labels are provided to the learner. However, to provide a stronger training signal and to encourage attention coefficients to highlight salient events, we follow recent works [7, 51] and use some additional regularization terms that encode a prior knowledge we retain about the dynamics of abnormality. Often, the presence of anomalous activities is characterized by the properties of sparseness and smoothness. Namely, anomalies appear rarely (i.e., normal events dominate) and transitions between the two modalities usually occur throughout multiple frames. Such a peculiarity represents a prior we would like to enforce over the scores learned by the model: the majority of them should be close to zero and have similar values for video segments. In formal terms, the first constraint can be injected by penalizing the l 1 norm [11] of the attention weights, as follows:</p><formula xml:id="formula_3">L sp = ??? 1 ,<label>(3)</label></formula><p>while the second one can be carried out by imposing adjacent coefficients to vary as little as possible:</p><formula xml:id="formula_4">L sm = T ?1 t=1 (? t ? ? t+1 ) 2 .<label>(4)</label></formula><p>Alignment loss Our main contribution consists in adding a consistency-based regularization term to the overall objective function. Overall, the idea is to generate two slightly different sequences out of a single one and, then, to encourage the model to produce the same attention coefficients for the two inputs.</p><p>To do so, we introduce a data augmentation function, shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, that allows us to forge different versions X A and X B from the same example X . In more detail, we split each sequence (x 1 , x 2 , . . . , x T ) into fixed-size blocks, whose length L is a hyperparameter we always set to 3; afterward, we randomly choose a feature vector within each block.</p><p>Once the variants X A and X B have been created, we ask the network to minimize the following objective function:</p><formula xml:id="formula_5">L a = T t=1 (? A t ? ? B t ) 2 ,<label>(5)</label></formula><p>where ? A and ? B are respectively the attention coefficients computed by the network for X A and X B . With this additional regularization term, we seek to enforce that not only adjacent time-steps should have the same weight, but also those lying within a wider temporal horizon.</p><p>Overall objective Finally, the objective function will be:</p><formula xml:id="formula_6">L ? L cl + ?L sp + ?L sm + ?L a ,<label>(6)</label></formula><p>where the parameters ?, ?, and ? give different weights to each loss component and L cl is the binary cross entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Proposal</head><p>During inference, we refine the anomaly scores by applying a post-processing step. Usually, two segments considered paramount by the network are interleaved by "holes", mostly due to noisy acquisitions or poor representations. The purpose of this phase is therefore to merge temporally close detections in a single retrieved candidate. In particular, as done in <ref type="bibr" target="#b3">[54]</ref>, we initially take out from the candidate set all those time-steps whose corresponding attention scores are lower than a certain threshold (in our experiments, &lt; 0.35). The remaining non-zero scores will be used to generate the temporal proposal.</p><p>To generate the proposals, we do not use the rough coefficients, but instead a more refined version. In particular, we compute a 1-d activation map in the temporal domain, called Temporal Class Activation Map (T-CAM) <ref type="bibr" target="#b3">[54]</ref>, which indicates the relevance of the segment t in the prediction of one of the two classes involved (normal vs anomalous). Each value a t of such activation map is computed as a t = g(x t ), i.e., the guess of the classifier g(?) (introduced in Sec. 3.1) if masking the contributions of all time-steps except the t-th one. Furthermore, we extract the Weighted T-CAM, which combines the attention weight and the T-CAM activation values, i.e., ? t = ? t ? a t . This operation let us emphasize the most important features for generating the proposal.</p><p>The last operation involves interpolating the weighted scores in the temporal axis and taking the bounding box that covers the largest connected component [36] to generate the final proposal <ref type="bibr" target="#b3">[54]</ref>. The anomaly score for each proposal is then computed as:</p><formula xml:id="formula_7">t end t=tstart = ? t ? a t t end ? t start ? 1 ,<label>(7)</label></formula><p>where t start and t end represent the beginning and the ending of a single proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct our experiments on the XD-Violence Dataset [47], a multi-modal dataset that contains scenes from different sources such as movies, sports, games, 77.81 <ref type="table">Table 2</ref>. Comparison with recent works. We report the frame-level AP score on XD-Violence for both unsupervised and weakly-supervised methods. All the competitors exploit the I3D network for extracting features from RGB frames. news, and live scenes. It holds a great variety concerning the devices used for video acquisition; indeed, the examples were captured by CCTV cameras, handheld cameras, or car driving recorders. There are 4754 videos for a total of 217 hours: among all these, 2405 are violent videos and 2349 are non-violent. The training set consists of 3954 examples; the test set, instead, features 800 ones, split into 500 violent and 300 non-violent videos. The dataset comes with multiple modalities such as RGB, optical flow, and audio; however, we restrict our analysis only to the RGB input domain.</p><p>XD-Violence also comes with segment-level ground truth labels; however, we only use video-level annotation during training, conforming to the weakly supervised setting. Differently, we exploit both the segment-level and framelevel annotations during the test phase, thus evaluating the model's capabilities for fine-grained localization.</p><p>Metrics We used the most popular metrics in anomaly detection settings, namely the Area Under Receiver Operating Characteristic Curve (AUC) and the Area Under Precision-Recall Curve (AP). While the AUC tends to be optimistic in the case of unbalanced datasets, the AP gives a more accurate evaluation of these scenarios. We assess the performance of the classification head by providing these two metrics at the video level; therefore, we use the attention scores, their interpolation, as well as the temporal proposals to assess the other grains.</p><p>Training Details We use the Adam optimizer [23] with a learning rate of 10 ?4 for the first 10 epochs and 10 ?5 for the remaining 40 epochs. The hyperparameters for the loss components are set to: ? = 2 ? 10 ?8 , ? = 0.002, ? = 0.5. The threshold for taking out the low weighted T-CAM scores is set to 0.35; the batch size equals 8.</p><p>Results Tab 1 reports the comparison between the baseline approach with and without the proposed alignment objective. It can be observed that its addition leads to a remarkable improvement in most of the metrics. In particular, we have a gain of about 1% in AUC for segment and frame level metrics, while the AP remains almost the same. The greatest improvement subsists for the temporal proposal metric, where we gain 5 points in AUC and around 4 points in AP. The video level metrics remain approximately the same but still very high.</p><p>When comparing our approach with other recent works (see Tab 2), it can be seen that it outperforms the unsupervised state-of-the-art methods; however, it is in turn surpassed by the weakly supervised ones. We conjecture that such a gap is mainly due to the bag representations inherent in these approaches, which could confer superior robustness; therefore, we leave to future works the extension of our idea to these methods. <ref type="figure" target="#fig_1">Fig 2 presents several qualitative results,</ref> showing two fight scenes in the first row, and a riot and an explosion in the second one. We notice that the scores rightly increase when an anomalous action begins. Unfortunately, they remain always close to the uncertainty regime (assuming a score around 0.5) and never tend towards discrete decisions. In future works, we are going to address also this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative analysis</head><p>Looking at the original videos, we could also explain why the scores yielded by the model are noisy and subject to local fluctuations. Indeed, as sudden characters or camera' movements are likely to intervene, they are mistaken for real anomalies by the model, which is susceptible to these visual discontinuities due to the lack of segment-level annotations during training. Differently, when relying on the entire video sequence, the model can conversely recognize them correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This work proposes a novel strategy to learn effective frame-level scores in weakly supervised settings when only video-level annotations are made available to the learner. Our proposal -which builds upon recent advances in the field of selfsupervised learning -relies on maximizing the alignment between the attention weights of two different augmentations of the same input sequence. We show that a base network equipped also with other common regularization strategies (e.g. sparsity and smoothness) brings even more improvements. We found that it is not enough to achieve state-of-the-art performance; however, we leave aside for future works a comprehensive investigation of its applicability to more advanced architectures.</p><p>2. Amraee, S., Vafaei, A., Jamshidi, K., Adibi, P.: Anomaly detection and localization in crowded scenes using connected component analysis. Multimedia Tools and Applications (2018) 3. Benezeth, Y., Jodoin, P.M., Saligrama, V., Rosenberger, C.: Abnormal events detection based on spatio-temporal co-occurences. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. IEEE (2009) 4. Bolelli, F., Allegretti, S., Baraldi, L., Grana, C.: Spaghetti labeling: Directed acyclic graphs for block-based connected components labeling. IEEE Transactions on Image Processing (2019) 5. Bolelli, F., Allegretti, S., Grana, C.: One dag to rule them all. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021) 6. Boschini, M., Buzzega, P., Bonicelli, L., Porrello, A., Calderara, S.: Continual semisupervised learning through contrastive interpolation consistency. arXiv preprint arXiv:2108.06552 (2021) 7. Cai, R., Zhang, H., Liu, W., Gao, S., Hao, Z.: Appearance-motion memory consistency network for video anomaly detection. In: Proceedings of the AAAI Conference on Artificial Intelligence (2021) 8. Calderara, S., Heinemann, U., Prati, A., Cucchiara, R., Tishby, N.: Detecting anomalies in people's trajectories using spectral graph analysis. Computer Vision and Image Understanding (2011) 9. Candeloro, L., Ippoliti, C., Iapaolo, F., Monaco, F., Morelli, D., Cuccu, R., Fronte, P., Calderara, S., Vincenzi, S., Porrello,   Wang, X., Che, Z., Jiang, B., Xiao, N., Yang, K., Tang, J., Ye, J., Wang, J., Qi, Q.: Robust unsupervised video anomaly detection by multipath frame prediction. IEEE transactions on neural networks and learning systems (2021) 46. Wu, J., Zhang, W., Li, G., Wu, W., Tan, X., Li, Y., Ding, E., Lin, L.: Weaklysupervised spatio-temporal anomaly detection in surveillance video. In: International Joint Conferences on Artificial Intelligence (2021) 47. Wu, P., Liu, J., Shi, Y., Sun, Y., Shao, F., Wu, Z., Yang, Z.: Not only look, but also listen: Learning multimodal violence detection under weak supervision. In: Proceedings of the European Conference on Computer Vision (2020) 48. Xie, Q., Dai, Z., Hovy, E., Luong, T., Le, Q.: Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems (2020) 49. Xu, H., Das, A., Saenko, K.: R-c3d: Region convolutional 3d network for temporal activity detection. In: IEEE International Conference on Computer Vision (2017) 50. Yang, L., Peng, H., Zhang, D., Fu, J., Han, J.: Revisiting anchor mechanisms for temporal action localization. IEEE Transactions on Image Processing (2020) 51. Yu, G., Wang, S., Cai, Z., Zhu, E., Xu, C., Yin, J., Kloft, M.: Cloze test helps:</p><p>Effective video anomaly detection via learning to complete video events. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 583-591 (2020)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed framework. (left) Augmentation function sampling two slightly different sequences out of a single one. The original sequence gets split into windows and for each, we randomly sample a single feature vector. This is done twice to obtain two sequences X A and X B . (right) Both sequences are separately fed to the model, obtaining two sequences of attention weights ?A and ?B, pulled closer together by the alignment loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative examples of the capabilities of our model to perform anomaly localization. The temporal proposal scores are indicated with a blue line, while the weighted T-CAM scores and the ground truth are shown in green and red respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A., et al.: Predicting wnv circulation in italy using earth observation data and extreme gradient boosting model. Remote Sensing (2020) 10. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2017) 11. Cascianelli, S., Costante, G., Crocetti, F., Ricci, E., Valigi, P., Luca Fravolini, M.: Data-based design of robust fault detection and isolation residuals via lasso optimization and bayesian filtering. Asian Journal of Control (2021) 12. Cascianelli, S., Costante, G., Devo, A., Ciarfuglia, T.A., Valigi, P., Fravolini, M.L.: The role of the input in natural language video description. IEEE Transactions on Multimedia (2019) 13. Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2018) 14. Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (2016) 15. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International Conference on Machine Learning (2020) 16. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. Ieee (2005) 17. Feng, J.C., Hong, F.T., Zheng, W.S.: Mist: Multiple instance self-training framework for video anomaly detection. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2021) 18. Gao, J., Yang, Z., Chen, K., Sun, C., Nevatia, R.: Turn tap: Temporal unit regression network for temporal action proposals. In: IEEE International Conference on Computer Vision (2017) 19. Hasan, M., Choi, J., Neumann, J., Roy-Chowdhury, A.K., Davis, L.S.: Learning temporal regularity in video sequences. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2016) 20. Hu, X., Hu, S., Huang, Y., Zhang, H., Wu, H.: Video anomaly detection using deep incremental slow feature analysis network. IET Computer Vision (2016) 21. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017) 22. Kim, J., Grauman, K.: Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. IEEE (2009) 23. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference on Learning Representations (2014) 24. Koshti, D., Kamoji, S., Kalnad, N., Sreekumar, S., Bhujbal, S.: Video anomaly detection using inflated 3d convolution network. In: International Conference on Inventive Computation Technologies (ICICT). IEEE (2020) 25. Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks for action segmentation and detection. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2017) 26. Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: IEEE International Conference on Computer Vision (2019) 27. Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the 25th ACM international conference on Multimedia (2017) 28. Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>F.: Autoloc: Weaklysupervised temporal action localization in untrimmed videos. In: Proceedings of the European Conference on Computer Vision (2018) 40. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos via multi-stage cnns. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2016) 41. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D., Kurakin, A., Li, C.L.: Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems (2020) 42. Sultani, W., Chen, C., Shah, M.: Real-world anomaly detection in surveillance videos. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2018) 43. Tian, Y., Pang, G., Chen, Y., Singh, R., Verjans, J.W., Carneiro, G.: Weaklysupervised video anomaly detection with robust temporal feature magnitude learning. In: IEEE International Conference on Computer Vision (2021) 44. Wang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmednets for weakly supervised action recognition and detection. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2017) 45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Video Level Segment Level Frame Level Proposal Frame Level For different levels and metrics, results of our model with and without the proposed align loss. There is an improvement on almost all metrics when leveraging the proposed term.</figDesc><table><row><cell cols="3">Align Loss AUC% AP% AUC% AP% AUC%</cell><cell cols="2">AP%</cell><cell>AUC% AP%</cell></row><row><cell>-</cell><cell cols="2">97.91 98.36 84.39 66.75 85.14</cell><cell cols="2">68.01</cell><cell>84.57 65.96</cell></row><row><cell>?</cell><cell cols="2">97.79 98.28 85.49 66.87 90.23</cell><cell cols="2">71.68</cell><cell>85.65 66.05</cell></row><row><cell></cell><cell>Supervision</cell><cell>Method</cell><cell></cell><cell>AP%</cell></row><row><cell></cell><cell></cell><cell>SVM baseline</cell><cell></cell><cell>50.78</cell></row><row><cell></cell><cell>Unsupervised</cell><cell>OCSVM [38]</cell><cell></cell><cell>27.25</cell></row><row><cell></cell><cell></cell><cell cols="2">Hasan et al. [19]</cell><cell>30.77</cell></row><row><cell></cell><cell></cell><cell cols="3">Ours (no align loss) 68.01</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>71.68</cell></row><row><cell></cell><cell>Weakly Supervised</cell><cell cols="3">Sultani et al. [42] 75.68</cell></row><row><cell></cell><cell></cell><cell>Wu et al. [47]</cell><cell></cell><cell>75.41</cell></row><row><cell></cell><cell></cell><cell>RTFM [43]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Liu, W., Luo, W., Lian, D., Gao, S.: Future frame prediction for anomaly detectiona new baseline. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2018) 30. Long, F., Yao, T., Qiu, Z., Tian, X., Luo, J., Mei, T.: Gaussian temporal awareness networks for action localization. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2019) 31. Lv, H., Zhou, C., Cui, Z., Xu, C., Li, Y., Yang, J.: Localizing anomalies from weakly-labeled videos. IEEE transactions on image processing (2021) 32. Medel, J.R., Savakis, A.: Anomaly detection in video using predictive convolutional long short-term memory networks. arXiv preprint arXiv:1612.00390(2016)33. Mehran, R., Oyama, A., Shah, M.: Abnormal crowd behavior detection using social force model. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. IEEE (2009) 34. Monti, A., Porrello, A., Calderara, S., Coscia, P., Ballan, L., Cucchiara, R.: How many observations are enough? knowledge distillation for trajectory forecasting. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2022) 35. Morris, B.T., Trivedi, M.M.: Trajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach. IEEE Transactions on Pattern Analysis and Machine Intelligence (2011) 36. Nguyen, P., Liu, T., Prasad, G., Han, B.: Weakly supervised action localization by sparse temporal pooling network. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (2018) 37. Porrello, A., Vincenzi, S., Buzzega, P., Calderara, S., Conte, A., Ippoliti, C., Candeloro, L., Di Lorenzo, A., Dondona, A.C.: Spotting insects from satellites: modeling the presence of culicoides imicola through deep cnns.</figDesc><table><row><cell>: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European Conference on Computer Vision (2018) Conference on Signal-Image Technology &amp; Internet-Based Systems (SITIS). IEEE (2019) 38. Sch?lkopf, B., Williamson, R.C., Smola, A., Shawe-Taylor, J., Platt, J.: Support vector method for novelty detection. In: Advances in Neural Information Processing Systems (1999) 29. In: 2019 15th International 39. Shou, Z., Gao, H., Zhang, L., Miyazawa, K., Chang, S.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work has been supported in part by the InSecTT project, funded by the Electronic Component Systems for European Leadership Joint Undertaking under grant agreement 876038. The Joint Undertaking receives support from the European Union's Horizon 2020 research and innovation programme and AU, SWE, SPA, IT, FR, POR, IRE, FIN, SLO, PO, NED and TUR. The document reflects only the author's view and the Commission is not responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion-aware feature for improved video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
