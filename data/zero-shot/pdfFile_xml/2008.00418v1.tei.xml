<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blind Face Restoration via Deep Multi-scale Component Dictionaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Peng Cheng Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Blind Face Restoration via Deep Multi-scale Component Dictionaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face hallucination ? deep face dictionary ? guided image restoration ? convolutional neural networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent reference-based face restoration methods have received considerable attention due to their great capability in recovering highfrequency details on real low-quality images. However, most of these methods require a high-quality reference image of the same identity, making them only applicable in limited scenes. To address this issue, this paper suggests a deep face dictionary network (termed as DFDNet) to guide the restoration process of degraded observations. To begin with, we use K-means to generate deep dictionaries for perceptually significant face components (i.e., left/right eyes, nose and mouth) from high-quality images. Next, with the degraded input, we match and select the most similar component features from their corresponding dictionaries and transfer the high-quality details to the input via the proposed dictionary feature transfer (DFT) block. In particular, component AdaIN is leveraged to eliminate the style diversity between the input and dictionary features (e.g., illumination), and a confidence score is proposed to adaptively fuse the dictionary feature to the input. Finally, multi-scale dictionaries are adopted in a progressive manner to enable the coarse-to-fine restoration. Experiments show that our proposed method can achieve plausible performance in both quantitative and qualitative evaluation, and more importantly, can generate realistic and promising results on real degraded images without requiring an identity-belonging reference. The source code and models are available at https://github.com/csxmli2016/DFDNet.</p></div>
			</abstract>
		</profileDesc>
		<revisionDesc>
				<date type="submission" when="-1" />
		</revisionDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blind face restoration (or face hallucination) aims at recovering realistic details from real low-quality (LQ) image to its high-quality (HQ) one, without knowing the degradation types or parameters. Compared with single image restoration tasks, e.g., image super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>, denoising <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, and deblurring <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, blind image restoration suffers from more challenges, yet is of great practical value in restoring real LQ images.</p><p>Recently, benefited from the carefully designed architecture and the incorporation of related priors in deep neural convolutional networks, the restoration results tend to be more plausible and acceptable. Though great achievements have been made, the real LQ images usually contain complex and diverse distributions that are impractical to synthesize, making the blind restoration problem intractable. To solve this issue, reference-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47]</ref> have been suggested by using reference prior in image restoration task to improve the process of network learning and alleviate the dependency of network on degraded input. Among these methods, GFRNet <ref type="bibr" target="#b25">[26]</ref> and GWAINet <ref type="bibr" target="#b6">[7]</ref> adopt a frontal HQ image as reference to guide the restoration of degraded observation. However, these two methods suffer from two drawbacks. 1) They have to obtain a frontal HQ reference which is from the same identity with LQ image. 2) The differences of poses and expressions between the reference and degraded input will affect the reconstruction performance. These two requirements limit their applicative ability to some specific scenarios (e.g., old film restoration or phone album that supports identity group).</p><p>In this paper, we present a DFDNet by building deep face dictionaries to address the aforementioned difficulties. We note that the four face components (i.e., left/right eyes, nose and mouth) are similar among different people. Thus, in this work, we off-line build face component dictionaries by adopting K-means on large amounts of HQ face images. This manner can obtain more accurate component reference without requiring the corresponding identity-belonging HQ images, which makes the proposed model applicable in most face restoration scenes. To be specific, we firstly use pre-trained VggFace <ref type="bibr" target="#b2">[3]</ref> to extract the multi-scale features of HQ face images in different feature scale (e.g., output of different convolutional layers). Secondly, we adopt RoIAlign <ref type="bibr" target="#b13">[14]</ref> to crop their component features based on the facial landmarks. K-means is then applied on these features to generate the K clusters for each component on different feature levels. After that, component adaptive instance normalization (CAdaIN) is proposed to norm the corresponding dictionary feature which helps to eliminate the effect of style diversity (i.e., illumination or skin color). Finally, with the degraded input, we match and select the dictionary component clusters which have the smallest feature distance to guide the following restoration process in an adaptive and progressive manner. A confidence score is predicted to balance the input component feature and the selected dictionary feature. In addition, we use multi-scale dictionaries to guide the restoration progressively which further improves the performance. Compared with the former reference-based methods (i.e., GFRNet <ref type="bibr" target="#b25">[26]</ref> and GWAINet <ref type="bibr" target="#b6">[7]</ref>), which have only one HQ reference, our DFDNet has more component candidates to be selected as a reference, thus making our model achieve superior performance.</p><p>Extensive experiments are conducted to evaluate the performance of our proposed DFDNet. The quantitative and qualitative results show the benefits of deep multi-scale face dictionaries brought in our method. Moreover, DFDNet can also generate plausible and promising results on real LQ images. Without requiring identity-belonging HQ reference, our method is flexible and practical in most face restoration applications.</p><p>To sum up, the main contributions of this work are:</p><p>-We use deep component dictionaries as reference candidates to guide the degraded face restoration. The proposed DFDNet can generalize to face images without requiring the identity-belonging HQ reference, which is more applicative and efficient than those reference-based methods. -We suggest a DFT block by utilizing CAdaIN to eliminate the distribution diversity between the input and dictionary clusters for better dictionary feature transfer, and we also propose a confidence score to adaptively fuse the dictionary feature to the input with different degradation level. -We adopt a progressive manner for training DFDNet by incorporating the component dictionaries in different feature scales. This can make our DFDNet learn coarse-to-fine details. -Our proposed DFDNet can achieve promising performance on both synthetic and real degraded images, showing its potential in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss recent works about single image and reference-based image restoration methods which are closely related to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single Image Restoration</head><p>Along with the benefits brought by deep CNNs, single image restoration has achieved great success in many tasks, e.g., image super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>, denoising <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, deblurring <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>, and compression artifact removal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. Due to the specific facial structure, there are also several well-developed methods for face hallucination <ref type="bibr">[2, 4-6, 15, 37, 39, 40, 48]</ref>. Among these methods, Huang et al. <ref type="bibr" target="#b14">[15]</ref> suggest to ultra-resolve a very low resolution face image by using the neural networks to predict the wavelet coefficients of HQ images. Cao et al. <ref type="bibr" target="#b1">[2]</ref> propose reinforcement learning to discover the attended regions and then enhance them with a learnable local network. To better recover the structure details, there are also some methods that incorporate the image prior knowledge in the restoring process. Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose to use semantic segmentation probability maps as class prior to recover class-aware textures on natural image super-resolution task. It firstly takes the LR images through a segmentation network to generate the class probability maps. And then these maps and LQ features are fused together by spatial feature transformation. As for face images, Shen et al. <ref type="bibr" target="#b32">[33]</ref> propose to learn a global semantic face prior as input to impose local structure on the output. Similarly, Xu et al. <ref type="bibr" target="#b38">[39]</ref> use a multi-tasks model to predict the facial components heatmaps and use them for incorporating structure information. Chen et al. <ref type="bibr" target="#b3">[4]</ref> learn the facial geometry prior (i.e., landmarks heatmaps and parsing maps) and take them to recover the high-resolution results. Yu et al. <ref type="bibr" target="#b39">[40]</ref> develop a facial attribute-embedded network by incorporating face attributes vector in the LR feature space. Kim et al. <ref type="bibr" target="#b5">[6]</ref> adopt a progressive manner to generate the successive higher resolution output and propose a facial attention loss on landmarks to constrain the structure of reconstruction. However, most of these facial prior knowledge mainly focus on geometry constrains (i.e., landmarks or heatmaps), which may not bring direct facial details for the restoration of LQ image. Thus, most of these single image restoration methods failed to generate plausible and realistic details on real LQ face images because of the ill-posed problem and the limitation of a single image or facial structure prior brought to the learning process of networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reference-Based Image Restoration</head><p>Due to the limitation of single image restoration methods on real-world LQ images, there are some works that use an additional image to guide the restoration process, which can bring the object structure details to the final result. As for natural image restoration, Zhang et al. <ref type="bibr" target="#b46">[47]</ref> utilize a reference image which has similar content with a LR image and then adopt a global matching scheme to search the similar content patches. These reference feature patches are then used to swap the texture feature of LR images. This method can achieve great visual improvements. However, it is very time and memory consuming in searching similar patches from the global content. Moreover, the requirement of reference further limits its application, because finding a natural image with a similar content for each LR input is also terrible and sometimes it is impossible to obtain these types of image. Different from natural image, face owns specific structures and share the similar components on different images of the same identity. Based on this observation, two reference-based methods have been developed for face restoration. Li et al. <ref type="bibr" target="#b25">[26]</ref> and Dogan et al. <ref type="bibr" target="#b6">[7]</ref> use a fixed frontal HQ reference for each identity to provide identity-aware features to benefit the restoration process. However, we note that face images are usually taken under unconstrained conditions, e.g., different background, poses, expressions, illuminations, etc. To solve this problem, they utilize a WarpNet to predict flow field to warp the reference to align with the LQ image. However, the alignment still does not solve all the differences between the reference and input, i.e., mouth close to open. Besides, the warped reference is usually unnatural and may take obvious artifacts to the final reconstruction result. We note that each component between different identity has the similar structure (i.e., teeth, nose, and eyes). It is intuitive to split the whole face into different parts, and generate the representative components for each one. To achieve this goal, we firstly use K-means on HQ images to cluster different component features off-line. Then we match the LQ features from the conducted component dictionaries to select the one with the similar structures to guide the left eyes dictionaries mouth dictionaries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-1 Components</head><p>Dictionaries Dic1,c</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionaries Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DFT Block</head><p>Dictionaries Generation   latter restoration. Moreover, with the conducted dictionaries, we do not require an identity-belonging reference anymore, and more component candidates can be selected as reference. It is much more accurate and effective than only one face image in reference-based restoration and can be applied in the unconstrained applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionaries Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Inspired by the former reference-based image restoration methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>, this work attempts to overcome the limitation of requiring reference image in face restoration. Given a LQ image I d , our proposed DFDNet aims to generate plausible and realistic HQ one? h with the conducted component dictionaries. The whole pipeline is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. In the first stage ( <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>), we firstly generate the deep component dictionaries from the high-quality images I h via k-means. These dictionaries can be selected as candidate component references. In the second stage ( <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>), for each component of the degraded observation I d , our DFDNet selects the dictionary features that have the most similar structure with the input. Specially, we re-norm the whole dictionaries via component AdaIN (termed as CAdaIN) based on the input component to eliminate the distribution or style diversity. The selected dictionary features are then utilized to guide the restoration process via dictionary feature transformation. Furthermore, we introduce a confidence score on the selected dictionary feature to generalize different degradation levels through weighted feature fusion. The progressive manner from coarse to fine is also beneficial to the restoration process. In the following, we first describe the off-line generation of multi-scale deep component dictionaries. Then the details of our proposed DFDNet along with the dictionaries feature transfer (DFT) blocks are interpreted. The objective functions for training are finally presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Off-line Generation of Component Dictionaries</head><p>To build the deep component dictionaries that cover the most types of faces, we adopt FFHQ dataset <ref type="bibr" target="#b17">[18]</ref> due to its high-quality and considerable variation in terms of age, ethnicity, pose, expression, etc. We utilize DeepPose <ref type="bibr" target="#b31">[32]</ref> and Face ++1 to recognize their poses and expressions (i.e., anger, disgust, fear, happiness, neutral, sadness and surprise), respectively, to balance the distribution of each attribute. Among these 70,000 high-quality images of FFHQ, we select 10,000 ones to build our dictionaries. Given a high-quality image I h , we first use pre-trained VggFace <ref type="bibr" target="#b2">[3]</ref> to extract its features on different scales. With the facial landmarks L h detected by dlib <ref type="bibr" target="#b19">[20]</ref>, we utilize RoIAlign <ref type="bibr" target="#b13">[14]</ref> to crop and re-sample these four components on each scale to a fixed size. We then adopt K-means <ref type="bibr" target="#b29">[30]</ref> to generate K clusters for each component, resulting in our component dictionaries. In particular, for handling 256 ? 256 images, the feature sizes of left/right eyes, nose and mouth on scale-1 are set to 40/40, 25, 55, respectively. The sizes are down-sampled one by one by two times for the following scale-{2, 3, 4}. These dictionary feature can be formulated as:</p><formula xml:id="formula_0">Dics,c = FDic I h |L h ; ?V gg ,<label>(1)</label></formula><p>where s ? {1, 2, 3, 4} is the dictionary scale, c ? {left eye, right eye, nose, mouth} is the type of components, and ? V gg is the fixed parameters from VggFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Face Dictionary Network</head><p>After building the high-quality component dictionaries, our DFDNet is then proposed to transfer the dictionary features to the degraded input I d . The</p><p>proposed DFDNet can be formulated as:</p><formula xml:id="formula_1">I = F(I d |L d , Dic; ?),<label>(2)</label></formula><p>where L d and Dic represent the facial landmarks of I d and the component dictionaries in Eqn. 1, respectively. ? denotes the learnable parameters of DFDNet.</p><p>To guarantee the features of I d and Dic in the same feature space, we take the pre-trained VggFace model as the encoder of DFDNet, which has the same network architecture and parameters in the dictionary generation network ( <ref type="figure" target="#fig_1">Fig. 1  (a)</ref>). Suppose that the encoder of DFDNet is different from VggFace or trainable in the training phase, it easily generates different features which are inconsistent with the pre-conducted dictionaries. For better transferring the dictionary feature to the input components, we suggest a DFT block and use it in a progressive manner. It mainly contains five parts, i.e., RoIAlign, CAdaIN, Feature Match, Confidence Score and Reverse RoIAlign. As for the encoder features of I d , we first utilize RoIAlign to generate four component regions. We note that these input components may have different distribution/style with the cluster of conducted dictionaries Dic s,c , we here suggest a component adaptive instance norm <ref type="bibr" target="#b15">[16]</ref> (CAdaIN) to re-norm each cluster in the dictionaries. The feature match scheme is then utilized to select the cluster with the similar texture. In addition, a confidence score is predicted based on the residual between the selected cluster and the input feature to better provide complementary details on input. The reverse RoIAlign is finally adopted to paste the restored features to the corresponding locations. For better transformation of restored features to the decoder, we modify the UNet <ref type="bibr" target="#b30">[31]</ref> and propose to use spatial feature transform (SFT) <ref type="bibr" target="#b34">[35]</ref> to transfer the dictionary features to the degraded input.</p><p>CAdaIN. We note that face images are usually under unconstrained conditions, e.g., different illuminations, skin color. To eliminate the effect of these diversities between the input components and dictionaries, we adopt component AdaIN (CAdaIN) to re-norm the clusters in component dictionaries for accurate feature matching. AdaIN <ref type="bibr" target="#b15">[16]</ref> can remain the structure while translate the content to the desired style. Denote </p><p>where s and c are the dictionary scale and the type of components defined in Eqn. 1. ? and ? are the mean and standard deviation. The re-normed dictionaries RDic k s,c has the similar distribution with input components F d s,c , which can not only eliminate the style difference, but also facilitate the feature match scheme. </p><p>The input component feature F d s,c matches across all the clusters in the re-normed component dictionaries to select the most similar one. F d s,c has the same size with k-th cluster in the corresponding dictionaries, thus this inner product operation can be regarded as a convolutional layer with zero bias and weights of F c s,d performed over all the clusters. This is very efficient to obtain the dictionaries' similarity scores. Among all the scores S s,c , we select the re-normed cluster with the highest similarity as the matched dictionaries, termed as RDic * s,c . This selected component feature RDic * s,c is then utilized to provide the high-quality details to guide the restoration of the input components in the following section.</p><p>Confidence Score. We note that the slight degradation of input (e.g., ?2 super-resolution) relies little on the dictionaries and vice versa. To generalize our DFDNet to different degradation level, we take the residual between F d s,c</p><p>and RDic * s,c as input to predict a confidence score that performs on the selected dictionary feature RDic * s,c . The result is expected to contain the absent highquality details which can add back to F d s,c . The output of confidence score can be formulated by:</p><formula xml:id="formula_4">Fs,c = F d s,c + RDic * s,c * F Conf (RDic * s,c ? F d s,c ; ?C ),<label>(5)</label></formula><p>where ? C is the learnable parameters of confidence score block F Conf . Inspired by SFT <ref type="bibr" target="#b34">[35]</ref>, which is proposed to learn a feature modulation function that incorporates some prior condition through affine transformation. The scale ? and shift ? parameters are learned from the restored featuresF s with two convolutional layers. The scale-s SFT layer is formulated as:</p><formula xml:id="formula_5">SF Ts = ? F decoder s + ? ,<label>(6)</label></formula><p>where ? and ? are both element-wise weights which have the same shape (i.e., height, width, number of channels) with F decoder s . After the progressive DFT block, our DFDNet can gradually learn the fine details for the final result?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Objective</head><p>The learning objective for training our DFDNet contains two parts, 1) reconstruction loss that constrains the result? close to the ground-truth I h , 2) adversarial loss <ref type="bibr" target="#b10">[11]</ref> for recovering realistic details.</p><p>Reconstruction Loss. We adopt mean square error (MSE) on both pixel and feature space (perceptual loss <ref type="bibr" target="#b16">[17]</ref>). The whole reconstruction loss is defined as,</p><formula xml:id="formula_6">Lrec = ? l2 ? ? I h 2 + M m=1 ?p,m CmHmWm ?m(?) ? ?m(I h ) 2<label>(7)</label></formula><p>where ? m denotes the m-th convolution layer of VggFace model ? . C, H and W are the channel, height, and width for the m-th feature. ? l2 and ? p,m are the trade-off parameters. The first term tends to generate blurry results, while the second one (perceptual loss) is beneficial for improving visual quality for the reconstruction results. The combination of the two terms is common in computer vision tasks and also is effective in the stable training of neural networks. In our experimental settings, we set M equal to 4.</p><p>Adversarial Loss. It is widely used to generate realistic details in image restoration tasks. In this work, we adopt multi-scale discriminators <ref type="bibr" target="#b33">[34]</ref> at different size of the restoration results. Moreover, for stable training of each discriminator, we adopt SNGAN <ref type="bibr" target="#b27">[28]</ref> by incorporating the spectral normalization after each convolution layer. The objective function for training multi-scale discriminators is defined as:</p><formula xml:id="formula_7">adv,Dr = R r E I h ?r ?P (I h ?r ) min 0, Dr(I h ?r ) ? 1 +E? ?r ?P (??r) min 0, ?1 ? Dr(? ?r ) ,<label>(8)</label></formula><p>where ?r denotes the down-sampling operation with scale factor r and r ? {1, 2, 4, 8}. Similarly, the loss for training generator F is defined as:</p><formula xml:id="formula_8">adv,G = ??a,r R r E I d ?P (I d ) Dr F I d |L d , Dic; ? ?r ,<label>(9)</label></formula><p>where ? a,r is the trade-off parameters for each scale discriminator. To sum up, the full objective function for training our DFDNet can be written as the combination of reconstruction and adversarial loss,</p><formula xml:id="formula_9">L = rec + adv,G .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Since the performance of reference-based methods are usually superior to other single image or face restoration methods <ref type="bibr" target="#b25">[26]</ref>, in this paper, we mainly compare our DFDNet with reference-based (i.e., GFRNet <ref type="bibr" target="#b25">[26]</ref>, GWAINet <ref type="bibr" target="#b6">[7]</ref>) and face prior-based methods (i.e., Shen et al. <ref type="bibr" target="#b32">[33]</ref>, Kim et al. <ref type="bibr" target="#b5">[6]</ref>). We also report the results of single natural image (i.e., RCAN <ref type="bibr" target="#b45">[46]</ref>, ESRGAN <ref type="bibr" target="#b35">[36]</ref>) and face (i.e., WaveletSR <ref type="bibr" target="#b14">[15]</ref>) super-resolution methods. Among these methods, Shen et al. <ref type="bibr" target="#b32">[33]</ref> and Kim et al. <ref type="bibr" target="#b5">[6]</ref> can only handle 128 ? 128 images, while others can restore 256 ? 256 images. For fair comparisons, our DFDNet is trained on these two sizes (termed as DFDNet128 and DFDNet256). RCAN <ref type="bibr" target="#b45">[46]</ref> and ESRGAN <ref type="bibr" target="#b35">[36]</ref> were originally trained on the natural images, thus we retrain them using our training data for further fair comparison (termed as *RCAN and *ESRGAN). WaveletSR <ref type="bibr" target="#b14">[15]</ref> was also retrained by using our training data with their released training code (termed as *WaveletSR). Following <ref type="bibr" target="#b25">[26]</ref>, PSNR, SSIM and LPIPS <ref type="bibr" target="#b44">[45]</ref> are reported on the super-resolution task (?4 and ?8) which also has the random injection of Gaussian noise and blur operation for quantitatively evaluating on the blind restoration task. In terms of qualitative comparison, we demonstrate the comparisons on the synthetic and real-world low-quality images. More visual results including high resolution restoration performance (i.e., 512 ? 512) can be found in our supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>As mentioned in Section 3.1, we select 10,000 images from FFHQ <ref type="bibr" target="#b17">[18]</ref> to build our component dictionaries. We note that GFRNet, GWAINet and WaveletSR adopt VggFace2 <ref type="bibr" target="#b2">[3]</ref> as their training data, we also use it for training and validating our DFDNet for fair comparison. To evaluate the generality of our method, we build two test datasets, i.e., 2,000 test images from VggFace2 <ref type="bibr" target="#b2">[3]</ref> which are not overlapped with the training data, and another 2,000 images from CelebA <ref type="bibr" target="#b26">[27]</ref>.</p><p>Each of them has a high-quality reference from the same identity for running GFRNet and GWAINet. To synthesize the training data that approximate to the real LQ images, we adopt the same degradation model suggested in GFRNet <ref type="bibr" target="#b25">[26]</ref>,</p><formula xml:id="formula_10">I d = (I h ? k) ?r + n ? JP EGq<label>(11)</label></formula><p>where k denotes two common types of blur kernel, i.e., Gaussian blur with ? {1 : 0.1 : 5} and 32 motion blur kernels from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>. Down-sampler r, Gaussian noise n ? and JPEG compression quality q are randomly sampled from {1 : 0.1 : 8}, {0 : 1 : 15} and {40 : 1 : 80}, respectively. The trade-off parameters for training DFDNet are set as follows: ? l2 = 100, ? p,1 = 0.5, ? p,2 = 1, ? p,3 = 2, ? p,4 = 4, ? a,1 = 4, ? a,2 = 2, ? a,4 = 1, ? a,8 = 1. The Adam optimizer <ref type="bibr" target="#b20">[21]</ref> is adopted to train our DFDNet with learning rate lr = 2 ? 10 ?4 , ? 1 = 0.5 and ? 2 = 0.999. lr is reduced by 2 times when the reconstruction loss on validation set becomes non-decreasing. The whole model including the generation of multi-scale component dictionaries and the training of DFDNet are executed on a server with 128G RAM and 4 Tesla V100. It takes 4 days to train our DFDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Synthetic Images</head><p>Qualitative evaluation. The quantitative results of these competing methods on super-resolution task are shown in <ref type="table">Table 1</ref>. We can have the following observations: 1) Compared with all the competing methods, our DFDNet is superior to others by a large margin on two datasets and two super-resolution tasks (i.e., at least 0.4 dB in ?4 and 0.3 dB in ?8 higher than the 2-nd best method). 2) <ref type="table">Table 1</ref>: Quantitative comparisons on two datasets and two tasks (?4 and ?8).</p><p>VggFace2 <ref type="bibr" target="#b2">[3]</ref> CelebA <ref type="bibr">[</ref>  Even though the retrained *RCAN and *ESRGAN have achieved great improvements, the performance is still inferior to GFRNet, GWAINet and our DFDNet, mainly due to the lack of high-quality facial references. 3) With the same training data, reference-based methods (i.e., GFRNet <ref type="bibr" target="#b25">[26]</ref> and GWAINet <ref type="bibr" target="#b6">[7]</ref>) outperform other methods, but are still inferior to our DFDNet, which can be attributed to the incorporation of high-quality component dictionaries and the progressive dictionary feature transfer manner. Given a LQ image, our DFDNet has more candidates to be selected as component reference, resulting in the flexible and effective restoration. 4) Our component dictionaries are conducted on FFHQ <ref type="bibr" target="#b17">[18]</ref> and DFDNet is trained on VggFace2 <ref type="bibr" target="#b2">[3]</ref>, but the performance on CelebA <ref type="bibr" target="#b26">[27]</ref> still outperforms other methods, indicating the great generalization of our DFDNet.</p><p>Visual Comparisons. <ref type="figure" target="#fig_5">Figs. 2 and 3</ref> show the restoration results of these competing methods on ?4 and ?8 super-resolution tasks. Shen et al. <ref type="bibr" target="#b32">[33]</ref> and Kim et al. <ref type="bibr" target="#b5">[6]</ref> were proposed to handle face deblur and super-resolution problems. Since they only released their test model, we did not re-implement them with the same training data and degradation model in this paper, resulting in their poor performance. The retrained *RCAN, *ESRGAN and *WaveletSR still limited in generating plausible facial structure, which may be caused by the lack of    reasonable guidance for face restoration. In terms of reference-based methods, GFRNet <ref type="bibr" target="#b25">[26]</ref> and GWAINet <ref type="bibr" target="#b6">[7]</ref> generate plausible structures but fail to restore realistic details. In contrast to these competing methods, our DFDNet can reconstruct promising structure with richer details on these notable face regions (i.e., eyes and mouth). Moreover, even though the degraded input is not frontal, our DFDNet can also have plausible performance (2-nd rows in <ref type="figure" target="#fig_5">Figs. 2 and 3</ref>).</p><p>Performance on Real-world Low-quality Images. Our goal is to restore the real low-quality images without knowing the degradation types and parameters. To evaluate the performance of our DFDNet on blind face restoration, we select the real images from Google Image with face resolution lower than 80 ? 80 and each of them has an identity-belonging high-quality reference for running GFRNet <ref type="bibr" target="#b25">[26]</ref> and GWAINet <ref type="bibr" target="#b6">[7]</ref>. Here we only show the visual results on competing methods with top-5 quantitative performance in <ref type="figure" target="#fig_9">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To evaluate the effectiveness of our proposed DFDNet, we conduct two groups of ablative experiments, i.e., the cluster number K for each component dictionary, and the progressive dictionary feature transfer block (DFT). For the first one, we generate different number of clusters in our component dictionaries. In this paper, we consider the cluster K ? {16, 64, 128, 256, 512}. For each variant, we retrain our DFDNet256 with the same experimental settings but with different cluster numbers, which are defined as Ours(#K). The quantitative results on our VggFace2 test data are shown in <ref type="table" target="#tab_3">Table 2</ref>. One can see that Ours(#64) has nearly the same performance with GFRNet <ref type="bibr" target="#b25">[26]</ref>. We analyze that because GFRNet <ref type="bibr" target="#b25">[26]</ref> adopts alignment between reference and degraded input, making Ours(#16) performs poorer than it. By increasing the cluster numbers, our DFDNet tends to achieve better results. We note that Ours(#256) performs on par with Ours(#512) but has less time-consuming in feature match. Thus, we adopt Ours(#256) as our default model. Visual comparisons between these five variants are also presented in <ref type="figure">Fig. 5</ref>. We can see that when K is larger, the restoration results tend to be clear and are much more realistic, indicating the effectiveness of our dictionaries in guiding the restoration process. For the second one, to evaluate the effectiveness of our progressive DFT block, we consider the following variants    <ref type="bibr" target="#b2">[3]</ref> test data are reported in <ref type="table" target="#tab_4">Table 3</ref>. We can have the following observations. (i) By increasing the number of DFT block, obvious gains (at least 2.2 dB in ?4 and 0.6 dB in ?8) are achieved, indicating the effectiveness of our progressive manner. (ii) The performance is severely degraded when removing the CAdaIN. This may be caused by the inconsistent distribution of degraded feature and dictionaries, resulting in the wrong matched features for restoration. (iii) With the incorporation of confidence score, which can help balance the input and the matched dictionary feature, our DFDNet can also achieve plausible improvements. <ref type="figure" target="#fig_10">Fig. 6</ref> shows the restoration results of these variants. We can see that compared with Ours(0DFT ) and Ours(2DFT ), Ours(Full ) is much clear and contains rich details. Results of Ours(-Ada) are inconsistent with ground-truth (i.e., mouth region in 1-st row). By the way, when the degradation is slight (1-st row), Ours(-CS ) which directly swaps the dictionary feature to the degraded image can easily change the original content (mouth region), making the undesired modification of face components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a blind face restoration model, i.e., DFDNet, to solve the limitation of reference-based methods. To eliminate the dependence of identitybelonging high-quality reference, we firstly suggest traditional K-means on large amount of high-quality images to cluster perceptually significant facial component. For dictionary feature transfer, we then propose a DFT block by addressing the following problems, distribution diversity between degraded input and dictionary feature with proposed component AdaIN, feature match scheme with fast inner product similarity, and generalization to degradation level with the confidence score. Finally, the multi-scale component dictionaries are incorporated in the multiple DFT blocks in a progressive manner, which can make our DFDNet learn the coarse-to-fine details for face restoration. Experiments validate the effectiveness of our DFDNet in handling the synthetic and real-world low-quality images. Moreover, we did not require an identity-belonging reference, showing the practical value in wide scenes in the real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Architecture of our DFDNet for dictionary feature transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our proposed method. It mainly contains two parts: (a) the off-line generation of multi-scale component dictionaries from large amounts of high-quality images which have diverse poses and expressions. K-means is adopted to generate K clusters for each component (i.e., left/right eyes, nose and mouth) on different feature scales. (b) The restoration process and dictionary feature transfer (DFT) block that are utilized to provide the reference details in a progressive manner. Here, DFT-i block takes the Scale-i component dictionaries for reference in the same feature level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>F d s,c and Dic k s,c as the c-th component features of the input I d and the k-th cluster from the component dictionaries at scale s, respectively. The re-normed dictionaries RDic s,c by CAdaIN is formulated by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Feature Match.</head><label></label><figDesc>As for the input component feature F d s,c and the re-normed dictionaries RDic s,c , we adopt inner product to measure the similarity between the F d s,c and all the clusters in RDic s,c . For k-th cluster in component dictionary, the similarity is defined as: S k s,c = F d s,c , RDic k s,c ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Reverse RoIAlign.</head><label></label><figDesc>After all the input components are processed by the former section, here we utilize a reverse operation of RoIAlign by takingF s,c and c ? {left/right eyes, nose and mouth} to their original locations of F d s,c . Denote the result of reverse RoIAlignF s . This manner can easily keep and translate other features (e.g., background) to the decoder for better restoration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Visual comparisons of these competing methods on ?4 SR task. Close-up in the right bottom of GFRNet is the required guidance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparisons of these competing methods on ?8 SR task. Close-up in the right bottom of GFRNet is the required guidance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons of competing methods with top performance on real-world low-quality images. Close-up at the right bottom is the required guidance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Restoration results of our DFDNet variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>: 1 )</head><label>1</label><figDesc>Ours(Full ): the final model in this paper, 2) Ours(0DFT ): our DFDNet by removing all the DFT blocks and directly using SFT to transfer the encoder feature to the decoder, 3) Ours(2DFT ): our DFDNet with two DFT blocks (i.e., DFT-{3,4} block), 4) Ours(-Ada) and Ours(-CS ): by removing the CAdaIN and Confidence Score in all the DFT blocks of final</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>PSNR? SSIM? LPIPS? PSNR? SSIM? LPIPS? PSNR? SSIM? LPIPS? PSNR? SSIM? LPIPS? Shen et al. [33] 20.56 .745 .080 18.79 .717 .126 21.04 .751 .079 18.64 .714 .131 Kim et al. 24.30 .878 .236 21.70 .823 .273 24.51 .884 .247 21.42 .820 .279 GFRNet [26] 27.13 .912 .132 23.37 .856 .269 27.32 .915 .124 23.12 .DFDNet256 27.54 .923 .114 23.73 .872 .239 27.77 .925 .103 23.69 .872 .241</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27]</cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell></cell><cell>?4</cell><cell></cell><cell></cell><cell>?8</cell><cell></cell><cell>?4</cell><cell>?8</cell></row><row><cell></cell><cell cols="2">[6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">20.99 .759 .095</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.72 .749 .104</cell></row><row><cell></cell><cell>DFDNet128</cell><cell></cell><cell cols="7">25.76 .893 .035 23.42 .841 .071 25.92 .899 .031 23.40 .839 .080</cell></row><row><cell></cell><cell>RCAN [46]</cell><cell></cell><cell cols="7">24.87 .889 .283 21.36 .819 .295 24.93 .892 .267 21.11 .814 .302</cell></row><row><cell></cell><cell>*RCAN</cell><cell></cell><cell cols="7">25.32 .896 .247 22.94 .836 .271 25.47 .901 .217 22.84 .831 .283</cell></row><row><cell></cell><cell cols="5">ESRGAN [36] 24.13 .876 .223</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.31 .878 .210</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>*ESRGAN</cell><cell></cell><cell cols="3">24.91 .891 .194</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.04 .896 .193</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="9">WaveletSR [15] 852 .273</cell></row><row><cell></cell><cell cols="2">GWAINet [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">23.41 .860 .260</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.38 .859 .270</cell></row><row><cell></cell><cell>Input</cell><cell cols="2">Shen et al.</cell><cell cols="2">*RCAN</cell><cell cols="4">*ESRGAN *WaveletSR</cell><cell>GFRNet</cell><cell>Ours</cell><cell>Ground-truth</cell></row><row><cell>Ours</cell><cell>Ground-truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Input</cell><cell cols="2">Shen et al.</cell><cell cols="2">Kim et al.</cell><cell>*RCAN</cell><cell></cell><cell>*WaveletSR</cell><cell>GFRNet</cell><cell>GWAINet</cell><cell>Ours</cell><cell>Ground-truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Among these competing methods, only GFRNet<ref type="bibr" target="#b25">[26]</ref> is proposed to handle blind face restoration, thus can well generalize to real degraded images. However, its results still contain obvious artifacts due to the inconsistent reference of only one high-quality image. With the incorporation of component dictionaries, our DFDNet can generate plausible and realistic results, especially in the eyes and mouth region, indicating the effectiveness of our DFDNet in handling real degraded observations. Moreover, our DFDNet does not require the identity-belonging reference, showing practical values in wide applications.</figDesc><table><row><cell>Input</cell><cell>RCAN</cell><cell>WaveletSR</cell><cell>GWAINet</cell><cell>GFRNet</cell><cell>Ours</cell><cell></cell></row><row><cell>Input</cell><cell>*RCAN</cell><cell>WaveletSR</cell><cell>GWAINet</cell><cell>GFRNet</cell><cell>Ours</cell><cell></cell></row><row><cell>Input</cell><cell>Ours(#16)</cell><cell>Ours(#64)</cell><cell>Ours(#128)</cell><cell>Ours(#256)</cell><cell>Ours(#512)</cell><cell>Ground-truth</cell></row><row><cell>Input</cell><cell>Ours(#16)</cell><cell>Ours(#64)</cell><cell>Ours(#128)</cell><cell>Ours(#256)</cell><cell>Ours(#512)</cell><cell>Ground-truth</cell></row><row><cell cols="7">Figure 5: Restoration results of our DFDNet with different cluster numbers.</cell></row><row><cell>Input</cell><cell>Ours(0DFT)</cell><cell>Ours(2DFT)</cell><cell>Ours(-Ada)</cell><cell>Ours(-CS)</cell><cell>Ours(Full)</cell><cell>Ground-truth</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on cluster number. Ours(#16) 26.79 .908 .144 .23.21 .839 .257 Ours(#64) 27.15 .914 .126 23.38 .856 .266 Ours(#128) 27.43 .919 .120 23.56 .867 .248 Ours(#256) 27.54 .923 .114 23.73 .872 .239 Ours(#512) 27.55 .923 .110 23.75 .873 .231</figDesc><table><row><cell>Methods</cell><cell>?4</cell><cell>?8</cell></row></table><note>PSNR?SSIM?LPIPS?PSNR?SSIM?LPIPS?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons on variants of DFT. Ours(0DFT ) 25.30 .896 .239 23.06 .839 .253 Ours(2DFT ) 26.43 .905 .161 23.24 .848 .261 Ours(-Ada) 25.47 .897 .190 22.97 .836 .270 Ours(-CS ) 27.23 .914 .129 23.51 .862 .246 Ours(Full) 27.54 .923 .114 23.73 .872 .239 model, respectively. The quantitative results on our VggFace2</figDesc><table><row><cell>Methods</cell><cell>?4</cell><cell>?8</cell></row></table><note>PSNR?SSIM?LPIPS?PSNR?SSIM?LPIPS?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.faceplusplus.com.cn/emotion-recognition/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is partially supported by the National Natural Science Foundation of China (NSFC) under Grant No.s 61671182, U19A2073 and Hong Kong RGC RIF grant (R5001-18).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling the performance of image restoration from motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>FG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep face deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Progressive face super-resolution via attention to facial landmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K D S K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exemplar guided face image super-resolution without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep generative adversarial compression artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">One-to-many network for visually pleasing compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deblurgan-v2: Deblurring (orders-ofmagnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep semantic face deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning to superresolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bm3d-net: A convolutional neural network for transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep plug-and-play super-resolution for arbitrary blur kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
