<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
							<email>hanshengchen97@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Intelligent Vehicles</orgName>
								<orgName type="department" key="dep2">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyao</forename><surname>Huang</surname></persName>
							<email>huangyuyao@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Intelligent Vehicles</orgName>
								<orgName type="department" key="dep2">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tian</surname></persName>
							<email>tianwei@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Intelligent Vehicles</orgName>
								<orgName type="department" key="dep2">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Intelligent Vehicles</orgName>
								<orgName type="department" key="dep2">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xiong</surname></persName>
							<email>xionglu@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Intelligent Vehicles</orgName>
								<orgName type="department" key="dep2">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object localization in 3D space is a challenging aspect in monocular 3D object detection. Recent advances in 6DoF pose estimation have shown that predicting dense 2D-3D correspondence maps between image and object 3D model and then estimating object pose via Perspective-n-Point (PnP) algorithm can achieve remarkable localization accuracy. Yet these methods rely on training with ground truth of object geometry, which is difficult to acquire in real outdoor scenes. To address this issue, we propose MonoRUn, a novel detection framework that learns dense correspondences and geometry in a self-supervised manner, with simple 3D bounding box annotations. To regress the pixel-related 3D object coordinates, we employ a regional reconstruction network with uncertainty awareness. For self-supervised training, the predicted 3D coordinates are projected back to the image plane. A Robust KL loss is proposed to minimize the uncertainty-weighted reprojection error. During testing phase, we exploit the network uncertainty by propagating it through all downstream modules. More specifically, the uncertainty-driven PnP algorithm is leveraged to estimate object pose and its covariance. Extensive experiments demonstrate that our proposed approach outperforms current state-of-the-art methods on KITTI benchmark. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular 3D object detection is an active research area in computer vision. Although deep-learning-based 2D object detection has achieved remarkable progress <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">30]</ref>, the 3D counterpart still poses a much greater challenge on accurate object localization, since a single image cannot provide explicit depth information. To address this issue, a large number of works leverage geometrical priors and solve the object pose (position and orientation in camera frame) via  <ref type="figure">Figure 1</ref>. 3D reconstruction is conducted by regressing the pixelrelated object coordinate map, which can be visualized as local point cloud in object space. For self-supervision, the object coordinates are reprojected to recover the image coordinate map. To focus on foreground pixels, we estimate the aleatoric uncertainty of network prediction. The coordinate uncertainty can be further propagated to estimate the pose covariance.</p><p>2D-3D constraints. These constraints either require extra keypoint annotations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, or exploit centers, corners and edges of ground truth bounding boxes <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b27">27]</ref>. Yet the accuracy largely depends on the number and quality of available constraints, and the performance degrades in occlusion and truncation cases with fewer visible keypoints. A more robust approach is using dense 2D-3D correspondences, in which every single foreground pixel is mapped to a 3D point in the object space. This has proven successful in monocular 6DoF pose estimation tasks <ref type="bibr" target="#b15">[15]</ref>. Current state-of-the-art dense correspondence methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b40">40]</ref> rely on both ground truth pose and 3D object model, so that target 3D coordinate map and object mask can be rendered for training supervision. This requirement restricts the training data to synthetic or simple laboratory scenes, where exact or pre-reconstructed 3D models are readily available. However, 3D object detection in real scenes (e.g., driving scenes) mostly deals with category-level objects, where acquiring accurate 3D models for all instances is impractical. An intuitive idea could be using LiDAR points to generate sparse coordinate maps for supervision. However, the persisting challenge is the deficiency of LiDAR points on specific objects or parts, e.g., on distant objects or reflective materials.</p><p>A workaround to the lack of ground truth is leveraging self-supervision. Typically, Wang et al. <ref type="bibr" target="#b35">[35]</ref> adopted a selfsupervised network to directly learn object pose with the given ground truth 3D geometry. Our work, on the contrary, adopts the opposite idea: learning the 3D geometry from ground truth pose in a self-supervised manner during training, and then solving the object pose via 2D-3D correspondences during testing.</p><p>In this paper, we propose the MonoRUn, a novel monocular 3D object detection method using selfsupervised reconstruction with uncertainty propagation. MonoRUn can extend off-the-shelf 2D detectors by appending a 3D branch to the region of interest (RoI) within each predicted 2D box. The 3D branch regresses dense 3D object coordinates in the RoI, which effectively builds up the geometry and 2D-3D correspondences.</p><p>To overcome the need for supervised foreground segmentation, we estimate the uncertainty of the predicted coordinates and adopt an uncertainty-driven PnP algorithm, which focuses on the low-uncertainty foreground. Furthermore, by forward propagating the uncertainty through PnP module, we can estimate the pose covariance matrix, which is used for scoring the detection confidence.</p><p>Self-supervision is conducted by projecting the predicted 3D coordinates back to the image via ground truth object pose and camera intrinsic parameters. To minimize the reprojection error with uncertainty awareness, we propose the Robust KL loss that minimizes the KL divergence between the predicted Gaussian distribution and the ground truth Dirac distribution. This novel loss function is the key to the state-of-the-art performance for the MonoRUn network.</p><p>To summarize, our main contributions are as follows:</p><p>? We propose a novel monocular 3D object detection network with uncertainty awareness, which can be trained without extra annotations (e.g., keypoints, 3D models, masks). To the best of our knowledge, this is the first dense correspondence method employed for 3D detection in real driving scenes. ? We propose the Robust KL loss for general deep regression with uncertainty awareness, and demonstrate its superiority over the plain KL loss in previous work. ? Extensive evaluation on KITTI <ref type="bibr" target="#b9">[10]</ref> benchmark shows significant improvement of our approach compared to current state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D Object Detection The majority of previous methods can be roughly divided into the following two categories, based on how depth information is derived. 1) With off-the-shelf monocular depth estimators. Representatively, Pseudo-LiDAR <ref type="bibr" target="#b39">[39]</ref> converts the estimated depth map to 3D point cloud and takes advantage of existing LiDAR-based 3D object detection pipeline. D 4 LCN [8] uses depth map as guidance to generate dynamic depth-wise convolutional filters, which can extract 3D information from RGB image more effectively. These methods largely benefit from the pre-trained depth estimator, e.g., DORN <ref type="bibr" target="#b8">[9]</ref>, which may have generalization issues.</p><p>2) With 2D-3D geometrical constraints. Deep-MANTA <ref type="bibr" target="#b3">[4]</ref> annotates the training data with 36-keypoint 3D vehicle templates. A network is trained to find the the best-matched template and regress 2D keypoint coordinates, and vehicle pose is computed via EPnP <ref type="bibr" target="#b20">[20]</ref> algorithm. RTM3D <ref type="bibr" target="#b21">[21]</ref> detects virtual keypoints (corners and center of the 3D bounding box) using a CenterNet-like <ref type="bibr" target="#b41">[41]</ref> network. Another commonly used constraint is 2D box and 3D box consistency, first used by Mousavian et al. <ref type="bibr" target="#b27">[27]</ref>. The above methods suffer from constraint deficiency by truncation or occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Correspondence and 3D Reconstruction</head><p>Most existing work uses ground truth geometry to train deep correspondence mapping networks. Nevertheless, some have explored end-to-end training via differentiable PnP algorithm without ground truth geometry.</p><p>1) With geometry supervision. Pix2Pose <ref type="bibr" target="#b28">[28]</ref> directly regresses normalized object coordinates (NOC) of each object pixel. DPOD <ref type="bibr" target="#b40">[40]</ref> predicts two-channel UV correspondences that map the object surface to 3D coordinates. Regarding category-level objects, Wang et al. <ref type="bibr" target="#b36">[36]</ref> demonstrated that the scale-invariant NOC can handle unseen instances in a given category. These methods are only tested on synthetic or simple indoor data.</p><p>2) Without geometry supervision. Brachmann and Rother <ref type="bibr" target="#b0">[1]</ref> proposed an approximate PnP back-propagation method to train an end-to-end network for Structure from Motion (SfM) problem. A further development is the BPnP <ref type="bibr" target="#b4">[5]</ref>, which is an exact PnP back-propagation approach. However, both methods train the networks in conjunction with reprojection loss as regularization term, which is essentially self-supervision. The potential of selfsupervision alone is not investigated in these researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty Estimation</head><p>The uncertainty in deep learning consists of aleatoric and epistemic uncertainty <ref type="bibr" target="#b18">[18]</ref>. The former captures noise inherent in observations, while the latter represents the uncertainty of model parameters. Kendall and Gal <ref type="bibr" target="#b18">[18]</ref> introduced heteroscedastic regression for deep networks to directly output data-dependent aleatoric uncertainty, which can be learned with a KL loss <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref> function. Yet the plain KL loss is sensitive to outliers and is not well-balanced against other loss functions, leaving room for improvement.  <ref type="figure">Figure 2</ref>. Training and testing pipeline of MonoRUn. The uncertainty-aware variables in red are modeled by Monte Carlo approach or by probabilistic models (e.g., Gaussian models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation and Approach Overview</head><p>Given an RGB image, the aim of 3D object detection is to localize and classify all objects of interest, yielding a 3D bounding box with class label for each object. The 3D box can be parameterized with dimensions d = l h w T and pose p = ? t x t y t z T , where ? is the object yaw angle and t x , t y , t z is the bottom center of the box in camera coordinate system. Based on off-the-shelf 2D object detectors, we aim to predict a 3D object coordinate map using the RoI features within each 2D box. For self-supervision, given the ground truth pose p gt and camera model, we can project the 3D coordinates back to the image, obtaining the reprojected 2D coordinates (u rp , v rp ). The objective is to recover the original 2D coordinates (u, v). However, if we simply minimize the reprojection error without foreground segmentation, the network will be disrupted by large errors (hence large uncertainty) on irrelevant background points. Therefore, we design an uncertainty-aware reconstruction module that estimates the aleatoric uncertainty of (u rp , v rp ), and the network is optimized by minimizing the uncertainty-weighted reprojection error using the proposed Robust KL loss.</p><p>During inference, we adopt the uncertainty-driven PnP module, through which the network uncertainty is propagated to object pose, represented with a multivariate normal distribution. This distribution is used by the scoring head to compute the detection confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Supervised Reconstruction Network</head><p>To deal with category-level objects of various sizes, we employ two network branches to predict the 3D dimensions and the dimension-invariant normalized object coordinates (NOC) <ref type="bibr" target="#b36">[36]</ref> respectively. Then, the object coordinate vector x OC is the element-wise product (denoted by ?) of NOC vector x NOC and dimension vector d:</p><formula xml:id="formula_0">x OC = x NOC ? d.<label>(1)</label></formula><p>The first branch is called the global extractor, which extracts the global understanding of an object and predicts the 3D dimensions. The second branch is called the NOC decoder, which predicts the dense NOC map using convolutional layers. Since convolutional layers have limited capability of understanding the global context, we let the global extractor predict an additional global latent vector to enhance the NOC decoder. This latent vector potentially encodes the occlusion, truncation and shape of the object, and is found to be beneficial for aleatoric uncertainty estimation (as shown in Section 4.4). Details about the network are presented as follows.</p><p>Global Extractor As shown in <ref type="figure">Fig. 2</ref>, a 7?7 RoI feature map is extracted from a higher level of the feature pyramid. The features are then flattened and fed into the global extraction head, which extracts a 16-channel global latent vector and predicts the 3D dimensions. The dimensions can be directly supervised by the annotated 3D bounding box sizes. For this network branch, we adopt two 1024-channel fully connected layers, as shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOC Decoder</head><p>The NOC decoder network is designed to aggregate the global latent vector and local convolutional features for NOC prediction. This is realized by incorporating the Excitation operation from the Squeeze-Excitation Network <ref type="bibr" target="#b16">[16]</ref>. As shown in <ref type="figure">Fig. 4</ref>, the channel size of the latent vector is first expanded to 256. Then, a channel-wise addition is conducted before the upsampling layer. Apart from predicting the three-channel NOC map, the NOC de-  <ref type="figure">Figure 3</ref>. The global extraction head predicts the dimensions and latent vector, while the MLP score head exploits both the detection results and the global feature to predict the detection score.</p><formula xml:id="formula_1">Conv 3?3, ReLu Conv 3?3, ReLu Conv 3?3, ReLu Upsample Conv 3?3, ReLu Conv 1?1 latent vector 16?1?1 FC 256?1?1</formula><p>NOCs 3?28?28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI features 256?14?14</head><p>256?14?14 std devs 2?28?28 <ref type="figure">Figure 4</ref>. The NOC decoder network aggregates the local features and global embeddings by channel-wise addition, and outputs the dense NOCs as well as standard deviations of reprojected coordinates. For upsampling, we adopt the CARAFE layer <ref type="bibr" target="#b37">[37]</ref>, which is more efficient than the deconvolutional layer often used in instance segmentation.</p><p>coder network is also responsible for estimating the twochannel aleatoric uncertainty map, as explained in the next paragraph.</p><p>Self-Supervision with Aleatoric Uncertainty Given the ground truth of object pose, the predicted object coordinates can be projected back to the image plane. The reprojection error of the pixel (u, v) is formulated as:</p><formula xml:id="formula_2">r (u,v) = K(Rx OC (u,v) + t) ? u v<label>(2)</label></formula><p>with the camera projection function K(?), rotation matrix R, and translation vector t = t x t y t z T . To focus on minimizing foreground error without instance segmentation, we introduce the aleatoric uncertainty in our work. More specifically, we use univariate Gaussian distribution to model the reprojected 2D coordinates, and let the network predict the means and standard deviations, which are optimized using the Robust KL loss. Formally we could follow the uncertainty propagation path in <ref type="figure">Fig. 2</ref> and predict the uncertainty of intermediate variables at first. Practically, leveraging the flexibility of deep networks, we take a shortcut by letting the NOC decoder directly regress the standard deviations of reprojected 2D coordinates, as shown in <ref type="figure">Fig. 4</ref>.</p><p>Additional Epistemic Uncertainty Estimating the epistemic uncertainty is essential for safety-critical applications such as autonomous driving. Following <ref type="bibr" target="#b18">[18]</ref>, we compute the mean and variance of x OC using Monte Carlo dropout during inference. We insert a channel dropout layer <ref type="bibr" target="#b34">[34]</ref> after RoI Align <ref type="bibr" target="#b10">[11]</ref> and a 1D dropout layer after each FC layer. Since PnP algorithms handle 2D projection variances more efficiently, we first transform the 3D variances of object coordinates into 2D variances of reprojected coordinates, which is then combined with the aleatoric uncertainty. Details are shown in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Robust KL Loss</head><p>By definition, KL loss is derived from the KL divergence between the predicted distribution and target distribution. Assuming Gaussian priors, the KL divergence is as follows:</p><formula xml:id="formula_3">D KL (N tgt N pred ) = 1 2 ? 2 tgt ? 2 pred + (? pred ? ? tgt ) 2 ? 2 pred ? 1 + log ? 2 pred ? 2 tgt .<label>(3)</label></formula><p>For fixed target distribution, log ? 2 tgt is constant and can be omitted in the minimization. Assuming narrow target (Dirac-like), (? pred ? ? tgt ) 2 dominates much more than ? 2 tgt . Let y = ? tgt , the minimization objective is simplified as:</p><formula xml:id="formula_4">L KL = 1 2? 2 pred (? pred ? y) 2 + 1 2 log ? 2 pred .<label>(4)</label></formula><p>We call Eq. (4) the Gaussian KL loss. Hereafter we omit the subscript (?) pred for brevity. To capture heteroscedastic aleatoric uncertainty in regression, Kendall and Gal <ref type="bibr" target="#b18">[18]</ref> proposed to directly predict the data-dependent mean ? and log variance log ? 2 using deep networks, which is optimized by Eq. (4). Apparently, the first term in Eq. (4) is a weighted L2 loss, where errors with higher uncertainty are less punished. Despite its probabilistic origin, the Gaussian KL loss has two defects when applied to a deep regression model:</p><p>? As a generalization form of L2 loss, the Gaussian KL loss is not robust to outliers; ? The gradient w.r.t. ? tends to increase as the denominator 2? 2 decays during training, whereas regular L2 or L1 losses have decreasing or constant gradient, leading to loss imbalance in multi-task learning.</p><p>Regarding the first problem, an alternative KL loss derived from Laplacian distribution is used in work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">18]</ref>:</p><formula xml:id="formula_5">L LapKL = ? 2 ? |? ? y| + log ?.<label>(5)</label></formula><p>Same as the L1 loss, this function is not differentiable at ? = y. To overcome this issue, we design a mixed KL loss, written as a function of weighted error e = (? ? y)/? and standard deviation ?:</p><formula xml:id="formula_6">L mKL = ? ? ? 1 2 e 2 + log ?, |e| ? ? 2, ? 2|e| ? 1 + log ?, |e| &gt; ? 2.<label>(6)</label></formula><p>It can be verified that this function is differentiable w.r.t. both ? and ? (on condition that ? &gt; 0). The mixed KL loss can be regarded as an extension of Huber loss (smooth L1), which is robust to outliers and easier to optimize. The second problem is caused by the increasing weight 1/? as the denominator ? decays during training. This can be mitigated by normalizing the weight such that its batch-wise mean equals unity. Inspired by Batch Normalization <ref type="bibr" target="#b17">[17]</ref>, we perform online estimate of the mean weight w = E[1/?] using exponential moving average:</p><formula xml:id="formula_7">w ? ?? + (1 ? ?) 1 N N i=1 1 ? i ,<label>(7)</label></formula><p>where ? is the momentum, N is the number of samples in a batch. The finalized Robust KL loss is simply the weightnormalized mixed KL loss:</p><formula xml:id="formula_8">L RKL = 1 w L mKL .<label>(8)</label></formula><p>In practice, directly optimizing ? can lead to gradient explosion. Therefore, we let the network predict the logarithmic standard deviation log ? as an alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Uncertainty-Driven PnP</head><p>Maximum Likelihood Estimation To solve PnP with uncertainty is to perform the maximum likelihood estimation (MLE) of pose p, in which the negative log likelihood (NLL) function is the sum of squared reprojection error r (u,v) measured in Mahalanobis distance:</p><formula xml:id="formula_9">p * = arg min p 1 2 (u,v)?RoI r T (u,v) ? ?1 (u,v) r (u,v) (9) with ? (u,v) = diag[? 2 urp , ? 2 vrp ] (u,v)</formula><p>, where ? urp , ? vrp are the predicted standard deviations of the reprojected coordinates. This minimization can be solved efficiently using the Levenberg-Marquardt algorithm. Covariance Estimation For probabilistic object localization, we also need to estimate the covariance of p * , which can be approximated by the inverse Hessian of the NLL at p * <ref type="bibr" target="#b29">[29]</ref>:</p><formula xml:id="formula_10">Cov[p * ] ? H(p * ) ?1 .<label>(10)</label></formula><p>To avoid computing the second derivative during inference, we approximate the exact Hessian using the Gauss-Newton matrix J(p * ) T J(p * ), with J(p * ) = ?r all /?p T p * , where r all = r 1 /? 1 r 2 /? 2 . . . r 2n /? 2n T (flattened vector of all weighted reprojection errors).</p><p>Online Covariance Calibration In practice, using Eq. (10) can result in a much lower covariance than the actual one. This is mainly because Eq. (9) assumes that the reprojection error of each point is independent, whereas the network outputs are apparently correlated. Therefore, we further conduct online covariance calibration using a 4?1 learnable calibration vector k:</p><formula xml:id="formula_11">? p * = exp(diag k) J(p * ) T J(p * ) ?1 exp(diag k). (11)</formula><p>The calibration vector can be learned by applying the multivariate Gaussian KL loss:</p><formula xml:id="formula_12">L calib = 1 2 (p * ?p gt ) T ? ?1 p * (p * ?p gt )+ 1 2 log det ? p * ,<label>(12)</label></formula><p>where p * is detached and only the calibration vector is optimized. Despite the defects of Gaussian KL loss stated in Section 3.3, it is sufficient for the simple calibration task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Scoring Head</head><p>The confidence score of a detected object, denoted by c, can be decomposed into localization score Pr[Fg] (the probability of detecting a foreground object) and classification score Pr[Cls] (the probability of predicting the correct class label). For 3D object detection, the score can be expressed as the product of 3D localization conditional probability and 2D score:</p><formula xml:id="formula_13">c 3D = Pr[Fg 3D |Fg 2D ] c 3DLoc Pr[Fg 2D ] Pr[Cls] c 2D .<label>(13)</label></formula><p>The 2D score c 2D is given by the 2D detection module. Thus, we only need to predict the 3D localization score c 3DLoc . Since the 3D branch is trained only with positive 2D samples, the predicted c 3DLoc is naturally conditional. By sampling object poses from the estimated distribution N (p * , Cov[p * ]), the 3D localization score can be computed via Monte Carlo integration w.r.t. 3D IoU, as we show in the supplementary material. Due to slow 3D IoU calculation, Monte Carlo scoring has adverse effects on inference time. Therefore, we practically adopt the multi-layer perceptron (MLP) approach <ref type="figure">(Fig. 3)</ref>, which is faster, end-toend trainable, and capable of fusing both pose uncertainty and network features to predict a more reliable score. To train the MLP scoring branch, we use the same binary crossentropy loss as in <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref>:</p><formula xml:id="formula_14">L score = ?c tgt log c 3DLoc ? (1 ? c tgt ) log(1 ? c 3DLoc ),<label>(14)</label></formula><p>where c 3DLoc is the output of MLP, which is bounded to 0 1 by logistic activation function, c tgt is a clamped linear function w.r.t. the 3D IoU between prediction and ground truth:</p><formula xml:id="formula_15">c tgt = max(0, min(1, 2IoU 3D ? 0.5)).<label>(15)</label></formula><p>The performance comparison between Monte Carlo and MLP is presented in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Network Training</head><p>The proposed MonoRUn network can be trained with three different setups, which are compared in the experiments. Fully Self-Supervised Reconstruction (Without Extra Supervision) In this mode, neither the LiDAR point supervision nor the end-to-end PnP is used. The 3D reconstruction process is trained in a fully self-supervised manner, except that 3D dimensions are directly supervised. The overall loss function is formulated as:</p><formula xml:id="formula_16">L self = L 2D + L proj + L dim + L score + ?L calib ,<label>(16)</label></formula><p>where L 2D is the 2D detection loss, L proj is a Robust KL loss on self-supervised reprojection error, L dim is a smooth L1 loss on dimension error, and ? is a hyperparameter for calibration loss, which is set to 0.01. LiDAR Supervision Direct NOC loss can be imposed by converting foreground LiDAR points into sparse ground truth of NOC map. In this case, the aleatoric uncertainty is unnecessary. Thus, we adopt the weighted smooth L1 loss:</p><formula xml:id="formula_17">L NOC = 1 i w i i w i L SmoothL1 (x NOC i,pred ? x NOC i,LiDAR ),<label>(17)</label></formula><p>where x NOC i denotes the i-th element of the NOC tensor, w i equals 1 where x NOC i,LiDAR is available and 0 elsewhere. The overall loss becomes:</p><formula xml:id="formula_18">L LiDAR = L self + L NOC .<label>(18)</label></formula><p>Without specific statement, we train all the models using this setup. End-to-End Training Incorporating the PnP backpropagation approach in <ref type="bibr" target="#b4">[5]</ref>, we apply smooth L1 loss on the Euclidean errors of estimated translation vector and yaw angle. Details of the PnP derivatives and loss functions are presented in the supplementary materials. Since end-to-end training is unstable at the beginning, we use a similar training protocol to <ref type="bibr" target="#b0">[1]</ref>, i.e., applying end-to-end training as refinement after self-supervised training. This setup is only investigated in ablation studies for pure comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate the proposed model on the KITTI-Object benchmark <ref type="bibr" target="#b9">[10]</ref>. It consists of 7481 training images and 7518 test images as well as the corresponding point clouds, comprising a total of 80256 labeled objects in eight classes. Each object is assigned to one of three difficulty levels according to truncation, occlusion and 2D box height. The training images are further split into 3712 training and 3769 validation images <ref type="bibr" target="#b5">[6]</ref>. The official benchmark evaluates detection performance on three classes: Car, Pedestrian and Cyclist. Evaluation metrics are based on precision-recall curves with 3D IoU threshold of 0.7 or 0.5. We adopt the official metric that computes the 40-point interpolated average precision (AP) <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>2D Detector We use pre-trained Faster R-CNN <ref type="bibr" target="#b30">[30]</ref> with ResNet-101 <ref type="bibr" target="#b11">[12]</ref> as backbone. We adopt a six-level feature pyramid network <ref type="bibr" target="#b23">[23]</ref>, in which an additional upsampled level is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Module</head><p>We set the dropout rate to 0.5 for 1D dropout layers and 0.2 for channel dropout layers. Network outputs (dimensions, NOCs) are normalized w.r.t. the mean and standard deviation calculated from training data. When training with multiple classes, we predict a set of class-specific latent vectors, dimensions and NOCs.</p><p>Data Augmentation During training, we apply random flip and photometric distortion augmentation. We set two NOC decoder branches at the last 1?1 convolutional layer for original and mirrored objects respectively.</p><p>Training Schedule The network is trained by the AdamW <ref type="bibr" target="#b24">[24]</ref> optimizer with a weight decay of 0.01. We take a batch size of 6 on two Nvidia RTX 2080 Ti GPUs, and train the network using cosine learning rate decay with a base learning rate of 0.0002. Total epochs are 32 for the full training set and 50 for the split training set. For end-toend training, we append a second cycle of 15 epochs with a reduced base learning rate of 0.00003.</p><p>Testing Configuration For epistemic uncertainty, we use 50 Monte Carlo dropout samples as in <ref type="bibr" target="#b18">[18]</ref>. By default, we only sample the global extractor to estimate the dimension uncertainty (see discussions in Section 4.4). During postprocessing, we use 3D non-maximum suppression (NMS) with an IoU threshold of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the State of the Art</head><p>We evaluate two variations of our model (fully selfsupervised and plus LiDAR supervision) on the official test set and the validation split. <ref type="table">Table 1</ref> lists the top methods from the official leaderboard. We observe that: (1) When trained with LiDAR supervision, our method outperforms the state of the art by a wide margin. Note that the top three competitors also use extra supervision and even extra data (by using depth estimators pre-trained on the much larger KITTI-Depth dataset). (2) When trained without extra supervision, our method still outperforms the nondepth methods (RTM3D <ref type="bibr" target="#b21">[21]</ref>, MonoPair <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>In this section, all models are trained and evaluated with the train/val split. We show the mean value of all six AP metrics (mAP) for 3D Car detection. All results are presented in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervision versus LiDAR Supervision</head><p>While self-supervision alone can achieve the state-of-the-art performance (28.57), using only LiDAR supervision leads to very poor performance (18.84), which demonstrates the importance of self-supervision. Nonetheless, the overall best result is reached with both supervisions together (31.21). As revealed in <ref type="figure" target="#fig_0">Fig. 5b</ref>, the self-supervised geometry does not necessarily match the true surface and is therefore prone to overfitting, which can be alleviated by the shaperegularizing effect from LiDAR supervision.  <ref type="table">Table 3</ref>. Results of ablation studies on reprojection loss function, LiDAR supervision, end-to-end training, epistemic uncertainty and latent vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust KL Loss</head><p>Laplacian mixture and weight normalization make important contributions to the Robust KL loss, totaling an mAP increase of 1.74 compared to the Laplacian KL loss.</p><p>End-to-End Refinement We observe that the performance of end-to-end refinement is strongly related to the baseline performance. For the baseline trained with Laplacian KL loss (29.47), end-to-end refinement boosts the mAP by 0.26 <ref type="bibr">(29.73)</ref>. For the baseline trained with Robust KL loss (31.21), however, end-to-end refinement slightly worsens the performance (31.09). This validates that selfsupervised training with Robust KL loss can better optimize the network than end-to-end training via differentiable-PnP.</p><p>Epistemic Uncertainty Estimating the epistemic uncertainty of box dimensions alone shows improvement to the baseline (31.47 vs 31.21). When sampling the full reconstruction network, we observe adverse effect on the detection performance <ref type="bibr">(31.16)</ref>.</p><p>Latent vector When disabling the latent vector, the performance drops significantly (29.78 vs 31.21). To find out how the latent vector impacts the network performance, we evaluate the network sensitivity to the latent vector by reset-(a) 3D bounding boxes in images and bird's-eye views (BEV). Red indicates detected boxes (along with 95% confidence ellipse), ground truth boxes are color coded by their occlusion levels: fully visible, partly occluded, difficult to see.</p><p>(b) The image patches and their corresponding reconstruction results. Note that LiDAR helps regularizing the shape, which alleviates overfitting. Future work may explore other shape regularization techniques, e.g., using prior models.   <ref type="table">Table 4</ref>. Performance sensitivity to the latent vector.</p><p>ting it to zero during inferring. As shown in <ref type="table">Table 4</ref>, only the network with aleatoric uncertainty is sensitive to the latent vector. This implies that the latent vector encodes very important information for the estimation of aleatoric uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Reliability of the Localization Uncertainty</head><p>As the online covariance calibration is conducted on the training data, the model tends to be overconfident on the testing data. To assess the reliability of the localization uncertainty (w.r.t. the translation vector t), we discretize the ground truth object distance t z into a number of bins. For the samples in each bin, we calculate the mean covariance matrix of prediction (? t * ), as well as the covariance matrix of the actual localization error (Cov[t * ? t gt ]). Ideally, these two covariance matrices should be equal. For comparison, we calculate the Gaussian entropy of the two covariance matrices respectively:</p><formula xml:id="formula_19">H pred = 1 2 log det 2?e? t * ,<label>(19)</label></formula><formula xml:id="formula_20">H actual = 1 2 log det(2?e Cov[t * ? t gt ]).<label>(20)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Fig. 6</ref>, on the train split, the predicted uncertainty is very close to the actual error. This demonstrates the effectiveness of covariance calibration. For the unseen validation data, since epistemic sampling does not cover the full network, the model generally predicts overconfident results,  which can be roughly corrected by applying an empirical covariance scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented the MonoRUn framework, a novel monocular 3D object detector with state-of-the-art performance and high practicality.</p><p>To employ dense correspondence method for 3D detection in real driving scenes, we overcame the deficiency of geometry supervision by self-supervised reconstruction with uncertainty awareness. Meanwhile, we made uncertainty-aware deep regression networks easier to optimize by proposing the Robust KL loss. Finally, we are among the first to explore probabilistic 3D object localization by uncertainty propagation through PnP, which may open up new possibilities for downstream tasks such as robust tracking and motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head><p>As mentioned in the main paper, this supplementary material discusses the epistemic uncertainty, end-to-end training and Monte Carlo scoring in detail. Apart from that, it also provides the complete evaluation results on the official KITTI benchmark, including precision-recall plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Details on Epistemic Uncertainty</head><p>For the estimation of epistemic uncertainty, we adopt the Monte Carlo dropout approach described in <ref type="bibr" target="#b18">[18]</ref>. By sampling the reconstruction network, we can estimate the epistemic uncertainty of object coordinates x OC . The predictive mean and variance are approximated by:</p><formula xml:id="formula_21">E[x OC ] ? x OC = 1 N MC NMC i x OC i ,<label>(21)</label></formula><p>Var</p><formula xml:id="formula_22">[x OC ] ? 1 N MC ? 1 NMC i (x OC i ? x OC ) 2 ,<label>(22)</label></formula><p>where x OC i is the output of the i-th sampled network, N MC is the number of Monte Carlo samples (set to 50).</p><p>Next, we need to transform the variances of 3D object coordinates into the variances of 2D reprojected coordinates. Strict 3D-2D variance projection requires knowing the object pose in advance, which is unavailable before the PnP module. Thus, we use the following approximation:</p><formula xml:id="formula_23">? ? ? Var[u norm rp ] ? 1 2 (Var[x OC ] + Var[z OC ]), Var[v norm rp ] ? Var[y OC ],<label>(23a)</label></formula><p>where u norm rp , v norm rp are the normalized reprojected coordinates (invariant to depth). Note that this approximation does not take object orientation into consideration, thus Eq. 23a simply averages the horizontal variances.</p><p>Finally, the reconstruction module outputs the combined uncertainty of reprojected 2D coordinates:</p><formula xml:id="formula_25">? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 u norm rp ,comb ? 1 N MC NMC i ? 2 u norm rp ,i + Var[u norm rp ], ? 2 v norm rp ,comb ? 1 N MC NMC i ? 2 v norm rp ,i + Var[v norm rp ],<label>(24a)</label></formula><p>where ? 2 u norm rp ,i , ? 2 v norm rp ,i is aleatoric uncertainty predicted by the i-th sampled network. It is to be observed that all the 2D variances above are in the normalized scale. The actual variances should be further multiplied by a scale factor (f /t z ) 2 , where f is camera focal length in pixels and t z is the z-component of object pose. Since t z is unknown beforehand, we only apply this factor to the final pose covariance as a correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Details on End-to-End Training</head><p>End-to-end training is only investigated in the ablation studies, and is not included in our final training setup. Nevertheless, here we elaborate on the details of differentiable PnP and end-to-end training loss.</p><p>Regarding differentiable PnP, we generally follow the approach in BPnP <ref type="bibr" target="#b4">[5]</ref>, with the code completely reimplemented for higher efficiency and uncertainty awareness. The details are as follows.</p><p>Differentiating the PnP Algorithm The derivative of the PnP result p * is as follows:</p><formula xml:id="formula_27">?p * ?(?) T = ?H ?1 ?J T r all ?(?) T p * ,<label>(25)</label></formula><p>where (?) stands for PnP inputs x OC and ?.</p><p>Proof. Recall the MLE of object pose:</p><formula xml:id="formula_28">p * = arg min p 1 2 (u,v)?RoI r T (u,v) ? ?1 (u,v) r (u,v) = arg min p 1 2 r T all r all .<label>(26)</label></formula><p>The gradient of the NLL function ( 1 2 r T all r all ) w.r.t. p is as follows: g = J T r all <ref type="bibr" target="#b27">(27)</ref> with J = ?r all ?p T . When the optimization (Eq. 26) converges to p * , the gradient always satisfies</p><formula xml:id="formula_29">g p * = 0.<label>(28)</label></formula><p>Therefore, the total derivative of g w.r.t. any PnP input equals zero:</p><formula xml:id="formula_30">Dg p * D(?) T = ?g p * ?p * T ?p * ?(?) T + ?g p * ?(?) T = 0,<label>(29)</label></formula><p>which implies that</p><formula xml:id="formula_31">?p * ?(?) T = ? ?g p * ?p * T ?1 ?g p * ?(?) T = ?H ?1 ?J T r all ?(?) T p * .<label>(30)</label></formula><p>Implementation Details of PnP The PnP forward process is implemented with Ceres Solver on CPU, while the backward process is implemented with the Autograd package of PyTorch on GPU. With our efficient implementation, the backward overhead of computing the exact second derivatives is negligible for training.</p><p>End-to-End Training Loss Leveraging the differentiable PnP, we apply smooth L1 loss on the Euclidean errors of estimated translation vector t * and yaw angle ? * :</p><formula xml:id="formula_32">L trans = L SmoothL1 ( t * ? t gt ),<label>(31)</label></formula><formula xml:id="formula_33">L rot = L SmoothL1 cos ? * sin ? * ? cos ? gt sin ? gt .<label>(32)</label></formula><p>The overall loss for end-to-end training is:</p><formula xml:id="formula_34">L e2e =L 2D + L dim + L score + ?L calib + L trans + L rot + L NOC ,<label>(33)</label></formula><p>where the reprojection loss is replaced by translation and rotation losses, and the NOC loss is added as regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Details on Monte Carlo Scoring</head><p>By sampling p i from the distribution N (p * , ? p * ), the 3D localization score can be computed using Monte Carlo integration:</p><formula xml:id="formula_35">c 3DL = 1 N MC NMC i=1 f IoU 3D p * d , p i d ,<label>(34)</label></formula><p>where vectors in the form p d T represent 3D boxes used for computing 3D IoU; f (?) is technically a step function with a hard IoU threshold, which is 1 for IoU 3D ? IoU threshold and 0 otherwise. In practice, we use the clamped linear function:</p><p>f (IoU 3D ) = max(0, min(1, 2IoU 3D ? 0.5)). <ref type="table" target="#tab_6">Table 5</ref> illustrates the performance of Monte Carlo and MLP scoring methods, along with the baseline of using only 2D detection score. We observe that, although Monte Carlo scoring proves effective compared to the baseline, its performance is still much lower than the MLP scoring network. This validates that fusing both pose uncertainty and network feature can produce a more reliable confidence score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Complete Evaluation Results on the Official KITTI Benchmark</head><p>The official KITTI-Object benchmark consists of four tasks (2D detection, orientation similarity, 3D detection, BEV detection) on three classes (Car, Pedestrian, Cyclist).</p><p>All metrics are based on the precision-recall curves given the IoU threshold or rotation threshold. We tested two variants of our model: with LiDAR supervision and without Li-DAR supervision. The results are shown in <ref type="table">Tables 6 and 7  and Figures 7</ref>    <ref type="figure">Figure 9</ref>. Cyclist, with LiDAR supervision.  <ref type="figure">Figure 10</ref>. Car, without LiDAR supervision (Fully self-supervised reconstruction).  <ref type="figure">Figure 11</ref>. Pedestrian, without LiDAR supervision (Fully selfsupervised reconstruction).  <ref type="figure">Figure 12</ref>. Cyclist, without LiDAR supervision (Fully selfsupervised reconstruction).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of detection and reconstruction results on the KITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Gaussian entropy of predicted and actual covariance matrices vs object distance, tested on the Car category of train and val splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>BEV detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Reprojection Self-Supervised Training Uncertainty-Aware Reconstruction Testing with Uncertainty PropagationSelf-Supervised Reconstruction Network</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Global Extractor</cell><cell></cell></row><row><cell>RGB image</cell><cell>feature pyramid</cell><cell>RoI features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>&amp; 2D boxes</cell><cell>256?7?7</cell><cell cols="2">latent vector</cell><cell cols="2">dimensions</cell><cell>Scoring Head</cell><cell>score</cell></row><row><cell></cell><cell>RoI Align</cell><cell>RoI features 256?14?14</cell><cell>NOC Decoder</cell><cell>NOCs 3?28?28</cell><cell></cell><cell>OCs 3?28?28</cell><cell>Uncertainty PnP</cell><cell>pose</cell></row><row><cell cols="2">2D Detector</cell><cell></cell><cell>KL Loss Robust</cell><cell></cell><cell></cell><cell></cell><cell>Tensor Vector</cell><cell>Without uncertainty Input or ground truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>With uncertainty</cell></row><row><cell cols="2">NOC: normalized object coordinate OC: object coordinate</cell><cell>RoI 2D crds. 2?28?28</cell><cell cols="3">reprojected 2D crds. 2?28?28</cell><cell>pose</cell><cell>Simple propagation Uncertainty propagation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>7]) on the official test set. (3) Our method achieves state-of-the-art accuracy within a reasonable runtime (0.070 s, including Monte Carlo and PnP), whereas the top three competitors spend Method Test AP IoU ?0.7Val AP IoU ?0.<ref type="bibr" target="#b4">5</ref> Val AP IoU ?0.7 3D detection performance of Car category on KITTI official test set and validation set. The 1 st , 2 nd and 3 rd place are color coded. * indicates using the pre-trained depth estimator DORN<ref type="bibr" target="#b8">[9]</ref>. Wang et al.<ref type="bibr" target="#b38">[38]</ref> pointed out that the training data of DORN overlaps with KITTI-Object validation data, causing the 3D detectors to overfit. We use gray to indicate the values affected by overfitting. 3D detection performance of Pedestrian and Cyclist categories on KITTI official test set. Red indicates the best.</figDesc><table><row><cell>Time</cell></row><row><cell>(sec)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>By comparing the performance of smooth L1 loss (26.35), Laplacian KL loss (29.47), mixed KL loss (30.05) and Robust KL loss (31.21), we observe incremental improvements. The largest performance gap is between L SmoothL1 and L LapKL , which reveals the contribution of aleatoric uncertainty. Moreover, both Gaussian-</figDesc><table><row><cell>L proj (Self)</cell><cell>L NOC (LiDAR)</cell><cell>E2E Epistemic</cell><cell>Latent vector</cell><cell>mAP</cell></row><row><cell>L RKL</cell><cell></cell><cell></cell><cell></cell><cell>31.21</cell></row><row><cell>L RKL</cell><cell></cell><cell></cell><cell></cell><cell>28.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>18.84</cell></row><row><cell>L SmoothL1</cell><cell></cell><cell></cell><cell></cell><cell>26.35</cell></row><row><cell>L LapKL</cell><cell></cell><cell></cell><cell></cell><cell>29.47</cell></row><row><cell>L mKL</cell><cell></cell><cell></cell><cell></cell><cell>30.05</cell></row><row><cell>L LapKL</cell><cell></cell><cell></cell><cell></cell><cell>29.73</cell></row><row><cell>L RKL</cell><cell></cell><cell></cell><cell></cell><cell>31.09</cell></row><row><cell>L RKL</cell><cell></cell><cell>dim</cell><cell></cell><cell>31.47</cell></row><row><cell>L RKL</cell><cell></cell><cell>full</cell><cell></cell><cell>31.16</cell></row><row><cell>L RKL</cell><cell></cell><cell></cell><cell></cell><cell>29.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison between different scoring methods, based on the evaluation results on KITTI validation set.</figDesc><table><row><cell>Scoring Method</cell><cell>mAP</cell></row><row><cell>2D score only</cell><cell>25.40</cell></row><row><cell>Monte Carlo</cell><cell>28.19</cell></row><row><cell>MLP</cell><cell>31.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>to 12.</figDesc><table><row><cell>Benchmark</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>Car (2D det.)</cell><cell>95.48</cell><cell>87.91</cell><cell>78.10</cell></row><row><cell>Car (orientation)</cell><cell>95.44</cell><cell>87.64</cell><cell>77.75</cell></row><row><cell>Car (3D det.)</cell><cell>19.65</cell><cell>12.30</cell><cell>10.58</cell></row><row><cell>Car (BEV det.)</cell><cell>27.94</cell><cell>17.34</cell><cell>15.24</cell></row><row><cell>Ped. (2D det.)</cell><cell>73.05</cell><cell>56.40</cell><cell>51.40</cell></row><row><cell>Ped. (orientation)</cell><cell>63.28</cell><cell>47.82</cell><cell>43.23</cell></row><row><cell>Ped. (3D det.)</cell><cell>10.88</cell><cell>6.78</cell><cell>5.83</cell></row><row><cell>Ped. (BEV det.)</cell><cell>11.70</cell><cell>7.59</cell><cell>6.34</cell></row><row><cell>Cyclist (2D det.)</cell><cell>67.47</cell><cell>49.13</cell><cell>43.41</cell></row><row><cell>Cyclist (orientation)</cell><cell>49.04</cell><cell>34.36</cell><cell>30.22</cell></row><row><cell>Cyclist (3D det.)</cell><cell>1.01</cell><cell>0.61</cell><cell>0.48</cell></row><row><cell>Cyclist (BEV det.)</cell><cell>1.14</cell><cell>0.73</cell><cell>0.66</cell></row><row><cell cols="3">Table 6. With LiDAR supervision.</cell><cell></cell></row><row><cell>Benchmark</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>Car (2D det.)</cell><cell>95.65</cell><cell>87.76</cell><cell>80.12</cell></row><row><cell>Car (orientation)</cell><cell>95.48</cell><cell>87.33</cell><cell>79.51</cell></row><row><cell>Car (3D det.)</cell><cell>16.04</cell><cell>10.53</cell><cell>9.11</cell></row><row><cell>Car (BEV det.)</cell><cell>24.02</cell><cell>15.98</cell><cell>13.52</cell></row><row><cell>Ped. (2D det.)</cell><cell>71.27</cell><cell>55.80</cell><cell>49.47</cell></row><row><cell>Ped. (orientation)</cell><cell>60.13</cell><cell>46.00</cell><cell>40.61</cell></row><row><cell>Ped. (3D det.)</cell><cell>11.18</cell><cell>6.53</cell><cell>5.73</cell></row><row><cell>Ped. (BEV det.)</cell><cell>12.03</cell><cell>7.27</cell><cell>6.20</cell></row><row><cell>Cyclist (2D det.)</cell><cell>68.85</cell><cell>50.32</cell><cell>44.36</cell></row><row><cell>Cyclist (orientation)</cell><cell>37.29</cell><cell>27.95</cell><cell>25.27</cell></row><row><cell>Cyclist (3D det.)</cell><cell>0.69</cell><cell>0.38</cell><cell>0.40</cell></row><row><cell>Cyclist (BEV det.)</cell><cell>0.94</cell><cell>0.55</cell><cell>0.42</cell></row><row><cell cols="4">Table 7. Without LiDAR supervision (Fully self-supervised recon-</cell></row><row><cell>struction).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning less is more -6d camera localization via 3d surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1827" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Ordinal Regression Network for Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11859" to="11868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7677" to="7686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Wanli Ouyang, and Xin Fan</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudi</forename><surname>Pawitan</surname></persName>
		</author>
		<title level="m">All Likelihood: Statistical Modelling and Inference Using Likelihood</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10526" to="10535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self6d: Selfsupervised monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Carafe: Content-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3007" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Task-aware monocular depth estimation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
