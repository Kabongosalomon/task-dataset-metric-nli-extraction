<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Quality-aware Dynamic Memory for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
							<email>yang.yujiu@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Quality-aware Dynamic Memory for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>video object segmentation, memory bank</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, several spatial-temporal memory-based methods have verified that storing intermediate frames and their masks as memory are helpful to segment target objects in videos. However, they mainly focus on better matching between the current frame and the memory frames without explicitly paying attention to the quality of the memory. Therefore, frames with poor segmentation masks are prone to be memorized, which leads to a segmentation mask error accumulation problem and further affect the segmentation performance. In addition, the linear increase of memory frames with the growth of frame number also limits the ability of the models to handle long videos. To this end, we propose a Quality-aware Dynamic Memory Network (QDMN) to evaluate the segmentation quality of each frame, allowing the memory bank to selectively store accurately segmented frames to prevent the error accumulation problem. Then, we combine the segmentation quality with temporal consistency to dynamically update the memory bank to improve the practicability of the models. Without any bells and whistles, our QDMN achieves new state-of-the-art performance on both DAVIS and YouTube-VOS benchmarks. Moreover, extensive experiments demonstrate that the proposed Quality Assessment Module (QAM) can be applied to memory-based methods as generic plugins and significantly improves performance. Our source code is available at https://github.com/workforai/QDMN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a video and the first frame's annotations of single or multiple objects, semi-supervised video object segmentation (Semi-VOS or One-shot VOS) aims at segmenting these objects in subsequent frames. Semi-VOS is one of the most challenging tasks in computer vision with many potential applications, including interactive video editing, augmented reality, and autonomous driving. <ref type="figure">Fig. 1</ref>: Visual comparison of memory frames of different qualities. The first row shows the memory frames of MiVOS <ref type="bibr" target="#b5">[6]</ref>. The second row shows the memory frames of our method. The yellow box area illustrates the error accumulation.</p><p>Unlike other segmentation tasks that aim to look for the relationship between features and specific categories, the critical problem of Semi-VOS lies in how to make full use of the spatial-temporal information to recognize the target objects. Consequently, the methods that perform matching with historical reference frames have received tremendous attention in recent years. Some works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref> utilize the first frame and the previous adjacent frame as references. Due to limited reference information, these approaches tend to fail miserably under challenging scenarios, e.g., the target objects disappear for a while or are drastically deformed. To excavate more information, the Space-Time Memory Network (STM) <ref type="bibr" target="#b28">[29]</ref> utilizes a memory network to memorize intermediate frames and their segmentation masks as references, which has been proved effective and has served as the current mainstream framework. Many approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7]</ref> further develop the feature extraction and memory readout process of STM and have achieved excellent performance.</p><p>However, these methods mainly focus on optimizing the matching process while ignoring the impact of the matching target, i.e., memory bank, on the segmentation results. Specifically, previous methods select memory frames in a straightforward way, i.e., storing at fixed frame intervals. This approach has two weaknesses: <ref type="bibr" target="#b0">(1)</ref> Frames with poor segmentation results may be memorized and provide an erroneous reference for subsequent frames, which leads to an error accumulation problem. As shown in the first row of <ref type="figure">Fig. 1</ref>, if there are inaccurately segmented masks in the memory bank, the segmentation quality of subsequent frames will be greatly degraded. Such an observation inspires us to pay more attention to the design of the memory bank. Since the matching-based approaches rely on a memory bank to identify the target objects, the memory bank's quality (especially the correctness) is very important. <ref type="bibr" target="#b1">(2)</ref> In existing methods, the size of the memory bank would infinitely expand with the growth of frame number, which makes the models incapable of handling long videos and greatly limits their practicality.</p><p>Therefore, the way of designing the memory bank is a significant issue for spatial-temporal memory-based methods. Generally speaking, we believe that the design of the memory bank should meet the following principles: (1) Accu-racy: In a one-shot scenario, the memory bank should be composed of the annotated frame and frames that are segmented as accurately as possible to obtain correct supervision information. (2) Temporal consistency: Considering the continuity of motion, the state of objects in adjacent frames tends to be similar. In other words, the masks of adjacent frames are of great reference to the current frame. Based on these two principles, we can selectively store frames with more reference information as memory and dynamically update the memory bank to handle videos of arbitrary length.</p><p>To this end, we propose a Quality-aware Dynamic Memory Network (QDMN), which introduces a simple but effective structure called Quality Assessment Module (QAM) in this paper to evaluate each frame's segmentation result and decide whether a frame can be added to the memory bank as a reference. Being aware of the segmentation quality limits the impact of noise and provides the accuracy credentials for dynamically updating the memory bank. Besides, since the objects in adjacent frames share a similar status to the current target, we introduce a temporal regularization to penalize the outdated memory. Extensive experiments demonstrate that the dynamic updating strategy of the memory bank designed according to the principles of accuracy and temporal consistency is reasonable and effective. By designing a high-quality memory bank and introducing temporal consistency, our method achieves new state-of-the-art performance on both DAVIS <ref type="bibr" target="#b32">[33]</ref> and Youtube-VOS <ref type="bibr" target="#b46">[47]</ref> benchmark without any bells and whistles. Furthermore, we also verify that memory-based methods can gain significant improvement by simply applying our QAM as a generic plugin for video object segmentation tasks.</p><p>Our contributions can be summarized as follows. Firstly, we pinpoint the design of the memory bank as the Achilles heel of the Semi-VOS task and propose the strategy for designing a high-quality memory bank. Secondly, we present QDMN for Semi-VOS, which can selectively memorize high-quality frames and take advantage of the temporal consistency. Thirdly, QDMN can effectively control the number of memory frames to avoid memory explosion. Experiments show that our method surpasses the existing methods on both DAVIS and YouTube-VOS datasets. Furthermore, QAM can be used as a generic plugin to improve memory-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Propagation-based Methods. Propagation-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41]</ref> treat semi-supervised video object segmentation as a mask propagation task. MaskTrack <ref type="bibr" target="#b30">[31]</ref> concatenates the previous adjacent frame's segmentation mask with the current image as input and online fine-tunes the network. AGSS-VOS <ref type="bibr" target="#b23">[24]</ref> proposes an attention-guided decoder to combine the instance-specific branch and instance-agnostic branch. Based on mask confidence and mask concentration, SAT <ref type="bibr" target="#b2">[3]</ref> selectively propagates the entire image or local region to the next frame. The propagation-based method takes advantage of the strong prior provided by the previous adjacent frame. It can better deal with the appear-ance change of the target object, but it has fatal shortcomings in the problem of occlusion and error accumulation.</p><p>Detection-based Methods. Detection-based methods divide the Semi-VOS task into three subtasks: detection, tracking and segmentation. DyeNet <ref type="bibr" target="#b19">[20]</ref> utilizes RPN <ref type="bibr" target="#b33">[34]</ref> to generate proposals and applies the re-identification module to perform matching. PReMVOS <ref type="bibr" target="#b25">[26]</ref> uses Mask RCNN <ref type="bibr" target="#b11">[12]</ref> to obtain coarse masks and performs optical flow, re-identification to achieve good performance. Huang et al <ref type="bibr" target="#b15">[16]</ref> and Sun et al <ref type="bibr" target="#b37">[38]</ref> integrate segmentation into tracking with a dynamic template bank. Detection-based methods rely heavily on the detectors, which dramatically limits the performance of such methods.</p><p>Matching-based Methods. Matching-based methods perform matching between reference frames and the current frame to identify target objects, which has raised great attention for excellent performance and robustness. PML <ref type="bibr" target="#b3">[4]</ref> proposes a pixel-level embedding network with the nearest neighbor classifier. FEELVOS <ref type="bibr" target="#b39">[40]</ref> and CFBI <ref type="bibr" target="#b49">[50]</ref> perform global and local matching with the first frame and the previous adjacent frame, respectively. AOT <ref type="bibr" target="#b50">[51]</ref> associates multiple target objects into the same embedding space by employing an identification mechanism. STM <ref type="bibr" target="#b28">[29]</ref> leverages the memory network to memorize intermediate frames as references, which has been proved effective and has served as the current mainstream framework. Based on STM, KMN <ref type="bibr" target="#b34">[35]</ref> and RMNet <ref type="bibr" target="#b45">[46]</ref> propose to perform local-to-local matching instead of non-local. SwiftNet <ref type="bibr" target="#b41">[42]</ref> and AFB-URR <ref type="bibr" target="#b22">[23]</ref> reduce memory duplication redundancy by calculating the similarity between query and memory. LCM <ref type="bibr" target="#b13">[14]</ref> emphasizes the importance of the first frame and the previous adjacent frame. STCN <ref type="bibr" target="#b6">[7]</ref> improves the feature extraction and performs reasonable matching by decoupling the image and masks. Following the memory-based idea, there are still many variants of STM, such as JOINT <ref type="bibr" target="#b26">[27]</ref>, EGMN <ref type="bibr" target="#b24">[25]</ref>, MiVOS <ref type="bibr" target="#b5">[6]</ref>, DMN-AOA <ref type="bibr" target="#b21">[22]</ref>, HMMN <ref type="bibr" target="#b35">[36]</ref>, and so on.</p><p>Although these methods have achieved great performance, they mainly focus on better matching the current frame with the memory frames. In other words, previous works dedicate to optimizing the matching process while neglecting the importance of matching with the correct object. Besides, they do not take into account that the size of the memory bank grows linearly with the length of the video, which greatly impacts the application of the models in real scenarios due to the hardware memory limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overall architecture of our QDMN is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Similar to STM <ref type="bibr" target="#b28">[29]</ref>, during video processing, the current frame (t-th frame) is considered as the query, and the past reference frames with segmentation masks are considered as the memory. The query and memory are encoded into pairs of key and value maps through visual encoders and corresponding convolution layers. To highlight the temporal consistency of video, the query feature f t is first enhanced with the prior mask to obtain the enhanced feature f e . Then the enhanced feature is encoded into pairs of key K Q and value V Q through corresponding convolution layers. The Space-Time Memory Read block performs pixel-level matching between K Q and the memory key K M . The relative matching similarity is used to address the memory value V M , and the corresponding values are combined to the decoder for segmentation. Finally, the Quality Assessment Module (QAM) evaluates the quality of the segmentation result and decides whether the query frame can become a memory frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quality Assessment Module</head><p>Designing the memory bank is a significant issue for memory network-based methods. For existing strategy, frames with erroneous masks may be memorized, which leads to an error accumulation problem. To alleviate this problem and ensure the accuracy of the memory bank, inspired by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, we propose the Quality Assessment Module (QAM) to evaluate the segmentation quality and decide whether a frame can be added to the memory bank as a reference.</p><p>QAM is a simple structure but effective module composed of a score encoder, four convolution layers, and two MLP layers. It takes the query image I t and its segmentation mask M t as input and outputs the predicted quality scores. Since the feature extraction process of the score encoder Enc s is the same as that Score: 0.64 Score: 0.92 Score: 0.39 Score: 0.81 of the memory encoder Enc M (both takes images with segmentation masks as input), we directly use the memory encoder as the score encoder, which helps to save calculations and parameters. Specifically, the structure of the score encoder Enc s and the memory encoder Enc M is the same, and the parameters are shared. The QAM first takes the query image I t ? R 3?H?W and its segmentation mask M t ? R 1?H?W into the score encoder to obtain the score feature map f s ? R C?H/16?W/16 , where H?W are resolutions of the input image. Then, f s is input to the convolution layers and fully connected layers to learn the segmentation quality score S A t for the current frame. The process of segmentation quality assessment can be expressed as:</p><formula xml:id="formula_0">f s = Enc s (I t ? M t ); S A t = Fc(Conv (f s )),<label>(1)</label></formula><p>where ? denotes the concatenation operation. t is the index of the current frame.</p><p>Conv and Fc denote convolution and fully connected layers with sigmoid nonlinear function, respectively. During training, the target value of the quality score is defined as mask IoU between the segmentation mask and ground truth. The specific calculation process is as follows:</p><formula xml:id="formula_1">loss = 1 N N i=1 (S A i ? maskIoU (M i , GT i )) 2 ,<label>(2)</label></formula><p>where S A i represents the quality score of the segmentation result for i-th object, M i indicates the segmentation result, GT i is the ground truth. N indicates the total number of objects.</p><p>Since QAM evaluates the segmentation quality for each object individually, we take the average of all object scores in one frame as the quality score of this frame. In addition, considering that the segmentation difficulty varies for different video scenes, we normalize the quality scores of all frames in a video to better measure the relative quality of the segmentation results, which helps to memorize more helpful information under challenging scenarios. Specifically, the final quality score of each frame is its initial predicted score divided by the score of the first frame. Formally, the process can be expressed as:</p><formula xml:id="formula_2">S A t = 1 N N i=1 S A t? S A 1 ,<label>(3)</label></formula><p>where N represents the total number of objects in the t-th frame,S A t indicates the quality score of the segmentation result in frame t,S A 1 represents the quality score of the first frame. <ref type="figure" target="#fig_1">Figure 3</ref> shows some visualization results of the quality assessment, the first two columns are the same video, and the last two columns represent another video. We can observe that the driver is considered part of the car in the first column, which is a bad case. The pink zebra in the third column is not recognized, and the orange zebra is matched with similar background objects.</p><p>For the hard case, our QAM identifies these suboptimal results well, which shows that the segmentation accuracy of a frame is consistent with its quality score. Extensive experiments also verify this. With QAM, the memory bank can selectively memorize frames whose quality scores are higher than the memory threshold ?, that is, frames with accurate segmentation masks. In this way, even if a frame is poorly segmented owing to fast object motion or other factors, it will not affect the subsequent frames or cause error accumulation. for k in M emory.keys() do # k is the relative index of the frame in the memory bank 12: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamically Updated Memory Bank</head><formula xml:id="formula_3">S C k = exp(k ? j) 13: S R k = S A k + S C k 14: if S R k &lt; S R min<label>then</label></formula><formula xml:id="formula_4">t = t + 1 23: end while 1</formula><p>The infinite increase of the memory frames with the growth of frame number greatly limits the practicability of the model in the realworld scenario. Thus, it is necessary to limit the size of the memory bank and update it dynamically to adapt to new scenarios.</p><p>Due to the temporal consistency of video, the appearance of the target objects in adjacent frames is similar. The masks of adjacent frames are more instructive for the segmentation of current frame. Combining the above analysis and considering accuracy, we suggest dynamically updating the memory bank in accordance with these two principles (Algorithm.1). Specifically, when the memory bank reaches a certain storage limit, we will dynamically update the memory bank to handle different video scenes. For quantifying the temporal consistency and measuring the distance between each memory frame and the current frame, we compute the temporal consistency score S C as:</p><formula xml:id="formula_5">S C k = e ?|t?k| ,<label>(4)</label></formula><p>where k is the index of each memory frame, t is the index of the current frame. Based on the accuracy score S A and the temporal consistency score S C , the reference score of each memory frame in the memory bank can be calculated by S R k =S A k + S C k . By removing the memory frames with the lowest reference score, the memory bank is dynamically updated to handle different video scenarios and prevent the memory explosion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prior Enhancement Strategy</head><p>In addition to considering temporal consistency when designing a memory bank, we further utilize the prior provided by the previous adjacent frame to enhance temporal information. We adopt a similar module structure to SCM <ref type="bibr" target="#b53">[54]</ref> to introduce the prior information from the previous adjacent frame. Instead of introducing spatial constraint in the decoder like SCM, we utilize the prior information in the query encoding process to better learn the target object's appearance feature and avoid over-reliance on the prior information.</p><p>Specifically, in the query encoding process, the segmentation mask of the previous adjacent frame M t?1 ? R 1?H?W is downsampled and concatenated with the query's embedding f t ? R C?H/16?W/16 . Then the resultant feature goes through convolution and non-linear function to fuse information between channels, through which a prior feature map f p ? R 1?H/16?W/16 is produced. Finally, we perform an element-wise product between f p and f t to get the enhanced feature f e ? R C?H/16?W/16 . Formally, the process can be expressed as the following equation:</p><formula xml:id="formula_6">f e = Conv (f t ? M t?1 ) ? f t .<label>(5)</label></formula><p>Furthermore, we find that it is better to provide weak prior (mentioned above) than strong prior (masks of the previous frame have a great influence on the feature of the current frame). We found two primary reasons through experiments: the first one is that the prior information may be noisy, and providing a strong prior may lead to error accumulation; the second one is that providing strong prior makes the model overly dependent on it, which weakens its ability to extract features and identify objects. <ref type="table" target="#tab_6">Table 6</ref> shows the disadvantages of providing strong prior under challenging scenarios. In Section 5.3, we will describe the specific approach of providing strong prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Memory Read and Decoder</head><p>In the Space-Time Memory Read block <ref type="bibr" target="#b28">[29]</ref>, soft weights are first computed by measuring the similarities between query key K Q and memory key K M . Then the memory value V M is retrieved by a weighted summation with the soft weights and concatenated with query value V Q to get the output y. This operation can be summarized as:</p><formula xml:id="formula_7">y i = V Q i ? 1 Z ?j D(K Q i , K M j )V M j ,<label>(6)</label></formula><p>where i and j are the index of the query and the memory location, Z = ?j D(K Q i , K M j ) is the normalizing factor. D denotes the similarity measure (in our experiment is dot product).</p><p>Our decoder stays close to that of <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b28">29]</ref>. The decoder takes the output y of the Space-Time Memory Read block as input and predicts the object masks. It consists of an ASPP layer <ref type="bibr" target="#b1">[2]</ref>, a residual block, and two upsample blocks that upscale the feature map to the initial image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>Following the training strategy in MiVOS <ref type="bibr" target="#b5">[6]</ref>, we first pretrain our model on static image datasets <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref> and then perform main training on YouTube-VOS and DAVIS datasets. Besides, we also experiment with the synthetic dataset BL30K proposed in MiVOS, which is not used unless otherwise specified. During pretraining, each image is expanded into a pseudo video of three frames by random affine, horizontal flip, color and brightness augmentation. We randomly pick three frames in chronological order (with a ground-truth mask for the first frame) from a video to form a training sample in the main training. The range of random sampling varies with the training process. In the intermediate period of training, the sampling range is set larger to improve the robustness of the model, while at the end of the training, it is set smaller to narrow the gap between training and inference. Our models are trained end-to-end with two 32GB Tesla V100 GPUs with the Adam optimizer in PyTorch. The batch size is set to 28 during pretraining and 16 during main training. We adopt ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as backbone for all encoders. Bootstrapped cross-entropy loss <ref type="bibr" target="#b5">[6]</ref> is used for segmentation, and MSE loss is used for quality score evaluation. The initial learning rate is 2e-5. During inference, we choose the memory threshold ? of 0.8 by default. Ablation studies are conducted on a single 1080Ti GPU and DAVIS 2017 validation set in default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparisons with State-of-the-Art Methods</head><p>DAVIS 2016 <ref type="bibr" target="#b31">[32]</ref> is a single object benchmark for video object segmentation. As shown in <ref type="table" target="#tab_1">Table 1</ref>, QDMN trained without synthetic dataset still outperforms most previous methods (91.0 J &amp;F). With synthetic training data, QDMN surpasses all existing methods and achieves the performance of 92.0 J &amp;F. DAVIS 2017 <ref type="bibr" target="#b32">[33]</ref> is a multiple objects extension of DAVIS 2016. In the <ref type="table" target="#tab_1">Table 1</ref>, QDMN achieves an average score of 84.6 and 85.6 for training without synthetic data and with synthetic data, respectively. What's more, we also test our model on the challenging DAVIS 2017 testing split set. It achieves the best performance (81.9) compared to all previous methods. YouTube-VOS <ref type="bibr" target="#b46">[47]</ref> is a large-scale benchmark for video object segmentation. As shown in <ref type="table" target="#tab_2">Table 2</ref>, without synthetic training data, our QDMN also achieves state-of-the-art performance (83.0). If we use synthetic data for training, the overall score of QDMN will be boosted to 83.8. Qualitative results. The qualitative comparison between baseline and our QDMN are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We show the performance on two challenging scenarios, i.e., occlusion scenes and similar objects. Both STM <ref type="bibr" target="#b28">[29]</ref> and MiVOS <ref type="bibr" target="#b5">[6]</ref> have lost targets in the occlusion scene. STM lost targets in the scene with similar objects, while MiVOS identified other objects incorrectly. In contrast, our method can achieve satisfactory performance in challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generic Plugins</head><p>To further prove the effectiveness of our proposed QAM, we apply it as a general plugin to other methods. The results on the DAVIS2017 validation set are shown in <ref type="table">Table 3</ref> (the baseline performance is our re-implementation results). It can be seen that with QAM, the performance of these methods has been significantly boosted. Besides, QAM is easy to be deployed on other methods, and we hope that the QAM would shed light on the studies of related fields that need to memorize reference information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>The effectiveness of QAM. To demonstrate the effectiveness of the QAM, we conduct specific analyses from three dimensions.</p><p>(1) Accuracy of the predicted scores. We perform a histogram visualization of the distribution of the ground truth mask IoU and prediction scores at 0.05 intervals <ref type="figure">(Fig. 5</ref>). When multiple objects are in a frame, the average is taken. We can see that the quality score and ground truth mask IoU are positively correlated, which verifies the accuracy of the scores predicted by QAM.</p><p>(2) Memory Threshold. We test different memory thresholds ? on DAVIS 2017 test-dev set, and the results are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. We can see that it will hurt the segmentation effect if the threshold is set too high or too low. The reason is that if the threshold ? is too high, only a few intermediate frames will be memorized, leading to losing a lot of helpful information; if the ? is too low, the model may memorize some incorrect noise information. Besides, the performance is worst when the memory threshold is 0 (at this time, QAM does not filter poor segmentation masks), which proves the motivation of the QAM is correct.</p><p>(3) Applying QAM only at inference stage. To further prove that filtering out inaccurately segmented frames has a beneficial effect on segmentation, we construct experiments that adding QAM only at the inference stage. Specifically, for QAM, we load its parameters trained in QDMN. For other parts, we load the weights of the initial model (trained without QAM). As shown in <ref type="table" target="#tab_3">Table 4</ref>, the performance of all vanilla models has been improved after adding QAM, which shows the importance of filtering poorly segmented frames.  Component Analysis. We analyze the effectiveness of our modules in <ref type="table" target="#tab_4">Table 5</ref>. PE represents the prior enhancement strategy introduced to highlight temporal consistency. As shown in the table, both the QAM and PE bring remarkable performance improvement.</p><p>Dynamic Memory Updating Strategy. Due to the lack of a widely used large-scale long video dataset in this field, we choose to demonstrate the effectiveness of our proposed memory bank dynamic updating strategy by compress- <ref type="figure">Fig. 5</ref>: Distribution of the prediction score and the ground truth mask IoU.    <ref type="table">Table 3</ref>: Applying QAM as general plugin. w / QAM indicates that whether the QAM is deployed on this method.</p><p>ing the upper limit of the memory. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, The segmentation effect remains unaffected even at low memory upper limit, and the speed is improved as a result of our memory bank design strategy. The similar phenomenon is observed on the YouTube-VOS set, which illustrates the effectiveness of our dynamic updating strategy. Besides, we also perform analysis on long videos (without annotations). We find that previous memory network methods store up to about 70 frames and the memory explosion occurs, which greatly limits the practicability. But QDMN can handle videos of arbitrary length by setting upper memory limit and dynamically updating the memory. What's more, the FPS of previous methods will drop from 14 to about 2 before memory explodes, while the FPS of QDMN will stay around 7 after the initial drop (assuming the upper memory limit is 25). Enhancement Strategy. For PE, we directly concatenate the prior mask with the deepest layer feature of the current frame to provide a weak prior.</p><p>In contrast, we also try to provide a strong prior. Specifically, we extract the feature of the prior mask and fuse it with the middle layer features of the current frame. After convolution and downsampling, the fused features are added to the deepest layer features of the current frame. Compared with the current enhancement strategy, this approach can significantly enhance the influence of the prior mask. However, although this approach works well in common scenarios, the performance drops significantly under challenging situations, as shown in <ref type="table" target="#tab_6">Table 6</ref>. The reason for this phenomenon is that the strong prior makes the model overly dependent on it, which weakens the model's ability to recognize objects.</p><p>Speed Analysis. We also experiment with the impact of the proposed modules on the inference speed. With our modules, the FPS of baseline has changed from 8.6 to 7.8 on DAVIS2017 val set. The increased running time brought by QAM and PE is nearly negligible (no more than 10%), mainly because we directly use the feature extracted by the memory encoder for quality assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose that the design of the memory bank should follow the principles of accuracy and temporal consistency. To support this, we introduce a Quality-aware Dynamic Memory Network (QDMN) for semi-supervised video object segmentation, which selectively memorizes accurately segmented intermediate frames as references and emphasizes video temporal consistency. Without bells and whistles, our QDMN achieves new state-of-the-art performance on the popular benchmark YouTube-VOS and DAVIS with almost no additional inference time. Furthermore, the QAM also has a remarkable improvement for other approaches as a general plugin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Analysis on Long Videos</head><p>To further prove the rationality and effectiveness of our dynamic memory updating strategy, we show the qualitative results on long videos (more than 2000 frames) in <ref type="figure" target="#fig_6">Fig. 8</ref>. In general, the most common practice for updating memory in practical applications is to retain the most recent memory frames. However, this approach is difficult to deal with object appearance changes or scene changes. As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, only retaining the most recent memory frames may cause the memory bank losing perception of the target object but our dynamic updating strategy allows for superior segmentation effect. (a) is the results of retaining the most recent memory frames and (b) is applying our updating strategy. The memory frame storage limit is 25 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Failure Cases</head><p>The failure cases of QAM are the extremely difficult scenarios, e.g., lots of similar objects overlapping each other, in which almost every frame is poorly segmented. Take the <ref type="figure">Fig. 9</ref> as example, in this case, the target sheep are mixed with other background sheep. Along with the movement of the flock, it is difficult for the algorithm to correctly identify the target sheep as well as the outline of the sheep. Although we use relative quality scores in this scenario, QAM is less helpful. t=1 t=6 <ref type="figure">Fig. 9</ref>: The bad case of QAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Memory Interval</head><p>In order to avoid excessive memory redundancy in the same scene, we still choose to trigger the storage of every N frames, but the frame must meet our proposed principles. Otherwise, it will be deferred to the next frame to trigger. As <ref type="table" target="#tab_7">Table 7</ref> shows, it is not the smaller the memory interval, the better the segmentation effect. The reason is that although the smaller memory interval means more reference frames, excessive redundancy in the same scene affects the matching process. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of QDMN. (a) is the feature extraction of the reference frames in the memory bank. (b) QAM is the module used to evaluate whether the current frame can be added to the memory bank. (c) is the pipeline for predicting the segmentation result of the current frame I t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustrations of segmentation masks with different quality scores. The three rows represent the ground truth, segmentation results, and the quality scores predicted by QAM, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm: 1 ? then 5 :# to filter the inaccurately segmented frames 6 :j = j + 1 7 :</head><label>1567</label><figDesc>Pseudocode of Dynamic Memory Bank Input?memory bank M emory, video frames sequence {It} of length L 1: t = 2 # the ground truth mask of the first frame is given 2: j = 1 # the relative index of memory frames 3: while t ? L do 4: if S A t ? if len(M emory) ? ? then 8: M emory.add({j : [It, Mt, S A t ]}) # store It, Mt, S A t to the j position in memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Visual comparison of QDMN with baseline methods.Each row demonstrates five frames sampled from a video sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The quantitative results of different memory threshold ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>The performance for different memory upper limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results of our proposed dynamic memory updating strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other methods on DAVIS dataset. '*' indicates using synthetic training dataset<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">DAVIS2016</cell><cell cols="3">DAVIS2017 val</cell><cell cols="3">DAVIS2017 test-dev</cell></row><row><cell></cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>RANet [44]</cell><cell cols="8">86.6 87.6 87.1 63.2 68.2 65.7 53.4 56.2</cell><cell>55.3</cell></row><row><cell>FEELVOS [40]</cell><cell cols="8">81.1 82.2 81.7 69.1 74.0 71.5 55.2 60.5</cell><cell>57.8</cell></row><row><cell>RGMP [28]</cell><cell cols="8">81.5 82.0 81.8 64.8 68.6 66.7 51.3 54.4</cell><cell>52.8</cell></row><row><cell>DMVOS [45]</cell><cell cols="3">88.0 87.5 87.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STM [29]</cell><cell cols="8">88.7 89.9 89.3 79.2 84.3 81.8 69.3 75.2</cell><cell>72.2</cell></row><row><cell>KMN [35]</cell><cell cols="8">89.5 91.5 90.5 80.0 85.6 82.8 74.1 80.3</cell><cell>77.2</cell></row><row><cell>CFBI [50]</cell><cell cols="8">88.3 90.5 89.4 79.1 84.6 81.9 71.1 78.5</cell><cell>74.8</cell></row><row><cell>GIEL [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">80.2 85.3 82.7 72.0 78.3</cell><cell>75.2</cell></row><row><cell>SwiftNet [42]</cell><cell cols="6">90.5 90.3 90.4 78.3 83.9 81.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RMNet [46]</cell><cell cols="8">88.9 88.7 88.8 81.0 86.0 83.5 71.9 78.1</cell><cell>75.0</cell></row><row><cell>SSTVOS [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">79.9 85.1 82.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LCM [14]</cell><cell cols="8">89.9 91.4 90.7 80.5 86.5 83.5 74.4 81.8</cell><cell>78.1</cell></row><row><cell>MiVOS [6]</cell><cell cols="8">87.8 90.0 88.9 80.5 85.8 83.1 72.6 79.3</cell><cell>76.0</cell></row><row><cell>MiVOS* [6]</cell><cell cols="8">89.7 92.4 91.0 81.7 87.4 84.5 74.9 82.2</cell><cell>78.6</cell></row><row><cell>JOINT [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">80.8 86.2 83.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">RPCMVOS [48] 87.1 94.0 90.6 81.3 86.0 83.7 75.8 82.6</cell><cell>79.2</cell></row><row><cell>DMN-AOA [22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">81.0 87.0 84.0 74.8 81.7</cell><cell>78.3</cell></row><row><cell>HMMN [36]</cell><cell cols="8">89.6 92.0 90.8 81.9 87.5 84.7 74.7 82.5</cell><cell>78.6</cell></row><row><cell>STCN [7]</cell><cell cols="8">90.8 92.5 91.6 82.2 88.6 85.4 72.7 79.6</cell><cell>76.1</cell></row><row><cell>AOT-L [51]</cell><cell cols="8">89.7 92.3 91.0 80.3 85.7 83.0 75.3 82.3</cell><cell>78.8</cell></row><row><cell cols="10">QDMN (Ours) 90.2 91.7 91.0 81.8 87.3 84.6 74.2 81.2 77.7</cell></row><row><cell cols="10">QDMN* (Ours) 90.7 93.2 92.0 82.5 88.6 85.6 78.1 85.4 81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on YouTube-VOS 2018 val set. Seen and Unseen denote whether the categories exist in the training set. G is averaged overall score.</figDesc><table><row><cell>Methods</cell><cell>Seen</cell><cell></cell><cell cols="2">Unseen</cell><cell>G</cell></row><row><cell></cell><cell>J</cell><cell>F</cell><cell>J</cell><cell>F</cell><cell></cell></row><row><cell>STM [29]</cell><cell>79.7</cell><cell>84.2</cell><cell>72.8</cell><cell>80.9</cell><cell>79.4</cell></row><row><cell>AFB-URR [23]</cell><cell>78.8</cell><cell>83.1</cell><cell>74.1</cell><cell>82.6</cell><cell>79.6</cell></row><row><cell>GCM [21]</cell><cell>72.6</cell><cell>75.6</cell><cell>68.9</cell><cell>75.7</cell><cell>73.2</cell></row><row><cell>KMN [35]</cell><cell>81.4</cell><cell>85.6</cell><cell>75.3</cell><cell>83.3</cell><cell>81.4</cell></row><row><cell>G-FRTM [30]</cell><cell>68.6</cell><cell>71.3</cell><cell>58.4</cell><cell>64.5</cell><cell>65.7</cell></row><row><cell>SwiftNet [42]</cell><cell>77.8</cell><cell>81.8</cell><cell>72.3</cell><cell>79.5</cell><cell>77.8</cell></row><row><cell>GIEL [11]</cell><cell>80.7</cell><cell>85.0</cell><cell>75.0</cell><cell>81.9</cell><cell>80.6</cell></row><row><cell>SSTVOS [10]</cell><cell>80.9</cell><cell>-</cell><cell>76.6</cell><cell>-</cell><cell>81.8</cell></row><row><cell>RMNet [46]</cell><cell>82.1</cell><cell>85.7</cell><cell>75.7</cell><cell>82.4</cell><cell>81.5</cell></row><row><cell>LCM [14]</cell><cell>82.2</cell><cell>86.7</cell><cell>75.7</cell><cell>83.4</cell><cell>82.0</cell></row><row><cell>MiVOS [6]</cell><cell>80.0</cell><cell>84.6</cell><cell>74.8</cell><cell>82.4</cell><cell>80.4</cell></row><row><cell>MiVOS* [6]</cell><cell>81.1</cell><cell>85.6</cell><cell>77.7</cell><cell>86.2</cell><cell>82.6</cell></row><row><cell>JOINT [27]</cell><cell>81.5</cell><cell>85.9</cell><cell>78.7</cell><cell>86.5</cell><cell>83.1</cell></row><row><cell>HMMN [36]</cell><cell>82.1</cell><cell>87.0</cell><cell>76.8</cell><cell>84.6</cell><cell>82.6</cell></row><row><cell>DMN-AOA [22]</cell><cell>82.5</cell><cell>86.9</cell><cell>76.2</cell><cell>84.2</cell><cell>82.5</cell></row><row><cell>STCN [7]</cell><cell>81.9</cell><cell>86.5</cell><cell>77.9</cell><cell>85.7</cell><cell>83.0</cell></row><row><cell>AOT-L [51]</cell><cell>82.5</cell><cell>87.5</cell><cell>77.9</cell><cell>86.7</cell><cell>83.7</cell></row><row><cell>QDMN (Ours)</cell><cell>82.0</cell><cell>86.8</cell><cell>77.5</cell><cell>85.5</cell><cell>83.0</cell></row><row><cell>QDMN* (Ours)</cell><cell>82.7</cell><cell>87.5</cell><cell>78.4</cell><cell>86.4</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effect of adding QAM only in the inference stage Methods J &amp;F (?=0) J &amp;F (?=0.8)</figDesc><table><row><cell>STM</cell><cell>81.5</cell><cell>82.5?</cell></row><row><cell>KMN</cell><cell>82.6</cell><cell>83.4?</cell></row><row><cell>MiVOS</cell><cell>82.7</cell><cell>83.5?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of proposed components.</figDesc><table><row><cell cols="2">QAM PEM J</cell><cell>F J &amp;F</cell></row><row><cell></cell><cell cols="2">80.3 85.5 82.9</cell></row><row><cell>?</cell><cell cols="2">81.7 87.1 84.3 ?</cell></row><row><cell></cell><cell cols="2">? 81.1 86.1 83.6 ?</cell></row><row><cell>?</cell><cell cols="2">? 81.8 87.3 84.6 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of different enhancement strategy. "Weak" means providing weak prior (PE). "Strong" means providing strong location prior.</figDesc><table><row><cell>Strategy</cell><cell>DAVIS</cell><cell>YouTube-VOS</cell></row><row><cell>Weak</cell><cell cols="2">J F J &amp;F J F 81.8 87.3 84.6 79.8 86.2 83.0 G</cell></row><row><cell cols="3">Strong 82.4 87.9 85.2 77.5 83.8 80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Experiment results of different memory interval on DAVIS2017 val set.</figDesc><table><row><cell>Memory Interval</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>3</cell><cell cols="2">82.3 88.1</cell><cell>85.2</cell></row><row><cell>5</cell><cell cols="3">82.7 88.6 85.7</cell></row><row><cell>7</cell><cell cols="2">81.6 87.8</cell><cell>84.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This research was supported in part by the National Natural Science Foundation of China under Grant No. U1903213, the Shenzhen Key Laboratory of Marine IntelliSense and Computation (NO. ZDSYS20200811142605016.)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-aware tracker for realtime video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07941</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05210</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Video object segmentation using global and instance embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning position and target consistency for memory-based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04329</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FSS-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Video object segmentation with dynamic memory networks and adaptive object alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">AGSS-VOS: attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03679</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning dynamic network using a reuse gate function in semi-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hierarchical memory matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11404</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<title level="m">Hierarchical image saliency detection on extended CSSD. TPAMI pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast template matching and update for video object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FEELVOS: fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swiftnet: Real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">DMVOS: discriminative matching for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACMMM</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient regional memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12934</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Reliable propagation-correction modulation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02638</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spatial constrained memory network for semi-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR Workshops</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Enhanced memory network for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
