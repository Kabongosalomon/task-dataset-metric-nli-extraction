<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Landmarks Detection by Self-Iterative Regression based Landmarks-Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<email>hgqi@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Landmarks Detection by Self-Iterative Regression based Landmarks-Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cascaded Regression (CR) based methods have been proposed to solve facial landmarks detection problem, which learn a series of descent directions by multiple cascaded regressors separately trained in coarse and fine stages. They outperform the traditional gradient descent based methods in both accuracy and running speed. However, cascaded regression is not robust enough because each regressor's training data comes from the output of previous regressor. Moreover, training multiple regressors requires lots of computing resources, especially for deep learning based methods. In this paper, we develop a Self-Iterative Regression (SIR) framework to improve the model efficiency. Only one self-iterative regressor is trained to learn the descent directions for samples from coarse stages to fine stages, and parameters are iteratively updated by the same regressor. Specifically, we proposed Landmarks-Attention Network (LAN) as our regressor, which concurrently learns features around each landmark and obtains the holistic location increment. By doing so, not only the rest of regressors are removed to simplify the training process, but the number of model parameters is significantly decreased. The experiments demonstrate that with only 3.72M model parameters, our proposed method achieves the stateof-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Facial landmarks detection is one of the most important techniques in face analysis, such as face recognition, facial animation and 3D face reconstruction. It aims to detect the facial landmarks such as eyes, nose and mouth, namely predicting the location parameters of landmarks.</p><p>Researchers usually regard this task as a typical non-linear least squares problem <ref type="bibr" target="#b22">(Xiong and la Torre 2013)</ref>. The Newton's method and its variants are the traditional gradient based solution, whose convergence rate is quadratic and is guaranteed to converge, provided that the initial estimate is sufficiently close to the minimum. However, when the objective function is not differentiable(e.g. SIFT <ref type="bibr" target="#b12">(Lowe 2004)</ref>) or the Hessian matrix is not positive definite, the method won't works well <ref type="bibr" target="#b22">(Xiong and la Torre 2013;</ref><ref type="bibr" target="#b23">Xiong and la Torre 2015)</ref>.</p><p>In recent years, cascaded regression based methods <ref type="bibr" target="#b6">(Doll?r, Welinder, and Perona 2010;</ref><ref type="bibr"></ref> Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>(a) Cascaded Regression.</p><p>(b) Self-Iterative Regression. <ref type="figure">Figure 1</ref>: Facial landmarks detection process of Cascaded Regression(a) and Self-Iterative Regression(b). To predict the landmarks' location parameters, the CR based methods require multiple regressors, while SIR just need one regressor and updates parameters iteratively. <ref type="bibr" target="#b22">Xiong and la Torre 2013;</ref><ref type="bibr" target="#b14">Ren et al. 2014;</ref><ref type="bibr" target="#b29">Zhu et al. 2016;</ref><ref type="bibr" target="#b19">Tzimiropoulos 2015;</ref><ref type="bibr" target="#b19">Tu 2008)</ref> have been proposed and applied to solve the non-linear least squares problem. They usually train multiple regressors to predict the parameters' increment sequentially, which outperform the traditional gradient descent based methods in both accuracy and running speed. Moreover, deep learning based cascaded regression methods <ref type="bibr" target="#b17">(Sun, Wang, and Tang 2013;</ref><ref type="bibr" target="#b26">Zhang et al. 2014b;</ref><ref type="bibr" target="#b18">Trigeorgis et al. 2016;</ref><ref type="bibr" target="#b21">Xiao et al. 2016;</ref><ref type="bibr" target="#b25">Zhang et al. 2014a</ref>) are widely leveraged for this task because of the powerful ability to extract the discriminative feature. However, when applying cascaded regression system, three main problems arise: (1) Each regressor just works well in its local data space, when previous regressor predicts the false descent direction, the final results are very likely to drift away; (2) In general, higher accuracy can be obtained by adding more cascaded regressors, while it will increase model storage memory and computing resources;</p><p>(3) Subsequent regressors usually cannot be activated for training until previous regressors finished their training process, which increases the system complexity.</p><p>In this paper, we develop a Self-Iterative Regression (SIR) framework to solve the above issues. By means of the powerful representation of Convolutional Neural Network (CNN), we only train one regressor to learn the descent directions in coarse and fine stages together. The training data is obtained by random sampling in the parameter space, and in the test-ing process, parameters are updated iteratively by calling the same regressor, which is dubbed Self-Iterative Regression. The testing process is illustrated in <ref type="figure">Figure 1(b)</ref>. The experimental results show that for deep learning based method, one regressor achieves comparable performance to state-ofthe-art multiple cascaded regressors and significantly reduce the training complexity. Moreover, to obtain discriminative landmarks features, we proposed a Landmarks-Attention Network (LAN), which focuses on the appearance around landmarks. It first concurrently extracts local landmarks' features and then obtains the holistic increment, which significantly reduces the dimension of the final feature layer and the number of model parameters. The contributions of this paper are summarized as follows: 1. We propose a novel regression framework called SIR to solve the non-linear least squares problem, which simplifies the cascaded regression framework and obtains stateof-the-art performance in facial landmarks detection task. 2. The Landmarks-Attention Network (LAN) is developed to independently learn discriminative features around each landmarks, which significantly reduces the dimension of feature layer and the number of model parameters. 3. Experimental results on several publicly available benchmarks demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we will review related works in solving nonlinear least squares problems, especially facial landmarks detection problem.  <ref type="bibr" target="#b25">(Zhang et al. 2014a)</ref>, which consists of multiple Stacked Auto-encoder Networks (SANs). The first SAN quickly predicts the preliminary location of landmarks by a low-resolution image, and the subsequent SANs then refine the location with higher and higher resolution.</p><p>Trigeorgis et al. proposed the Mnemonic Descent Method (MDM) <ref type="bibr" target="#b18">(Trigeorgis et al. 2016)</ref>, which regards the nonlinear least squares optimization as a dynamic process. The Recurrent Neural Network (RNN) is introduced to maintain an internal memory unit that accumulates the history information so as to relate the cascaded refinement process.</p><p>Jo?o et al. proposed a iterative error feedback (Carreira et al. 2016) method to solve the human pose extimation problems. Same with MDM, their training data is generated by previous stages, while ours is obtained by random sampling in coarse stages and fine stages, which simplifies the training process.</p><p>Xiao et al. <ref type="bibr" target="#b21">(Xiao et al. 2016</ref>) propose a Long Short Term Memory (LSTM) based recurrent attentive-refinement network, which also follows the pipeline of cascaded regressions. Instead of updating all landmarks location together, it first extracts reliable landmarks by a CNN and then infers locations of the rest noisy landmarks, resulting in improved accuracy. However, these deep cascaded regression methods usually require more computing resources and also suffer from the same drawbacks as discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascaded Regression</head><p>Before introducing our method, we begin with the cascaded regression framework in brief for better understanding. As illustrated in <ref type="figure">Figure 1</ref>(a), in the training process of cascaded regression, K regressors (R 1 , R 2 , ? ? ? , R K ) are trained sequentially. Each regressor R k is computed by minimizing the expected loss between the predicted and the optimal parameters's increment. It is formulated as</p><formula xml:id="formula_0">R k = argmin R k i ?? * k,i ? R k (x k,i ) 2 2 , k = 1, 2, ? ? ? , K,</formula><p>(1) where x k,i is i th example in k th regression process, ?? * k,i = ? * i ? ? k,i is the corresponding target increment, i.e., the difference between ground truth parameter ? * i and present parameter ? k,i . After obtaining R k , the target parameter is updated by Equ. <ref type="formula">(2)</ref>,</p><formula xml:id="formula_1">? k+1,i = ? k,i + R k (x k,i ).</formula><p>(2) Then, new training dataset will be generated according to the updated parameter for the next regression (Xiong and la Torre 2013).</p><p>In the testing process, parameter will be sequentially refined by these cascaded regressors in Equ 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Iterative Regression</head><p>In this section, we will describe our facial landmarks detection method including the Gaussian random sampling and the Landmarks-Attention Network in detail. The overall procedure is presented in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian random Sampling</head><p>Generating training data is key important process in our method. Cascaded regression generates training data according to previous regressor, while our method obtains it (c) Iterative predicting and updating process. The training process consists of (a) and (b), while the testing process consists of (b) and (c). In the figure, one of the dimension of facial Landmarks Model parameter S is showed, and ? is landmarks' location parameter.</p><p>by random sampling, which includes most possible landmarks distribution from coarse stages to fine stages. Let (x j , y j ) be the j th landmark's position coordinates and ? = (x 1 , y 1 , x 2 , y 2 , ..., x M , y M ) be all landmarks' location parameters, where M is the total number of facial landmarks. It is not a good choice to directly sample in location parameter ? since its dimension is so high that the training process will be hard to converge and is very likely to generate unreasonable face shape. To improve the effectiveness of sampling process, we indirectly obtain sampling location ? according to a new facial landmarks model that is similar to 3D Morphable Model (3DMM) <ref type="bibr" target="#b1">(Blanz and Vetter 1999)</ref>. Facial landmarks distribution will be represented by pose and shape parameter. Facial Landmarks Model. We obtain intrinsic face shape parameter by Principal Component Analysis (PCA) and pose parameter(including 2D translation, in-plane rotation and scale) by geometry transformation. The shape, translation, rotation angle and scale coefficient are represented by ?, t 2d , ?, f respectively. Finally, facial landmarks model parameters can be represented by S = [?, t 2d , ?, f ]. S and ? are two kinds of representation for facial landmarks. S can be converted to ? by</p><formula xml:id="formula_2">?(S) = f * R ? * (S 0 + A * ?) + t 2d ,<label>(3)</label></formula><p>where S 0 is the mean shape, A is the PCA shape matrix and</p><formula xml:id="formula_3">R ? = cos ? ? sin ? sin ? cos ? is the rotation matrix with angle ?.</formula><p>Random sampling in facial landmarks model S and then converting to location parameter ? makes the sampling process easier to control and generates more reasonable landmarks' distribution. Sampling space. For each face I, let S gt represent its ground truth facial landmarks model parameters. We random select values in each dimension of S obeying distribution D which is a union set of two Gaussian distribution. The sampling space of each face is represented by</p><formula xml:id="formula_4">D ? {N (S 0 , ?) ? N (S gt , ?)},<label>(4)</label></formula><p>where N (?, ?) represents Gaussian distribution, and ? is its standard deviation. We adopt this sampling distribution because training regressor around mean location and ground truth location affects the performance in coarse and fine stages, respectively, and the final location error usually obeys Gaussian distribution. The value of standard deviation ? affects the final performance. System with larger ? will contain more training space which makes the system more robust, while the final accuracy may decrease because sampling probability around ground truth will decrease and vise versa. The effect of ? will be discussed in the Experiments section.</p><p>For i-th image in the t-th sampling period, sampling parameter S t,i is obtained by random selecting a value in Equ. (4). We then calculate location parameters ? t,i by Equ (3) and extract patches P t,i in location ? t,i . Finally, we set P t,i as the training input data and set ?? t,i = ? * i ? ? t,i as regressor's corresponding target increment. The process is also summarized in Algorithm 1, and the training data is represented as</p><formula xml:id="formula_5">T t=1 N i=1 (P t,i , ?? t,i ),<label>(5)</label></formula><p>where T is the number of sampling period, N is the number of images in raw dataset. The sampling process is illustrated in <ref type="figure" target="#fig_0">Figure 2(a)</ref>. By the sampling process, we obtained nearly unlimited training data and the training space contains most possible landmarks' distribution from coarse stages to fine stages. The sampled training data is online generated to save the system memory.</p><p>Algorithm 1 Sampling process of SIR Input: Raw face landmarks dataset:</p><formula xml:id="formula_6">N i=1 (I i , (S gt ) i ) Output: Training dataset: T t=1 N i=1 (P t,i , ?? t,i ) 1: for t = 1 to T do 2:</formula><p>for i = 1 to N do Set P t,i as the regressor's input data; 7:</p><p>Set ?? t,i = ? * i ? ? t,i as regressor's target increment; 8: end for 9: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landmarks-Attention Network</head><p>In this section, we will describe the structure of the proposed regressor. Our goal is to learn a mapping between appearance features and landmarks' location increment. Previous works usually first obtain robust initialization location by extracting features in the whole image and then refine the location by many refinement networks <ref type="bibr" target="#b21">(Xiao et al. 2016;</ref><ref type="bibr" target="#b17">Sun, Wang, and Tang 2013)</ref> or stack all landmarks patches to directly extract all landmarks features <ref type="bibr" target="#b18">(Trigeorgis et al. 2016)</ref>. They either require a number of model parameters or generate indiscriminative features. Thus we propose a Landmarks-Attention Network (LAN) to overcome the above two drawbacks. Our regressor is a single CNN which concurrently pays attention to appearance feature around each facial landmark. Specifically, for each landmarks patch, we extract features by several convolutional and pooling layers, then concatenate these independent feature vectors and add two fully connected layers to learn a holistic location increment. The structure of each feature extraction sub-network is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(b) and the detailed information of the sub-network is presented in <ref type="table" target="#tab_1">Table 1</ref>. Compared to the previous networks, our proposed model has three advantages: (1) The landmarks feature extracted by independent sub-networks can be more discriminative, as showed in <ref type="figure" target="#fig_7">Figure 6;</ref> (2) Concatenating all independent features vectors and adding fully connected layers can obtain a holistic landmarks location increment, especially when some landmarks are occluded or blurred; (3) Our network is very light, whose parameters number(3.72M in total) is far less than other CNN models (e.g., AlexNet (Krizhevsky, Sutskever, and Hinton 2012) contains about 60M parameters and VGGNet (Simonyan and Zisserman 2014) contains about 138M parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>The training process is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>  (P t,i , ?? t,i ). Then, the above described LAN is trained to learn the descent directions in coarse and fine stages together. This process can be formulated as</p><formula xml:id="formula_7">R ? = argmin R? 1 T ? N T t=1 N i=1 ?? t,i ? R ? (P t,i ) 2 2 ,<label>(6)</label></formula><p>where R ? is the target self-iterative regressor (i.e., LAN), and t indicates the t th sampling period.</p><p>Since the training space of SIR includes most possible landmarks distribution from coarse stages to fine stages, the training process will generate a Descent Direction Map (DDM) in the sampling space where each sample's descent direction roughly points toward the ground truth. As illustrated in <ref type="figure">Figure 3 (b)</ref>, SIR is more robust than CR because the former can cover more training space and isn't affected by the optimization path. When the previous regressor predicts false descent directions, SIR can still converge to the ground truth while CR is prone to drift away.</p><p>(a) Cascaded Regression (b) Self-Iterative Regression <ref type="figure">Figure 3</ref>: (a) Typical cascaded regression process: starting from initial value, parameters are updated and close to the ground truth (such as init ? C 1 ? C 2 ? gt) by regressors R k (k = 1, 2, 3, ...). Once one regressor predicts the false direction, the final result is prone to drift away; (b) SIR Descent Direction Map: the training space of SIR includes distribution from coarse stages to fine stages and all descent directions are pointed to ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Iterative Updating</head><p>For the testing process, similar to the cascaded regression methods, starting from initial location parameters ? 0 , we iteratively update the location parameters ? k and extract new patches P k till converges. The process is presented in Algorithm 2, and facial landmarks location parameter is updated by, ? k+1 = ? k + R ? (P k ), k = 0, 1, ? ? ? . (7)</p><p>Algorithm 2 Self-Iterative updating process of SIR Input: Regressor R ? , Initial location ? 0 , Total iteration times K Output: Prediction of facial landmarks' location ? K 1: for k = 0 to K ? 1 do 2:</p><p>Extract patches P k in location ? k 3:</p><formula xml:id="formula_8">? k+1 = ? k + R ? (P k ) 4: end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we perform experiments to demonstrate the effectiveness of the proposed SIR compared to state-of-theart methods. Specifically, we evaluate the proposed method model by <ref type="formula">(1)</ref>   <ref type="bibr" target="#b15">(Sagonas et al. 2016)</ref>; and competition testing set (600 faces in total) including 300 indoor and 300 outdoor faces images. Metrics. Normalized Mean Error (NME) measures landmarks' mean location error normalized by inter-pupil (eyes centers) distance <ref type="bibr" target="#b28">(Zhu et al. 2015;</ref><ref type="bibr" target="#b14">Ren et al. 2014)</ref> or interocular (outer eye corners) distance <ref type="bibr" target="#b18">(Trigeorgis et al. 2016)</ref>. Cumulative Error Distribution (CED) curve is the cumulative distribution function of the normalized error, which can avoid heavily impacted by some big failures <ref type="bibr" target="#b25">(Yang et al. 2015)</ref>. We also calculated another two evaluation metrics, namely Area-Under-the-Curve (AUC ? ) and Failure Rate (FR ? ). Similar as MDM <ref type="bibr" target="#b18">(Trigeorgis et al. 2016</ref>), we consider mean point-to-point error greater than 0.08 as a failure, i.e., ? = 0.08. Implementation Detail. We perform the experiments based on a machine with Core i7-5930k CPU, 32 GB memory and GTX 1080 GPU with 8G video memory. The detected faces are resized into 256 ? 256 and the location patch size is 57 ? 57. For CNN structure, the Rectified Linear Unit (ReLU) is adopted as the activation function, and the optimizer is the Adadelta (Zeiler 2012) approach, learning rate is set to 0.1 and weight decay is set to 1e ? 4. Training the CNN requires around 2 days.</p><p>Comparison with State-of-the-arts As shown in <ref type="table" target="#tab_2">Table 2</ref>, we compare the proposed method with several state-of-the-art facial landmarks detection methods in the public testing set. Specifically, the common subset consists of LFPW testing set (224 faces) and HELEN testing set (330 faces) and the challenging subset is IBUG dataset (135 faces). Thus the the full set (689 faces) of the union of the common (554 faces) and challenging subsets (135 faces). The NME results shows that SIR performs comparatively with RAR <ref type="bibr" target="#b21">(Xiao et al. 2016</ref>) and outperform other existing methods <ref type="bibr" target="#b2">Burgos-Artizzu, Perona, and Doll?r 2013;</ref><ref type="bibr" target="#b22">Xiong and la Torre 2013;</ref><ref type="bibr" target="#b14">Ren et al. 2014;</ref><ref type="bibr" target="#b28">Zhu et al. 2015;</ref><ref type="bibr" target="#b9">Kowalski and Naruniec 2016;</ref><ref type="bibr" target="#b18">Trigeorgis et al. 2016;</ref><ref type="bibr" target="#b21">Xiao et al. 2016)</ref>. Besides, more visual results are also illustrated in <ref type="figure" target="#fig_9">Figure 9</ref>. In the more challenging IBUG subset, our method achieves robust performance in large pose, expression and illumination environment. On the other hand, we evaluate SIR in the competition testing set. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, the SIR method outperform the state-of-the-art methods <ref type="bibr" target="#b4">(Cech et al. 2016;</ref><ref type="bibr" target="#b5">Deng et al. 2016;</ref><ref type="bibr" target="#b6">Fan and Zhou 2016;</ref><ref type="bibr" target="#b0">Baltrusaitis, Robinson, and Morency 2013;</ref><ref type="bibr" target="#b24">Yan et al. 2013;</ref><ref type="bibr" target="#b27">Zhou et al. 2013;</ref><ref type="bibr" target="#b7">Jaiswal, Almaev, and Valstar 2013;</ref><ref type="bibr" target="#b14">Milborrow, Bishop, and Nicolls 2013;</ref><ref type="bibr" target="#b7">Kamrul Hasan, Pal, and Moalem 2013;</ref><ref type="bibr" target="#b13">Martinez and Valstar 2016)</ref> according to the CED curve. Moreover, <ref type="table" target="#tab_3">Table 3</ref> presents the quantitative results for both the 51-point and 68-point error metrics (i.e., AUC and Failure Rate at a threshold of 0.08 of the normalised error), compared to existing methods <ref type="bibr" target="#b8">(Kazemi and Sullivan 2014;</ref><ref type="bibr" target="#b19">Tzimiropoulos 2015;</ref><ref type="bibr" target="#b0">Asthana et al. 2014;</ref><ref type="bibr" target="#b22">Xiong and la Torre 2013;</ref><ref type="bibr" target="#b27">Zhou et al. 2013;</ref><ref type="bibr" target="#b24">Yan et al. 2013;</ref><ref type="bibr" target="#b28">Zhu et al. 2015;</ref><ref type="bibr" target="#b18">Trigeorgis et al. 2016</ref>). The promising performances on two metrics indicate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Cascaded Regression</head><p>As discussed before, previous cascaded regression methods adding more regressors can achieve better performance, but increase the number of model parameters, computing resources and storage space, especially for deep learning based methods. Different from them, our method obtains state-(a) CED curve of facial 51-points.   of-the-art performance by iterative call the same regressor rather than adding any more regressors.</p><p>Our method reduces the model complexity while keeps the performance in two folds: (1) the proposed network focuses on the landmarks' local feature, which significantly reduces the dimension of final feature layer;</p><p>(2) only one CNN module is required to iteratively predict the location parameters, while cascaded regression usually requires at least three regressors <ref type="bibr" target="#b18">(Trigeorgis et al. 2016;</ref><ref type="bibr" target="#b22">Xiong and la Torre 2013)</ref>.</p><p>To prove the effectiveness of SIR, we add a baseline CR method which extracts features by the same LAN while adopts cascaded regression framework. Both baseline CR and SIR is updated for 4 times before the stable performance. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our method requires parameters and memory far less than other cascaded regression based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Analysis</head><p>In this section, we perform analyses on the effect of several important modules in our method to the final performance. Effect of different feature extraction networks. In SIR framework, we adopt the Landmarks-Attention Network (we call it SIR-LAN) to extract landmarks patches features separately, while some works stack all landmarks patches and then extract the whole features directly (we call it SIR-Stack), as illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. To demonstrate the effectiveness of our network, we conduct an experiment by SIR framework to compare the above two networks with the same number of CNN layers and model parameters, the structure of SIR-Stack is showed in <ref type="figure" target="#fig_6">Figure 5</ref>. The result illustrated in <ref type="figure" target="#fig_7">Figure 6</ref> shows that the proposed network extracting patches features separately performs significantly better than previous methods extracting patches feature together (e.g., MDM <ref type="bibr" target="#b18">(Trigeorgis et al. 2016)</ref>).  Effect of iteration times. From <ref type="figure">Figure 7</ref>, we can find that the accuracy will be improved by adding iteration times before the stable performance (i.e., 4 iterations) is achieved. When increasing iteration times, more model memory will be added in baseline CR.</p><p>Figure 7: Effect of iteration times. Top: Comparison between SIR and baseline CR in accuracy. With the increase of iteration times, both SIR and baseline CR can decrease the detection error and SIR performs better than baseline CR. Bottom: Comparison between SIR and baseline CR in in Model Memory. Increasing the iteration times will increase its model memory of baseline CR, while SIR doesn't because it can iteratively call itself.</p><p>Effect of Gaussian sampling space parameters. As one of the most important processes, random sampling space significantly affects the final robustness and accuracy. As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, the NME results are presented by varying the standard deviation ? of Gaussian sampling. Appropriate values lead to promising performance so that we set ? = 0.2 in our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we develop a SIR framework solve the nonlinear least squares problems. Compared with cascaded regression, it only needs to train a single regressor to learn descent directions in coarse stages to fine stages together, and refines the target parameters iteratively by call the same regressor. Experimental results in the facial landmarks detection task demonstrate that the proposed self-iterative regressor achieves comparable accuracy to state-of-the-art methods, but significantly reduces the number of parameters and memory storage of the pre-trained models. In the future, we will extend the proposed method to other applications, such as human pose prediction, structure from motion and 3D face reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Training and testing process of the proposed SIR. (a) random sampling process. (b) Landmarks-Attention Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>location ? t,i by S t,i by Equ. (3); 5:Extract patches P t,i for image I i in location ? t,i ; 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) and (b). Since sampling period T can be large enough, online random sampling process can generate nearly unlimited training data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>comparing the performance of SIR vs. stateof-the-art and baseline cascaded regression; (2) comparing the number of model parameters and memory storage of pretrain models; and (3) studying the effect of the proposed feature extraction network(LAN), the number of iteration times and sampling space parameter. Datasets. The 300-W dataset is short for 300 faces in-thewild (Sagonas et al. 2016), which is designed for evaluating the performance of facial landmarks detection. The training set (3, 148 faces in total) consists of AFW dataset (Ramanan 2012), HELEN training set (Le et al. 2012) and LFPW training set (Belhumeur et al. 2011). Two testing sets are established, i.e., public testing set (689 faces in total) including HELEN testing set (Le et al. 2012), LFPW testing set (Belhumeur et al. 2011) and IBUG dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) CED curve of facial 68-points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>CED curve results comparison in 300-W competition testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Structure of SIR-Stack Network. For a fair comparison, we adopt the same number of CNN layers and model parameters(3.72M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison between SIR-LAN and SIR-Stack in the competition testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Performances of different Gaussian sampling in the 300-W public testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Several facial landmarks detection results in 300-W public testing and competition testing set. Blue dot in each sub-picture indicates ground truth landmarks location and yellow dot indicates the predicted location of SIR. Pictures for the five rows are from HELEN testing set, LFPW testing set, IBUG set, 300-W competition testing Indoor and Outdoor set respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Feature extraction sub-network of Landmarks-Attention Network for each patch.</figDesc><table><row><cell>Layer</cell><cell>Input Tensor</cell><cell>Kernel</cell><cell>Output Tensor</cell></row><row><cell>conv1</cell><cell>57 ? 57 ? 3</cell><cell>3 ? 3 ? 3 ? 16</cell><cell>57 ? 57 ? 16</cell></row><row><cell>pool1</cell><cell>57 ? 57 ? 16</cell><cell>2 ? 2</cell><cell>29 ? 29 ? 16</cell></row><row><cell>conv2</cell><cell>29 ? 29 ? 16</cell><cell>2 ? 2 ? 16 ? 32</cell><cell>29 ? 29 ? 32</cell></row><row><cell>pool2</cell><cell>29 ? 29 ? 32</cell><cell>2 ? 2</cell><cell>15 ? 15 ? 32</cell></row><row><cell>conv3</cell><cell>15 ? 15 ? 32</cell><cell>2 ? 2 ? 32 ? 64</cell><cell>15 ? 15 ? 64</cell></row><row><cell>pool3</cell><cell>15 ? 15 ? 64</cell><cell>2 ? 2</cell><cell>8 ? 8 ? 64</cell></row><row><cell>fc1</cell><cell>8 ? 8 ? 64 (1 ? 1 ? 4096)</cell><cell>4096 ? 10</cell><cell>1 ? 1 ? 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>NME (inter-pupil normalization) results in the public testing set. The top two performance are shown in boldface.</figDesc><table><row><cell>Methods</cell><cell>Common subset</cell><cell>Challenging subset</cell><cell>Full set</cell></row><row><cell>RCPR(2013)</cell><cell>6.18</cell><cell>17.26</cell><cell>8.35</cell></row><row><cell>ESR(2012)</cell><cell>5.28</cell><cell>17.00</cell><cell>7.58</cell></row><row><cell>SDM(2013)</cell><cell>5.57</cell><cell>15.40</cell><cell>7.50</cell></row><row><cell>LBF(2014)</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>CFAN(2014)</cell><cell>5.55</cell><cell>-</cell><cell>-</cell></row><row><cell>CFSS(2015)</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>Kowalski et al.(2016)</cell><cell>4.62</cell><cell>9.48</cell><cell>5.57</cell></row><row><cell>MDM(2016)</cell><cell>4.83</cell><cell>10.14</cell><cell>5.88</cell></row><row><cell>RAR(2016)</cell><cell>4.12</cell><cell>8.35</cell><cell>4.94</cell></row><row><cell>SIR</cell><cell>4.29</cell><cell>8.14</cell><cell>5.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results using AUC 0.08 (%) and FR 0.08 (%) in the competition testing set. 51-points and 68points are two groups of facial landmarks and 51-points is part of 68-points.</figDesc><table><row><cell></cell><cell cols="2">51-points</cell><cell cols="2">68-points</cell></row><row><cell>Method</cell><cell>AUC</cell><cell>Failure</cell><cell>AUC</cell><cell>Failure</cell></row><row><cell>ERT(2014)</cell><cell>40.60</cell><cell>13.50</cell><cell>32.35</cell><cell>17.00</cell></row><row><cell>PO-CR(2015)</cell><cell>47.65</cell><cell>11.70</cell><cell>-</cell><cell>-</cell></row><row><cell>Chehra(2014)</cell><cell>31.12</cell><cell>39.30</cell><cell>-</cell><cell>-</cell></row><row><cell>Intraface(2013)</cell><cell>38.47</cell><cell>19.70</cell><cell>-</cell><cell>-</cell></row><row><cell>Balt et al.(2013)</cell><cell>37.65</cell><cell>17.17</cell><cell>19.55</cell><cell>38.83</cell></row><row><cell>zhou et al.(2013)</cell><cell>53.29</cell><cell>5.33</cell><cell>32.81</cell><cell>13.00</cell></row><row><cell>Yan et al.(2013)</cell><cell>49.07</cell><cell>8.33</cell><cell>34.97</cell><cell>12.67</cell></row><row><cell>CFSS(2015)</cell><cell>50.79</cell><cell>7.80</cell><cell>39.81</cell><cell>12.30</cell></row><row><cell>MDM(2016)</cell><cell>56.34</cell><cell>4.20</cell><cell>45.32</cell><cell>6.80</cell></row><row><cell>SIR</cell><cell>58.11</cell><cell>2.83</cell><cell>46.56</cell><cell>4.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods in the public testing set, with the first and the second best results highlighted. DL indicates whether the method is based on deep learning.</figDesc><table><row><cell>Method</cell><cell cols="4">DL NME # params model memory</cell></row><row><cell>RCPR(2013)</cell><cell>N</cell><cell>8.35</cell><cell>-</cell><cell>91.3MB</cell></row><row><cell>LBF(2014)</cell><cell>N</cell><cell>6.32</cell><cell>-</cell><cell>36.6MB</cell></row><row><cell>CFSS(2015)</cell><cell>N</cell><cell>5.76</cell><cell>-</cell><cell>225.2MB</cell></row><row><cell>MDM(2016)</cell><cell>Y</cell><cell>5.61</cell><cell>80.56M</cell><cell>322.3MB</cell></row><row><cell>RAR(2016)</cell><cell>Y</cell><cell>4.94</cell><cell>15.65M+</cell><cell>62.6MB+</cell></row><row><cell>baseline CR</cell><cell>Y</cell><cell>6.23</cell><cell>14.88M</cell><cell>62.4MB</cell></row><row><cell>SIR</cell><cell>Y</cell><cell>5.04</cell><cell>3.72M</cell><cell>15.6MB</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asthana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
	<note>Breiman 2001] Breiman, L. 2001. Random forests</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perona</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
	<note>Face alignment by explicit shape regression</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view facial landmark detection by using a 3d shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">M3 csr. IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approaching human level facial landmark localization by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welinder</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona ; Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Cascaded pose regression</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Localizing facial keypoints with global descriptor search, neighbour alignment and locally linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Almaev</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valstar ; Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moalem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. [Kamrul Hasan, Pal, and Moalem</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face alignment using k-cluster regression forests with weighted splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imagenet classification with deep convolutional neural networks. In NIPS</title>
		<meeting><address><addrLine>Hinton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">L2,1-based regression and prediction accumulation across views for robust facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="36" to="44" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Martinez and Valstar</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiview active shape models with sift descriptors for the 300-w face landmark challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishop</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagonas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Simonyan and Zisserman</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Tang ; Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trigeorgis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view facial landmark detector learned by the structured output SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Uric?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<idno>abs/1212.5701</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>An empirical study of recent face alignment methods</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
