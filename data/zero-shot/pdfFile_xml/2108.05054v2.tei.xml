<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Coarse-to-Fine Approach in Single Image Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jin</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo-Won</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Pyo</forename><surname>Hong</surname></persName>
							<email>jphong@dali.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
							<email>sjko@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Coarse-to-Fine Approach in Single Image Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coarse-to-fine strategies have been extensively used for the architecture design of single image deblurring networks. Conventional methods typically stack sub-networks with multi-scale input images and gradually improve sharpness of images from the bottom sub-network to the top subnetwork, yielding inevitably high computational costs. Toward a fast and accurate deblurring network design, we revisit the coarse-to-fine strategy and present a multi-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct features. First, the single encoder of the MIMO-UNet takes multi-scale input images to ease the difficulty of training. Second, the single decoder of the MIMO-UNet outputs multiple deblurred images with different scales to mimic multi-cascaded U-nets using a single U-shaped network. Last, asymmetric feature fusion is introduced to merge multi-scale features in an efficient manner. Extensive experiments on the GoPro and Real-Blur datasets demonstrate that the proposed network outperforms the state-of-the-art methods in terms of both accuracy and computational complexity. Source code is available for research purposes at https://github.com/ chosj95/MIMO-UNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image deblurring aims to recover a latent sharp image from a blurry image <ref type="bibr" target="#b2">[3]</ref>. Even with the rapid development of camera modules in the last few decades, blur artifact still exists when camera and/or objects move. Blurry images are not only visually unpleasant but significantly degrade the performance of vision systems including surveillance <ref type="bibr" target="#b31">[32]</ref> and autonomous driving systems <ref type="bibr" target="#b3">[4]</ref>, necessitating accurate and efficient image deburring techniques.</p><p>Owing to the success of deep learning, convolutional neural network (CNN)-based image deblurring methods have been extensively studied and showed promising performance. Early CNN-based image deblurring meth-* equal contribution ? corresponding author <ref type="figure">Figure 1</ref>. Comparison between the proposed and conventional methods in terms of the PSNR and runtime. The runtime of the methods is reported as the runtime measured using the released test code of each method on our environment (filled) and the runtime provided in each paper (blank). ods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref> commonly exploit CNN as a blur kernel estimator and construct two-stage image deblurring framework, i.e., CNN-based blur kernel estimation stage and kernel-based deconvoltion stage. On the other hand, recent CNN-based image deblurring methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref> aim to directly learn the complicated relationship between blurry-sharp image pairs in an end-to-end manner. As a pioneering technique, a deep multi-scale CNN for dynamic scene deblurring (DeepDeblur) <ref type="bibr" target="#b19">[20]</ref> is introduced to directly regress a sharp image from a blurry image. DeepDeblur consists of multiple stacked sub-networks to handle multiscale blur, where each sub-network takes a down-scaled image and gradually recovers a sharp image in a coarse-tofine manner. Motivated by the success of DeepDeblur, various CNN-based image deblurring methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref> have been introduced with remarkable performance improvements. Although these methods try to improve the deblurring performance in different aspects, their coarse-tofine strategies are similar in that multiple sub-networks are stacked. In other words, a coarse-to-fine network design principle has proven to be effective in image deblurring. However, such efficiency comes at the cost of the inevitable increase in the computational complexity and memory usage, making the conventional methods difficult to be used for cost and time-sensitive environments such as mobile devices, vehicles, and robots. Recently, a light-weight CNN is presented for efficient single image deblurring <ref type="bibr" target="#b32">[33]</ref>. Specifically, by using optical flow and global motion of blurry images as extra supervision for network training, they design a shallower architecture compared to that of conventional deblurring networks. However, such shallow architecture failed in obtaining deblurring accuracy comparable to stateof-the-art methods.</p><p>In this paper, we revisit the coarse-to-fine scheme and present a novel deblurring network called multi-input multioutput UNet (MIMO-UNet) that can handle multi-scale blur with low computational complexity. The proposed MIMO-UNet is a single encoder-decoder-based U-shaped network that has three distinct features.</p><p>First, the single decoder of the MIMO-UNet outputs multiple deblurred images, and therefore we name our decoder as multi-output single decoder (MOSD). The MOSD is simple but can mimic conventional network architectures composed of stacked sub-networks and guide the decoder layers to gradually recover latent sharp images in a coarseto-fine manner. Second, the single encoder of the MIMO-UNet takes multi-scale input images; thus, our encoder is called multi-input single encoder (MISE). Last, asymmetric feature fusion (AFF) is introduced to merge multi-scale features in an efficient manner. The AFF takes features from different scales and merges multi-scale information flow across the encoder and the decoder to improve the deblurring performance. Extensive experiments demonstrate the superiority of the proposed MIMO-UNet compared to the state-of-the-art methods in terms of the PSNR as well as the computational complexity as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we review the conventional image deblurring methods that adopt a coarse-to-fine strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DeepDeblur</head><p>As a pioneering work, DeepDeblur directly learns the relation between blurry-sharp image pairs in an end-to-end manner by adopting a coarse-to-fine strategy <ref type="bibr" target="#b19">[20]</ref>. Nah et al. also introduced the real-world image deblurring dataset named the GoPro dataset. Specifically, using a sequence of sharp images captured at 240 fps using a GoPro camera, a blurry image, B, is obtained by averaging successive sharp images as follow:</p><formula xml:id="formula_0">B = 1 M M ?1 i=0 S[i],<label>(1)</label></formula><p>where M and S[i] represent the number of sampled sharp images and the i th sharp image, respectively. To construct a blurry and sharp image pair for training, the ground-truth sharp image for B is chosen by selecting the middle image from the sampled sharp images.</p><p>To adopt a coarse-to-fine strategy in CNN for gradual recovery of latent sharp images, DeepDeblur uses multiple stacks of sub-networks as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a). Each subnetwork consists of a sequence of convolutional layers that maintains the spatial resolution of input feature maps. Different scales of input images are fed into the sub-networks, and the resultant image from a coarser scale sub-network is concatenated with the input of a finer scale sub-network to enable coarse-to-fine information transfer. The reconstruc-tion procedure of DeepDeblur is formulated as follows:</p><formula xml:id="formula_1">S n = H D ? D n (B n ; (? n+1 ) ? ) + B n ,<label>(2)</label></formula><p>where H D ? D n is the n th sub-network of DeepDeblur parameterized by ? D n . B n and? n are blurry and deblurred images at the n th scale, respectively, and ? denotes the up-sampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">PSS-NSC</head><p>Inspired by the success of DeepDeblur, Gao et al. presented parameter selective sharing and nested skip connections (PSS-NSC) <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b), the architecture of PSS-NSC is similar to that of DeepDeblur, but has two distinct features. First, each sub-network is structured as an encoder-decoder-based U-Net with symmetric skip connections that directly transfers the feature maps from the encoder to the decoder. Second, since every subnetwork commonly aims to recover a sharp image from a blurry image, most network parameters are shared among sub-networks. Therefore, the memory requirement of PSS-NSC is significantly reduced, but the computational complexity is still demanding because the final sharp image is generated after passing through the three sub-networks. The reconstruction procedure of PSS-NSC is formulated as follows:?</p><formula xml:id="formula_2">n = H P (? P n ,? P ) (B n ; (? n+1 ) ? ) + B n ,<label>(3)</label></formula><p>where H P (? P n ,? P ) represents the n th sub-network of PSS-NSC with exclusive parameter ? P n and shared parameter ? P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">MT-RNN</head><p>The network architecture of multi-temporal recurrent neural networks (MT-RNN) <ref type="bibr" target="#b21">[22]</ref> is illustrated in Figure 2(c). In MT-RNN, a single U-shaped network is repeated seven times, and the feature maps from the decoder at the previous iteration are transferred to the encoder at the next iteration as green colored arrows. For each iteration, MT-RNN is trained to predict an averaged image obtained using a different number of M in Eq. 1, where M decreases as the iteration proceeds. Due to the repeated application of a single U-shaped network, MT-RNN has low memory usage but low runtime efficiency. The reconstruction procedure of PSS-NSC is formulated as follows:</p><formula xml:id="formula_3">? i , F i = H M ? M B i ;? i?1 , F i?1 ,<label>(4)</label></formula><p>where i refers to an iteration index. H M ? M is the network of MT-RNN parameterized by ? M . B i ,? i , and F i are input blurry image, estimated latent image, and feature maps at the i th iteration, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>We propose MIMO-UNet that fully exploits multi-scale features extracted from an input image. <ref type="figure">Figure 3</ref> shows the overall architecture of MIMO-UNet. The architecture of MIMO-UNet is based on a single U-Net <ref type="bibr" target="#b25">[26]</ref> with significant modifications for efficient multi-scale deblurring. The encoder and decoder of MIMO-UNet are composed of three encoder blocks (EBs) and decoder blocks (DBs). The following subsections detail the three special features of MIMO-UNet, i.e., MISE, MOSD, and AFF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-input single encoder</head><p>It has been demonstrated that different levels of blur in images can be better handled from multi-scale images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. Various CNN-based deblurring methods have also adopted this idea by taking a blurry image with a different scale as an input of each sub-network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In our MIMO-UNet, not a sub-network but an EB takes a blurry image with a different scale as an input. In other words, in addition to the downsized feature extracted from the above EB, we extract the feature from the downsampled blurry image and then combine both features. By taking advantage of the complementary information from the downsized feature and the feature obtainable from the downsampled image, our EB is expected to handle diverse image blurs effectively. The use of multi-scale images as an input for a single U-Net has also proven to be effective in other tasks such as depth map super-resolution <ref type="bibr" target="#b5">[6]</ref> and object detection <ref type="bibr" target="#b20">[21]</ref>.</p><p>We first extract the features from the downsampled image using a shallow convolutional module (SCM) as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(a). Considering efficiency, we use two stacks of 3 ? 3 and 1 ? 1 convolutional layers. We concatenate the features from the last 1 ? 1 layer with the input B k , and further refine the concatenated features using an additional 1 ? 1 convolutional layer. The output of the SCM at the k th level is denoted as SCM out k , where we use SCM for the second and third levels as shown in <ref type="figure">Figure 3</ref>.</p><p>For the fusion of SCM out k with the output of the k ? 1 th level EB, EB out k ?1 , we apply a convolutional layer with a stride of 2 to EB out k ?1 , resulting in EB out</p><formula xml:id="formula_4">k?1 ? . The two fea- tures EB out k?1 ? and SCM out k</formula><p>have the same size and thus can be fused. Here, we exploit a feature attention module (FAM) to actively emphasize or suppress the features from the previous scale and learn the spatial/channel importance of the features from SCM. We experimentally demonstrate that this module increases the performance compared to general feature fusion approaches as detailed in Sec. 4.3.</p><p>In particular, EB out k?1 ? and SCM out k are element-wise multiplied with each other, and then the multiplied features are passed through a 3 ? 3 convolutional layer. The output of the 3 ? 3 convolutional layer is expected to include com-</p><formula xml:id="formula_5">EB 1 DB 2 DB 1 Feature attention module Convolution transpose " ! " " " # " # Convolution Concatenation C Residual blocks Element-wise summation ! ?? ?32 ? 2 ? 2 ?64 ? 4 ? 4 ?128 EB 2 EB 3 DB 3 AFF 1 AFF 2 SCM 2 SCM 3 C C ? 4 ? 4 ?128 ? 2 ? 2 ?64 ?? ?32 ? 4 ? 4 ?3 ? 2 ? 2 ?3</formula><p>?? ?3 <ref type="figure">Figure 3</ref>. The architecture of the proposed network.</p><p>plementary information for deblurring, and finally added to EB out k?1 ? to be further refined through following residual blocks, where we used eight modified residual blocks <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-output single decoder</head><p>In MIMO-UNet, different DBs have feature maps with different sizes. We consider that these multi-scale feature maps can be used to mimic multi-stacked sub-networks. Unlike the intermediate supervision at the sub-network as the conventional coarse-to-fine networks, we apply the intermediate supervision to each DB. The image reconstruction in each level can be formulated as follows:</p><formula xml:id="formula_6">S n = o(DB n (AFF out n ; DB out n+1 )) + B n , n = 1, 2, o(DB n (EB out n )) + B n , n = 3,<label>(5)</label></formula><p>where AFF out n , EB out n , and DB out n are the outputs of the n th level asymmetric feature fusion (AFF) module, EB, and DB, respectively. Since the output of DB is a feature map not an image, mapping function o is required for generating an intermediate output image, where we use a single convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Asymmetric feature fusion</head><p>In most conventional coarse-to-fine image deblurring networks, only the features from the coarser-scale subnetwork are used for the finer-scale sub-networks, making information flow inflexible. One exceptional method is to cascade the whole network in horizontal or vertical direction, allowing top-to-bottom and bottom-to-top information flow <ref type="bibr" target="#b34">[35]</ref>. Inspired by dense connection between intra-scale features <ref type="bibr" target="#b12">[13]</ref>, we present an asymmetric feature fusion (AFF) module as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(c) to allow information flow from different scales within a single U-Net. Each AFF takes the outputs of all EBs as an input and combines multi-scale features using convolutional layers. The output of the AFF is delivered to its corresponding DB. More specifically, the first-level and second-level AFFs, AFF 1 and AFF 2 , are formulated as follows:</p><formula xml:id="formula_7">out 1 EB out 3 EB out 2 EB 1?1 Conv 3?3 Conv Resize Resize Resize C out AFF k (c) (a) k B 3?3 Conv 1?1 Conv 3?3 Conv 1?1 Conv out SCM k C 1?1 Conv (b) out SCM k out (EB ) k?3 ?3 Conv</formula><formula xml:id="formula_8">AFF out 1 = AFF 1 EB out 1 , EB out 2 ? , EB out 3 ? AFF out 2 = AFF 2 EB out 1 ? , EB out 2 , EB out 3 ? ,<label>(6)</label></formula><p>where AFF out n represents the outputs of the n th AFF. Up- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>Likewise with other multi-scale deblurring networks, we use the multi-scale content loss function <ref type="bibr" target="#b19">[20]</ref>, where we found that L1 loss produces better results than MSE loss for our network. The content loss L cont is defined as follows:</p><formula xml:id="formula_9">L cont = K k=1 1 t k ? k ? S k 1 ,<label>(7)</label></formula><p>where K is the number of levels. We divide the loss by the number of total elements t k for normalization. Recent studies also suggest the auxiliary loss terms in addition to the content loss for the performance improvement <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. In image enhancement and restoration tasks, auxiliary loss terms that minimize the distance between the input and output in the feature space have been widely used and showed promising results <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37]</ref>. Since the purpose of deblurring is to restore the lost high-frequency component, it is essential to reduce the difference in the frequency space. To this end, we present multi-scale frequency reconstruction (MSFR) loss function. The MSFR loss measures the L1 distance between multi-scale ground-truth and deblurred images in the frequency domain as follows:</p><formula xml:id="formula_10">L M SF R = K k=1 1 t k F(? k ) ? F(S k ) 1 ,<label>(8)</label></formula><p>where F denotes the fast Fourier transform (FFT) that transfers image signal to the frequency domain. The final loss function for training our network is determined as follows: where we experimentally set ? = 0.1.</p><formula xml:id="formula_11">L total = L cont + ?L M SF R ,<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and implementation details</head><p>We used the GoPro <ref type="bibr" target="#b19">[20]</ref> and RealBlur <ref type="bibr" target="#b24">[25]</ref> training datasets for training our models which consist of 2,103 and 3,758 pairs of blurred and sharp images. The GoPro and Real blur test datasets were used for testing, where the number of image pairs are 1,111 and 980, respectively. For testing on the GoPro test dataset, we trained our model using only the GoPro training dataset.</p><p>For every training iteration, we randomly sampled four images and then randomly cropped the sampled images with the size of 256 ? 256. For data augmentation, each patch was horizontally flipped with a probability of 0.5. For deblurring of images in the GoPro dataset, we trained our network for 3,000 epochs which were sufficient for convergence. The learning rate was initially set to 10 ?4 and decreased by the factor of 0.5 at every 500 epochs. For deblurring of images in the RealBlur dataset, we trained our network for 1,000 epochs, and used the same initial learning rate but decreased it by the factor of 0.5 at every 200 epochs. Our experiments were conducted on Intel i5-8400 and NVIDIA Titan XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance comparison</head><p>We compared MIMO-UNet with state-of-the-art deblurring networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>. Considering the trade-off between the computational complexity and deblurring accuracy, we evaluated the following three variants of MIMO-UNet: 1) MIMO-UNet employing 8 residual blocks for each EB and DB, 2) MIMO-UNet+ employing 20 residual blocks for each EB and DB, and 3) MIMO-UNet++ estimating the resultant image using MIMO-UNet+ with geometric self-ensemble <ref type="bibr" target="#b15">[16]</ref>. The quantitative results on the GoPro test dataset are reported in <ref type="table" target="#tab_0">Table 1</ref> els is provided as the runtime measured using the released test code of each model on our PC (left) and the runtime reported in each paper (right). MIMO-UNet+ and MIMO-UNet++ were slower than MIMO-UNet but still performed deblurring in 0.014s and 0.040s, respectively. The average PSNR of MIMO-UNet++ was obtained as 32.68 dB. MIMO-UNet showed the average processing time of 0.008s and the average PSNR of 31.73 dB. These three models demonstrate the best trade-off between the accuracy and computational complexity as shown in <ref type="figure">Figure 1</ref>. <ref type="bibr" target="#b0">1</ref> Due to the stacked sub-networks, DeepDeblur, SRN, PSS-NSC, DMPHN, and SAPHN required large computational costs as shown in <ref type="table" target="#tab_0">Table 1</ref>. Compared with these methods, MIMO-UNet+ was faster but achieved still higher PSNR scores. Although SRN, PSS-NSC, and MT-RNN employ fewer parameters than the proposed methods, these methods repetitively use parameters in the procedure, and therefore they are slower than the our slowest model MIMO-UNet++. Especially, MIMO-UNet++ was 4.05 times faster and 0.02 dB higher in terms of PSNR compared to MPRNet that is the best method among the conventional methods. The single network-based methods, such as RADN and SVDN, achieved high runtime efficiency compared to the stacked sub-networks. However, MIMO-UNet outperforms SVDM, and MIMO-UNet+ outperforms RADN, in terms of both runtime and PSNR. To validate the effectiveness of the proposed method on the real case scenario, we also evaluated our methods on the recent RealBlur dataset <ref type="bibr" target="#b24">[25]</ref>. As listed in <ref type="table">Table 2</ref> in <ref type="figure" target="#fig_2">Figure 5</ref> and <ref type="figure" target="#fig_3">Figure 6</ref>, respectively. For the reproduction of results, we used the author-released network models trained on each dataset, i.e., SRN, PSS-NSC, DMPHN, MT-RNN, and MPRNet were used for the GoPro dataset, and DeblurGAN-v2 and SRN for the RealBlur dataset, respectively. Although the resultant images obtained by the conventional networks exhibit much less blur compared to the input blurry images, local details and structures were not sufficiently deblurred as can be noticed from the magnified image regions, whereas our method produced sharper images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We conducted experiments to analyze the effectiveness of each component of MIMO-UNet on the GoPro test dataset. First, we evaluated the effectiveness of different feature fusion methods in MISE. The proposed FAM was compared with the conventional fusion methods: concatenation and element-wise sum, and achieved the highest performance as listed in <ref type="table">Table 3</ref>. Second, we tested MIMO-UNet without MOSD, MISE, AFF, and/or MSFR. For comparison, a baseline model was trained without using any of the four components, resulting the average PSNR of 31.16 dB. As shown in <ref type="table">Table 4</ref>, compared with the baseline model, MOSD improved PSNR by 0.17 dB. The standalone use of MISE showed a marginal effect because multi-scale information is difficult to be used in a simple U-Net. However, when used with MOSD, MISE contributed to the further performance improvement of PSNR by 0.05 dB. AFF improved PSNR by 0.17 dB compared to the baseline model, and the performance gain was further increased to 0.23 dB when AFF was used with MISE. With MISE, MOSD, and AFF, the network achieved 0.30 dB higher PSNR, and finally, the network trained using MSFR achieved 0.57 dB higher PSNR compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object detection performance evaluation</head><p>Single image deblurring can also boost the performance of computer vision tasks when used as a preprocessing technique. Object detection is one of the best examples in which single image deblurring can be used to improve the performance. With the advances in the CNNs, object detection methods have adopted CNNs and achieved significant improvements <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. However, most of these methods assume blur-free input images, and therefore they often fail to detect objects in blurry images. <ref type="figure" target="#fig_4">Figure 7(a)</ref> illustrates the failure case of PFPNet <ref type="bibr" target="#b12">[13]</ref>, which is one of the state-ofthe-art object detectors, in detecting objects from a blurry image, depicting its vulnerability to blurry inputs. When the same PFPNet was applied to the deblurred image obtained using MIMO-UNet++, many of the false negative examples could be successfully detected as shown in <ref type="figure" target="#fig_4">Figure 7</ref>(b). <ref type="figure">Figure 8</ref>. Object detection performance evaluation. Following the measurement <ref type="bibr" target="#b13">[14]</ref>, we use the bounding boxes obtained from the sharp images as the ground-truth.</p><p>Last, we compared the proposed MIMO-UNet++ with the other deblurring techniques in terms of their effectiveness in the object detection task as preprocessing. Similar to the previous experiment, PSS-NSC and DMPHN with the author-provided codes were used for comparison. Although PFPNet was trained using the PASCAL VOC dataset <ref type="bibr" target="#b1">[2]</ref> that contains 20 different classes, the blurry images in the GoPro dataset primarily contain only three classes among them, i.e., car, person, and potted plant. Therefore, the average precision (AP) of each object class was measured for the performance evaluation. As shown in <ref type="figure">Figure 8</ref>, the proposed MIMO-UNet++ resulted in the best performance in object detection. Moreover, since the proposed method recorded the fastest execution time, it is most suitable as a preprocessing technique for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a fast and accurate image deblurring network. Instead of stacking multiple subnetworks for coarse-to-fine deblurring, we presented a single U-Net that has distinct features, enabling much simpler but more effective coarse-to-fine deblurring. The encoder of the network is modified to take multi-scale input images and combine features from different sources. The decoder of the network is also changed to output multi-scale deblurred images during decoding such that coarse-to-fine deblurring can be better performed. A feature fusion method is also introduced to asymmetrically combine multi-scale features for dynamic image deblurring. The experimental results demonstrate that our method outperforms the other conventional methods in regard to the speed and accuracy trade-off.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of coarse-to-fine image deblurring network architectures: (a) DeepDeblur, (b) PSS-NSC, (c) MT-RNN, and (d) proposed MIMO-UNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The structures of sub-modules: (a) SCM, (b) feature attention, and (c) AFF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Several examples on the GoPro test dataset. For clarity, the magnified parts of the resultant images are displayed. From left-top to right-bottom: Blurry images, ground-truth images, and the resultant images obtained by SRN, PSS-NSC, DMPHN, MT-RNN, MPRNet, and MIMO-UNet++, respectively. sampling (?) and down-sampling (?) are applied such that the features from different scales can be concatenated. Each DB of MIMO-UNet can thus exploit multi-scale features, resulting the improved deblurring performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Several examples on the RealBlur test dataset. For clarity, the magnified parts of the resultant images are displayed. From left to right: Blurry images, ground-truth images, and the resultant images obtained by DeblurGAN-v2, SRN, and MIMO-UNet++, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>The examples of object detection result from (a) blurry image and (b) resultant image obtained by MIMO-UNet++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The average PSNR and SSIM on the GoPro test dataset. The SAPHNs with ? and ? denote the models with and without offsets, respectively. We employ stacked(4) version for DMPHN. The runtime and parameters are expressed in seconds and millions.</figDesc><table><row><cell>. For a fair comparison, the runtime of the mod-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, MIMO-UNet++ recorded the best and the second best performance in terms of PSNR and SSIM, respectively. The several resultant images from the GoPro and RealBlur test datasets are shown 1 Runtime measured using GPU synchronization mode can be find in on our website.</figDesc><table><row><cell cols="2">Model</cell><cell cols="2">PSNR SSIM</cell></row><row><cell cols="4">DeblurGAN-v2 [15] 29.69 0.870</cell></row><row><cell cols="2">SRN [31]</cell><cell cols="2">31.38 0.909</cell></row><row><cell cols="2">MPRNet [34]</cell><cell cols="2">31.76 0.922</cell></row><row><cell cols="2">MIMO-UNet+</cell><cell cols="2">31.92 0.919</cell></row><row><cell cols="2">MIMO-UNet++</cell><cell cols="2">32.05 0.921</cell></row><row><cell cols="4">Table 2. The average PSNR and SSIM on the RealBlur test</cell></row><row><cell>dataset [25].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Method Concat. Element-wise sum FAM</cell></row><row><cell>PSNR</cell><cell>31.66</cell><cell>31.60</cell><cell>31.73</cell></row><row><cell></cell><cell cols="2">Table 3. Ablation studies on FAM.</cell><cell></cell></row><row><cell cols="4">MISE MOSD AFF MSFR PSNR Params.</cell></row><row><cell></cell><cell></cell><cell>31.16</cell><cell>6.46</cell></row><row><cell></cell><cell></cell><cell>31.17</cell><cell>6.72</cell></row><row><cell></cell><cell></cell><cell>31.33</cell><cell>6.47</cell></row><row><cell></cell><cell></cell><cell>31.33</cell><cell>6.54</cell></row><row><cell></cell><cell></cell><cell>31.38</cell><cell>6.73</cell></row><row><cell></cell><cell></cell><cell>31.38</cell><cell>6.54</cell></row><row><cell></cell><cell></cell><cell>31.39</cell><cell>6.80</cell></row><row><cell></cell><cell></cell><cell>31.46</cell><cell>6.81</cell></row><row><cell></cell><cell></cell><cell>31.73</cell><cell>6.81</cell></row><row><cell cols="4">Table 4. Effectiveness of different components of MIMO-UNet on</cell></row><row><cell cols="2">the GoPro test dataset.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegement</head><p>This work was supported by Samsung Electronics Co., Ltd (IO201210-08026-01)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time stereo vision for urban traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Joos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium</title>
		<meeting>the IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical features driven residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for direct text deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Hradi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemc?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip?roubek</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf., volume</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Ichimura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02336</idno>
		<title level="m">Spatial frequency loss for learning convolutional autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DSLR-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3277" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Focal frequency loss for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyong-Keun</forename><surname>Kook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jee-Young</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun-Cheon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blur-kernel bound estimation from pyramid statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1012" to="1016" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="783" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient featurized image pyramid network for single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7336" to="7344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="327" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11882" to="11889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jucheol</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5572" to="5581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3606" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A coprime blur scheme for data security in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3066" to="3072" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3555" to="3564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14821" to="14831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image demoireing with learnable bandpass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
