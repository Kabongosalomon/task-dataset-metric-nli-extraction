<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Technical Report: Temporal Aggregate Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
							<email>sener@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dibyadip</forename><surname>Chatterjee</surname></persName>
							<email>dibyadip@comp.nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
							<email>ayao@comp.nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Technical Report: Temporal Aggregate Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report extends our work presented in <ref type="bibr" target="#b8">[9]</ref> with more experiments. In [9], we tackle long-term video understanding, which requires reasoning from current and past or future observations and raises several fundamental questions. How should temporal or sequential relationships be modelled? What temporal extent of information and context needs to be processed? At what temporal scale should they be derived? [9] addresses these questions with a flexible multi-granular temporal aggregation framework. In this report, we conduct further experiments with this framework on different tasks and a new dataset, EPIC-KITCHENS-100. Our code and models can be found in https://github.com/dibschat/tempAgg</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we utilize the temporal aggregates model presented in <ref type="bibr" target="#b8">[9]</ref> for next action anticipation, action, and activity recognition in long-range videos, see <ref type="figure">Fig. 2</ref>. We also test our method on the new EPIC-KITCHENS-100 dataset. Our model is described in detail in <ref type="bibr" target="#b8">[9]</ref>, and we refer the reader to this paper for further detail.</p><p>An overview of the building blocks of our temporal aggregates framework can be found in <ref type="figure">Fig. 1</ref>. We split video streams into snippets of equal length and max-pool the frame features within the snippets. We then create ensembles of multi-scale feature representations that are aggregated bottom-up based on scaling and temporal extent. Based on different start and end frames i and j and number of snippets K, we define two types of snippet features: 'recent' features {R} from recent observations and "spanning" features {S} drawn from the long-term video. The recent snippets cover a couple of seconds (or up to a minute, depending on the temporal granularity) before the current time point, while spanning snippets refer to the long-term past and may last up to ten minutes. In <ref type="figure">Fig. 1</ref> we use two starting points to compute the "recent past" snippet features and represent each with K R = 3 number of snippets (   <ref type="figure">Figure 1</ref>. Model overview: In this example we use 3 scales for computing the "spanning past" snippet features SK 1 , SK 2 , SK 3 , and 2 starting points to compute the "recent past" snippet features, Ri 1 , Ri 2 , by max-pooling over the frame features in each snippet. Each recent snippet is coupled with all the spanning snippets in our Temporal Aggregation Block (TAB). An ensemble of TAB outputs is used for next action anticipation. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp;</head><p>). And we use three scales to compute the "spanning past" snippet features with K = {7, 5, 3} ( , &amp;</p><p>). Key to both types of representations is the ensemble of snippet features from multiple scales.</p><p>Our framework is build in a bottom up manner, starting with the recent and spanning features R and S, which are coupled with non-local blocks (NLB) within coupling blocks (CB). Non-local operations <ref type="bibr" target="#b11">[12]</ref> are applied to capture relationships amongst the spanning snippets and between spanning and recent snippets. Two such NLBs are combined in a Coupling Block (CB) which calculates attention-reweighted recent and spanning context representations. Each recent with all spanning representations are coupled via individual CBs and their outputs are combined in a Temporal Aggregation Block (TAB). Outputs of different TABs are then chained together for the task of interest.  <ref type="figure">Figure 2</ref>. Our temporal aggregates model <ref type="bibr" target="#b8">[9]</ref> is very flexible in that it can be utilized for different tasks easily. <ref type="table">Table 1</ref>. Model parameters for activity recognition on Breakfast.</p><formula xml:id="formula_0">Dataset # spanning scope (s) K R {K} Breakfast entire video 5 {10, 15, 20}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Implementation Details</head><p>We train our models using the Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with batch size 10, learning rate 10 ?4 and dropout rate 0.3. We train for k epochs (where k=15 if task=anticipation &amp; k=25 if task=recognition) and decrease the learning rate by a factor of 10 every 10 th epoch. We use 512-D vectors for all non-classification linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recognizing Long-range Complex Activities</head><p>To validate our model further on a new task, we experiment on classifying long-range complex activities. Since these videos include multiple actions and are several minutes long, it becomes more challenging to model their temporal structure compared to short-term single action videos, see <ref type="figure">Fig. 2</ref> "activity recognition". Recently, <ref type="bibr" target="#b4">[5]</ref> proposed a neural layer, "Timeception", which uses multiscale temporal-only convolutions for modelling minuteslong complex activity videos, such as "cooking a meal". Placed on top of backbone CNNs, the permutation invariant convolution layer, PIC <ref type="bibr" target="#b5">[6]</ref>, also aims at modelling only the temporal dimension. PIC is invariant to temporal permutations as it models their correlations regardless of their order, which helps to handle different action orderings in videos. It also uses pairs of key-value kernels to learn the most representative visual signals in long and noisy videos.</p><p>We experiment on the Breakfast actions dataset <ref type="bibr" target="#b7">[8]</ref>, which contains 1712 videos of 10 complex activities such as "making coffee". In our model, we divide videos into</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Fine-tuning Accuracy (%) I3D no 64.3 I3D + Timeception <ref type="bibr" target="#b4">[5]</ref> no 69.3 I3D + ours no 80.8</p><p>I3D yes 80.6 I3D + Timeception <ref type="bibr" target="#b4">[5]</ref> yes 86.9 I3D+ PIC <ref type="bibr" target="#b5">[6]</ref> yes 89.8 three partitions and use each partition as a recent snippet. We use the entire video for computing our spanning snippets. The model parameters are presented in <ref type="table">Table 1</ref>.</p><p>We report our comparisons in <ref type="table" target="#tab_2">Table 2</ref> on Breakfast actions using two types of I3D features, where one is the features from an I3D model trained on Kinetics only, and the other is the features from an I3D model fine-tuned on the Breakfast dataset. Our method outperforms Timeception [5] by 11.4%, and the I3D backbone by 16.5%. <ref type="bibr" target="#b5">[6]</ref> use the fine-tuned I3D features on Breakfast and shows a 3.1% improvement over Timeception <ref type="bibr" target="#b4">[5]</ref>. Fine-tuning improves the accuracy by 16.3% and shows that there is room for improvement for our method using better feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Experiments on EPIC-KITCHENS-100</head><p>Epic-Kitchens-100 <ref type="bibr" target="#b0">[1]</ref> is the recently released extension to Epic-Kitchens-55 <ref type="bibr" target="#b1">[2]</ref>. It is the largest egocentric dataset with 100 hours of egocentric recordings capturing participants' daily kitchen activities with a head-mounted camera. There are around 90K pre-trimmed segments extracted from 700 long videos. Each segment is annotated with an  <ref type="table">Table 5</ref>. Action recognition results on EPIC-KITCHENS-100 validation and test sets. We report our results for modalities RGB, Flow, Obj and ROI and late fusion of the predictions from all these modalities (Fusion).</p><p>action composed of a verb and noun classes, e.g., "pour water". There are 4,025 actions composed of 97 verbs and 300 nouns. The dataset provides RGB and optical flow images, as well as bounding boxes extracted by a hand-object detection framework <ref type="bibr" target="#b9">[10]</ref>.</p><p>Parameters: The spanning scales {K}, recent scale K R , recent starting points {i} and recent ending points {j} are given in <ref type="table">Table 3</ref>. In our work, we anticipate or recognize the action classes directly rather than anticipating or recognizing the verbs and nouns independently <ref type="bibr" target="#b1">[2]</ref> which is shown to outperform the latter <ref type="bibr" target="#b2">[3]</ref>. We use the training and validation sets provided by <ref type="bibr" target="#b0">[1]</ref> for selecting our model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Features</head><p>We use the appearance (RGB), motion (optical flow), and object-based features provided by <ref type="bibr" target="#b3">[4]</ref> for reporting the baseline results on EPIC-100. They independently train two CNNs using the TSN <ref type="bibr" target="#b10">[11]</ref> framework on RGB and flow im-ages for action recognition on EPIC-Kitchens-100. <ref type="bibr" target="#b3">[4]</ref> also trains object detectors to recognize the 352 object classes of the EPIC-KITCHENS-100 dataset. We additionally extract regions of interest (ROI) features from this pre-trained TSN model (on RGB), provided by <ref type="bibr" target="#b3">[4]</ref>, for the hand-object interaction regions in frames. We use the interacting hand-object bounding boxes provided by <ref type="bibr" target="#b9">[10]</ref> and consider the union of these boxes to be our ROI for each frame. The RGB features from this ROI help our model ignore the background clutter, which adversely affects our performance as it focuses primarily on interacting regions. The feature dimensions are 1024, 1024, and 352, 1024 for appearance, motion, object, and ROI features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Anticipation on EPIC-KITCHENS-100</head><p>The anticipation task of EPIC-KITCHENS-100 requires anticipating the future action ? ? = 1s before it starts. We train our model separately for each feature modality (appearance, motion, object and RoI) with the parameters described in <ref type="table">Table 3</ref> and apply late fusion to the predictions from all these modalities by average voting to compute our final results. <ref type="table">Table 4</ref> summarizes our results (class-mean top-5 recall (%)) for validation and hold-out test sets on EPIC-KITCHENS-100 for all (overall) and unseen participants and tail classes. Overall the ensemble of all modalities improves action scores for overall, unseen and tail classes while training our model solely on RGB performs better for verb and noun scores. Our submission in the challenge leaderboard 1 is named as "temporalAgg".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Recognition on EPIC-KITCHENS-100</head><p>For recognition, we classify pre-trimmed action segments. We train our model separately for each feature modality using the model parameters described in <ref type="table">Table 3</ref>. During inference, similar to our anticipation, a late fusion of the predictions from modalities RGB, Flow, Obj, and ROI is used.</p><p>Following the EPIC-KITCHENS-100 protocol <ref type="bibr" target="#b0">[1]</ref>, we report Top-1/5 accuracies on both the validation and test sets for all (overall) and unseen participants and tail classes in <ref type="table">Table 5</ref>. Fusing all modalities improves all scores significantly for all evaluation categories. Our submission in the challenge leaderboard 2 is "temporalAgg".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Comparisons to methods developed for recognizing long-range complex activities, Timeception [5] and PIC [6] on the Breakfast Actions dataset. Our method outperforms Time- ception [5] by a significant margin showing the superiority of our method in modelling long-range activities.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Dataset details and our respective model parameters for anticipation and recognition. s and e refers to the start and end times of the segments for action recognition. Action anticipation results (class-mean top-5 recall) on EPIC-KITCHENS-100 validation and test sets. We report our results for RGB, Flow, Obj and ROI modalities and the late fusion of the predictions from all these modalities (Fusion).</figDesc><table><row><cell>Task</cell><cell cols="2"># segments</cell><cell></cell><cell cols="3">{i}/{i, j}(in seconds (s))</cell><cell></cell><cell cols="2">spanning scope (s)</cell><cell>K R</cell><cell>{K}</cell></row><row><cell cols="2">Anticipation</cell><cell>90K</cell><cell></cell><cell cols="3">{t?1.6, t?1.2, t?0.8, t?0.4}</cell><cell></cell><cell></cell><cell>6</cell><cell>2</cell><cell>{2, 3, 5}</cell></row><row><cell cols="2">Recognition</cell><cell>90K</cell><cell cols="5">{(s, e), (s ? 1, e + 1), (s ? 2, e + 2), (s ? 3, e + 3)}</cell><cell cols="2">(s ? 6, e + 6)</cell><cell>5</cell><cell>{2, 3, 5}</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overall</cell><cell></cell><cell cols="3">Unseen Participants</cell><cell></cell><cell cols="2">Tail Classes</cell></row><row><cell>Split</cell><cell>Modality</cell><cell></cell><cell>Verb</cell><cell>Noun</cell><cell>Act.</cell><cell>Verb</cell><cell>Noun</cell><cell>Act.</cell><cell cols="2">Verb</cell><cell>Noun</cell><cell>Act.</cell></row><row><cell></cell><cell>RGB</cell><cell></cell><cell>24.22</cell><cell>29.76</cell><cell>13.02</cell><cell>27.04</cell><cell>22.95</cell><cell>12.21</cell><cell cols="2">16.23</cell><cell>22.93</cell><cell>10.41</cell></row><row><cell></cell><cell>Flow</cell><cell></cell><cell>18.90</cell><cell>18.68</cell><cell>7.27</cell><cell>26.53</cell><cell>18.86</cell><cell>9.54</cell><cell cols="2">10.65</cell><cell>12.53</cell><cell>5.25</cell></row><row><cell>Val</cell><cell>Obj</cell><cell></cell><cell>20.45</cell><cell>27.64</cell><cell>10.45</cell><cell>24.17</cell><cell>24.71</cell><cell>11.45</cell><cell cols="2">12.55</cell><cell>19.31</cell><cell>7.36</cell></row><row><cell></cell><cell>ROI</cell><cell></cell><cell>21.22</cell><cell>26.61</cell><cell>11.62</cell><cell>25.49</cell><cell>19.16</cell><cell>10.10</cell><cell cols="2">13.36</cell><cell>19.91</cell><cell>9.10</cell></row><row><cell></cell><cell>Fusion</cell><cell></cell><cell>23.15</cell><cell>31.37</cell><cell>14.73</cell><cell>28.01</cell><cell>26.23</cell><cell>14.47</cell><cell cols="2">14.50</cell><cell>22.47</cell><cell>11.75</cell></row><row><cell>Test</cell><cell>Fusion</cell><cell></cell><cell>21.76</cell><cell>30.59</cell><cell>12.55</cell><cell>17.86</cell><cell>27.04</cell><cell>10.46</cell><cell cols="2">13.59</cell><cell>20.62</cell><cell>8.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Overall</cell><cell></cell><cell cols="3">Unseen Participants</cell><cell cols="2">Tail Classes</cell></row><row><cell></cell><cell></cell><cell cols="3">Top-1 Accuracy (%)</cell><cell cols="2">Top-5 Accuracy (%)</cell><cell cols="3">Top-1 Accuracy (%)</cell><cell cols="2">Top-1 Accuracy (%)</cell></row><row><cell cols="2">Split Modality</cell><cell cols="2">Verb Noun</cell><cell>Act.</cell><cell>Verb Noun</cell><cell>Act.</cell><cell cols="2">Verb Noun</cell><cell>Act.</cell><cell cols="2">Verb Noun</cell><cell>Act.</cell></row><row><cell></cell><cell>RGB</cell><cell cols="3">59.92 45.14 36.87</cell><cell cols="2">86.72 71.09 56.97</cell><cell cols="3">47.51 31.46 27.51</cell><cell cols="2">26.82 21.89 18.04</cell></row><row><cell></cell><cell>Flow</cell><cell cols="3">62.81 37.51 32.84</cell><cell cols="2">87.70 63.22 53.17</cell><cell cols="3">53.80 32.86 28.26</cell><cell cols="2">27.23 8.00 12.72</cell></row><row><cell>Val</cell><cell>Obj</cell><cell cols="3">49.89 41.70 30.97</cell><cell cols="2">84.02 73.56 54.00</cell><cell cols="3">48.36 34.84 27.63</cell><cell cols="2">23.64 20.63 14.27</cell></row><row><cell></cell><cell>ROI</cell><cell cols="3">57.75 43.16 35.48</cell><cell cols="2">86.36 70.87 56.65</cell><cell cols="3">47.32 34.65 26.76</cell><cell cols="2">26.42 20.16 16.94</cell></row><row><cell></cell><cell>Fusion</cell><cell cols="3">66.00 53.35 45.26</cell><cell cols="2">89.39 80.40 66.93</cell><cell cols="3">56.62 44.60 38.12</cell><cell cols="2">30.57 25.26 22.42</cell></row><row><cell>Test</cell><cell>Fusion</cell><cell cols="3">62.68 51.66 42.65</cell><cell cols="2">86.28 73.85 64.05</cell><cell cols="3">56.25 46.33 36.06</cell><cell cols="2">24.99 18.00 15.92</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rollingunrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pic: Permutation invariant convolution for recognizing long-range activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08275</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding human hands in contact at internet scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
