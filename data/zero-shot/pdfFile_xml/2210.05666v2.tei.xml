<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Lao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<email>hszhao@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Point Transformer (PTv1) <ref type="bibr" target="#b0">[1]</ref> introduces the self-attention networks to 3D point cloud understanding. Combining the vector attention <ref type="bibr" target="#b1">[2]</ref> with a U-Net style encoder-decoder framework, PTv1 achieves remarkable performance in several 3D point cloud recognition tasks, including shape classification, object part segmentation, and semantic scene segmentation.</p><p>In this work, we analyze the limitations of Point Transformer (PTv1) <ref type="bibr" target="#b0">[1]</ref> and propose a new elegant and powerful backbone named Point Transformer V2 (PTv2). Our PTv2 improves upon PTv1 with several novel designs, including the advanced grouped vector attention with improved position encoding, and the efficient partition-based pooling scheme.</p><p>The vector attention layers in PTv1 utilize MLPs as the weight encoding to map the subtraction relation of query and key into an attention weight vector that can modulate the individual channels of the value vector. However, as the model goes deeper and the number of channels increases, the number of weight encoding parameters also increases drastically, leading to severe overfitting and limiting the model depth. To address this problem, we present grouped vector attention with a more parameter-efficient formulation, where the vector attention is divided into groups with shared vector attention weights. Meanwhile, we show that the well-known multi-head attention <ref type="bibr" target="#b2">[3]</ref> and the vector attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> are degenerate cases of our proposed grouped vector attention. Our proposed grouped vector attention inherits the merits of both vector attention and multi-head attention while being more powerful and efficient. Furthermore, point positions provide important geometric information for 3D semantic understanding. Hence, the positional relationship among 3D points is more critical than 2D pixels. However, previous 3D position encoding schemes mostly follow the 2D ones and do not fully exploit the geometric knowledge in 3D coordinates. To this end, we strengthen the position encoding mechanism by applying an additional position encoding multiplier to the relation vector. Such a design strengthens the positional relationship information in the model, and we validate its effectiveness in our experiments.</p><p>Moreover, it is worth noting that the irregular, non-uniform spatial distributions of points are significant challenges to the pooling modules for point cloud processing. Previous point cloud pooling approaches rely on a combination of sampling methods (e.g. farthest point sampling <ref type="bibr" target="#b3">[4]</ref> or grid sampling <ref type="bibr" target="#b4">[5]</ref>) and neighbor query methods (e.g. kNN or radius query), which is time-consuming and not spatially well-aligned. To overcome this problem, we go beyond the pooling paradigm of combining sampling and query, and divide the point cloud into non-overlapping partitions to directly fuse points within the same partition. We use uniform grids as partition divider and achieve significant improvement.</p><p>In conclusion, we propose Point Transformer V2, which improves Point Transformer <ref type="bibr" target="#b0">[1]</ref> from several perspectives:</p><p>? We propose an effective grouped vector attention (GVA) with a novel weight encoding layer that enables efficient information exchange within and among attention groups.</p><p>? We introduce an improved position encoding scheme to utilize point cloud coordinates better and further enhance the spatial reasoning ability of the model.</p><p>? We design the partition-based pooling strategy to enable more efficient and spatially betteraligned information aggregation compared to previous methods.</p><p>We conducted extensive analysis and controlled experiments to validate our designs. Our results indicate that PTv2 outperforms predecessor works and sets the new state-of-the-art on various 3D understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Image transformers. With the great success of ViT <ref type="bibr" target="#b5">[6]</ref>, the absolute dominance of convolution in vision tasks is shaken by Vision Transformer, which becomes a trend in 2D image understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. ViT introduces the far-reaching scaled dot-product self-attention and multi-head self-attention theory <ref type="bibr" target="#b2">[3]</ref> in NLP into vision by considering image patches as tokens. However, operating global attention on the entire image consumes excessive memory. To solve the memory consumption problem, Swin Transformer <ref type="bibr" target="#b6">[7]</ref> introduces the grid-based local attention mechanism to operate the transformer block in a sequence of shifted windows.</p><p>Point cloud understanding. Learning-based methods for processing 3D point clouds can be classified into the following types: projection-based, voxel-based, and point-based networks. An intuitive way to process irregular inputs like point clouds is to transform irregular representations into regular ones. Projection-based methods project 3D point clouds into various image planes and utilize 2D CNN-based backbones to extract feature representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. An alternative approach operates convolutions in 3D by transforming irregular point clouds into regular voxel representations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Those voxel-based methods suffer from inefficiency because of the sparsity of point clouds until the introduction and implementation of sparse convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Point-based methods extract features directly from the point cloud rather than projecting or quantizing irregular point clouds onto regular grids in 2D or 3D <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5]</ref>. The recently proposed transformer-based point cloud understanding approaches, introduced in the next paragraph, are also categorized into point-based methods.</p><p>Point cloud transformers. Transformer-based networks belong to the category of point-based networks for point cloud understanding. During the research upsurge of vision transformers, at almost the same period, Zhao et al. <ref type="bibr" target="#b0">[1]</ref> and Guo et al. <ref type="bibr" target="#b20">[21]</ref> published their explorations of applying attention to point cloud understanding, becoming pioneers in this direction. The PCT <ref type="bibr" target="#b20">[21]</ref> proposed by Guo et al. performs global attention directly on the point cloud. Their work, similar to ViT, is limited</p><formula xml:id="formula_0">Unpooling U ! U " U " U ! ? U " U # MLP ? + Softmax ? $ ? ? % AGG &amp; ' ? ( &amp; ) &amp; ' &amp; ) U ! U " U " U ! ? U " U # MLP MLP ? + Gouped AGG Softmax ? ? $ ? ? * &amp; &amp; ) ' ? ( &amp; ) &amp; '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PTv1 PTv2</head><p>Pooling </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Point Transformer V2</head><p>We analyze the limitations of Point Transformer V1 (PTv1) <ref type="bibr" target="#b0">[1]</ref> and propose our Point Transformer V2 (PTv2), including several improved modules upon PTv1. We begin by introducing the mathematical formulations and revisiting the vector self-attention used in PTv1 in Sec. 3.1. Based on the observation that the parameters of PTv1 increases drastically with the increased model depth and channel size, we propose our powerful and efficient grouped vector attention in Sec. 3.2. Further, we introduce our improved position encoding in Sec. 3.3 and the new pooling method in Sec. 3.4. We finally describe our network architecture in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation and Background</head><p>Problem formulation. Let M = (P, F) be a 3D point cloud scene containing a set of points</p><formula xml:id="formula_1">x i = (p i , f i ) ? M,</formula><p>where p i ? R 3 represents the point position, and f i ? R c represents the point features. Point cloud semantic segmentation aims to predict a class label for each point x i , and the goal of scene classification is to predict a class label for each scene M. M(p) denotes a mapping function that maps the point at position p to a subset of M denoted as "reference set". Next, we revisit the self-attention mechanism used in PTv1 <ref type="bibr" target="#b0">[1]</ref>.</p><p>Local attention. Conducting the global attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> over all points in a scene is computationally heavy and infeasible for large-scale 3D scenes. Therefore, we apply local attention where the attention for each point x i works within a subset of points, i.e., reference point set, M(p i ).</p><p>Shifted-grid attention <ref type="bibr" target="#b6">[7]</ref>, where attention is alternatively applied over two sets of non-overlapping image grids, has become is a common practice <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> for image transformers. Similarly, the 3D space can be split into uniform non-overlapping grid cells, and the reference set is defined as the points within the same grid, i.e., M(p i ) = {(p j , f j ) | p j in the same grid cell as p i }. However, such attention relies on a cumbersome shift grid operation to achieve a global receptive field, and it does not work well on point clouds where the point densities within different grids are not consistent.</p><p>PTv1 adopts neighborhood attention, where the reference point set is a local neighborhood of the given point, i.e., M(</p><formula xml:id="formula_2">p i ) = {(p j , f j ) | p j ? Neighborhood(p i )}. Specifically, the neighborhood point set M(p i ) is defined as the k nearest neighboring (kNN) points of p i in PTv1. Our experiments (Sec. 4.3)</formula><p>show that neighborhood attention is more effective than shifted-grid attention, so our approach adopts the neighborhood attention.</p><p>Scalar attention and vector attention. Given a point x i = (p i , f i ) ? M, we apply linear projections or MLPs to project the point features f i to the feature vectors of query q i , key k i , and value v i , each with c h channels. The standard scalar attention (SA) operated on the point x i and its reference point set M(p i ) can be represented as follows,</p><formula xml:id="formula_3">w ij = q i , k j / ? c h , f attn i = xj ?M(pi) Softmax(w i ) j v j ,<label>(1)</label></formula><p>The attention weights in the above formulation are scalars computed from the scaled dot-product <ref type="bibr" target="#b2">[3]</ref> between the query and key vectors. Multi-head scalar attention (MSA) <ref type="bibr" target="#b2">[3]</ref> is an extension of SA which runs several scalar attentions in parallel. MSA is widely applied in transformers, and we will show in Sec. 3.2 that MSA is a degenerate case of our proposed grouped vector attention.</p><p>Instead of the scalar attention weights, PTv1 applies vector attention, where the attention weights are vectors that can modulate the individual feature channels. In SA, the scalar attention is computed by the scaled dot-product between the query and key vectors. In vector attention, a weight encoding function encodes the relation between query and key to a vector. The vector attention <ref type="bibr" target="#b1">[2]</ref> is formulated as follows,</p><formula xml:id="formula_4">w ij = ?(?(q i , k j )), f attn i = xj ?M(pi) Softmax(W i ) j v j ,<label>(2)</label></formula><p>where is the Hadamard product. ? is a relation function (e.g., subtraction). ? : R c ? R c is a learnable weight encoding (e.g., MLP) that computes the attention vectors to re-weight v j by channels before aggregation. <ref type="figure" target="#fig_1">Fig. 2</ref> (a) shows a method using vector attention with linear weight encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grouped Vector Attention</head><p>In vector attention, as the network goes deeper and there are more feature encoding channels, the number of parameters for the weight encoding layer increases drastically. The large parameter size restricts the efficiency and generalization ability of the model. In order to overcome the limitations of vector attention, we introduce the grouped vector attention, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>.</p><p>Attention groups. We divide channels of the value vector v ? R c evenly into g groups (1 ? g ? c). The weight encoding layer outputs a grouped attention vector with g channels instead of c channels. Channels of v within the same attention group share the same scalar attention weight from the grouped attention vector. Mathematically,</p><formula xml:id="formula_5">w ij = ?(?(q i , k j )), f attn i = M(pi) xj g l=1 c/g m=1 Softmax(W i ) jl v lc/g+m j ,<label>(3)</label></formula><p>where ? is the relation function and ? : R c ? R g is the learnable grouped weight encoding defined in the next paragraph. The second equation in Eq. 3 is the grouped vector aggregation. <ref type="figure" target="#fig_1">Fig. 2</ref> (a) presents a vanilla GVA implemented by a fully connected weight encoding, the number of the grouped weight encoding function parameters reduced compared with the vector attention ( <ref type="figure" target="#fig_1">Fig. 2  (b)</ref>), leading to a more powerful and efficient model. GVA is a generalized formulation of VA and MSA. Our GVA degenerates to vector attention (VA) when g = c, and it degenerates to multi-head self-attention (MSA) if ? in Eq. 3 is defined as follows,</p><formula xml:id="formula_6">?(r) = r ? ? ? ? 1 1?cg 0 1?cg ? ? ? 0 1?cg 0 1?cg 1 1?cg ? ? ? 0 1?cg . . . . . . . . . . . . 0 1?cg 0 1?cg ? ? ? 1 1?cg ? ? ? ? g ? c g T 1 ? c g ,<label>(4)</label></formula><p>where c g = c/g and r ? R 1?c . </p><formula xml:id="formula_7">?(r) = r ? ? ? ? p 1 0 1?cg ? ? ? 0 1?cg 0 1?cg p 2 ? ? ? 0 1?cg . . . . . . . . . . . . 0 1?cg 0 1?cg ? ? ? p g ? ? ? ? g ? c g T ,<label>(5)</label></formula><formula xml:id="formula_8">?(r) = Linear ? Act ? Norm(?(r)),<label>(6)</label></formula><p>where c g = c/g, p 1 , . . . , p g ? R c g are learnable parameters, and ? represents function composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Position Encoding Multipler</head><p>Different from the discrete, regular-grid pixels in 2D images, points in the 3D point cloud are unevenly distributed in a continuous Euclidean Metric space, making the spatial relationship in 3D point cloud much more complicated than 2D images. In transformers and attention modules, the spatial information is obtained with the position encoding ? bias (p i ? p j ) added to the relation vector ?(q i , k j ) as a bias.</p><p>Due to the generalization limitation of vector attention in PTv1 mentioned in Sec. 3.2, adding more position encoding capacity to vector attention will not help to improve the performance. In PTv2, the grouped vector attention has an effect of reducing overfitting and enhancing generalization. With grouped vector attention restricting the capacity of the attention mechanism, we strengthen the position encoding with an additional multiplier ? mul (p i ? p j ) to the relation vector, which focuses on learning complex point cloud positional relations. As shown in <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>, our improved position encoding is as follows,</p><formula xml:id="formula_9">w ij = ?(? mul (p i ? p j ) ?(q i , k j ) + ? bias (p i ? p j )),<label>(7)</label></formula><p>where is the Hadamard product. ? mul , ? bias : R d ? R d are two MLP position encoding functions, which take relative positions as input. Position encoding multiplier compliments group vector attention to achieve a good balance of network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Partition-based Pooling</head><p>Traditional sampling-based pooling procedures adopted by other point-based methods use a combination of sampling and query methods. In the sampling stage, farthest point sampling <ref type="bibr" target="#b3">[4]</ref> or grid sampling <ref type="bibr" target="#b4">[5]</ref> is used to sample points reserved for the following encoding stage. For each sampled point, a neighbor query is performed to aggregate information from the neighboring points. In these sampling-based pooling procedures, the query sets of points are not spatially-aligned since the information density and overlap among each query set are not controllable. To address the problem, we propose a more efficient and effective partition-based pooling approach, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Pooling. Given a point set M = (P, F), we partition M into subsets [M 1 , M 2 , ..., M n ] by separating the space into non-overlapping partitions. We fusion each subset of points M i = (P i , F i ) from a single partition as follows,</p><formula xml:id="formula_10">f i = MaxPool({f j U | f j ? F i }), p i = MeanPool({p j | p j ? P i }),<label>(8)</label></formula><p>where (p i , f i ) is the position and features of pooling point aggregated form subset M i , and U ? R c?c is the linear projection. Collecting the pooling points from n subsets gives us the point set</p><formula xml:id="formula_11">M = {p i , f i } n i=1</formula><p>for the next stage of encoding. In our implementation, we use uniform grids to partition the point cloud space, and thus our partition-based pooling is also called grid pooling.</p><p>Unpooling. The common practice of unpooling by interpolation is also applicable to partition-based pooling. Here we introduce a more straightforward and efficient unpooling method. To unpool the fused point set M back to M, the point locations in M are record from the pooling process, and we only need to obtain the features for each point in M. With the help of the grid-based partitioning [M 1 , M 2 , ..., M n ] during the pooling stage, we can map point feature to all points from the same subset,</p><formula xml:id="formula_12">f up i = f j , if (p i , f i ) ? M j .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Architecture</head><p>Backbone structure. Following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1]</ref>, we adopt the U-Net architecture with skip connections. There are four stages of encoders and decoders with block depths [2, 2, 6, 2] and [1, 1, 1, 1], respectively. The grid size multipliers for the four stages are [x3.0, x2.5, x2.5, x2.5], representing the expansion ratio over the previous pooling stage. The attention is conducted in a local neighborhood, described in "neighborhood attention" in Sec. 3.1. In Sec. 4.3 we compare the neighborhood attention with shift-grid attention.</p><p>The initial feature dimension is 48, and we first embed the input channels to this number with a basic block with attention groups of 6. Then, we double this feature dimension and attention groups each time entering the next encoding stage. For the four encoding stages, the feature dimensions are <ref type="bibr">[96,</ref><ref type="bibr">192,</ref><ref type="bibr">384,</ref><ref type="bibr">384]</ref>, and the corresponding attention groups are <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Output head. For point cloud semantic segmentation, we apply an MLP to map point features produced by the backbone to the final logits for each point in the input point set. For point cloud classification, we apply global average pooling over the point features produced by the encoding stages to obtain a global feature vector, followed by an MLP classifier for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To validate the effectiveness of the proposed method, we conduct experimental evaluations on ScanNet v2 <ref type="bibr" target="#b43">[44]</ref> and S3DIS <ref type="bibr" target="#b44">[45]</ref> for semantic segmentation, and ModelNet40 <ref type="bibr" target="#b45">[46]</ref> for shape classification. Implementation details are available in the appendix.   <ref type="bibr" target="#b3">[4]</ref>, we use mean class-wise intersection over union (mIoU) as the evaluation metric for validation and test set of ScanNet v2. And we use mean class-wise intersection over union (mIoU), mean of class-wise accuracy (mAcc), and overall point-wise accuracy (OA) for evaluating performance on S3DIS area5.</p><p>Performance comparison. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show the results of our PTv2 model compared with previous methods on ScanNet v2 and S3DIS, respectively. Our PTv2 model outperforms prior methods in all evaluation metrics. Notably, PTv2 significantly outperforms PTv1 [1] by 4.8% mIoU on the ScanNet v2 validation set.</p><p>Visualization. The qualitative results of point cloud semantic segmentation are shown in <ref type="figure" target="#fig_3">Fig. 3</ref> and <ref type="figure" target="#fig_4">Fig. 4</ref>. Our PTv2 model is able to predict semantic segmentation results that are quite close the ground-truth. It is worth noting that our model can capture the detailed structure information and predict the correct semantics for challenging scenarios. For example, in the S3DIS scenes with chairs, PTv2 is able to cleanly predict the chair legs and armrests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shape Classification</head><p>Data and metric. We test our proposed PTv2 model for 3D point cloud classification on ModelNet40 dataset. The ModelNet40 <ref type="bibr" target="#b45">[46]</ref> dataset consists of 12,311 CAD models belonging to 40 object categories. 9,843 models are split out for training, and the rest 2,468 models are reserved for testing. Following the common practice in the community, we report the class-average accuracy (mAcc) and overall accuracy (OA) on the test set.</p><p>Performance comparison. We test our PTv2 model and compare it with previous models on the ModelNet40 dataset for shape classification. Results are shown in    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct ablation studies to examine the effectiveness of each module in our design. The ablation study results are reported on ScanNet v2 validation set.</p><p>Attention type. We first investigate the effects of different attention designs. We experiment with two types of local attention introduced in Sec. 3.1, namely shifted-grid attention and neighborhood attention <ref type="bibr" target="#b0">[1]</ref>. Then, to validate the effectiveness of our proposed grouped vector attention (denoted as "GVA"), we compare it with the commonly-used multi-head self-attention (denoted as "MSA"). We use the vanilla position encoding in PTv1 <ref type="bibr" target="#b0">[1]</ref> and our proposed partition-based pooling scheme in all of the experiments in <ref type="table" target="#tab_5">Table 4</ref>. It shows neighborhood attention performs significantly better than shiftedgrid attention, indicating that the neighborhood attention is better suited for point clouds which are non-uniformly distributed. Moreover, our proposed grouped vector attention consistently outperforms the commonly-used multi-head self-attention with both shifted-grid attention and neighborhood attention. So our grouped vector attention is not only more efficient, but also more effective, than multi-head self-attention. The comparison between GVA and MSA indicates the effectiveness of the learnable parameters in the grouped linear layer of the grouped weight encoding in Sec. 3.2.</p><p>Weight encoding. We study the effects of different weight encoding functions ? in <ref type="table" target="#tab_7">Table 6</ref>. The weight encoding functions are introduced in Sec. 3.1 and Sec. 3.2, and different attention mechanisms adopt different weight encoding functions. We use the vanilla position encoding in PTv1 <ref type="bibr" target="#b0">[1]</ref> and our proposed grid pooling scheme in all of the experiments in <ref type="table" target="#tab_7">Table 6</ref>. We experimented with the following weight encoding functions: (1) The weight encoding for multi-head scalar attention in Eq. 4, denoted as "MSA". (2) Weight encoding as a linear layer denoted as "L". (3) The grouped linear layer, which is ? in Eq. 5, denoted as "GL". (4) The linear layer followed by batch normalization, activation, and another linear layer, denoted as "L+N+A+L". (5) The grouped linear layer, followed by batch normalization, activation, and a linear layer, denoted as "GL+N+A+L". (5) is also the grouped weight encoding function used for our grouped vector attention, introduced as ? in Eq. 5. Results in <ref type="table" target="#tab_7">Table 6</ref> demonstrate that our grouped weight encoding function outperforms other compared designs. Specifically, comparing (1), (3) and (5), GL slightly outperforms MSA but adding additional inter-group information exchange combined with proper normalization and activation can boost the performance to be better than MSA. Moreover, the comparison between (5) and (4) and the comparison between (3) and (2) both indicate that our grouped linear layer outperforms the naive linear layer, even though the grouped linear layer has g times fewer parameters and requires less computing than the linear layer.      Pooling methods. In Sec. 3.4 we discuss the potential limitations of the sampling-based pooling in PTv1 and propose a new pooling and unpooling scheme based on non-overlapping partitions. We also name a simple and effective grid-based implement of our partition-based pooling as grid pooling.</p><p>To further examine the superiority of our method, we experiment with different pooling-unpooling schemes in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>For our partition-based pooling implemented by a grid, the base grid size is 0.02 meters, which is identical to the voxelization grid size during data pre-processing. The grid size multipliers are the grid size expansion ratio over the previous pooling stage. For example, [?4.0, ?2.0, ?2.0, ?2.0] means that the grid sizes are: [0.08, 0.16, 0.32, 0.64] meters, respectively. We choose a relatively large value for initial grid sizes (?3.0 and ?4.0) to provide sufficiently large receptive fields, which is analogous to the common practice in image transformers <ref type="bibr" target="#b5">[6]</ref>. For subsequent pooling stages, we observe that ?2.0 grid size increase results in an approximate pooling ratio of 4 for the point cloud, while ?2.5 grid size increase results in an approximate pooling ratio of 6. We choose the same sampling ratio of 4 and 6 for sampling-based pooling to ensure a fair comparison.</p><p>The results in <ref type="table" target="#tab_6">Table 5</ref> illustrate that our partition-based pooling achieves higher mIoU than the sampling-based method. For sampling-based pooling with farthest point sampling, the performance decreases significantly when the sampling ratio increases from 4 to 6. However, for our partition-based pooling implemented by grid, we observe that initial grid size and subsequent grid size multipliers do not significantly affect the overall performance, so we can use larger grid sizes to reduce the number of points in each stage to save memory.</p><p>Module design. We ablate different modules introduced in our PTv2: grouped vector attention (VGA), position encoding multiplier (PE Mul), partition-based pooling implemented by grid (Grid Pool), and partition map unpooling (Map Unpool) and the results are illustrated in <ref type="table" target="#tab_8">Table 7</ref>. The model adopts in Experiment I is PTv1 <ref type="bibr" target="#b0">[1]</ref>, which serves as a baseline result of our design. Benefiting from structural parameter adjustments and better data processing, which are also shared with the rest of the experiments, our baseline result increased from 70.6% to 72.3%. Experiment II to V add each of our proposed components in turns, gradually increasing our baseline result to 75.4%. The increasing mIOU indicates the effectiveness of each component.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Complexity and Latency</head><p>We further conduct model complexity and latency studies to examine the superior efficiency of several design in our work. We record the amortized forward time for each scan in the ScanNet v2 validation set with batch size 4 on a single TITAN RTX.</p><p>Pooling methods. <ref type="table" target="#tab_9">Table 8</ref> shows the forward time and mIoU of PTv2 with different pooling methods and pooling ratios. We compare our pooling method with two classical sampling-based pooling methods: FPS-kNN and Grid-kNN. FPS-kNN pooling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref> uses farthest point sampling (FPS) to sample a specified number of points and then query k nearest neighbor points for pooling. We call the pooling method in Strided KPConv <ref type="bibr" target="#b4">[5]</ref> Grid-kNN pooling, as it uses a uniform grid to sample points and then applies the kNN method to index neighbors. This leads to uncontrollable overlaps of the pooling receptive fields. As shown in the table, our grid pooling method is not only faster but also achieves higher mIoUs.</p><p>Module design. <ref type="table" target="#tab_10">Table 9</ref> summarizes comparisons of model complexity, time consumption, and evaluation performances on ScanNet v2 validation set. Meanwhile, we drop the first batch of forwarding time for GPU preparation. In <ref type="table" target="#tab_10">Table 9</ref>, GVA refers to grouped vector attention. L refers to grouped weight encoding implemented by a single Linear. GL refers to grouped weight encoding implemented by a Grouped Linear. GL-N-A-L refers to the grouped linear layer, followed by batch normalization, activation, and a linear layer as grouped weight encoding function. GP refers to partition-based pooling implemented by grid. PEM refers to the position encoding multiplier. To ensure fair comparison, PTv1 is set to be the same depth and feature dimensions as our model architecture of PTv2. By comparing experiment 1 and 2 , we can study the effect of GVA. The same spirit goes on for experiment 3 , 4 , and 5 , where each experiments adds one additional module, so that we can study the effect of the added module respectively.</p><p>Comparing experiments 1 , 2 , 3 , and 4 , the introduction of grouped vector attention (GVA) with grouped weight encoding dramatically improves the model performance and slightly reduces execution time. The comparison between 4 and 5 indicates that the grid pooling strategy can significantly speed up the network and further enhance the generalization ability of our model. Position encoding multiplier is the only design that increases the number of model parameters, but experiment 6 demonstrates its effectiveness in improving performance. Meanwhile, our model is still lightweight compared to voxel-based backbones, such as MinkUNet42 <ref type="bibr" target="#b17">[18]</ref> with 37.9M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose Point Transformer V2 (PTv2), a powerful and efficient transformer-based backbone for 3D point cloud understanding. Our work makes several non-trivial improvements upon Point Transformer V1 <ref type="bibr" target="#b0">[1]</ref>, including the grouped vector attention, improved position encoding, and partitionbased pooling. Our PTv2 model achieves state-of-the-art performance on point cloud classification and semantic segmentation benchmarks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Depths of Decoder Blocks</head><p>We show ablation experiments on the depths of each decoder block with different unpooling methods in <ref type="table" target="#tab_0">Table 12</ref>. Applying at least one attention block in each decoding stage can significantly improve the model performance, and this phenomenon is more evident while utilizing our mapping pooling. But deeper decoders (from a depth of 1 to 2 for each decoder block) do not improve performance. These phenomena are reasonable since the naive unpooling methods such as interpolation and mapping require a learnable block to optimize the sampled features. In 2D, deconvolution with stride is widely used to unpooling and optimize features simultaneously, and such a process usually does not require a deep network.  <ref type="table" target="#tab_0">Table 13</ref> shows an additional ablation study on PE Multiplier. PE Multiplier does not work well with PTv1 since PTv1 already overfits to the training set. Adding more capacity to the PTv1 will not help improve the performance. On PTv2, the group vector attention (GVA) has an effect of reducing overfitting and enhancing generalization. With GVA restricting the capacity of the attention mechanism, the addition of PE Multiplier can focus on learning complex point cloud positional relations. PE Multiplier compliments group vector attention to achieve a good balance of network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Ablation Study on Position Encoding Multiplier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Comparison of Pooling Methods</head><p>We compare three different feature-level pooling methods: FPS-kNN, Grid-kNN, and our partitionbased pooling implemented by grid. FPS-kNN pooling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref> uses the farthest point sampling (FPS) to sample a specified number of points and then query k nearest neighbor points for pooling. Strided KPConv <ref type="bibr" target="#b4">[5]</ref> uses a uniform grid to sample points and then applies the kNN method to index neighbors, leading to an uncontrollable overlapping of the pooling receptive field. We name this method Grid-kNN Pooling. Our method values non-overlapping receptive fields and fusion points within each non-overlaping grid cell. To distinguish it from the previous sampling-based method, we name it Grid Pooling.</p><p>Benchmark with synthetic data. <ref type="table" target="#tab_0">Table 14</ref> provides benchmark results for the different pooling methods with synthetic data. We generate n points uniformly at random in the unit cube space. The sampling ratio r the sample size divided by the population size. For comparison, we keep the sampling ratio r the same for different pooling methods. For Grid-kNN and Grid Pooling, the grid size is computed as (n ? r) ? 1 3 since the point cloud is sampled uniformly at random. We show the combined time of pooling and uppooling for these three methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of the attention, position encoding, and pooling mechanisms between PTv1 and PTv2. Top-left: the vector attention (Sec. 3.1) with position encoding (Sec. 3.3) in PTv1 . Bottom-left: our grouped vector attention (Sec. 3.2, denoted by red) with improved position encoding (Sec. 3.3, denoted by blue) in PTv2. Top-right: the sampling-based pooling and interpolation-based unpooling in PTv1. Bottom-right: our partition-based pooling and unpooling in PTv2 (Sec. 3.4). by memory consumption and computational complexity. Meanwhile, based on the vector attention theory proposed in SAN [2], Point Transformer [1] proposed by Zhao et al. directly performs local attention between each point and its adjacent points, which alleviated the memory problem mentioned above. Point Transformer achieves remarkable results in multiple point cloud understanding tasks and state-of-art results for several competitive challenges.In this work, we analyze the limitations of the Point Transformer<ref type="bibr" target="#b0">[1]</ref>, and propose several novel architecture designs for the attention and pooling module, to improve the effectiveness and efficiency of the Point Transformer. Our proposed model, Point Transformer V2, performs better than the Point Transformer across a variety of 3D scene understating tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of various weight encoding functions. Each square represents a scalar, and each row of them represents a vector. The three rows represent relation vector, weight vector, and value vector from top to bottom. The attention groups are separated by dash lines. For demonstration, we assume the feature dimension is 4 and the number of attention groups (applicable to b, c, d) is 2. Lines with different colors refer to different operations, blue lines represent learnable parameters act on input relation scalar, while red lines represent multiply by the input relation scalar. Orange lines identify which value feature is affected by the input scalar weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Grouped linear. Inspired by the weight encoding function of MSA, we design the grouped linear layer ?(r) : R c ? R g where different groups of the input vector are projected with different parameters independently. Grouped linear further reduce the number of parameters in the weight encoding function. Our final adopted grouped weight encoding function is composed of the grouped linear layer, normalization layer, activation layer, and a fully connected layer to allow inter-group information exchange. Mathematically,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of semantic segmentation results on ScanNet v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of semantic segmentation results on S3DIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Grid~1 / 4 [</head><label>4</label><figDesc>?3.0, ?2.0, ?2.0, ?2.0] 75.2 1/4 [?4.0, ?2.0, ?2.0, ?2.0] 75.0 1/6 [?3.0, ?2.5, ?2.5, ?2.5] 75.4 1/6 [?4.0, ?2.5, ?2.5, ?2.5] 74.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semantic segmentation on ScanNet v2.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Val</cell><cell>Test</cell></row><row><cell>PointNet++ [4]</cell><cell cols="3">point 53.5 55.7</cell></row><row><cell>3DMV [26]</cell><cell>point</cell><cell>-</cell><cell>48.4</cell></row><row><cell>PanopticFusion [27]</cell><cell>point</cell><cell>-</cell><cell>52.9</cell></row><row><cell>PointCNN [28]</cell><cell>point</cell><cell>-</cell><cell>45.8</cell></row><row><cell>PointConv [29]</cell><cell cols="3">point 61.0 66.6</cell></row><row><cell cols="4">JointPointBased [30] point 69.2 63.4</cell></row><row><cell>PointASNL [31]</cell><cell cols="3">point 63.5 66.6</cell></row><row><cell>SegGCN [32]</cell><cell>point</cell><cell>-</cell><cell>58.9</cell></row><row><cell>RandLA-Net [33]</cell><cell>point</cell><cell>-</cell><cell>64.5</cell></row><row><cell>KPConv [5]</cell><cell cols="3">point 69.2 68.6</cell></row><row><cell>JSENet [34]</cell><cell>point</cell><cell>-</cell><cell>69.9</cell></row><row><cell>FusionNet [35]</cell><cell>point</cell><cell>-</cell><cell>68.8</cell></row><row><cell>SparseConvNet [17]</cell><cell cols="3">voxel 69.3 72.5</cell></row><row><cell>MinkUNet [18]</cell><cell cols="3">voxel 72.2 73.6</cell></row><row><cell>PTv1 [1]</cell><cell cols="2">point 70.6</cell><cell>-</cell></row><row><cell>PTv2 (ours)</cell><cell cols="3">point 75.4 75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation on S3DIS Area 5.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>OA</cell><cell cols="2">mAcc mIoU</cell></row><row><cell>PointNet [19]</cell><cell>point</cell><cell>-</cell><cell>49.0</cell><cell>41.1</cell></row><row><cell>SegCloud [36]</cell><cell>point</cell><cell>-</cell><cell>57.4</cell><cell>48.9</cell></row><row><cell>TanConv [37]</cell><cell>point</cell><cell>-</cell><cell>62.2</cell><cell>52.6</cell></row><row><cell>PointCNN [28]</cell><cell cols="2">point 85.9</cell><cell>63.9</cell><cell>57.3</cell></row><row><cell>PointWeb [20]</cell><cell cols="2">point 87.0</cell><cell>66.6</cell><cell>60.3</cell></row><row><cell>HPEIN [38]</cell><cell cols="2">point 87.2</cell><cell>68.3</cell><cell>61.9</cell></row><row><cell>GACNet [39]</cell><cell cols="2">point 87.8</cell><cell>-</cell><cell>62.9</cell></row><row><cell>PAT [40]</cell><cell>point</cell><cell>-</cell><cell>70.8</cell><cell>60.1</cell></row><row><cell cols="2">ParamConv [41] point</cell><cell>-</cell><cell>67.0</cell><cell>58.3</cell></row><row><cell>SPGraph [42]</cell><cell cols="2">point 86.4</cell><cell>66.5</cell><cell>58.0</cell></row><row><cell>SegGCN [32]</cell><cell cols="2">point 88.2</cell><cell>70.4</cell><cell>63.6</cell></row><row><cell>MinkUNet [18]</cell><cell>voxel</cell><cell>-</cell><cell>71.7</cell><cell>65.4</cell></row><row><cell>PAConv [43]</cell><cell>point</cell><cell>-</cell><cell>-</cell><cell>66.6</cell></row><row><cell>KPConv [5]</cell><cell>point</cell><cell>-</cell><cell>72.8</cell><cell>67.1</cell></row><row><cell>PTv1 [1]</cell><cell cols="2">point 90.8</cell><cell>76.5</cell><cell>70.4</cell></row><row><cell>PTv2 (ours)</cell><cell cols="2">point 91.1</cell><cell>77.9</cell><cell>71.6</cell></row></table><note>Data and metric. For semantic segmentation, we experiment on ScanNet v2 [44] and S3DIS [45]. The ScanNet v2 dataset contains 1,513 room scans reconstructed from RGB-D frames. The dataset is divided into 1,201 scenes for training and 312 for validation. Point clouds for the model input are sampled from vertices of reconstructed meshes, and each sampled point is assigned a semantic label from 20 categories (wall, floor, table, etc.). The S3DIS dataset for semantic scene parsing consists of 271 rooms in six areas from three different buildings. Following a common protocol [36, 4, 1], area 5 is withheld during training and used for testing. Different from ScanNet v2, points of S3DIS are densely sampled on the mesh surfaces and annotated into 13 categories. Following a standard protocol</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Input</cell><cell cols="3">Ground truth</cell><cell>PTv2</cell><cell></cell><cell></cell><cell>Input</cell><cell cols="2">Ground truth</cell><cell>PTv2</cell></row><row><cell>floor</cell><cell>wall</cell><cell>cabinet</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>door</cell><cell>window</cell><cell>bookshelf</cell><cell>picture</cell></row><row><cell>counter</cell><cell>desk</cell><cell>curtain</cell><cell cols="2">refrigerator</cell><cell>bathtub</cell><cell cols="2">shower curtain</cell><cell>toilet</cell><cell>sink</cell><cell>other furniture</cell></row></table><note>, demonstrating that our proposed PTv2 model achieves state-of-the-art performance on ModelNet40 shape classification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Shape classification on ModelNet40.</figDesc><table><row><cell>Method</cell><cell cols="2">mAcc (%) OA (%)</cell></row><row><cell>PointNet [19]</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell>PointNet++ [4]</cell><cell>-</cell><cell>91.9</cell></row><row><cell>PointCNN [28]</cell><cell>88.1</cell><cell>92.5</cell></row><row><cell>PointConv [29]</cell><cell>-</cell><cell>92.5</cell></row><row><cell>KPConv [5]</cell><cell>-</cell><cell>92.9</cell></row><row><cell>DGCNN [47]</cell><cell>90.2</cell><cell>92.9</cell></row><row><cell>RS-CNN [48]</cell><cell>-</cell><cell>92.9</cell></row><row><cell>PointASNL [31]</cell><cell>-</cell><cell>92.9</cell></row><row><cell>DensePoint [49]</cell><cell>-</cell><cell>93.2</cell></row><row><cell>PosPool [50]</cell><cell>-</cell><cell>93.2</cell></row><row><cell>GBNet [51]</cell><cell>91.0</cell><cell>93.8</cell></row><row><cell>PCT [21]</cell><cell>-</cell><cell>93.2</cell></row><row><cell>PA-DGC [43]</cell><cell>-</cell><cell>93.9</cell></row><row><cell>CurveNet [52]</cell><cell>-</cell><cell>94.2</cell></row><row><cell>PTv1 [1]</cell><cell>90.6</cell><cell>93.7</cell></row><row><cell>PTv2 (ours)</cell><cell>91.6</cell><cell>94.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Attention type ablation.</figDesc><table><row><cell>Local Type</cell><cell>Mechanism Type</cell><cell>mIoU (%)</cell></row><row><cell>Shifted-Grid</cell><cell>MSA GVA</cell><cell>71.6 72.5</cell></row><row><cell>Neighborhood</cell><cell>MSA GVA</cell><cell>73.9 75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Pooling method ablation.</figDesc><table><row><cell>Pooling</cell><cell>Pooling</cell><cell>Grid Size</cell><cell>mIoU</cell></row><row><cell>Method</cell><cell>Ratio</cell><cell>Multipliers</cell><cell>(%)</cell></row><row><cell>FPS</cell><cell>1/4 1/6</cell><cell>--</cell><cell>74.4 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Weight encoding ablation.</figDesc><table><row><cell>ID Weight encoding</cell><cell>mIoU (%)</cell></row><row><cell>(1) MSA</cell><cell>73.9</cell></row><row><cell>(2) L</cell><cell>73.8</cell></row><row><cell>(3) GL</cell><cell>74.1</cell></row><row><cell>(4) L+N+A+L</cell><cell>74.7</cell></row><row><cell>(5) GL+N+A+L (ours)</cell><cell>75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Module design ablation.</figDesc><table><row><cell>ID</cell><cell>GVA</cell><cell>PE Mul</cell><cell>Grid Pool</cell><cell>Map Unpool</cell><cell>mIoU (%)</cell></row><row><cell>I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.3</cell></row><row><cell>II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73.8</cell></row><row><cell>III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.4</cell></row><row><cell>IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.9</cell></row><row><cell>V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Model performance and amortized latency with different pooling methods.</figDesc><table><row><cell></cell><cell cols="2">FPS-kNN [4]</cell><cell cols="2">Grid-kNN [5]</cell><cell cols="2">Grid pooling (ours)</cell></row><row><cell>Pooling Rate</cell><cell>1/4</cell><cell cols="5">1/6~1/4~1/6~1/4~1/6</cell></row><row><cell>Time (ms)</cell><cell>1007</cell><cell>785</cell><cell>389</cell><cell>356</cell><cell>318</cell><cell>266</cell></row><row><cell>mIoU (%)</cell><cell>74.4</cell><cell>72.9</cell><cell>74.1</cell><cell>73.4</cell><cell>75.2</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Model parameters and amortized latency of several networks.</figDesc><table><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell cols="2">PTv1 1 + GVA (L)</cell><cell>1 + GVA (GL)</cell><cell>1 + GVA (GL-N-A-L)</cell><cell>4 + GP</cell><cell>5 + PEM</cell></row><row><cell cols="2">Params (M) 11.4</cell><cell>9.8</cell><cell>9.6</cell><cell>9.6</cell><cell>9.6</cell><cell>12.8</cell></row><row><cell>Time (ms)</cell><cell>1023</cell><cell>991</cell><cell>951</cell><cell>971</cell><cell>220</cell><cell>266</cell></row><row><cell>mIoU (%)</cell><cell>72.3</cell><cell>73.0</cell><cell>73.2</cell><cell>74.2</cell><cell>75.0</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Data augmentation.</figDesc><table><row><cell></cell><cell>Drop Points Rotate Flip Scale Jitter Distort Chromatic Grid Size</cell></row><row><cell>ScanNet v2</cell><cell>0.02m</cell></row><row><cell>S3DIS</cell><cell>0.05m</cell></row><row><cell>ModelNet40</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Training setting.</figDesc><table><row><cell></cell><cell cols="6">Epoch Learning Rate Weight Decay Scheduler Optimizer Batch Size</cell></row><row><cell>ScanNet v2</cell><cell>600</cell><cell>0.005</cell><cell>0.02</cell><cell>Cosine</cell><cell>AdamW</cell><cell>16</cell></row><row><cell>S3DIS</cell><cell>3000</cell><cell>0.005</cell><cell>0.05</cell><cell>MultiStep</cell><cell>AdamW</cell><cell>16</cell></row><row><cell>ModelNet40</cell><cell>300</cell><cell>0.05</cell><cell>0.0001</cell><cell>MultiStep</cell><cell>SGD</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Results of different decoder depths for two upsampling strategy (mIoU%).</figDesc><table><row><cell>Decoder Depths</cell><cell cols="3">[0, 0, 0, 0] [1, 1, 1, 1] [2, 2, 2, 2]</cell></row><row><cell>Grid Mapping</cell><cell>73.5</cell><cell>75.4</cell><cell>74.8</cell></row><row><cell>Neighbors Interpolation</cell><cell>73.6</cell><cell>74.4</cell><cell>73.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Position encoding multiplier ablation.</figDesc><table><row><cell></cell><cell>PTv1</cell><cell></cell><cell>PTv2</cell><cell></cell></row><row><cell>PE Multiplier</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Params (M)</cell><cell>11.4</cell><cell>14.6</cell><cell>9.6</cell><cell>12.8</cell></row><row><cell>Time (ms)</cell><cell>1023</cell><cell>1055</cell><cell>220</cell><cell>266</cell></row><row><cell>mIoU (%)</cell><cell>72.3</cell><cell>72.1</cell><cell>75.0</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Combined pooing and unpooling time comparison.</figDesc><table><row><cell></cell><cell>n=10K</cell><cell>n=20K</cell><cell>n=40K</cell><cell>n=80K</cell><cell cols="2">n=160K n=320K</cell></row><row><cell>r=1/2</cell><cell>36</cell><cell>91</cell><cell>255</cell><cell>955</cell><cell>3644</cell><cell>14336</cell></row><row><cell>r=1/4</cell><cell>20</cell><cell>39</cell><cell>131</cell><cell>488</cell><cell>1842</cell><cell>7207</cell></row><row><cell>r=1/6</cell><cell>15</cell><cell>28</cell><cell>90</cell><cell>334</cell><cell>1241</cell><cell>4811</cell></row><row><cell>r=1/8</cell><cell>12</cell><cell>22</cell><cell>70</cell><cell>256</cell><cell>942</cell><cell>3632</cell></row><row><cell></cell><cell cols="6">(b) Grid-kNN pooling and unpooling [5] execution time (ms).</cell></row><row><cell></cell><cell>n=10K</cell><cell>n=20K</cell><cell>n=40K</cell><cell>n=80K</cell><cell cols="2">n=160K n=320K</cell></row><row><cell>r=1/2</cell><cell>4</cell><cell>6</cell><cell>10</cell><cell>17</cell><cell>52</cell><cell>141</cell></row><row><cell>r=1/4</cell><cell>4</cell><cell>6</cell><cell>9</cell><cell>17</cell><cell>36</cell><cell>109</cell></row><row><cell>r=1/6</cell><cell>4</cell><cell>5</cell><cell>9</cell><cell>16</cell><cell>23</cell><cell>69</cell></row><row><cell>r=1/8</cell><cell>4</cell><cell>6</cell><cell>9</cell><cell>16</cell><cell>22</cell><cell>67</cell></row><row><cell></cell><cell cols="6">(c) Grid pooling and unpooling (ours) execution time (ms).</cell></row><row><cell></cell><cell>n=10K</cell><cell>n=20K</cell><cell>n=40K</cell><cell>n=80K</cell><cell cols="2">n=160K n=320K</cell></row><row><cell>r=1/2</cell><cell>0.94</cell><cell>0.98</cell><cell>1.01</cell><cell>1.02</cell><cell>1.03</cell><cell>1.35</cell></row><row><cell>r=1/4</cell><cell>0.93</cell><cell>0.97</cell><cell>0.98</cell><cell>0.98</cell><cell>1.00</cell><cell>1.33</cell></row><row><cell>r=1/6</cell><cell>0.93</cell><cell>0.96</cell><cell>0.97</cell><cell>0.98</cell><cell>1.00</cell><cell>1.32</cell></row><row><cell>r=1/8</cell><cell>0.93</cell><cell>0.96</cell><cell>1.03</cell><cell>0.98</cell><cell>1.00</cell><cell>1.32</cell></row></table><note>(a) FPS-kNN pooling and unpooling [4, 1] execution time (ms).</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by HKU Startup Fund and HKU Seed Fund for Basic Research. We also appreciate the supporting of computing resources by SmartMore Corporation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the appendix, we provide more experiment details in Sec. A, more experiment results in Sec. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head><p>This section describes the model architectures adopted in the experiments and describes the experimental settings for each dataset in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Architecture</head><p>In <ref type="figure">Fig. 5</ref>, we show the detailed network architectures for semantic segmentation and shape classification. Tuples under each stage block indicate the number of sampled points and feature dimensions of attention blocks, and the number of sampled points is determined by grid sizes, as specified in the main paper.</p><p>Label: chair  <ref type="bibr" target="#b43">[44]</ref> dataset is under the MIT license, while S3DIS <ref type="bibr" target="#b44">[45]</ref> and ModelNet40 <ref type="bibr" target="#b45">[46]</ref> have custom licenses that only allow academic use.</p><p>Data preprocessing and augmentation. For S3DIS and ModelNet40 datasets, we adopt the data preprocessing of PTv1 <ref type="bibr" target="#b0">[1]</ref> with slight adjustments to the augmentation. For ScanNet v2, we estimate normal vectors for points as additional feature input. The data augmentation strategies are different for each dataset, as shown in <ref type="table">Table 10</ref>. The detailed settings for each type of data augmentation will be available in our open-source code.</p><p>Training details. Our specific model training settings are available in <ref type="table">Table 11</ref>. For the segmentation task, AdamW is used to reduce overfitting of the model. Scheduler with a cosine annealing policy has a better performance on ScanNet v2, which has more data than S3DIS. We use cross-entropy loss for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Quantitative Results</head><p>In this section, more quantitative results are provided to validate and analyze our proposed network architecture.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>ICLR, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Maskedattention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<idno>CVPR, 2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno>CVPR, 2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03605,2022.4</idno>
		<title level="m">Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Panopticfusion: Online volumetric semantic mapping at the level of stuff and things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaku</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohsuke</forename><surname>Kaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A unified point-based framework for 3d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yueh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geometric back-projection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
