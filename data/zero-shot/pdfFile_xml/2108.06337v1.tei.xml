<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Path Learning for Domain Adaptation of Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
							<email>fangwen@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
							<email>wqzhang@fudan.edu.cnfawe</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Path Learning for Domain Adaptation of Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation for semantic segmentation enables to alleviate the need for large-scale pixel-wise annotations. Recently, self-supervised learning (SSL) with a combination of image-to-image translation shows great effectiveness in adaptive segmentation. The most common practice is to perform SSL along with image translation to well align a single domain (the source or target). However, in this single-domain paradigm, unavoidable visual inconsistency raised by image translation may affect subsequent learning. In this paper, based on the observation that domain adaptation frameworks performed in the source and target domain are almost complementary in terms of image translation and SSL, we propose a novel dual path learning (DPL) framework to alleviate visual inconsistency. Concretely, DPL contains two complementary and interactive single-domain adaptation pipelines aligned in source and target domain respectively. The inference of DPL is extremely simple, only one segmentation model in the target domain is employed. Novel technologies such as dual path image translation and dual path adaptive segmentation are proposed to make two paths promote each other in an interactive manner. Experiments on GTA5?Cityscapes and SYNTHIA?Cityscapes scenarios demonstrate the superiority of our DPL model over the state-of-the-art methods. The code and models are available at: https: //github.com/royee182/DPL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decades, significant progress <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref> in semantic segmentation has been achieved with Deep Convolutional Neural Networks. The empirical observation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41]</ref> demonstrates that the leading performance is partially attributed to a large volume of training data, thus dense pixel-level annotations are required in supervised learning, which is laborious and time-consuming. To avoid this painstaking task, researchers resort to train segmentation models on synthetic but photo-realistic largescale datasets such as GTA5 <ref type="bibr" target="#b26">[27]</ref> and SYNTHIA <ref type="bibr" target="#b27">[28]</ref> with computer-generated annotations. However, due to the cross-domain differences, these well-trained models usually undergo significant performance drops when tested on realistic datasets (e.g., Cityscapes <ref type="bibr" target="#b5">[6]</ref>). Therefore, unsupervised domain adaptation (UDA) methods have been widely adopted to align the domain shift between the rich-labeled source data (synthetic images) and the unlabeled target data (real images).</p><p>Two commonly used paradigms in unsupervised domain adaptive segmentation are image-to-image translation based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> and self-supervised learning (SSL) based methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b11">12]</ref>. The most common practice for image-to-image translation based methods is to translate synthetic data from source domain (denote as domain-S) to target domain (denote as domain-T ) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref> to reduce the visual gap between different domains. Then adaptive seg-mentation is trained on translated synthetic data. However, by only applying the image-to-image translation to domain adaptation task, the results are always unsatisfying. One of the leading factors is that image-to-image translation may change the image content involuntarily and introduce visual inconsistency between raw images and translated images. Training on translated images with uncorrected groundtruth labels of source images introduces noise which disturbs the domain adaptation learning.</p><p>A combination of SSL and image-to-image translation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15]</ref> has been demonstrated great effectiveness in the UDA field. SSL utilizes a well-trained segmentation model to generate a set of pseudo labels with high confidence for unlabeled target data, then the adaptive segmentation training can be divided into two parallel parts, namely supervised part (training is performed on source data with ground-truth labels) and SSL part (training is performed on target data with pseudo labels). In this paradigm, the most prevalent practice is to perform adaptation to well align a single domain, i.e., either source domain (named domain-S adaptation) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> or target domain (named domain-T adaptation) <ref type="bibr" target="#b42">[43]</ref>. However, both domain-S and domain-T adaptation heavily rely on the quality of image-to-image translation models, where visual inconsistency is always unavoidable. For domain-T adaptation (as shown in <ref type="figure" target="#fig_0">Figure 1.(a)</ref>), visual inconsistency brings in misalignment between translated source images and uncorrected groundtruth labels, which disturbs the supervised part. In contrast, domain-S adaptation (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.(b)) avoids image translation on source images, but simultaneously introduces visual inconsistency between target images and the corresponding translated images. Defective pseudo labels generated by unaligned images disturb the SSL part.</p><p>Notice the above single-domain adaptation pipelines are almost complementary in terms of the two training parts, i.e., visual inconsistency caused by image translation disturbs the training of supervised part in domain-T adaptation and SSL part in domain-S adaptation. In contrast, SSL part in domain-T adaptation and supervised part in domain-S adaptation are unaffected. It is natural to raise a question: could we combine these two complementary adaptation pipelines into a single framework to make good use of each strength and make them promote each other? Based on this idea, we propose the dual path learning framework which considers two pipelines from opposite domains to alleviate unavoidable visual inconsistency raised by image translations. We name two paths used in our framework as path-T (adaption is performed in domain-T ) and path-S (adaption is performed in domain-S), respectively. Path-S assists path-T to learn precise supervision from source data. Meanwhile, path-T guides path-S to generate highquality pseudo labels which are important for SSL in return. It is worth noting that path-S and path-T are not two separated pipelines in our framework, interactions between two paths are performed throughout the training, which is demonstrated to be effective in our experiments. The whole system forms a closed-loop learning. Once the training has finished, we only retain a single segmentation model well aligned in target domain for testing, no extra computation is required. The main contributions of this work are summarized as:</p><p>? We present a novel dual path learning (DPL) framework for domain adaptation of semantic segmentation. DPL employs two complementary and interactive single-domain pipelines (namely path-T and path-S) in the training phase. In the testing, only a single segmentation model well aligned in target domain is used. The proposed DPL framework surpasses state-of-theart methods on representative scenarios.</p><p>? We present two interactive modules to make two paths promote each other, namely dual path image translation and dual path adaptive segmentation.</p><p>? We introduce a novel warm-up strategy for the segmentation models which helps adaptive segmentation in the early training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation. Domain adaptation is a broadly studied topic in computer vision. It aims to rectify the mismatch in cross-domains and tune the models toward better generalization at testing <ref type="bibr" target="#b23">[24]</ref>. A variety of domain adaptation methods for image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13]</ref> and object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref> have been proposed. In this paper, we focus on the unsupervised domain adaptation of semantic segmentation. Domain Adaptation for Semantic Segmentation. Semantic segmentation needs a large volume of pixel-level labeled training data, which is laborious and time-consuming in annotation. A promising solution to reduce the labeling cost is to train segmentation networks on synthetic dataset (e.g., GTA5 <ref type="bibr" target="#b26">[27]</ref> and SYNTHIA <ref type="bibr" target="#b27">[28]</ref>) with computergenerated annotations before testing on realistic dataset (e.g., Cityscapes <ref type="bibr" target="#b5">[6]</ref>). Although synthetic images have similar appearance to real images, there still exist domain discrepancies in terms of layouts, colors and illumination conditions, which always cripples the models' performance. Domain adaptation is necessary to align the synthetic and the real dataset <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b13">14]</ref>. Adversarial-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> are broadly explored in unsupervised domain adaptation, which align different domains at image-level <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref> or featurelevel <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11]</ref>. The image-level adaptation regards domain adaptation as an image synthesis problem, and aims to reduce visual discrepancy (e.g., lighting and object texture)   in cross-domains with unpaired image-to-image translation models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. However, the performance is always unsatisfactory by simply applying image translation to domain adaptation task. One reason is that image-to-image translation may change the image content involuntarily and further disturb the following segmentation training <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_0">S ? T Forward: Loss: S T ? Seg. Model T Seg. Model s L !"# T L $%&amp;'( T L !"# s L $%&amp;'( s L )*+ ( T (T ), s (T ')) L )*+ ( T (S ?), s (S )) DPPLG s L ,*- s (S, s ) L ,*- T (T, ( * ) L ,*- s (T ?, ( * ) L ./0 T (S ?,T ) L ./0 s (S,T ?) S ? T S T ' T (T ) s (T ?)</formula><p>In recent years, self-supervised learning (SSL) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> shows tremendous potential in adaptive segmentation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>. The key principle for these methods is to generate a set of pseudo labels for target images as the approximation to the ground-truth labels, then segmentation model is updated by leveraging target domain data with pseudo labels. CRST <ref type="bibr" target="#b51">[52]</ref> is the first work to introduce self-training into adaptive segmentation, it also alleviates category imbalance issue by controlling the proportion of selected pseudo labels in each category. Recent TPLD <ref type="bibr" target="#b11">[12]</ref> proposes a two-phase pseudo label densification strategy to obtain dense pseudo labels for SSL.</p><p>Two works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref> which explore the combination of image translation and SSL are closely related to ours. Label-Driven <ref type="bibr" target="#b42">[43]</ref> performs a target-to-source translation and a label-driven reconstruction module is used to reconstruct source and target images from the corresponding predicted labels. In contrast, BDL <ref type="bibr" target="#b15">[16]</ref> represents a bidirectional learning framework which alternately trains the image translation and the adaptive segmentation in target domain. Meanwhile, BDL utilizes a single-domain perceptual loss to maintain visual consistency. We will demonstrate this kind of design is suboptimal compared with the proposed dual path image translation module in Section 3.2. These two works demonstrate the combination of image translation and SSL can promote adaptive learning. Different from these single-domain adaptation methods, the proposed dual path learning framework integrates two comple-mentary single-domain pipelines in an interactive manner to address visual inconsistency problem by: 1) utilizing segmentation models aligned in different domains to provide cross-domain perceptual supervision for image translation; 2) combining knowledge from both source and target domain for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given the source dataset S (synthetic data) with pixellevel segmentation labels Y S , and the target dataset T (real data) with no labels. The goal of unsupervised domain adaptation (UDA) is that by only using S, Y S and T , the segmentation performance can be on par with the model trained on T with corresponding ground-truth labels Y T . Domain gap between S and T makes the task difficult for the network to learn transferable knowledge at once.</p><p>To address this problem, we propose a novel dual path learning framework named DPL. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>.(a), DPL consists of two complementary and interactive paths: path-S (adaptive learning is performed in source domain) and path-T (adaptive learning is performed in target domain). How to allow one of both paths provide positive feedbacks to the other is the key to success. To achieve this goal, we propose two modules, namely dual path image translation (DPIT) and dual path adaptive segmentation (DPAS). DPIT aims to reduce the visual gap between different domains without introducing visual inconsistency. In our design, DPIT unites general unpaired image translation models with dual perceptual supervision from two singledomain segmentation models. Notice any unpaired image translation models can be used in DPIT, we use Cycle-GAN <ref type="bibr" target="#b48">[49]</ref> as our default model due to its popularity and it provides bidirectional image translation inherently. We use In this section, we first describe our warm-up strategy in Section 3.1. Then, we introduce the key components of DPL: DPIT in Section 3.2 and DPAS in Section 3.3. Next, we revisit and summarize the whole training process in Section 3.4. Finally, testing pipeline of DPL is presented in Section 3.5.</p><formula xml:id="formula_1">T = G T ?S (T ) and S = G S?T (S) to denote translated S' s S Seg. Model T (Pseudo Label) (Revised Label) } Probability Map (S' ) s? (GT Label) s?T !"#$% % s?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single Path Warm-up</head><p>Perceptual supervision in DPIT and pseudo label generation in DPAS rely on the quality of segmentation models. To accelerate convergence of DPL, a warm-up process for segmentation models M S and M T is required. M S Warm-up. The warm-up for M S is easily conducted in a fully supervised way by using source dataset S with ground-truth labels Y S . M T Warm-up. It is difficult to directly train M T in a supervised manner since no labels can be accessed in target dataset T . A straightforward idea is to translate source images S to target domain by using naive CycleGAN, and then M T is trained on translated images S with approximate ground-truth labels Y S . Unfortunately, naive CycleGAN does not apply any constrains to preserve visual consistency between S and S , i.e., visual content may be changed when S is translated to S . Misalignment between S and Y S can disturb the training of M T .</p><p>To address this issue, we propose a novel label correction strategy as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. The core principle is to find a revised label Y S for S by considering both ground-truth labels Y S and segmentation predictions of S . Specially, we feed S into M T (which is initialized as M S at the beginning) to generate pseudo labels? S . Then label correction module revises raw ground-truth labels Y S by replacing pixel-wise labels in Y S with high-confidence pixel-wise labels in? S , which means the labels of content-changed areas have been approximately corrected by reliable predictions. Formally, define revised labels</p><formula xml:id="formula_2">Y S = {Y (i,j) S } (1 ? i ? H, 1 ? j ? W ) as: Y (i,j) S = ? (i,j) S , if P (i,j,?) (S ) ? P (i,j,c) (S ) &gt; ? Y (i,j) S , else,<label>(1)</label></formula><p>where H and W denote the height and width of the input image respectively, P (?) is probability map predicted by segmentation model,? and c denote the category index of Y</p><formula xml:id="formula_3">(i,j) S and Y (i,j) S</formula><p>respectively, ? controls correction rate, we set ? = 0.3 empirically.</p><p>In addition, we also use M T to generate pseudo label? Y T for T . Now we have paired training data (S , Y S ) and (T ,? T ) which approximately lie in target domain for M T training. The overall loss is defined as:</p><formula xml:id="formula_4">L M T = L seg (S , Y S ) + L seg (T ,? T ) + ? adv L adv (S , T ),<label>(2)</label></formula><p>where L adv represents typical adversarial loss as used in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43]</ref> to further align target domain, L seg indicates the commonly used per-pixel segmentation loss:</p><formula xml:id="formula_5">L seg (I, Y ) = ? 1 HW H i=1 W j=1 C c=1 Y (i,j,c) log P (i,j,c) (I),<label>(3)</label></formula><p>where I and Y denote input image (raw image or translated image) and corresponding labels (ground-truth labels or pseudo labels), respectively.</p><p>Once warm-up procedure is finished, we obtain preliminary segmentation models which are approximately aligned in the corresponding domain. These well-initialized models facilitate the training of DPIT and DPAS, which will be described in next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual Path Image Translation</head><p>Image-to-image translation aims to reduce the gap in visual appearance (e.g., object textures and lighting) between source and target domain. As discussed in Section 1, unavoidable visual inconsistency caused by image translation may mislead the subsequent adaptive segmentation learning, and thus extra constraints to maintain visual consistency are required. </p><formula xml:id="formula_6">+ L P er (F T (T ), F S (T )),<label>(4)</label></formula><p>where L P er is perceptual loss as in <ref type="bibr" target="#b15">[16]</ref>, F S (?) and F T (?) represent perceptual feature extracted by M S and M T respectively.</p><p>Besides the supervision of dual perceptual loss, DPIT is also supervised by general adversarial and reconstruction loss. The overall loss of DPIT can be formulated as:</p><formula xml:id="formula_7">L DP IT = L S GAN (S, T ) + L T GAN (S , T ) + ? Recon L S Recon (S, G T ?S (S )) + ? Recon L T Recon (T , G S?T (T )) + ? DualP er L DualP er (S, S , T , T ),<label>(5)</label></formula><p>1 Perceptual feature denotes the probability map before softmax layer of segmentation model. where L S GAN (L T GAN ) and L S Recon (L T Recon ) are GAN loss and reconstruction loss as in <ref type="bibr" target="#b48">[49]</ref>, ? Recon and ? DualP er denote the weights of reconstruction loss and dual perceptual loss respectively. We set ? Recon = 10 and ? DualP er = 0.1 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dual Path Adaptive Segmentation</head><p>Once DPIT is symmetrically trained, translated images S = G S?T (S) and T = G T ?S (T ) are fed into dual path adaptive segmentation (DPAS) module for subsequent learning. As shown in <ref type="figure" target="#fig_3">figure 2.(a)</ref>, DPAS utilizes selfsupervised learning with combination of well-trained image translation for adaptive segmentation learning, i.e., segmentation models are trained on both source images (or translated source images) with ground-truth labels and target images (or translated target images) with pseudo labels. The core of DPAS is to generate high-quality pseudo labels of target images by combining predicted results from two paths. The training process of DPAS can be formulated as two alternative steps: 1) dual path pseudo label generation; 2) dual path segmentation training. Dual Path Pseudo Label Generation. The labels of target dataset are unavailable in unsupervised domain adaptation tasks. Self-supervised learning (SSL) has been demonstrated great success when the labels of dataset are insufficient or noisy. The way to generate pseudo labels plays an important role in SSL. As described in Section 1, in path-T , visual inconsistency brings in misalignment between translated source images S and uncorrected ground-truth labels Y S , which disturbs the training of M T . Similar issue exists in path-S (see <ref type="figure" target="#fig_0">Figure 1</ref>). Inspired by the observation that two paths from opposite domains are almost complementary, we take full advantages of two paths and present a novel dual path pseudo label generation (DPPLG) strategy to generate high-quality pseudo labels as shown in <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>Concretely, let P S (?) = Softmax(F S (?)) and P T (?) = Softmax(F T (?)) denote probability map predicted by M S and M T , respectively. In path-T , target images can be directly fed into M T to generate P T (T ). In contrast, path-S requires image translation to generate T = G T ?S (T ), then P S (T ) can be obtained by feeding T into M S . Finally, enhanced probability map P * which is used for generating pseudo labels of target images can be obtained by a weighted sum of two separate probability maps P T (T ) and P S (T ):</p><formula xml:id="formula_8">P * = 1 2 P T (T ) + 1 2 P S (T ),<label>(6)</label></formula><p>Following common practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12]</ref>, we use max probability threshold (MPT) to select the pixels with higher confidence in P * as pseudo labels of unlabeled target images. Concretely, define pseudo labels? * = {?</p><formula xml:id="formula_9">(i,j,c) * } (1 ? i ? H, 1 ? j ? W, 1 ? c ? C) as: ? (i,j,c) * = ? ? ? ? ? 1, if c = argmax c (P (i,j,c) * ) and P (i,j,c) * &gt; ? 0, else,<label>(7)</label></formula><p>where ? denotes threshold to filter pixels with low prediction confidence. We set ? = 0.9 as default according to <ref type="bibr" target="#b15">[16]</ref>.</p><p>Though path-S and path-T can use respective pseudo labels generated by themselves, we will demonstrate the benefits by using shared pseudo label? * in Section 4.4. Dual Path Segmentation Training. Now we introduce the process of dual path segmentation training. Concretely, for path-T , the objective is to train a well generalized segmentation model M T in target domain. Training data for M T includes two part, translated source images S = G S?T (S) with ground-truth labels Y S , and raw target images T with pseudo labels? * generated by DPPLG. In contrast, path-S requires good generalization in source domain. Similarly, M S is trained on source images S with ground-truth labels Y S and translated images T = G T ?S (T ) with shared pseudo labels? * . Besides the supervision from segmentation loss, we also utilize a discriminator on top of the features of the segmentation model to further decrease the domain gap as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. The overall loss function of dual path segmentation can be defined as:</p><formula xml:id="formula_10">L DualSeg = L T seg (S , Y S ) + L T seg (T ,? * ) + L S seg (S, Y S ) + L S seg (T ,? * ) + ? adv (L T adv (S , T ) + L S adv (S, T )),<label>(8)</label></formula><p>where L S adv and L T adv denote typical adversarial loss, L S seg and L T seg are per-pixel segmentation loss as defined in Equation 3, ? adv controls contribution of adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training pipeline</head><p>Algorithm 1 summarizes the whole training process of DPL. First, M S and M T are initialized by the proposed warm-up strategy. Next, we train DPIT to provide welltranslated images for subsequent learning. At last, following the common practice that self-supervised learning is conducted in an iterative way <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12]</ref>, DPAS is trained N times for domain adaptation. We use superscript (n) to refer to the n-th iteration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Testing Pipeline</head><p>As shown in <ref type="figure" target="#fig_3">Figure 2.(b)</ref>, the inference of DPL is extremely simple, we only retain M T when testing on target images. Though DPL already shows the superiority over the state-of-the-art methods, we explore an optional dual path testing pipeline named DPL-Dual to boost performance by considering predictions from two paths. Concretely, we first generate probability map P T (T ) and P S (T ) from two well-trained segmentation models M T and M S respectively, then an average function is used to generate final probability map P F = (P S (T ) + P T (T ))/2. Though DPL-Dual promotes the performance, extra computation is introduced. We recommend DPL-Dual as an optional inference pipeline when computation cost is secondary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Following common practice, We evaluate our framework in two common scenarios, GTA5 <ref type="bibr" target="#b26">[27]</ref>?Cityscapes <ref type="bibr" target="#b5">[6]</ref> and SYNTHIA <ref type="bibr" target="#b27">[28]</ref>?Cityscapes. GTA5 consists of 24,996 images with the resolution of 1914 ? 1052 and we use the 19 common categories between GTA5 and Cityscapes for training and testing. For SYNTHIA dataset, we use the SYTHIA-RAND-CITYSCAPES set which contains 9,400 images with resolution 1280 ? 760 and 16 common categories with Cityscapes. Cityscapes is split into training set, validation set and testing set. Training set contains 2,975 images with resolution 2048 ? 1024. Following common practice, we report the results on the validation set which contains 500 images with same resolution. All ablation studies are performed on GTA5?Cityscapes, and comparison with state-of-the-art is performed on both GTA5?Cityscapes and SYNTHIA?Cityscapes. We use category-wise IoU and mIoU to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Following common practice, we use DeepLab-V2 <ref type="bibr" target="#b3">[4]</ref> with ResNet-101 <ref type="bibr" target="#b7">[8]</ref> and FCN-8s <ref type="bibr" target="#b17">[18]</ref> with VGG16 <ref type="bibr" target="#b29">[30]</ref> as our semantic segmentation models. The discriminator used in adversarial learning is similar to <ref type="bibr" target="#b24">[25]</ref>, which has 5 convolutional layers with kernel size 4 ? 4 with channel number {64, 128, 256, 512, 1} and stride of 2. For each of convolutional layer except the last one, a leaky ReLU <ref type="bibr" target="#b41">[42]</ref> layer parameterized by 0.2 is followed. The discriminator is implemented over the softmax output of segmentation model. For DPIT, following <ref type="bibr" target="#b15">[16]</ref>, we adopt the architecture of Cy-cleGAN with 9 blocks and use the proposed dual perceptual loss to maintain visual consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>When training DPIT, the input image is randomly cropped to the size 512?256 and it is trained for 40 epochs.  The learning rate of first 20 epochs is 0.0002 and decreases to 0 linearly after 20 epochs. Following <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b15">16]</ref>, in Equation 5, ? Recon is set to 10, ? DualP er is set to 0.1, respectively. For DPAS training, the input images are resized to the size 1024 ? 512 with batch size 4. For DeepLab-V2 with ResNet-101, we adopt SGD as optimizer and set initial learning rate with 5 ? 10 ?4 , which is decreased with 'poly' learning rate policy with power as 0.9. For FCN-8s with VGG16, we use Adam optimizer with momentum {0.9, 0.99} and initial learning rate is set to 2 ? 10 ?5 . The learning rate is decreased with 'step' policy with step size 50000 and drop factor 0.1. For adversarial learning, ? adv is set to 1 ? 10 ?3 for DeepLab-V2 and 1 ? 10 ?4 for FCN-8s in Equation 2 and 8. The discriminator is trained with Adam optimizer with the initial learning rate 2 ? 10 ?4 . The momentum parameters are set as 0.9 and 0.99. All ablation studies are conducted on the first iteration (N = 1). We set N = 4 when comparing with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments</head><p>Dual Path Image Translation Improves Translation Quality. DPIT encourages visual consistency through dual perceptual loss computed by segmentation models M S and M T . To demonstrate the effectiveness of DPIT, we compare it with: 1) naive CycleGAN, in which no perceptual loss is used to maintain visual consistency; 2) Single Path Image Translation (SPIT) used in BDL <ref type="bibr" target="#b15">[16]</ref>, which applies Cy-cleGAN and perceptual loss computed by single segmentation model aligned in target domain. Notice the only difference in this ablation study is that different image translation methods are used in DPL.   bels. We compare DPPLG with single path pseudo label generation (SPPLG) method, i.e., path-S and path-T generate respective pseudo labels by themselves. Meanwhile, we study three different strategies of DPPLG: 1) DPPLG-Max, which selects the prediction with maximum probability of two paths; 2) DPPLG-Joint, in which two paths generate pseudo labels separately and intersections are selected as final pseudo labels; 3) DPPLG-Weighted, which is the default strategy as described in Section 3.3. <ref type="table" target="#tab_1">Table 2</ref> shows the results. All of the DPPLG strategies have better performance than SPPLG, which means the joint decision of two complementary paths can improve the quality of pseudo labels.</p><p>We use DPPLG-Weighted as our pseudo label generation strategy due to the preeminent experimental result. The Effectiveness of Dual Path Adaptive Segmentation.</p><p>We show the stage-wise results of DPAS in <ref type="table" target="#tab_3">Table 3</ref>. When warm-up is finished, M T ) can still promote the performance, the improvement is limited. Ablation Study on Label Correction Strategy. In Section 3.1, we propose a label correction strategy for M T warm-up. Now we study different warm-up strategies as well as hyper parameters in <ref type="table" target="#tab_4">Table 4</ref>. Recall that label correction is used to find a revised label Y S by considering both ground-truth labels Y S and pseudo labels? S (see <ref type="bibr">Equation 1</ref>). We ablate two extreme cases: 1) directly leverage ground-truth labels Y S without label correction; 2) directly leverage pseudo labels? S without label correction. Results in <ref type="table" target="#tab_4">Table 4</ref> shows the superiority of our label correction module. We also study different ? which controls correction rate, from the table, we find ? is a less-sensitive hyper parameter which can be set as 0.3 by default. Comparison with State-of-the-art Methods. We evaluate DPL and DPL-Dual with state-of-the-art methods on two common scenarios, GTA5?Cityscapes and SYNTHIA? Cityscapes. For each scenario, we report the results on two segmentation models, ResNet101 and VGG16. <ref type="table" target="#tab_5">Table 5</ref> shows the results on the scenario GTA5?Cityscapes,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel dual path learning framework named DPL, which utilizes two complementary and interactive paths for domain adaptation of segmentation. Novel technologies such as dual path image translation and dual path adaptive segmentation are presented to make two paths interactive and promote each other. Meanwhile, a novel label correction strategy is proposed in the warmup stage. The inference of DPL is extremely simple, only one segmentation model well aligned target domain is used. Experiments on common scenarios GTA5?Cityscapes and SYNTHIA?Cityscapes demonstrate the superiority of our DPL over the state-of-the-art methods.</p><p>Analysis of Different Pseudo Label Generation Strategies. Effective pseudo label generation strategies aim to generate sufficient amount of pseudo labels with high-quality. To evaluate the quality of pseudo labels generated by different methods (i.e., SPPLG, DPPLG-Max, DPPLG-Joint and DPPLG-Weighted), we investigate the per-class accuracy, mean accuracy and selected-pixel ratio. <ref type="table" target="#tab_0">Table 10</ref> shows the comparison. We can observe that limited by the capability of single path model, SPPLG generates unreliable pseudo labels with the lowest mean accuracy. Among all dual path pseudo label generation strategies, DPPLG-Joint achieves the best mean accuracy but generates less amount of pseudo labels (lowest selected-pixel ratio). In contrast, DPPLG-Max has the highest selected-pixel ratio, but in the mean time, the low mean accuracy introduces more noise which disturbs the subsequent learning. Our default DPPLG-Weighted which achieves best mIoU performance makes a reasonable trade-off between mean accuracy and selected-pixel ratio and thus guarantees both quality and quantity of generated pseudo labels. The Effectiveness of M T Warm-Up. As described in Section 3.1, cross-domain perceptual supervision in DPIT and pseudo label generation in DPAS rely on the quality of segmentation models. Now we show more details to demonstrate the effectiveness of M T warm-up. Again, the warm-up of M S is simple, while the warm-up of M T is exclusive in our paper. A simpler way to avoid the particular warm-up procedure of M T it to directly initialize M (0) <ref type="table" target="#tab_0">Table 11</ref>, we show the importance of our warm-up strategy. Compared with the model achieving state-of-the-art performance (52.8 mIoU), the model trained without M T warm-up strategy only achieves mIoU of 49.0 and has worse performance in most categories.  Network designs of segmentation models. In our proposed dual path adaptive segmentation(DPAS) module, two segmentation models M S and M T are trained separately. We compare this 'Seperate' choice with two weight sharing designs: 1) Share-Part, where the first 4 blocks of ResNet101 are shared; 2) Share-All, where the whole segmentation model is shared. Weight sharing designs can reduce the number of parameters, while performance declined is observed in <ref type="table" target="#tab_0">Table 12</ref>.</p><formula xml:id="formula_11">T with M (0) S . In</formula><p>Weight coefficient in Eq.6. We re-define Eq.6 as P * = ?P T (T ) + (1 ? ?)P S (T ) and ablate ? in <ref type="table" target="#tab_0">Table 13</ref>.  Visualization of Pseudo Labels. In <ref type="figure" target="#fig_8">Figure 6</ref>, we visualize pseudo labels generated by different pseudo label generation strategies. Recall that path-S and path-T can generate respective pseudo label separately, which is named as single path pseudo label generation (SPPLG) strategy. We visualize pseudo label generated by SPPLG (in path-S and path-T respectively) and DPPLG for comparison. Obviously, DPPLG generates more accurate pseudo labels in both small objects (e.g., pole and t-light in row 1,2 and person in row 3) and indistinguishable categories (e.g., train in row 4, sidewalk in row 5 and car in row 6). The visualization further demonstrates the effectiveness of our DPPLG strategy. Visualization of Image Translation Results. In <ref type="figure">Figure 7</ref>, we show the qualitative results of different image translation methods, i.e., CycleGAN, single path image translation (SPIT) used in BDL <ref type="bibr" target="#b15">[16]</ref> and dual path image translation (DPIT) used in our DPL, on GTA5?Cityscapes scenario. As highlighted by red rectangles in <ref type="figure">Figure 7</ref>, CycleGAN and SPIT tend to generate vegetation-like artifacts in the sky to fool the discriminator, which can cause image distortion and visual inconsistency in the following adaptive learning. Benefit from the cross-domain perceptual loss which better maintains visual consistency, our DPIT achieves superior performance in terms of content preserving and style translation. Qualitative Comparison of Different Domain Adaption Methods. In <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>, we visualize segmentation results of different methods for qualitative comparisons. We can observe that DPL and DPL-Dual generate most visually pleasurable results which are similar to the ground truth among all methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of single-domain adaptation pipelines. S is source image with ground-truth label Y S , and T is target image. G S?T represents image translation from domain-S to domain-T and vice versa. S = G S?T (S) and T = G T ?S (T ) are translated images in the corresponding domain. M S and M T are semantic segmentation models in domain-S and domain-T , respectively.? T and? T represent the corresponding pseudo labels of T and T . Red dash rectangles denote that visual inconsistency raised by image translations disturbs domain adaptation learning in either supervised part or SSL part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training pipeline of DPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(a) Overview of DPL framework. Inputs are highlighted by orange rectangles. DPL consists of two complementary single-domain paths: path-S (learning is performed in source domain) and path-T (learning is performed in target domain). Dual path image translation (DPIT) and dual path adaptive segmentation (DPAS) are proposed to make two paths interactive and promote each other. In DPIT, unpaired image translation models (G T ?S and G S?T ) are supervised by general GAN loss and cross-domain perceptual loss. DPAS employs the proposed dual path pseudo label generation (DPPLG) module to produce pseudo labels? * of target images, then segmentation models (M S and M T ) are trained on both source images (or translated source images) with ground-truth labels and target images (or translated target images) with pseudo labels. (b) Testing of DPL. Only M T is used for inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of label correction strategy. Inputs are highlighted by orange rectangles. images in path-S and path-T respectively, where G T ?S and G S?T are image translation models in the corresponding path. DPAS utilizes translated images from DPIT and the proposed dual path pseudo label generation (DPPLG) module to generate high-quality pseudo labels for target images, then segmentation models M S (in path-S) and M T (in path-T ) are trained with both transferred knowledge in source domain and implicit supervision in target domain. The testing of DPL is extremely simple, we only retain M T for inference as shown in Figure 2.(b). The training process of DPL consists of two phases: single-path warm-up and DPL training. DPL benefits from well-initialized M S and M T , since both DPIT and DPAS rely on the quality of segmentation models. A simple but efficient warm-up strategy can accelerate the convergence of DPL. Once the warm-up finishes, DPIT and DPAS are trained sequentially in DPL training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of dual path pseudo label generation (DPPLG). Input is highlighted by orange rectangle.BDL<ref type="bibr" target="#b15">[16]</ref> introduces a perceptual loss to maintain visual consistency between paired images (i.e., raw images and corresponding translated images). Perceptual loss measures distance of perceptual features<ref type="bibr" target="#b0">1</ref> extracted from a welltrained segmentation model. In BDL, domain adaptation is only performed in target domain, as a result, the perceptual loss of paired images (S, S ) and (T , T ) is computed with identical segmentation model. Notice paired images are from two different domains (S and T are in source domain while T and S are in target domain), using segmentation model aligned in a single domain to extract features for perceptual loss computation may be suboptimal. Now we introduce our dual path image translation (DPIT) as illustrated in Figure 2.(a). DPIT is an bidirectional image translation model with cross-domain perceptual supervision. We use G S?T and G T ?S to denote image translation in Path-T and Path-S respectively. Cy-cleGAN is served as our default model since it provides bidirectional image translation inherently, however, any unpaired image translation algorithms can be used in DPIT. Different from BDL, DPIT makes use of two paths aligned in opposite domains and extracts perceptual features for paired images from their corresponding path to better maintain visual consistency. Concretely, DPIT utilizes M S to extract perceptual features for S and T , and M T to extract perceptual features for T and S , respectively. Then we can formulate our dual perceptual loss L DualP er as: L DualP er (S, S , T , T ) = L P er (F T (S ), F S (S))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Training process of DPL Input: S, Y S , T Output: M Equation 5for n ? 1 to N do DPAS generate?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>48.5, respectively. After first iteration, M (1) S achieves 49.6 (+13.5% improvement), and M (1) T achieves 51.8 (+6.8% improvement). The big improvements on two segmentation models demonstrates that the interactions between two complementary paths facilitate the adaptive learning mutually. Though subsequent iterations (M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Pseudo label visualization of different pseudo label generation strategies (GTA5?Cityscapes). (a) raw images from Cityscapes dataset; (b) ground-truth; (c) pseudo label generated by SPPLG (path-S); (d) pseudo label generated by SPPLG (path-T ); (e) pseudo label generated by DPPLG strategy. Red rectangles highlight the differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Raw images (b) CycleGAN (c) SPIT (d) DPIT Figure 7: Qualitative results of different image translation methods on GTA5?Cityscapes scenario. (a) raw images from GTA5 dataset; (b) translated images by CycleGAN; (c) translated images by single path image translation (SPIT); (d) translated images by our dual path image translation (DPIT). Red rectangles highlight the differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Qualitative comparison of different methods on GTA5?Cityscapes scenario. Qualitative comparison of different methods on GTA5?Cityscapes scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different image translation models.</figDesc><table><row><cell cols="2">Image translation module mIoU(M</cell><cell>(1) S ) mIoU(M T ) (1)</cell></row><row><cell>CycleGAN</cell><cell>41.4</cell><cell>48.5</cell></row><row><cell>SPIT</cell><cell>48.6</cell><cell>51.1</cell></row><row><cell>DPIT</cell><cell>49.6</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different pseudo label generation strategies.</figDesc><table><row><cell cols="2">Pseudo label generation strategy mIoU(M</cell><cell>(1) S ) mIoU(M T ) (1)</cell></row><row><cell>SPPLG</cell><cell>46.0</cell><cell>50.0</cell></row><row><cell>DPPLG-Max</cell><cell>49.2</cell><cell>50.6</cell></row><row><cell>DPPLG-Joint</cell><cell>49.1</cell><cell>50.3</cell></row><row><cell>DPPLG-Weighted</cell><cell>49.6</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the comparison.</cell></row><row><cell>By using perceptual loss to maintain visual consistency,</cell></row><row><cell>both SPIT and DPIT can significantly improve the adap-</cell></row><row><cell>tation performance compared with naive CycleGAN. Our</cell></row><row><cell>DPIT surpasses SPIT in both segmentation models (M S and</cell></row><row><cell>M T ) demonstrates that extracting aligned perceptual fea-</cell></row><row><cell>tures can further alleviate visual inconsistency caused by</cell></row><row><cell>image translation.</cell></row></table><note>The Effectiveness of Dual Path Pseudo Label Genera- tion. In our proposed DPPLG module, predictions from two paths jointly participate in the generation of pseudo la-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on stage-wise DPAS.</figDesc><table><row><cell cols="5">MS mIoU MT mIoU</cell></row><row><cell>M</cell><cell>(0) S</cell><cell>43.7 M</cell><cell>(0) T</cell><cell>48.5</cell></row><row><cell>M</cell><cell>(1) S</cell><cell>49.6 M</cell><cell>(1) T</cell><cell>51.8</cell></row><row><cell>M</cell><cell>(2) S</cell><cell>50.6 M</cell><cell>(2) T</cell><cell>52.4</cell></row><row><cell>M</cell><cell>(3) S</cell><cell>50.7 M</cell><cell>(3) T</cell><cell>52.6</cell></row><row><cell>M</cell><cell>(4) S</cell><cell>50.7 M</cell><cell>(4) T</cell><cell>52.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on M T warm-up.</figDesc><table><row><cell>Model</cell><cell cols="2">? mIoU</cell></row><row><cell>MT</cell><cell cols="2">0.2 47.4</cell></row><row><cell>MT</cell><cell cols="2">0.3 48.5</cell></row><row><cell>MT</cell><cell cols="2">0.5 47.3</cell></row><row><cell>MT w/ YS</cell><cell>-</cell><cell>46.2</cell></row><row><cell>MT w/? S</cell><cell>-</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on GTA5?Cityscapes scenario. Red: best result. Blue: second best result. 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 SIM [38] 90.6 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5 1.9 30.4 39.0 49.2 FADA[36] 92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2 Label-Driven[43] 90.8 41.4 84.7 35.1 27.5 31.2 38.0 32.8 85.6 42.1 84.9 59.6 34.4 85.0 42.8 52.7 3.4 30.9 38.1 49.5 Kim et al. [15] 92.9 55.0 85.3 34.2 31.1 34.9 40.7 34.0 85.2 40.1 87.1 61.0 31.1 82.5 32.3 42.9 0.3 36.4 46.1 50.2 FDA-MBT [44] 92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.</figDesc><table><row><cell>Segmentation Model</cell><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>ResNet101[8]</cell><cell>BDL[16]</cell><cell cols="20">91.0 4 50.5</cell></row><row><cell></cell><cell>TPLD [12]</cell><cell cols="20">94.2 60.5 82.8 36.6 16.6 39.3 29.0 25.5 85.6 44.9 84.4 60.6 27.4 84.1 37.0 47.0 31.2 36.1 50.3 51.2</cell></row><row><cell></cell><cell>DPL</cell><cell cols="20">92.5 52.8 86.0 38.5 31.7 36.2 47.3 34.9 85.5 39.9 85.2 62.9 33.9 86.8 37.2 45.3 20.1 44.1 42.4 52.8</cell></row><row><cell></cell><cell>DPL-Dual</cell><cell cols="20">92.8 54.4 86.2 41.6 32.7 36.4 49.0 34.0 85.8 41.3 86.0 63.2 34.2 87.2 39.3 44.5 18.7 42.6 43.1 53.3</cell></row><row><cell></cell><cell>TPLD [12]</cell><cell cols="20">83.5 49.9 72.3 17.6 10.7 29.6 28.3 9.0 78.2 20.1 25.7 47.4 13.3 79.6 3.3 19.3 1.3 14.3 33.5 34.1</cell></row><row><cell></cell><cell>BDL [16]</cell><cell cols="20">89.2 40.9 81.2 29.1 19.2 14.2 29.0 19.6 83.7 35.9 80.7 54.7 23.3 82.7 25.8 28.0 2.3 25.7 19.9 41.3</cell></row><row><cell></cell><cell cols="21">FDA-MBT [44] 86.1 35.1 80.6 30.8 20.4 27.5 30.0 26.0 82.1 30.3 73.6 52.5 21.7 81.7 24.0 30.5 29.9 14.6 24.0 42.2</cell></row><row><cell></cell><cell cols="21">Kim et al. [15] 92.5 54.5 83.9 34.5 25.5 31.0 30.4 18.0 84.1 39.6 83.9 53.6 19.3 81.7 21.1 13.6 17.7 12.3 6.5 42.3</cell></row><row><cell>VGG16[30]</cell><cell>SIM [38]</cell><cell cols="20">88.1 35.8 83.1 25.8 23.9 29.2 28.8 28.6 83.0 36.7 82.3 53.7 22.8 82.3 26.4 38.6 0.0 19.6 17.1 42.4</cell></row><row><cell></cell><cell cols="21">Label-Driven[43] 90.1 41.2 82.2 30.3 21.3 18.3 33.5 23.0 84.1 37.5 81.4 54.2 24.3 83.0 27.6 32.0 8.1 29.7 26.9 43.6</cell></row><row><cell></cell><cell>FADA[36]</cell><cell cols="20">92.3 51.1 83.7 33.1 29.1 28.5 28.0 21.0 82.6 32.6 85.3 55.2 28.8 83.5 24.4 37.4 0.0 21.1 15.2 43.8</cell></row><row><cell></cell><cell>DPL</cell><cell cols="20">88.9 43.6 83.4 33.8 24.7 28.0 37.6 26.2 84.1 40.3 81.5 54.9 25.0 83.0 27.7 48.6 4.8 29.1 32.0 46.2</cell></row><row><cell></cell><cell>DPL-Dual</cell><cell cols="20">89.2 44.0 83.5 35.0 24.7 27.8 38.3 25.3 84.2 39.5 81.6 54.7 25.8 83.3 29.3 49.0 5.2 30.2 32.6 46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art methods on SYNTHIA?Cityscapes scenario. Red: best result. Blue: second best result.</figDesc><table><row><cell>Segmentation Model</cell><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU (16)</cell><cell>mIoU (13)</cell></row><row><cell></cell><cell>Kim et al. [15]</cell><cell cols="3">92.6 53.2 79.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">1.6 7.5 78.6 84.4 52.6 20.0 82.1 34.8 14.6 39.4</cell><cell>-</cell><cell>49.3</cell></row><row><cell></cell><cell>BDL[16]</cell><cell cols="3">86.0 46.7 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell>51.4</cell></row><row><cell></cell><cell>SIM [38]</cell><cell cols="3">83.0 44.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">17.1 15.8 80.5 81.8 59.9 33.1 70.2 37.3 28.5 45.8</cell><cell>-</cell><cell>52.1</cell></row><row><cell></cell><cell cols="4">FDA-MBT [44] 79.3 35.0 73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1</cell><cell>-</cell><cell>52.5</cell></row><row><cell>ResNet101[8]</cell><cell>FADA[36]</cell><cell cols="17">84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2</cell><cell>52.5</cell></row><row><cell></cell><cell cols="4">Label-Driven[43] 85.1 44.5 81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">16.4 15.2 80.1 84.8 59.4 31.9 73.2 41.0 32.6 44.7</cell><cell>-</cell><cell>53.1</cell></row><row><cell></cell><cell>TPLD [12]</cell><cell cols="17">80.9 44.3 82.2 19.9 0.3 40.6 20.5 30.1 77.2 80.9 60.6 25.5 84.8 41.1 24.7 43.7 47.3</cell><cell>53.5</cell></row><row><cell></cell><cell>DPL</cell><cell cols="17">87.4 45.5 82.7 14.8 0.7 33.0 21.9 20.0 82.9 85.1 56.4 21.7 82.1 39.5 30.8 45.2 46.9</cell><cell>53.9</cell></row><row><cell></cell><cell>DPL-Dual</cell><cell cols="17">87.5 45.7 82.8 13.3 0.6 33.2 22.0 20.1 83.1 86.0 56.6 21.9 83.1 40.3 29.8 45.7 47.0</cell><cell>54.2</cell></row><row><cell></cell><cell>CrCDA [11]</cell><cell cols="17">74.5 30.5 78.6 6.6 0.7 21.2 2.3 8.4 77.4 79.1 45.9 16.5 73.1 24.1 9.6 14.2 35.2</cell><cell>41.1</cell></row><row><cell></cell><cell>TPLD [12]</cell><cell cols="17">81.3 34.5 73.3 11.9 0.0 26.9 0.2 6.3 79.9 71.2 55.1 14.2 73.6 5.7 0.5 41.7 36.0</cell><cell>41.3</cell></row><row><cell></cell><cell>Kim et al. [15]</cell><cell cols="3">89.8 48.6 78.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">0.0 4.7 80.6 81.7 36.2 13.0 74.4 22.5 6.5 32.8</cell><cell>-</cell><cell>43.8</cell></row><row><cell></cell><cell>BDL [16]</cell><cell cols="17">72.0 30.3 74.5 0.1 0.3 24.6 10.2 25.2 80.5 80.0 54.7 23.2 72.7 24.0 7.5 44.9 39.0</cell><cell>46.1</cell></row><row><cell>VGG16 [30]</cell><cell>FADA [36]</cell><cell cols="17">80.4 35.9 80.9 2.5 0.3 30.4 7.9 22.3 81.8 83.6 48.9 16.8 77.7 31.1 13.5 17.9 39.5</cell><cell>46.1</cell></row><row><cell></cell><cell cols="18">FDA-MBT [44] 84.2 35.1 78.0 6.1 0.4 27.0 8.5 22.1 77.2 79.6 55.5 19.9 74.8 24.9 14.3 40.7 40.5</cell><cell>47.3</cell></row><row><cell></cell><cell cols="18">Label-Driven [43] 73.7 29.6 77.6 1.0 0.4 26.0 14.7 26.6 80.6 81.8 57.2 24.5 76.1 27.6 13.6 46.6 41.1</cell><cell>48.5</cell></row><row><cell></cell><cell>DPL</cell><cell cols="17">82.7 37.3 80.1 1.6 0.9 29.5 20.5 33.1 81.7 82.9 55.6 20.2 79.2 26.3 6.8 45.5 42.7</cell><cell>50.2</cell></row><row><cell></cell><cell>DPL-Dual</cell><cell cols="17">83.5 38.2 80.4 1.3 1.1 29.1 20.2 32.7 81.8 83.6 55.9 20.3 79.4 26.6 7.4 46.2 43.0</cell><cell>50.5</cell></row><row><cell cols="8">DPL achieves state-of-the-art performance on both models</cell><cell></cell><cell cols="11">art with mIoU 42.7, DPL-Dual further promotes the perfor-</cell></row><row><cell cols="8">(with mIoU of 52.8 on ResNet101 and 46.2 on VGG16).</cell><cell></cell><cell cols="3">mance to 43.0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">DPL-Dual further achieves mIoU of 53.3 on ResNet101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">and 46.5 on VGG16. Domain gap between SYNTHIA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">and Cityscapes is much larger than that of GTA5 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Cityscapes, and their categories are not fully overlapped.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">We list both of the results for the 13-category and 16-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">category for a fair comparison with state-of-the-art meth-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">ods. Results are shown in Table 6, mIoU (13) and mIoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(16) represent adaptation methods are evaluated on 13 com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">mon categories and 16 common categories, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Once again, under 13-category metric, DPL achieves state-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">of-the-art result on both ResNet101 and VGG16, DPL-Dual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">further boosts performance. For 16-categories metric, the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">performance of DPL with ResNet101 is slightly worse since</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">the domain shift is much larger in {wall, fence, pole} cat-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">egories, and DPL with VGG16 still surpasses state-of-the-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Per-class accuracy, mean accuracy and selected-pixel ratio comparison among different pseudo label generation strategies.<ref type="bibr" target="#b48">49</ref>.9 98.8 81.4 63.7 68.8 80.0 25.5 96.9 93.5 99.8 95.6 65.69 98.2 93.1 67.5 10.8 91.5 85.4 96.9 64.5 49.1 50.3 DPPLG-Weighted 98.7 67.6 97.5 78.9 58.6 66.4 76.7 40.9 94.4 86.7 99.6 92.8 59.9 96.8 89.9 60.9 46.7 87.7 76.6 94.9 71.0</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mean acc</cell><cell>pixel ratio</cell><cell>mIoU (M (1) S )</cell><cell>mIoU (M (1) T )</cell></row><row><cell>SPPLG</cell><cell cols="21">96.6 62.2 96.6 69.0 48.9 57.6 62.8 35.0 90.3 85.3 99.6 88.3 51.3 96.2 82.3 51.4 44.9 68.3 61.1 92.3 77.0</cell><cell>46.0</cell><cell>50.0</cell></row><row><cell>DPPLG-Max</cell><cell cols="21">97.2 61.6 96.8 68.1 49.2 55.9 62.9 32.9 90.7 85.3 99.6 88.4 52.8 96.3 82.3 52.8 44.3 69.1 62.3 92.7 76.8</cell><cell>49.2</cell><cell>50.6</cell></row><row><cell>DPPLG-Joint</cell><cell cols="22">99.7 49.6</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>Evaluation of the necessity of M T warm-up. 40.7 22.5 34.1 40.7 27.2 82.4 35.6 81.4 59.3 28.9 84.8 31.8 39.7 25.7 35.4 38.2 49.0 ? 92.5 52.8 86.0 38.5 31.7 36.2 47.3 34.9 85.5 39.9 85.2 62.9 33.9 86.8 37.2 45.3 20.1 44.1 42.4 52.8</figDesc><table><row><cell>M T warm-up</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell></cell><cell cols="3">91.5 47.8 83.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Comparison of different network designs.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU (M S ) mIoU (M (1)</cell><cell>(1) T )</cell></row><row><cell>Share-Part</cell><cell>49.3</cell><cell>49.6</cell></row><row><cell>Share-All</cell><cell>48.6</cell><cell>49.2</cell></row><row><cell>Separate (default)</cell><cell>49.6</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Ablation study on weight coefficient ?.</figDesc><table><row><cell>?</cell><cell cols="2">mIoU (M S ) mIoU (M (1) T ) (1)</cell></row><row><cell>0</cell><cell>46.0</cell><cell>49.1</cell></row><row><cell>0.3</cell><cell>47.9</cell><cell>50.1</cell></row><row><cell>0.4</cell><cell>49.2</cell><cell>51.1</cell></row><row><cell>0.5 (default)</cell><cell>49.6</cell><cell>51.8</cell></row><row><cell>0.6</cell><cell>49.4</cell><cell>51.2</cell></row><row><cell>0.7</cell><cell>49.0</cell><cell>50.0</cell></row><row><cell>1</cell><cell>48.2</cell><cell>50.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. More Experiments</head><p>Category-Wise IoU for Ablation Studies. We use mIoU as evaluation metric for ablation studies in our paper. Now we report the detailed category-wise IoU for these ablation studies. <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref> present category-wise IoU of different image translation methods and pseudo label generation strategies, respectively. We can observe that our dual path image translation (DPIT) module and dual path pseudo label generation (DPPLG) strategy achieve best performance on most categories, which is consistent with the reported mIoU results in our paper. To further demonstrate the effectiveness of the proposed label correction strategy used in M T warm-up, <ref type="table">Table 9</ref> reports detailed results. Comparing with strategies without utilizing label correction (i.e., M T w/ Y S and M T w/? S ), almost all categories have a significant improvement by using our proposed label correction strategy. From the table, we also conclude that ? is a less-sensitive hyper parameter.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Qualitative Results</head><p>Visualization of Segmentation Results of DPL. In <ref type="figure">Figure 5</ref>, we visualize the segmentation results of different stages of DPL framework. M Step by step, segmentation results appear smoother with clearer boundaries among different classes. DPL and DPL-Dual further improve the performance on maintaining whole structures of small objectives (e.g., person and rider in row 1,2,3,4,6, sign and pole in row 7,8) and distinguishing similar categories(e.g., sidewalks and road in row 5,6). The stage-wise improvements demonstrate the effectiveness of the proposed label correction strategy, dual path image translation and dual path adaptive segmentation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dunit: Detection-based unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deblina</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Vizier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4787" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02424</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sanghyun Woo and In So Kweon. Twophase pseudo label densification for self-training based domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei Pan Inkyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pixel-level cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10620</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transferable representation learning with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3071" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning from scaleinvariant examples for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naseer</forename><surname>Subhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14449,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09222</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves ImageNet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Label-driven reconstruction for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04614</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised learning tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
