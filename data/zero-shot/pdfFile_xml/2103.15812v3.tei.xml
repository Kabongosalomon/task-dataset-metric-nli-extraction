<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LATENTKEYPOINTGAN: CONTROLLING GANS VIA LATENT KEYPOINTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingzhe</forename><surname>He</surname></persName>
							<email>xingzhe@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
							<email>wandt@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<email>rhodin@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LATENTKEYPOINTGAN: CONTROLLING GANS VIA LATENT KEYPOINTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) have attained photo-realistic quality in image generation. However, how to best control the image content remains an open challenge. We introduce LatentKeypointGAN, a two-stage GAN which is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their parts. A major difficulty that we address with suitable network architectures and training schemes is disentangling the image into spatial and appearance factors without domain knowledge and supervision signals. We demonstrate that LatentKeypointGAN provides an interpretable latent space that can be used to re-arrange the generated images by re-positioning and exchanging keypoint embeddings, such as generating portraits by combining the eyes, and mouth from different images. In addition, the explicit generation of keypoints and matching images enables a new, GAN-based method for unsupervised keypoint detection. 1 arXiv:2103.15812v3 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It is a long-standing goal to build generative models that depict the distribution of example images faithfully. While photo-realism is reached for well-constrained domains, such as faces, it remains challenging to make this image generation process interpretable and editable. Desired is a latent space that disentangles an image into parts and their appearances, which would allow a user to recombine and re-imagine a generated face interactively and artistically. There are promising attempts <ref type="bibr" target="#b29">(Lorenz et al., 2019)</ref> that use spatial image transformations, such as the thin plate spline (TPS) method, to create pairs of real and deformed images and impose an equivariance loss to discover keypoints and object appearance embeddings as the bottleneck of an autoencoder. Thereby, one can edit the image by moving the keypoints or modifying the appearance embedding. However, <ref type="bibr">Li et al. (2020)</ref> points out that the learned keypoints may be biased and only meaningful for the artificially introduced image transformations, the TPS transformation require careful parameter tuning for each <ref type="bibr">domain, and Xu et al. (2020b)</ref> shows that some geometry and appearance entanglement remains.</p><p>To avoid these shortcomings of existing autoencoder-based techniques, we propose an alternative GAN-based approach that does not rely on paired images and TPS transformation, thereby alleviating their domain-specific tuning and bias. Instead of an autoencoder, we use a GAN that includes keypoints as a latent embedding. <ref type="figure">Figure 1</ref> shows how this LatentKeypointGAN enables our main goal of editing the output image by changing the keypoint position, adding or removing points, and exchanging associated appearance embedding locally. Although entirely unsupervised, the learned keypoints meaningfully align with the image landmarks, such as a keypoint linked to the nose when generating images of faces. As a byproduct, we can learn a separate keypoint detector on generated image-keypoint pairs for unsupervised keypoint detection and to quantify localization accuracy.</p><p>The difficulty of such unsupervised learning approaches lies in finding the right implicit bias to facilitate the desired disentanglement <ref type="bibr" target="#b28">Locatello et al. (2019)</ref>. LatentKeypointGAN is designed as a two-stage GAN architecture that is trained end-to-end on the standard GAN objective. In the first step, a generator network turns the input values sampled from a normal distribution into 2D <ref type="figure">Figure 1</ref>: LatentKeypointGAN generates images (first column) with associated keypoints (second column), which enables local editing operations, such as moving, switching, and adding parts. keypoint locations and their associated encoding. We ensure with suitable layer connectivity that some of the encodings are correlated while others remain independent. These generated keypoints are then mapped to spatial heatmaps of increasing resolution. The heatmaps define the position of the keypoints and their support sets the influence range of their respective encodings. In the second step, a SPADE-like <ref type="bibr" target="#b36">(Park et al., 2019)</ref> image generator turns these spatial encodings into a complete and realistic image.</p><p>We summarize our contributions below:</p><p>1. Development of a GAN-based framework for image manipulation and keypoint detection; 2. Design of a keypoint generator that models dependent and independent factors explicitly; 3. Empirical study comparing different editing methods in terms of perceptual quality; 4. A new methodology for keypoint detection that contests established autoencoder methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the following, we discuss variants of deep generative models that learn to synthesize images from a collection of examples, focusing on methods modeling keypoints and those providing control on the image content.</p><p>GANs <ref type="bibr" target="#b17">(Goodfellow et al., 2014)</ref> are trained to generate images from a distribution that resembles the training distributions. Recent approaches attain photo-realism for portrait images. <ref type="bibr">Karras et al. (2018)</ref> train a progressive growth of the GAN's generator and discriminator. After that, <ref type="bibr">Karras et al. (2019;</ref><ref type="bibr" target="#b40">2020)</ref> integrate multiplicative neural network layers to build StyleGAN and StyleGAN2. StyleGAN gains some control on the generated high-resolution, high-quality faces, by adopting ideas from style transfer <ref type="bibr" target="#b15">(Gatys et al., 2016;</ref><ref type="bibr" target="#b23">Huang &amp; Belongie, 2017)</ref> and exchanging features of hidden layers between different samples. We build upon these network architectures and equip them with additional structure.</p><p>Editing GANs globally. More recently, efforts have been made on exploring the latent space of a pre-trained StyleGAN for image editing <ref type="bibr" target="#b41">(Shen et al., 2020;</ref><ref type="bibr">Jahanian* et al., 2020)</ref>. To allow editing real-world images, various encoders <ref type="bibr" target="#b50">(Zhu et al., 2020a;</ref><ref type="bibr" target="#b0">Abdal et al., 2019;</ref><ref type="bibr" target="#b40">2020;</ref><ref type="bibr" target="#b18">Guan et al., 2020;</ref><ref type="bibr">Wulff &amp; Torralba, 2020;</ref><ref type="bibr" target="#b39">Richardson et al., 2020)</ref> have been trained to project images into the latent space of StyleGANs. These methods provide control over the image synthesis process, such as for changing age, pose, and gender. To enable rig-like controls over semantic face parameters that are interpretable in 3D, such as illumination, <ref type="bibr">Tewari et al. (2020b;</ref><ref type="bibr">a)</ref>; <ref type="bibr" target="#b16">Ghosh et al. (2020)</ref>; <ref type="bibr" target="#b12">Deng et al. (2020)</ref> integrate 3D face models <ref type="bibr" target="#b5">(Blanz &amp; Vetter, 1999;</ref><ref type="bibr" target="#b46">Li et al., 2017)</ref> with GANs. Compared with these methods, our model focuses on detailed and local semantic controls. Instead of changing the face as a whole, our method is able to change a local patch without an obvious impact on other regions. Furthermore, our keypoints provide control handles for animation without manual rigging. Therefore, LatentKeypointGAN can be applied to many different objects and image domains.</p><p>Editing GANs locally. Instead of exchanging entire feature maps for style editing, local modifications are possible but require care to maintain consistent and artifact free synthesis. <ref type="bibr" target="#b11">Collins et al. (2020)</ref> propose to cluster the middle-layer feature maps of StyleGAN and to use the embedding to edit local appearance but require human supervision to select suitable clusters. Instead of using existing feature maps, <ref type="bibr" target="#b2">Alharbi &amp; Wonka (2020)</ref> and <ref type="bibr">Kim et al. (2021)</ref> generate spatially-variable maps by structured noise generators and then swap the pixel embedding of the maps to edit the images. However, this requires to draw source and target regions by hand for every instance. By contrast, our model can explicitly change the pose by modifying keypoint locations that are consistent across images. <ref type="bibr">Kwon &amp; Ye (2021)</ref> manipulate attention maps to edit the pose and local shape, but they do not demonstrate local appearance editing. While enabling different forms of editing, all of the methods above require some form of pixel-level selection of regions at test time or manual cluster selection out of hundreds of candidates. In addition, they do not demonstrate on out-of-distribution editing, such as moving parts relative to each other and adding or removing parts, which requires stronger local disentanglement. <ref type="bibr" target="#b45">Zhan et al. (2019)</ref> use a cycle consistency with a spatial transformation network to fuse a foreground object on top of a background reference. They can add a part but only if separate datasets with and without that part are available (e.g., with and without glasses). In sum, our keypoint-based editing is both more intuitive and enables additional editing capabilities.</p><p>Conditional image synthesis methods synthesize images that resemble a given reference input, such as category labels <ref type="bibr" target="#b33">(Mirza &amp; Osindero, 2014;</ref><ref type="bibr" target="#b35">Odena et al., 2017)</ref>, text <ref type="bibr" target="#b46">(Zhang et al., 2017;</ref><ref type="bibr" target="#b37">Reed et al., 2016)</ref>, and layout <ref type="bibr" target="#b49">(Zhao et al., 2019;</ref><ref type="bibr">Sun &amp; Wu, 2019)</ref>. A variant of the conditional GANs can do image-to-image translation <ref type="bibr">(Isola et al., 2017;</ref><ref type="bibr">Liu et al., 2017;</ref><ref type="bibr" target="#b52">Zhu et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr">Wang et al., 2018;</ref><ref type="bibr" target="#b36">Park et al., 2019)</ref>. These approaches aim to transfer images from one domain to another while preserving the image structure, such as mapping from day to night. <ref type="bibr" target="#b36">Park et al. (2019)</ref> pioneered using spatially-adaptive normalization to transfer segmentation masks to images, which we borrow and adapt to be conditioned on landmark position. To control individual aspects of faces, such as changing eye or nose shape, recent works condition on segmentation masks <ref type="bibr">(Lee et al., 2020;</ref><ref type="bibr" target="#b12">Yu et al., 2020;</ref><ref type="bibr" target="#b39">Richardson et al., 2020;</ref><ref type="bibr" target="#b53">Zhu et al., 2020b;</ref><ref type="bibr">Tan et al., 2020)</ref>, rough sketches <ref type="bibr">(Tseng et al., 2020;</ref><ref type="bibr">), landmarks (Yang et al., 2019</ref>, or word labels such as with or w/o glasses <ref type="bibr" target="#b9">(Cheng et al., 2020a)</ref>, of faces as input. <ref type="bibr" target="#b22">Hong et al. (2018)</ref> can superimpose a new object without requiring an exact mask, but still require one at training time. Compared with these methods, our model does not take any kind of supervision or other conditions at training time. It is trained in a totally unsupervised manner. Still, our method allows the landmarks to learn a meaningful location and semantic embedding that can be controlled at test time.</p><p>Unsupervised landmark discovery approaches aim to detect the landmarks from images without supervision. Most works train two-branch autoencoders, where shape and appearance are disentangled by training on pairs of images where one of the two is matching while the other factor varies. The essence of these methods is to compare the pair of images to discover disentangled landmarks. These pairs can stem from different views <ref type="bibr">(Suwajanakorn et al., 2018;</ref><ref type="bibr" target="#b38">Rhodin et al., 2019)</ref>, and frames of the same video <ref type="bibr" target="#b43">(Siarohin et al., 2019;</ref><ref type="bibr">Kulkarni et al., 2019;</ref><ref type="bibr" target="#b32">Minderer et al., 2019)</ref>. However, this additional motion information is not always available or is difficult to capture. Existing unsupervised methods trained on single images create pairs from spatial deformation and color shifts of a source image <ref type="bibr">(Xing et al., 2018;</ref><ref type="bibr" target="#b42">Shu et al., 2018;</ref><ref type="bibr">Thewlis et al., 2019;</ref><ref type="bibr">Li et al., 2020;</ref><ref type="bibr" target="#b10">Cheng et al., 2020b;</ref><ref type="bibr" target="#b13">Dundar et al., 2020)</ref>. However, the parameters of augmentation strategies, such as the commonly used thin plate spline deformation model <ref type="bibr">(Thewlis et al., 2017;</ref><ref type="bibr" target="#b47">Zhang et al., 2018;</ref><ref type="bibr">Jakab et al., 2018;</ref><ref type="bibr" target="#b29">Lorenz et al., 2019)</ref>, are difficult to calibrate and the learned keypoints may be biased to be only meaningful for the transformation <ref type="bibr">(Li et al., 2020)</ref>. Besides, the appearance information may be hidden in keypoint location <ref type="bibr">(Xu et al., 2020b)</ref>. The cycle consistency-based methods <ref type="bibr">(Wu et al., 2019;</ref><ref type="bibr">Xu et al., 2020b)</ref> cannot provide local editing ability. We show that our generator can well disentangle the appearance and keypoint location and control the images locally. Furthermore, the underlying idea of these methods, no matter whether they depend on multi-views, videos, or thin plate splines, is creating pairs of images to compare to find the clue of equivariant keypoints in comparison. On the other hand, our method does not need any paired correspondences. It is totally based on generation. Our GAN approach poses a viable alternative by training a keypoint detector on synthetic examples generated by LatentKeypointGAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LATENTKEYPOINTGAN METHOD AND METHODOLOGY</head><p>The unsupervised spatial disentanglement of image content that we target has been shown to be impossible without an implicit bias imposed by the network architecture <ref type="bibr" target="#b28">(Locatello et al., 2019)</ref>. To this end, our proposed LatentKeypointGAN architecture encodes position explicitly and appearance locally with two carefully designed sub-networks. First, the keypoint generator, K, defines the spatial arrangement of parts and their embedding. Subsequently, a spatial embedding layer, S, turns these sparse estimates into dense feature maps, and the image generator G up-scales these into a highresolution image. <ref type="figure">Figure 2</ref> shows the entire architecture. In the following, we explain architectural details and how we can train these networks end-to-end on a standard GAN objective, without any supervision on keypoints or masks. At inference time, the latent keypoints allow one to author the keypoint location and appearance interactively. Moreover, generated keypoint-image pairs can be used to train an independent keypoint detector E, which enables unsupervised keypoint localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KEYPOINT GENERATOR</head><p>The keypoint generator, K, learns the embeddings and spatial arrangement of image parts, such as eyes, nose, and mouth for describing a portrait image. It takes three Gaussian noise vectors as input z kp pose , z kp app , z bg emb ? N (0 Dnoise , I Dnoise?Dnoise ), where D noise is the dimension. Each vector is passed through a three-layer MLP to respectively generate the K keypoint coordinates k j ? [?1, 1] 2 , j = 1, ..., K, a global style vector w global ? R Dembed , and a background embedding w bg . Here K is a pre-defined hyperparameter. Special is our keypoint embedding w j that combines local and global factors,</p><formula xml:id="formula_0">w j = w global ? w j const ,<label>(1)</label></formula><p>with the elementwise product ?. The constant embedding w j const ? R Dembed is designed to encode the keypoint semantics, e.g., left or right eye. They are updated during the training but fixed during testing. The global style vector w global ? R Dembed can be regarded as a noise vector with learned distribution; it ensures that a different appearance is drawn for every generated image. We show in Appendix F that this factorization into dependent and independent factors is crucial and improves on straightforward variants that directly use an MLP. For instance, it facilitates learning the correlation of the two eyes of a face while still covering the wide variety of eye variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SPATIAL EMBEDDING LAYER</head><p>With keypoint coordinates and embeddings generated, we now turn these point-wise estimates into discrete style maps that are further processed by the convolutional generator. The style map S j ? R Dembed?H?W for each keypoint j is generated by broadcasting keypoint embedding w j ? R Dembed at each pixel of the heatmap H j ? R H?W and rescaling w j by the heatmap value H j (p),</p><formula xml:id="formula_1">S j (p) = H j (p)w j , where H j (p) = exp ? p ? k j 2 2 /?<label>(2)</label></formula><p>has Gaussian shape, is centered at the keypoint location k j , and ? controls the influence range. We also define a heatmap H bg for the background as the negative of all keypoint maps, H bg (p) = 1 ? max j {H j (p)} K j=1 . The background heatmap is multiplied with the independent noise vector w bg generated from z bg emb instead of keypoint embedding, but treated equally otherwise. Then we concatenate all K keypoint style maps {S j } K j=1 and the background style map S bg in the channel dimension to obtain the style map S ? R Dembed?H?W ?K+1 . How these style maps are used as conditional variables at different levels of the image generator is explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IMAGE GENERATOR</head><p>The image generator G follows the progressively growing architecture of <ref type="bibr">StyleGAN (Karras et al., 2019)</ref>, combined with the idea of spatial normalization from SPADE <ref type="bibr" target="#b36">(Park et al., 2019)</ref>, which was designed to generate images conditioned on segmentation masks. Please refer to Appendix E for additional details. In the following, we explain how we replace the role of manually annotated segmentation masks in the original with learned keypoints. Our generator starts from a learned 4 ? 4 ? 512 constant matrix and keeps applying convolutions and upsampling to obtain feature maps of increasing resolution. Following SPADE <ref type="bibr" target="#b36">(Park et al., 2019)</ref>, the original BatchNorm <ref type="bibr" target="#b25">(Ioffe &amp; Szegedy, 2015)</ref> layers are replaced with spatial adaptive normalization to control the content. By contrast to SPADE, we do not condition on annotated segmentation masks but instead on the learned feature maps introduced in Section 3.1, and we do not use the residual links <ref type="bibr" target="#b20">(He et al., 2016)</ref> because we found it detrimental in combination with progressive training. The generator increases resolution layer-by-layer with multiple adaptive normalization layers, requiring differently-sized feature maps. To create feature maps that match the respective spatial resolution of the generator, we apply Equation 2 multiple times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LOSS FUNCTIONS</head><p>Adversarial losses. We found it crucial for LatentKeypointGAN to use the non-saturating loss <ref type="bibr" target="#b17">(Goodfellow et al., 2014)</ref>,</p><formula xml:id="formula_2">L(G) GAN = E z?N log(exp(?D(G(z))) + 1)<label>(3)</label></formula><p>for the generator, and logistic loss,</p><formula xml:id="formula_3">L(D) GAN = E z?N log(exp(D(G(z))) + 1) + E x?pdata log(exp(?D(x)) + 1)<label>(4)</label></formula><p>for the discriminator, with gradient penalty <ref type="bibr" target="#b31">(Mescheder et al., 2018)</ref> applied only on real data,</p><formula xml:id="formula_4">L(D) gp = E x?pdata ?D(x).<label>(5)</label></formula><p>Other losses such as hinge loss <ref type="bibr" target="#b36">Park et al. (2019)</ref> failed.</p><p>Background loss. To further disentangle the background and the keypoints, and stabilize the keypoint location, we introduce a background penalty,</p><formula xml:id="formula_5">L(G) bg = E z1,z2 [(1 ? H bg 1 ) ? G(z 1 ) ? (1 ? H bg 2 ) ? G(z 2 )],<label>(6)</label></formula><p>where z 1 and z 2 share the same keypoint location and appearance input noise, and only differ at the background noise input. The H 1 and H 2 are the background heatmaps generated by z 1 and z 2 . With this penalty, we expect the keypoint location and appearance do not change with the background. The final loss for the discriminator and the generator are, respectively,</p><formula xml:id="formula_6">L(D) = L(D) GAN + ? gp L(D) gp , and L(G) = L(G) GAN + ? bg L(G) bg .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the improved quality and editing operations that our unsupervised LatentKeypointGAN approach for learning disentangled representations brings about. We show the types of edits in Section 4.3 and quantitatively and qualitatively evaluate our improved editing quality compared with existing unsupervised and supervised methods in Section 4.4 and appendix, respectively. To test the keypoint consistency, that keypoints stick to a particular object part, we compare to existing The first column is the source and the last the target. The images in-between are the result of the following operations. First row: pushing the eye keypoint distance from 0.8x to 1.2x. Note that the marked eye keypoints in this row are slightly shifted upward for better visualization. Second row: interpolating the hair keypoint to move the fringe from right to left. Third row: scaling the keypoint location and, therefore, the face from 1.15x to 0.85x. Fourth row: interpolating all keypoint locations, to rotate the head to the target orientation.</p><p>autoencoder frameworks for unsupervised landmark detection in Section 4.7. Even though not our core focus, we show that by further tailoring the proposed architecture for precise localization, this new GAN method reaches state-of-the-art accuracy on a keypoint detection benchmark.</p><p>Since our main goal of enhanced editing quality is hard to quantify, we also conducted a user study to further validate these improvements. The appendix contains exhaustive ablation tests on network architecture, hyperparameters, and training strategies to quantify the contributions of each individual component of our approach. The supplemental material contains additional video results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>CelebA <ref type="bibr" target="#b27">(Liu et al., 2015)</ref> contains 200k celebrity faces. We use this dataset to test our model's ability to discover the keypoints unsupervised. Following Thewlis et al. <ref type="formula" target="#formula_0">(2017)</ref>, we use three subsets of this dataset: CelebA training set without MAFL (160k images), MAFL training set (19k images), MAFL test set (1k images). MAFL <ref type="bibr" target="#b48">(Zhang et al., 2014</ref>) is a subset of CelebA. More details can be found in Section 4.7.</p><p>CelebA-HQ <ref type="bibr" target="#b27">(Liu et al., 2015)</ref> contains 30k celebrity face images selected from CelebA. Following <ref type="bibr" target="#b53">Zhu et al. (2020b)</ref>, we use the first 28k images as training set and the rest 2k images as testing set.</p><p>FlickrFaces-HQ (FFHQ) (Karras et al., 2019) consists of 70k high-quality portrait images, with more variation than CelebA <ref type="bibr" target="#b27">(Liu et al., 2015)</ref>. Therefore, we use this dataset to test our model's ability to disentangle the local representations of images.</p><p>BBC Pose <ref type="bibr" target="#b7">(Charles et al., 2013)</ref> consists of 20 videos of different sign-language signers with various backgrounds. We use this dataset to test our model's ability to edit human appearance.</p><p>LSUN bedroom <ref type="bibr">(Yu et al., 2015)</ref> consists of more than 3 million images of bedrooms, with very diverse spatial structure. We use this dataset to test our model's generalization ability to edit entire indoor scenes.</p><p>Hyperparameters. We use 512 ? 512 images for face editing on FFHQ, 256 ? 256 for FID calculation, comparison with SEAN <ref type="formula">(</ref> vised keypoint based methods on CelebA. Unless specified otherwise, we set ? = 0.01 and use 10 keypoints. For editing experiments on FFHQ and detection experiments on CelebA, we augment by randomly cropping the training images to a size of 70-100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INTERACTIVE EDITING</head><p>We show the capabilities of LatentKeypointGAN by changing the keypoint locations and exchanging the keypoint embeddings between different images. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, we can thereby edit the face direction, face size, and individual key parts by changing the keypoint locations. If only a subset of the keypoint embeddings is changed, the other parts are not significantly affected. <ref type="figure" target="#fig_1">Figure 5</ref> shows a heatmap of the area of interest. Since the GAN learns to generate a coherent face from its parts, global effects remain mostly unchanged, for instance, hairstyle and color. We discuss in Section 5 to which degree this is desired and what limitations remain.</p><p>Additional ablation studies, editing, and interpolation examples are shown in Appendix C, Appendix F, and the supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISENTANGLED REPRESENTATIONS</head><p>We first analyze how background and keypoint embeddings are disentangled and used for editing portrait images. Further examples are analyzed in the user study.</p><p>Disentangled keypoint embeddings. <ref type="figure" target="#fig_1">Figure 5</ref> shows editing operations of independent facial regions. We fix the background noise, z bg emb , and change some of the keypoint embeddings. This enables exchanging of eyes, mouth, or nose between persons. <ref type="figure" target="#fig_1">Figure 5</ref> includes heatmaps that visualize the difference between original and interpolated images. Their local activation highlights the spatial disentanglement of individual keypoint features.</p><p>Disentangled background. <ref type="figure" target="#fig_2">Figure 4</ref> shows a faithful change of backgrounds while keeping the face fixed. To this end, we fix the keypoint noise z kp pose , z kp app , and change only the background noise input, z bg emb . The local change in the three diverse examples shows that the background and keypoint encodings are disentangled well. The illumination and hair color is learned to be part of the background, which makes sense as a global feature cannot be attributed to individual keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Conditioned on FID score ? Pix2PixHD <ref type="bibr">(Wang et al., 2018)</ref> segmentation masks 23.69 SPADE <ref type="bibr" target="#b36">(Park et al., 2019)</ref> segmentation masks 22.43 SEAN <ref type="bibr" target="#b53">(Zhu et al., 2020b)</ref> segmentation  <ref type="table">Table 1</ref>: Image quality with respect to supervision type on CelebA-HQ. Our FID scores improve on mask-based solutions while providing similar editing capabilities and being unsupervised. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IMAGE QUALITY</head><p>We test our image quality on CelebA-HQ and compare it with an unsupervised autoencoder and segmentation mask-conditioned GANs, which require labeled training images. This comparison is biased in that the mask-conditioned GANs use the masks from the test set as input, while ours is unconditional. To make an as fair as possible comparison, we add a version that uses a selfsupervised keypoint detector (see Sec. 4.7) to condition the latent keypoint locations on the same test images as the baselines do. Scores are computed by using the Pytorch FID calculator <ref type="bibr" target="#b40">(Seitzer, 2020)</ref>. We followed <ref type="bibr" target="#b53">Zhu et al. (2020b)</ref> to use the first 28k images as the training set and the last 2k images as the test set. We list the results in <ref type="table">Table 1</ref>. Our approach attains better FID scores than those GANs conditioning on segmentation masks. It gets close to SEAN <ref type="bibr" target="#b53">(Zhu et al., 2020b)</ref>, which has an auto-encoder architecture and their reported FID is on reconstructed images instead of randomly generated appearances. In addition, the user study shows that on editing tasks, our method also exceeds the image editing quality of SEAN significantly (preferred by 94.78% of all responses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">GENERALIZATION TO OTHER DATASETS</head><p>LSUN Bedroom and BBC Pose. In <ref type="figure" target="#fig_3">Figure 6</ref>, we explore the editing ability of entire scenes on the LSUN bedroom dataset. No previous unsupervised part-based model has tried this difficult task before. We successfully interpolate the local appearance by changing the corresponding keypoint embeddings and translating the local key parts (window, bed) by moving the corresponding keypoints. We can also remove or add some parts by removing or adding peaks in the Gaussian Heatmaps. Since our learned representation is two-dimensional, it is not possible to rotate objects entirely. <ref type="figure" target="#fig_4">Figure 7</ref> explores the editing of individuals. Although artifacts remain due to the detailed background and motion blur in the datasets, pose and appearance can be exchanged separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ABLATION TESTS</head><p>We test different variants of our architecture and demonstrate the effectiveness and necessity of our design. We test 1) removing the background; 2) removing the global style vector; 3) using additive global style vector instead of multiplicative ones; 4) using contrastive learned keypoint embeddings instead of multiplicative ones; 5) removing the keypoint embedding; 6) removing keypoints. We show the detection result on MAFL and FID score on FFHQ in   <ref type="formula" target="#formula_0">(2019)</ref>). We use the same hyperparameters as before, showing the strong robustness to new settings. Thereby, although not our main goal, LatentKeypointGAN also provides an alternative methodology for unsupervised keypoint/landmark detection that avoids paired training and TPS deformation, which are notoriously difficult to tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">USER STUDY ON EDITING QUALITY</head><p>We conducted a user study to separately access image quality after editing, comparing against the unsupervised keypoint-based method <ref type="bibr" target="#b47">(Zhang et al., 2018)</ref> on CelebA <ref type="bibr" target="#b27">(Liu et al., 2015)</ref> and the maskconditioned method <ref type="bibr" target="#b53">(Zhu et al., 2020b)</ref> on CelebA-HQ <ref type="bibr" target="#b27">(Liu et al., 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS AND FUTURE WORK</head><p>For portrait images, the hair can mix with the background encoding, such that it is considered part of the background. Still, the hair can be changed by selecting a background embedding with the desired hairstyle. Moreover, the disentanglement into locally encoded features can lead to asymmetric faces, such as a pair of glasses with differently styled sides. For BBC Pose, the keypoints are not well localized. They are consistent across images with the same pose, which permits pose transfer, but are too inconsistent for keypoint detection. Limitations could be overcome by linking keypoints hierarchically with a skeleton and by reasoning about masks on top of keypoints. While the face orientation in portrait images can be controlled, we found that orientation changes on the bedroom images are not reliable. The orientation is mostly baked into the appearance encoding. We believe that it will be necessary to learn a 3D representation and datasets with less-biased viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present a GAN-based framework that is internally conditioned on keypoints and their appearance encoding, providing an interpretable hidden space that enables intuitive editing. We provide the idea and implementation to avoid issues with TPS and related deformation models that are common in existing autoencoders. This LatentKeypointGAN also facilitates the generation of image-keypoint pairs, thereby providing a new methodology for unsupervised keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ETHICS STATEMENT</head><p>This research provides a new unsupervised disentanglement and editing method. By contrast to existing supervised ones, e.g., those requiring manually annotated masks, ours can be trained on any image collection. This enables training on very diverse sets as well as on personalized models for a particular population and can thereby counter biases in the annotated datasets.</p><p>Photo-realistic editing tools have the risk of abuse via deep fakes. A picture of a real person could be altered to express something not intended by the person. In that regard, our method has the advantage of only enabling the editing of generated images; it does not enable modifying real images; it only works in the synthetic to real direction. However, future work could extend it with an appearance encoder, which bears some risk.</p><p>Another risk is that the network could memorize appearances from the training set and therefore reproduce unwanted deformations of the pictured subjects. While <ref type="bibr" target="#b6">Brock et al. (2018)</ref> and Karras et al. Our IRB approved the survey and we collected explicit and informed consent at the beginning of the study. Results are stored on secure servers. This research did not cause any harm to any subject.</p><p>On the technical side, we disclose all limitations in both the main paper and the supplementary. This research does not have conflicts with others. We credit the previous works in the Introduction (Section 1) and Related Work (Section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">REPRODUCIBILITY</head><p>We not only described our algorithm in Section 3 but also provide detailed information of our architecture, training strategy, and hyperparameters in the supplementary E. All the datasets we used are publicly available and the pre-processing is also described in Section 4. Furthermore, we provide the source code to facilitate future work in this direction of GAN-based generative image editing. In this appendix, we present additional details on the neural network architectures, the progressive training, and hyperparameters. Furthermore, we show more qualitative results in the supplemental videos that are embedded and explained in the project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>A KEYPOINTS ILLUSTRATION <ref type="figure" target="#fig_5">Figure 8</ref> shows the keypoints generated on all datasets. The keypoints are semantically meaningful and very consistent across different instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TUNING LATENTKEYPOINTGAN FOR KEYPOINTS DETECTION</head><p>We desire an architecture that encodes the location of parts solely in the keypoint locations to improve keypoint localization and the subsequent learning of a detector. Even though the convolutional generator is translation invariant, additional care is necessary to prevent leakage of global position at the image boundary and from the starting tensor. All these tuning steps are explained below. . By maintaining a fixed margin around the feature map and cropping after each upsampling, we effectively prevent the leaking of absolute position and the bias to the center because none of the generated pixels ever reaches a boundary condition. We use a 10-pixel margin. Note that such measures do not directly apply to autoencoders who are bound to the fixed resolution of the input.</p><p>Positional Encoded Starting Tensor. We remove the 4 ? 4 ? 512 starting tensor because it can encode absolute grid position. We replace it with the positional encoding M of difference between keypoints k 1 , ..., k K and the grid positions p,</p><formula xml:id="formula_8">M(p) = [sin(? * Linear([p ? k 1 , ..., p ? k K ])), cos(? * Linear([p ? k 1 , ..., p ? k K ]))]. (8)</formula><p>The underlying idea is to only encode relative distances to keypoints but not to the image boundary.</p><p>Larger Starting Tensor. We found that starting the feature map from 32 ? 32 instead of 4 ? 4 improves keypoint localization accuracy.</p><p>Background Handling. The background should be consistent across the whole image, but complete occlusion is inaccurate to model with the sparse keypoints and their limited support in the intermediate feature maps. Hence, we introduce the explicit notion of a foreground mask that blends in the background. The mask is generated as one additional layer in the image generator. To generate the background, we use a separate network that is of equivalent architecture to LatentKeypointGANtuned. Note that in background generation we use AdaIN <ref type="bibr" target="#b23">(Huang &amp; Belongie, 2017)</ref> instead of SPADE because there is no spatial sensitive representation, such as keypoints. Foreground and background are then blended linearly based on the foreground mask. We use an 1 ? 1 convolution layer to generate the final RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Aligned <ref type="formula">(</ref>  <ref type="table">Table 3</ref>: Landmark detection on CelebA (lower is better). The metric is the landmark regression (without bias) error in terms of mean L 2 distance normalized by inter-ocular distance. The bottom four rows shows our improvement step by step. We use the same number of keypoints as previous methods.</p><p>Simplification In addition, we remove the progressive training <ref type="bibr">(Karras et al., 2018)</ref> and use the ResBlocks as defined by <ref type="bibr" target="#b36">Park et al. (2019)</ref>. This step is for simplification (quicker training results on medium-sized images) and does not provide a measurable change in keypoint accuracy.</p><p>As shown in <ref type="table">Table 3</ref>, each step of improvement contributes significantly to the keypoint accuracy and consistent improvement on prior works on the two in-the-wild settings. Please see the main paper for the complete comparison. The FID of LatentKeypointGAN-tuned on CelebA of resolution 128 ? 128 is 18.34.</p><p>C IMAGE EDITING QUALITY COMPARISON <ref type="figure">Figure 9</ref> validates the improved editing quality, showing comparative quality to conditional GAN (supervised by paired masks), and superior quality to unsupervised methods (greater detail in the hair and facial features). Note that for SEAN <ref type="bibr" target="#b53">(Zhu et al., 2020b)</ref> we use the edited images (combine mask with different face appearance) instead of reconstructed images (combine mask with the corresponding face appearance) for fair comparison with other GAN methods. The qualitative improvement is further estimated in the subsequent user study.</p><p>The main paper reports FID scores for all those datasets where prior work does. To aid future comparisons we also report the remaining FID scores: 17.88 on FFHQ, 18.25 on CelebA, 30.53 on BBC Pose, and 18.89 on LSUN Bedroom. The FID is calculated by the generated 50k images and the resized original dataset.</p><p>We also qualitatively compare our method with a publicly available version 1 of <ref type="bibr" target="#b29">Lorenz et al. (2019)</ref> on LSUN Bedroom, using the standard parameters that work well on other datasets. As shown in <ref type="figure" target="#fig_7">Figure 10</ref>, their model generates trivial keypoints and fails to reconstruct the bedroom images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EDITING QUALITY SURVEY</head><p>We designed comparisons in 4 different aspects to demonstrate our editing quality. We compare to two methods, one unsupervised keypoint-based method <ref type="bibr" target="#b47">(Zhang et al., 2018)</ref> on CelebA <ref type="bibr" target="#b27">(Liu et al., 2015)</ref>, and one mask-conditioned method <ref type="bibr" target="#b53">(Zhu et al., 2020b)</ref> on CelebA-HQ <ref type="bibr" target="#b27">(Liu et al., 2015)</ref>. For both methods, we follow their experiment design in their papers to make a fair comparison. For each question, we ask the participants to chose from 3 options: 1) ours is better; 2) theirs is better; 3) both methods have equal quality. The order of the pair to compare (ours in the first or the second) in the questions is randomly generated by the function numpy.random.choice of the package Numpy in Python with random seed 1. The results are illustrated in  <ref type="figure">Figure 9</ref>: Image editing quality comparison. We compare the image editing quality with both, supervised (left) and unsupervised (right). LatentKeypointGAN improves on the methods in both classes. Study details. We invited 23 participants answering 35 questions in total, with 5 for each of the categories above and an additional 5 comparing to randomly selected real images from the celebA training set. The generated images are selected as follows. We generate 32 images with LatentKey-pointGAN. For the baselines we take the corresponding images from their paper (as implementations are not available). We then find our image (out of the 32) that best match the baseline images (in gender, pose, head size). If multiple images are similar, we use the first one.</p><p>Editing image quality. We edited the images by swapping the appearance embedding between different images. In each question, we show one image of ours and one image of theirs. We ask the participants to compare the resulting image quality.</p><p>Part disentanglement. To compare with <ref type="bibr" target="#b47">Zhang et al. (2018)</ref>, we moved part of the keypoints, as they did in their paper. To compare with <ref type="bibr" target="#b53">Zhu et al. (2020b)</ref>, we exchange the part embedding between different images, as they did in their paper. In each question, we show one pair of images of ours and one pair of images of theirs. We ask the participants to choose the one with better spatial disentanglement regardless of the image quality.</p><p>Identity preservation while changing expression. We compare identity preservation with <ref type="bibr" target="#b47">Zhang et al. (2018)</ref>. Following their paper, we change part (or all) of the keypoints to change the expression of the face. In each question, we show the two pairs of images. Each pair contains two images, one original image, and one edited image. We ask the participants to choose the pair that can better preserve the identity of the face regardless of the image quality, as quality is assessed separately.</p><p>Shape preservation while changing appearance. We compare the shape preservation with <ref type="bibr" target="#b53">Zhu et al. (2020b)</ref>. We edited the images by swapping the appearance embedding between different images. In each question, we show the two triplets of images. Each triplet contains three images, one shape source image and one appearance image, and one combined image. We ask the participants to choose the triplet where the combined image has the more similar shape as the shape source image regardless of the image quality.</p><p>Interpretation -comparative. This study confirms the findings of the main paper. Our method outperforms <ref type="bibr">Zhang et al. in all metrics in</ref>   <ref type="table" target="#tab_8">Table 4</ref>: Survey results. We compare 4 different aspects with other methods. The first one is the editing image quality. The second one is part disentanglement. The third one is identity preservation while changing expression. The last one is shape preservation while changing appearance.</p><p>quality. This confirms our claims of superior editing capability but may be surprising on the first glance since they attain a higher quality (better FID score) on unedited images. However, it can be explained with the limited editing capabilities of the mask-based approaches discussed next.</p><p>Participants give SEAN a higher shape preservation quality (47% in favour and 19% equal), which is expected since it conditions on pixel-accurate segmentation masks that explicitly encode the feature outline. However, the masks have the drawback that they dictate the part outline strictly, which leads to inconsistencies when exchanging appearance features across images. For instance, the strain direction of the hair and their outline must be correlated. This explains why our mask-free method attaining significantly higher image quality after editing operations (95% preferred ours). Hence, the preferred method depends on the use case. E.g., for the fine editing of outlines SEAN would be preferred while ours is better at combining appearances from different faces.</p><p>An additional strong outcome is that our unsupervised approach has equal disentanglement scores compared to SEAN; 50% judge them equal, with 29% giving preference to ours and only 22% giving preference to SEAN. Validating that LatentKeypointGAN enables localized editing.</p><p>Interpretation -realism. When comparing our GAN (without modifying keypoint location or appearance) to real images at resolution 128 ? 128 of the training set, 42% rate them as equal. Surprisingly 33% even prefer ours over the ground truth. This preference may be because the ground truth images have artifacts in the background due to the forced alignment that are smoothed out in any of the generated ones. Overall, these scores validate that the generated images come close to real images, even though minor artifacts remain at high resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EXPERIMENTS DETAILS</head><p>SPADE <ref type="bibr" target="#b36">(Park et al., 2019)</ref> As shown in <ref type="figure" target="#fig_8">Figure 11</ref>, SPADE takes two inputs, feature map and style map, and use the style map to calculate the mean and standard deviation, which is used to denormalize the batch normalized feature map. Formally speaking, let F i ? R N ?Ci?Hi?Wi be a i-th feature map in the network for a batch of N samples, where C i is the number of channels.</p><p>Here we slightly abuse the notation to denote N batched style maps of size (H i , W i ) as S i ? R N ?(K+1)Dembed?Hi?Wi . The same equation as for BatchNorm <ref type="bibr" target="#b25">(Ioffe &amp; Szegedy, 2015)</ref> is used to normalize the feature map, but now the denormalization coefficients stem from the conditional map, which in our case is the processed style map. Specifically, the resulting value of the spatial adaptive normalization is</p><formula xml:id="formula_9">A i n,c,y,x (S, F) = ? i c,y,x (S i n ) F i n,c,y,x ? ? i c ? i c + ? i c,y,x (S i n ),<label>(9)</label></formula><p>where n ? {1, ..., N } is the index of the sample, c ? {1, ..., C} is the index of channels of the feature map, and (y, x) is the pixel index. The ? i c and ? i c are the mean and standard deviation of channel c. The ? i c,y,x (S i n ) and ? i c,y,x (S i n ) are the parameters to denormalize the feature map. They are obtained by applying two convolution layers on the style map S i n .</p><p>Learning rate and initialization. We set the learning rate to 0.0001 and 0.0004 for the generator and discriminators, respectively. To let our model learn the coordinates reliably, we first set the learning rate of the MLP, which generates keypoint coordinates to 0.05x the generator learning rate,  i.e. 0.00005. We use Kaiming initialization <ref type="bibr" target="#b19">(He et al., 2015)</ref> for our network. We initialize the weights of the last layer of the MLP that generates the keypoint coordinates to 0.05x the Kaiming initialization.</p><p>Progressive Growing Training we adopt progressive growing training <ref type="bibr">(Karras et al., 2018)</ref> to allow larger batch size, which helps both on keypoint localization and local appearance learning. This is likely due to the BatchNorm that is essential in SPADE. We also tried to replace the BatchNorm in SPADE with LayerNorm <ref type="bibr" target="#b4">(Ba et al., 2016)</ref> and <ref type="bibr">PixelNorm (Karras et al., 2019)</ref>, but both of them cause mode collapse. We use a scheduled learning rate for the Keypoint Generator K. As illustrated in <ref type="figure" target="#fig_10">Figure 12</ref>, at each resolution stage, the training is divided into adapting period and non-adapting period. We set the learning rate of K to zero in the adapting period and back to normal in the nonadapting period. In the adapting period, the training follows <ref type="bibr">PGGAN (Karras et al., 2018)</ref> where the feature map is a linear combination of the larger resolution RGB image and current resolution RGB image. The coefficient ? gradually increases from 0 to 1. At the end of the adapting period, the network is fully adapted to generate higher resolution images. In the non-adapting period, the network generates high-resolution images without the linear combination. Following <ref type="bibr">StyleGAN (Karras et al., 2019)</ref>, we start from a 4 ? 4 ? 512 learned constant matrix, which is optimized during training and fixed during testing. We use the keypoint-based ConvBlock 2 and bilinear upsampling to obtain feature maps with increasing resolutions. Unlike <ref type="bibr">PGGAN (Karras et al., 2018)</ref> and <ref type="bibr">Style-GAN (Karras et al., 2019)</ref>, who generating RGB images from feature maps of all resolutions (from 4 ? 4 to 1024 ? 1024), we start generating RGB images from the feature maps of at least 64 ? 64 resolution. This is possible with the keypint generator and its spatially localized embeddings taking over the role of low feature maps. It helps to locate the keypoints more accurately. Adapting period Non-adapting period <ref type="bibr">Conv(4x4,</ref><ref type="bibr">s=2)</ref>, Conv(3x3, s=1), <ref type="bibr">(64,</ref><ref type="bibr">256,</ref><ref type="bibr">256)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(128,</ref><ref type="bibr">128,</ref><ref type="bibr">128)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(256,</ref><ref type="bibr">64,</ref><ref type="bibr">64)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(512,</ref><ref type="bibr">32,</ref><ref type="bibr">32)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(512,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(512,</ref><ref type="bibr">8,</ref><ref type="bibr">8)</ref> Conv(4x4, s=2), Conv(3x3, s=1), <ref type="bibr">(512,</ref><ref type="bibr">4,</ref><ref type="bibr">4)</ref> Conv <ref type="formula">(</ref>  <ref type="bibr">(512,</ref><ref type="bibr">4,</ref><ref type="bibr">4)</ref> means the output feature map has a resolution of 4 ? 4 and the channel size is 512. The toRGB blocks are 1?1 convolutions to generate the RGB images with the same resolution as corresponding feature maps. (Middle) LatentKeypointGAN discriminator. The number in the last parenthesis is the output dimension. For example, <ref type="bibr">(512,</ref><ref type="bibr">4,</ref><ref type="bibr">4)</ref> means the output feature map has a resolution of 4?4 and the channel size is 512. At each resolution, we apply two convolutions, one with stride 2 to downsample feature maps and one with stride 1 to extract features. Leaky ReLU <ref type="bibr" target="#b30">(Maas et al., 2013)</ref> is used after all convolutions except the linear layer in the last.. (Right) Progressive Training.. The adapting period is the same as <ref type="bibr">PGGAN (Karras et al., 2018)</ref> and <ref type="bibr">StyleGAN (Karras et al., 2019)</ref>. In the non-adapting period, we do not use the linear combination.  <ref type="table">Table 5</ref>: Setting for different datasets. For the Bedroom dataset, we do not use the background module and loss. For the BBC Pose dataset, we use ? = 0.025.</p><p>We lists the different ? s and different background setting for all experiments in <ref type="table">Table 5</ref>. In CelebA-HQ and FFHQ, the foreground is naturally disentangled from the background. The face can be freely moved on the image. However, in the Bedroom dataset, all objects and their parts are strongly correlated. For example, the bed cannot be moved to the ceiling, and the window cannot be moved to the floor. Therefore, we treat every object in the bedroom as a key part, even the floor, but the possible motion is restricted to plausible locations (see the supplementary video). A separate background embedding does not make sense. Therefore, we set the background (H bg = 0) and the background loss ? bg = 0 for the experiments on the Bedroom dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ABLATION TESTS F.1 ABLATION TEST ON THE NEURAL NETWORK ARCHITECTURE</head><p>We show the detection result on MAFL and FID score on FFHQ in <ref type="table" target="#tab_2">Table 2</ref> for different architectures we tested below. The detection follows Section 4.7. The FID is calculated at resolution 256 ? 256, by generated 50k images and the resized original dataset. Note that it is different from the main paper for simplicity since the goal of this part is ablation tests instead of comparing with others.</p><p>Removing background embedding. We remove the background embeddings from our architecture (z bg emb and w bg in <ref type="figure">Figure 2</ref>). In this case, the keypoint embedding controls the whole appear-ance of the image. In addition, as shown in <ref type="figure" target="#fig_2">Figure 14</ref>, the keypoints are not exactly located at the key parts, though they are still consistent among different views.</p><p>Removing global style vector. We remove the global style vector w global . Therefore, all the keypoint embeddings are constant. Only keypoint location and background embedding are different among the images. In this case, the keypoint embedding works equivalent to one-hot encoding, and cannot fully capture the variations on the key parts. Therefore, it leads to inaccurate keypoints, as shown in <ref type="figure" target="#fig_2">Figure 14</ref>. Furthermore, we observed that without w global , the network hides the appearance information in the keypoint location.</p><p>Changing keypoint embedding generation. We change the keypoint embedding generation in two ways. The first way is generating constant embedding w j const and global style vector w global just as before and then add them elementwisely instead of multiplying them. Formally speaking, for each keypoint j, its corresponding embedding is</p><formula xml:id="formula_10">w j = w global ? w j const ,<label>(10)</label></formula><p>where ? means elementwise addition. This gives slightly higher detection accuracy but lower image quality. We observe that in this case, the background controls the foreground appearance. However, different from Removing global style vector F.1, the appearance information is not hidden in keypoint locations. We believe this is because that w global works as noise to avoid the network from hiding foreground appearance information in keypoint location. As a result of good disentanglement of appearance and keypoint location, the keypoint detection accuracy slightly increases. However, again, in this setting, the keypoint embedding cannot fully capture the variations of the key parts. Therefore, the background takes the control of appearance.</p><p>The second way is to generate [w j ] K j=1 together from z kp app using MLP. In this case, there is no constant embeddings or global style vector. To force the embedding of the same keypoint to be similar, and the embedding for different keypoints to be different, we use Supervised Contrastive Losses <ref type="bibr">(Khosla et al., 2020)</ref>,</p><formula xml:id="formula_11">L contrastive (G) = ? j?J 1 |K(j)| k?K(j) log exp(w j ? w k /T ) a?A(j) exp(w j ? w a /T ) ,<label>(11)</label></formula><p>where A(j) = {i : w i , w j are in the same batch} K(j) = {i : w i , w j belong to the same keypoint in the same batch} J = {indices of all keypoint embeddings in the same batch} As shown in <ref type="figure" target="#fig_2">Figure 14</ref>, the keypoints are neither on the key parts nor consistent. We further visualize the embeddings with T-SNE and PCA in <ref type="figure" target="#fig_0">Figure 13</ref>. Although the contrastive learned embedding has comparable T-SNE with our multiplicative design, the PCA shows that our multiplicative embedding is linearly separable while contrastive learned embedding is not. Hence, we demonstrate that our original design of elementwise production is simple and effective.</p><p>Removing keypoint embedding. We remove the keypoint embedding w j entirely. In this case, we only have background embedding w bg and the keypoint location. Thus, instead of generating the style map S, we directly concatenate the keypoint heatmaps {H k } K k=1 and the broadcasted background style map to generate the style map without keypoint embedding. As shown in <ref type="figure" target="#fig_2">Figure 14</ref>, the keypoints are not meaningful or consistent. The keypoint location hides the appearance and is entangled with the background severely.</p><p>Removing keypoints. If we remove the keypoints, then SPADE <ref type="bibr" target="#b36">(Park et al., 2019)</ref> degenerates to AdaIN <ref type="bibr" target="#b23">(Huang &amp; Belongie, 2017)</ref>. Instead of using a style map S (2D), we now use a style vector (1D), which is the background embedding. In this case, we do not have the power of local controllability on the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 ABLATION TESTS ON THE HYPERPARAMETERS</head><p>Ablation test on the dimension of embeddings. Different numbers of embedding dimensions make the expression power vary. As shown in <ref type="table" target="#tab_13">Table 6</ref>, larger D embed leads to larger error on MAFL  <ref type="figure" target="#fig_0">Figure 13</ref>: Ablation study on multiplicative embedding. We show the T-SNE and PCA visualization of embeddings learned on FFHQ. The first two column shows keypoint embeddings and the last two column shows keypoint embeddings and background embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing background embedding</head><p>Removing global style vector</p><p>Removing keypoint embedding Changing to contrastive-learned embedding <ref type="figure" target="#fig_2">Figure 14</ref>: Ablation study on architecture. We show the keypoints for different architectures. but lower (better) FID. We use D embed = 128 in our main paper because the increase in error is small but the decrease of FID is significant.</p><p>Ablation Test on ? A too-small value for ? does not influence the image and will cause artifacts as shown in <ref type="figure" target="#fig_1">Figure 15</ref>. A too-large value for ? will disable the background embedding and control the background.</p><p>Ablation test on the number of keypoints. By selecting different numbers of keypoints, we can achieve different levels of control. In the second row of <ref type="figure" target="#fig_1">Figure 15</ref>, we use 6 keypoints rather than the default 10. Thereby, keypoints have a smaller area of effect. We observe that the background encoding then takes a larger role and contains the encoding of hair and beard, while the keypoints focus only on the main facial features (nose, mouth, and eyes).</p><p>Ablation test on the combination of number of keypoints and ? . The impact of keypoints depends on the combination of number of keypoints and ? . We test the pairwise combination between K = 1, 6, 8, 12, 16, 32 and ? = 0.002, 0.005, 0.01, 0.02. The FID is listed in <ref type="table">Table 7</ref>    <ref type="table">Table 7</ref>: FID scores of ablation tests on number of keypoints K and keypoint size ? on CelebA of resolution 128 ? 128. The lower means better. Neither K or ? significantly influence the image quality. Interestingly, the small artifacts when ? = 0.002 in <ref type="figure" target="#fig_1">Figure 15</ref> does not neither significantly influence the image quality.</p><p>tion error is listed in <ref type="table">Table 8</ref>. The image quality does not change much for different combinations. We illustrate samples of keypoints of each combination in <ref type="figure" target="#fig_3">Figure 16</ref> and editing results in <ref type="figure" target="#fig_4">Figure 17</ref> and <ref type="figure" target="#fig_5">Figure 18</ref>. If both the number of keypoints and ? are small, e.g., K = 1, and the ? = 0.002, then the background controls both foreground appearance and pose, and the keypoints are trivial. If both, the number of keypoints and ? , are large, e.g., K = 32, and ? = 0.02, then the keypoint appearance controls background and pose. While the model degenerates in extreme cases, we found the model to be robust for a wide range of values, i.e., K = 8, 12, 16 and ? = 0.005, 0.01. We summarize all the cases in <ref type="table">Table 9</ref>. Ablation Test on GAN Loss If we replace equation 3, 4 and 5 with the spectral norm <ref type="bibr" target="#b34">(Miyato et al., 2018)</ref> and hinge loss <ref type="bibr" target="#b34">(Miyato et al., 2018;</ref><ref type="bibr" target="#b36">Park et al., 2019)</ref> used in the original SPADE architecture, we get mostly static, meaningless latent keypoint coordinates. The object part location information is entangled with the key part appearance. The comparison is shown in <ref type="figure" target="#fig_13">Figure 21</ref>.</p><p>Ablation Test on Background Loss If we remove the background loss in Eq. 6, most keypoints are still at reasonable positions while some move to the background. As shown in <ref type="figure">Figure 20</ref>, the yellow keypoint is on the background while all the others are still on the foreground.</p><p>Removing Keypoint Scheduler. If we move the keypoint scheduler, i.e., updating keypoint generator during resolution adaption, the keypoint locations diverge and the appearance collapses, as shown in <ref type="figure" target="#fig_13">Figure 21</ref>. K = 1 K = 6 K = 8 K = 12 K = 16 K = 32 ? = 0.002 11.59% 7.86% 6.78% 7.09% 6.00% 5.35% ? = 0.005 8.65% 7.28% 6.39% 5.24% 5.13% 4.11% ? = 0.01 8.43% 7.91% 7.97% 6.06% 7.37% 8.84% ? = 0.02 8.71% 6.26% 7.13% 5.16% 6.80% 8.26% <ref type="table">Table 8</ref>: Normalized Error of ablation tests on number of keypoints K and keypoint size ? on CelebA of resolution 128 ? 128. For ? = 0.002, 0.005, the error decreases as K increases while for ? = 0.01, 0.02, the error first decreases and then increases. If both of them are large, e.g., K &gt; 16, ? &gt; 0.01, the appearance is entangled with the keypoints which results in a larger error. K = 1 K = 6 K = 8 K = 12 K = 16 K = 32 ? = 0.002 T T T T T T ? = 0.005 T ? = 0.01 T E ? = 0.02 E E <ref type="table">Table 9</ref>: Keypoint controllability. T denotes trivial keypoint, i.e., the background controls the entire image. E means entangled pose, appearance and background. means disentangled control and means inferior disentanglement, where one of the pairs {(pose, appearance), (pose, background), (appearance, background) is entangled.}. For a small keypoint size of ? = 0.0002 the model always gives trivial keypoints. With a large number of keypoints and a large keypoint size, i.e., K &gt; 16 and ? &gt; 0.01, our model gives entangled representations. Our model is robust in the range of K ? [8, 16] and ? ? [0.005, 0.01].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G FAILURE CASES AND LIMITATIONS</head><p>As described in the main text, our model sometimes generates asymmetric faces as shown in the first two images in <ref type="figure" target="#fig_14">Figure 22</ref>. In addition, the hair sometimes is entangled with the background, especially long hair, as shown in the right two images in <ref type="figure" target="#fig_14">Figure 22</ref>.</p><p>From the ablation tests in training strategy, we can see that this method can heavily depend on the state-of-the-art GAN training loss function and image-to-image translation architectures. In fact, we observed some image quality degeneration as the training goes on in the highest resolution (512?512). Therefore, we apply early stopping in the highest resolution. We expect that researchers will push GAN and spatially adaptive image-to-image translation even further. Then people can plug in our keypoint generator to receive higher image quality and more accurate keypoints. In this paper, we only provide the idea and focus on unsupervised local image editing without using any loss on pairs of images. <ref type="figure" target="#fig_3">Figure 16</ref>: Visualization for different combinations of number of keypoints and keypoint size ? . If both, the number of keypoints and the keypoint size ? , are small (top left), the keypoint is trivial. If both of them are large (bottom right), the keypoints distribute uniformly over the images instead of focusing on parts. <ref type="figure" target="#fig_4">Figure 17</ref>: Editing on different combinations of number of keypoints K and keypoint size ? . K=1, 6. Column 1: original image; column 2: part appearance source image used to swap appearance; column 3: the combined image with shape from the original images and the appearance from the part appearance source image; column4: we randomly swap a single keypoint close to the mouth; column 5: resulting difference map when changing the keypoint in the 4th column; column 6: move the face to the left and add another set of keypoints on the right; column 7: removing all keypoints. If ? = 0.0002, the keypoints are trivial, and cannot be used to change appearance. When K = 1, the keypoint also only have limited control even if ? = 0.02. The combination of K = 6, ? = 0.005 gives good spatial disentanglement. <ref type="figure" target="#fig_5">Figure 18</ref>: Editing on different combinations of number of keypoints K and keypoint size ? . K=8,12. For a small ? = 0.0002, the keypoint is trivial. When ? is large the background is entangled (K = 8, ? = 0.02) in some cases. We found the combinations of (K = 12, ? = 0.0005) and (K = 12, ? = 0.01) both give the best editing controllability. <ref type="figure">Figure 19</ref>: Editing on different combination of number of keypoints K and keypoint size ? . K=16,32. Extreme small ? (? = 0.0002) constantly gives trivial keypoints even if K is large (K = 32). When both, K and ? , are large (K = 32, ? &gt; 0.01), the keypoint embeddings control the background and the pose. We found the combinations of (K = 16, ? = 0.0005) and (K = 32, ? = 0.005) both give the best editing controllability.  The left top two images show asymmetric faces: different eye colors for the man and different blusher for the woman. The middle top two images show the entanglement of hair and background. The right top two images show that the pose of head is hidden in the relative positions of other keypoints than the keypoints on the head. We visualize the process of removing parts at the bottom. We sequentially remove the left eye, right eye, mouth, nose, and the entire face. Due to the entanglement of hair and background, the hair remains even if we remove the whole face.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Location and scale editing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc><ref type="bibr" target="#b53">Zhu et al., 2020b)</ref> on CelebA-HQ, and human pose experiments on BBCPose, and 128 ? 128 for experiments on LSUN Bedroom, and comparison with unsuper-Disentangled keypoint embeddings on FFHQ. The leftmost images are the source and the rightmost images are the target. The cross landmarks on the first column denote the parts to be changed. The second column shows the difference between the original image and the changed image. The third to the second to last columns show the interpolation between the original image and the target image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Disentangled Background.The background is changed while the faces are fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>25 w = 0.50 T a r g e t ( w =1) w = 0.75 Re m ov i n g b e d Re m ov i n g l i g ht Re m ov i n g w i n d ow Adding another bed Adding another bed Adding another drawing Editing on Bedroom. Left first row: interpolating the keypoint embedding on the curtain. Left second row: interpolating the keypoint embedding on the window. Right first row: changing the position of keypoint on the bed. Right second row: changing the position of the keypoints on the light. Left third row: removing parts from the bedroom. Right third row: adding parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Editing on BBC Pose. The first row shows the source image and the second row the editing results. First three columns: the human appearance is swapped with the small target image. Middle three columns: changing the position to the one in the overlayed target. Last three column: changing the background (the bottom right corner shows the difference).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FFHQFigure 8 :</head><label>8</label><figDesc>Keypoints. We show the keypoints on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Padding Margins.</head><label></label><figDesc>As pointed out by Islam et al. (2020); Alsallakh et al. (2021); Xu et al. (2020a); Kayhan &amp; Gemert (2020), convolutions with zero padding are very good at implicitly encoding absolute grid position at deeper layers. To prevent this, we follow Karras et al. (2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Lorenz et al. (2019) on LSUN Bedroom. (Left) Detected keypoints. The keypoints are static and do not have semantic meaning. (Right) Reconstructed images. The reconstruction completely fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>(Left) Spatially Adaptive Normalization. SPADE takes two inputs, feature map and style map. It first uses batch normalization to normalize the feature map. Then it uses the style map to calculate the new mean map and new standard deviation map for the feature map. (Right) Keypoint-based ConvBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Detailed architecture. (Left) LatentKeypointGAN generator. The numbers in the parenthesis is the output dimension of the Keypoint-based ConvBlock. For example,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F. 3 Figure 20 :</head><label>320</label><figDesc>ABLATION TESTS ON THE TRAINING STRATEGY Ablation Test on Background loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 21</head><label>21</label><figDesc>: (Left) GAN Loss Importance. Without gradient penality + logistic loss, as in SPADE, keypoint coordinates remain static. (Right) Scheduling the keypoint generator learning rate. Reducing the learning rate after each progressive up-scaling step prevents mode collapse and enables high-resolution training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 22 :</head><label>22</label><figDesc>(a) Training process. We visualize the image generated during the training. (b) Failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview. Starting from noise z, LatentKeypointGAN generates keypoint coordinates, k and their embeddings w. These are turned into feature maps that are localized around the keypoints, forming conditional maps for the image generation via SPADE blocks. The images are generated by toRGB blocks. At test time, the position and embedding of keypoints can be edited separately.</figDesc><table><row><cell>Keypoint Generator</cell><cell>Spatial embedding layer</cell><cell></cell><cell cols="3">Image generator</cell><cell></cell><cell></cell></row><row><cell>Keypoints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>coordinates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell>Heatmaps</cell><cell>concat</cell><cell>Const</cell><cell>SPADE</cell><cell>Block</cell><cell>SPADE</cell><cell>Block</cell></row><row><cell>MLP</cell><cell>broadcast multiplication</cell><cell>style map</cell><cell></cell><cell></cell><cell>toRGB</cell><cell></cell><cell>toRGB</cell><cell>toRGB</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>All contributions are crucial for the success and consistent for both detection and image quality tasks. The keypoint detection evaluation is explained in the subsequent Section 4.7. For simplicity, the FID is calculated at resolution 256 ? 256, between 50k generated images and the original dataset; slightly different from previous experiments.</figDesc><table><row><cell>4.7 UNSUPERVISED KEYPOINTS DISCOVERY</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">L 1 error % ? FID ?</cell></row><row><cell>full model</cell><cell>5.85%</cell><cell>23.50</cell></row><row><cell>w/o background</cell><cell>6.43%</cell><cell>25.67</cell></row><row><cell>w/o global style vector</cell><cell>6.76%</cell><cell>28.75</cell></row><row><cell>adding global style vector</cell><cell>5.29%</cell><cell>42.02</cell></row><row><cell>contrastive keypoint embedding</cell><cell>7.53%</cell><cell>28.47</cell></row><row><cell>w/o keypoint embedding</cell><cell>22.81%</cell><cell>32.41</cell></row><row><cell>w/o keypoint</cell><cell>-</cell><cell>34.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative ablation test on keypoint localization (L 1 ) and image quality(FID). A lower number is better. the-fly by our LatentKeypointGAN. Note, the LatentKeypointGAN is trained on the same number of training images as the other detectors, hence, results are comparable. For evaluation, we follow Thewlis et al. (2017) and subsequent methods: As the order and semantics of unsupervised keypoints are undefined, a linear regressor from the predicted keypoints to the 5 ground truth keypoints is learned on the MAFL training set. All images are resized to 128 ? 128 and the training and test set of the MAFL subset from CelebA are excluded when training the LatentKeypointGAN. The test error is the Euclidean distance to the ground truth on the MAFL test set, normalized by the inter-ocular distance in percent.High accuracy can therefore only be obtained when estimated keypoints move consistently with the human-annotated ones. LatentKeypointGAN strikes a low error of 5.9%, which lies between the original 8.0% byThewlis et al. (2017)  and the most recent 2.8% by<ref type="bibr" target="#b13">Dundar et al. (2020)</ref>. We further tuned our architecture for precise localization by enforcing stronger translation equivariance of the keypoints (see Appendix B for details). This improved variant, LatentKeypointGAN-tuned, reaches 3.3%, which gets close to Dunbar et al. (2.8%). On the in-the-wild variant of CelebA (more diverse head orientation, position, and scale), which is significantly more difficult, we reach an error of 5.6%. Here, we outperform all existing methods reporting on this metric (8.7% by Jakab et al.</figDesc><table><row><cell>We demonstrated that our keypoints</cell></row><row><cell>are semantically meaningful by edit-</cell></row><row><cell>ing the image via embeddings. In this</cell></row><row><cell>part, we further demonstrate the con-</cell></row><row><cell>sistency of our keypoints on the task</cell></row><row><cell>of unsupervised keypoint detection as</cell></row><row><cell>introduced by Thewlis et al. (2017).</cell></row><row><cell>We test whether keypoints consis-</cell></row><row><cell>tently align to the same image parts,</cell></row><row><cell>without randomly moving on the im-</cell></row><row><cell>age when other factors such as color</cell></row><row><cell>or texture are changed. To this end,</cell></row><row><cell>we train a standard ResNet detec-</cell></row><row><cell>tor (Xiao et al., 2018) supervised on</cell></row><row><cell>200,000 image-keypoint pairs gener-</cell></row><row><cell>ated on-</cell></row></table><note>(2018) and 11.4% by Lorenz et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The study confirms the main results: While Zhang et al. strike a high keypoint accuracy, its image quality (ours preferred by 92.17%) and editing results (our disentanglement preferred by 67.83%) are unsatisfactory. While Zhu et al. reach a better FID score before editing, after editing, ours is the most realistic (preferred by 94.78%). Moreover, our disentanglement comes close to the one ofZhu et al. (49.57% voted equal quality). Zhu et al. have better shape preservation because their masks provide silhouettes explicitly, but this likely also leads to the lower editing quality as masks and embeddings can become incompatible. For instance, the hair shape and strain direction need to be correlated. The full details are in the appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>training set is very small or the number of persons involved is limited, such as BBCPose. To mitigate the risk, we only use established and publicly available datasets, in particular those that collect pictures of celebrities or other public figures and also those not containing any person (bedroom dataset).</figDesc><table /><note>argue that GANs do not memorize training datasets, recently Feng et al. (2021) empirically proved that whether GANs memorize training datasets depends on the complexity of the training datasets. Therefore, our model has some risk of leaking such identity information if the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 1125-1134, 2017.    Ali Jahanian*, Lucy Chai*, and Phillip Isola. On the "steerability" of generative adversarial networks. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=HylsTT4FvB.Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object landmarks through conditional image generation. In Advances in neural information processing systems, pp. 4016-4027, 2018.Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4401-4410, 2019. Unsupervised learning of object keypoints for perception and control. In Advances in neural information processing systems, pp. 10724-10734, 2019. Gihyun Kwon and Jong Chul Ye. Diagonal attention and style-based gan for content-style disentanglement in image generation and translation. In Proceedings of the IEEE international conference on computer vision, pp. 13980-13989, 2021. Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. In Proceedings of the IEEE International Conference on Computer Vision, pp. 10531-10540, 2019. Supasorn Suwajanakorn, Noah Snavely, Jonathan J Tompson, and Mohammad Norouzi. Discovery of latent 3d keypoints via end-to-end geometric reasoning. In Advances in neural information processing systems, pp. 2059-2070, 2018. Xintao Wang, Kai Chen, Bolei Zhou, and Chen Change Loy. Positional encoding as spatial inductive bias in gans. arXiv preprint arXiv:2012.05217, 2020a. Yinghao Xu, Ceyuan Yang, Ziwei Liu, Bo Dai, and Bolei Zhou. Unsupervised landmark learning from unpaired data. arXiv preprint arXiv:2007.01053, 2020b.</figDesc><table><row><cell>Phillip Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5549-5558, 2020. Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), Hua, and Nenghai Yu. Semantic image synthesis via efficient class-adaptive normalization. arXiv preprint arXiv:2012.04644, 2020. Ayush Tewari, Mohamed Elgharib, Florian Bernard, Hans-Peter Seidel, Patrick P?rez, Michael Zollh?fer, Christian Theobalt, et al. Pie: Portrait image embedding for semantic control. arXiv preprint arXiv:2009.09485, 2020a. Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick P?rez, Michael Z?llhofer, and Christian Theobalt. Stylerig: Rigging stylegan for 3d control over portrait images, cvpr 2020. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, june 2020b. Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, and Ying Nian Wu. Deformable gen-erator network: Unsupervised disentanglement of appearance and geometry. arXiv preprint arXiv:1806.06298, 2018. 36(6Wei Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming He, Lu Yuan, Gang Rui Xu,</cell></row></table><note>This work was supported by the UBC Advanced Research Computing (ARC) GPU cluster, the Compute Canada GPU servers, and a Huawei-UBC Joint Lab project.Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz- ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8110-8119, 2020. Tero Karras, Miika Aittala, Samuli Laine, Erik H?rk?nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. arXiv preprint arXiv:2106.12423, 2021. Osman Semih Kayhan and Jan C van Gemert. On translation invariance in cnns: Convolutional layers can exploit absolute spatial location. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14274-14285, 2020. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020. Hyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, and Youngjung Uh. Exploiting spatial dimen- sions of latent in gan for real-time image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 852-861, 2021. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Tejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, and Volodymyr Mnih.):194:1-194:17, 2017. URL https://doi.org/10.1145/3130800.3130813. Weijian Li, Haofu Liao, Shun Miao, Le Lu, and Jiebo Luo. Unsupervised learning of landmarks based on inter-intra subject consistencies. arXiv preprint arXiv:2004.07936, 2020. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in neural information processing systems, pp. 700-708, 2017.James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object landmarks by factorized spatial embeddings. In Proceedings of the IEEE international conference on computer vision, pp. 5916-5925, 2017. James Thewlis, Samuel Albanie, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of land- marks by descriptor vector exchange. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6361-6371, 2019. Hung-Yu Tseng, Matthew Fisher, Jingwan Lu, Yijun Li, Vladimir Kim, and Ming-Hsuan Yang. Modeling artistic workflows for image generation and editing. In European Conference on Com- puter Vision, 2020. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High- resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8798-8807, 2018. Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, and Chen Change Loy. Transgaga: Geometry-aware unsupervised image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8012-8021, 2019. Jonas Wulff and Antonio Torralba. Improving inversion and generation diversity in stylegan using a gaussianized latent space. arXiv preprint arXiv:2009.06529, 2020.Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In European Conference on Computer Vision (ECCV), 2018.Xin Yang, Yuezun Li, Honggang Qi, and Siwei Lyu. Exposing gan-synthesized faces using landmark locations. In Proceedings of the ACM Workshop on Information Hiding and Multimedia Security, pp. 113-118, 2019. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>The entire study is in anonymous form in the supplemental document at survey/index.html.</figDesc><table><row><cell></cell><cell>Comparison with mask-conditioned methods (CelebA-HQ)</cell><cell cols="2">Unsupervised keypoint-based methods (CelebA 128 ? 128)</cell><cell></cell></row><row><cell>Ours</cell><cell>SPADE (Park et al., 2019) MaskGAN (Lee et al., 2020) SEAN (Zhu et al., 2020b)</cell><cell>Ours</cell><cell>Xu et al. (2020b)</cell><cell>Zhang et al. (2018)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>We also outperform SEAN in image editing</figDesc><table><row><cell>Aspect</cell><cell>Method to compare</cell><cell cols="3">In favour of ours In favour of others Equal quality</cell></row><row><cell>Editing image quality</cell><cell>Zhang et al. (2018)</cell><cell>92.17%</cell><cell>0.87%</cell><cell>6.96%</cell></row><row><cell cols="2">Editing image quality SEAN (Zhu et al., 2020b)</cell><cell>94.78%</cell><cell>2.61%</cell><cell>2.61%</cell></row><row><cell>Part disentanglement</cell><cell>Zhang et al. (2018)</cell><cell>67.83%</cell><cell>5.22%</cell><cell>26.95%</cell></row><row><cell cols="2">Part disentanglement SEAN (Zhu et al., 2020b)</cell><cell>28.69%</cell><cell>21.74%</cell><cell>49.57%</cell></row><row><cell>Identity preservation</cell><cell>Zhang et al. (2018)</cell><cell>55.65%</cell><cell>14.78%</cell><cell>29.57%</cell></row><row><cell>Shape preservation</cell><cell>SEAN (Zhu et al., 2020b)</cell><cell>33.91%</cell><cell>46.96%</cell><cell>19.13%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>and the detec-Dimension of embeddings D embed Keypoint detection error on MAFL ? FID score on FFHQ ?</figDesc><table><row><cell>16</cell><cell>4.61%</cell><cell>28.85</cell></row><row><cell>32</cell><cell>4.92%</cell><cell>26.14</cell></row><row><cell>64</cell><cell>5.66%</cell><cell>27.64</cell></row><row><cell>128</cell><cell>5.85%</cell><cell>23.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Quantitative ablation test on dimension of embeddings. For both metrics, the lower means better.</figDesc><table><row><cell>Ablation Test on</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Ablation Test on number of keypoints</cell></row><row><cell></cell><cell>K = 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>K = 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell cols="2">Background</cell><cell>Whole face</cell><cell>Only eyes</cell><cell>Pose</cell></row><row><cell cols="8">Figure 15: Ablation study on hyperparameters. (Left) Face generation with on FFHQ with ? =</cell></row><row><cell cols="8">0.002. We use the red circle to mark the artifacts in the images. (Right) Face generation on FFHQ</cell></row><row><cell cols="8">with number of keypoints 10 (top) and 6 (bottom). More keypoints lead to a stronger influence of</cell></row><row><cell cols="8">the keypoint embedding. However, the 6-keypoint version still provides control, e.g., glasses, nose</cell></row><row><cell cols="8">type, and pose. From left to right: original image, replaced background (difference map overlayed),</cell></row><row><cell cols="8">replaced keypoint embeddings (target image overlayed), exchanged eye embeddings, and keypoint</cell></row><row><cell>position exchanged.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">K = 1 K = 6 K = 8 K = 12 K = 16 K = 32</cell></row><row><cell cols="2">? = 0.002 19.35</cell><cell>18.94</cell><cell>17.77</cell><cell>17.69</cell><cell>20.44</cell><cell>19.29</cell></row><row><cell cols="2">? = 0.005 18.39</cell><cell>18.28</cell><cell>18.42</cell><cell>18.38</cell><cell>20.31</cell><cell>18.72</cell></row><row><cell>? = 0.01</cell><cell>19.49</cell><cell>19.60</cell><cell>20.31</cell><cell>18.14</cell><cell>19.25</cell><cell>17.91</cell></row><row><cell>? = 0.02</cell><cell>19.11</cell><cell>18.80</cell><cell>20.28</cell><cell>19.34</cell><cell>19.58</cell><cell>18.17</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/theRealSuperMario/unsupervised-disentangling/tree/ reproducing_baselines</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">? 4 and stride 2 to downsample the feature map to 0.5x. The second convolutions have a kernel size 3 ? 3 and stride 1 to extract features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generator. We illustrate the LatentKeypointGAN generator in <ref type="figure">Figure 12</ref>. The output image is linearly combined by the output of toRGB block, where the weights depend on the training stage.</p><p>Discriminator. We illustrate the discriminator in <ref type="figure">Figure 12</ref>. For each resolution, we use two convolutions followed by Leaky ReLU <ref type="bibr" target="#b30">(Maas et al., 2013)</ref>. The first convolution has a kernel size</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image2stylegan++: How to edit the embedded images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8296" to="8305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disentangled image generation through structured noise injection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazeed</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5134" to="5142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind the pad -{cnn}s can develop blind spots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narine</forename><surname>Bilal Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kokhlikyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orion</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reblitz-Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Domain adaptation for upper body pose tracking in signed tv broadcasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepfacedrawing: deep generation of face images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="73" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controllable image synthesis via segvae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of object landmarks via contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14787</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Editing in style: Uncovering the local semantics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5771" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled and controllable face image generation via 3d imitative-contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised disentanglement of pose, appearance and background from images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Pottorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09518</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">When do gans replicate? on the choice of dataset size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6701" to="6710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravir</forename><surname>Singh Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00149</idno>
		<title level="m">Gif: Generative interpretable faces</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01758</idno>
		<title level="m">Collaborative learning for faster stylegan embedding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic image manipulation through structured representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2713" to="2723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil Db</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248</idno>
		<title level="m">How much position information do convolutional neural networks encode? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised part-based disentangling of object shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10955" to="10964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object structure and dynamics from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="92" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1QRgziT-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural scene decomposition for multi-person motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<ptr target="https://github.com/mseitzer/pytorch-fid" />
		<title level="m">pytorch-fid: FID Score for PyTorch</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>Version 0.1.1</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9243" to="9252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="665" />
		</imprint>
	</monogr>
	<note>Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward realistic face photo-sketch synthesis via composition-aided gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial fusion gan for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3653" to="3662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
	<note>Zhiyuan He, and Honglak Lee</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Facial landmark detection by deep multi-task learning. 09</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-47</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8584" to="8593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">In-domain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00049</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5104" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">We start from generating 64 ? 64 images in the progressive training. The batch size for 64 2 , 128 2 , 256 2 , 512 2 images are 128, 64, 32, 8, respectively. We set ? gp = 10 and ? bg = 100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyperparameter Setting For all experiments in image generation, we use leaky ReLU</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>We set the learning rate to 0.0001 and 0.0004 for generator and discriminators, respectively. We set D noise = 256 and D embed = 128 for all experiments unless otherwise stated (in ablation tests</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
