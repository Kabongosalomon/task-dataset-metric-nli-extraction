<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmental Recurrent Neural Networks for End-to-end Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Speech Technology Research</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">The University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
							<email>s.renals@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Speech Technology Research</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Segmental Recurrent Neural Networks for End-to-end Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: end-to-end speech recognition</term>
					<term>segmental CRF</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. Essentially, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3% phone error rate (PER) from the first-pass decoding -the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech recognition is a typical sequence to sequence transduction problem, i.e., given a sequence of acoustic observations, the speech recognition engine decodes the corresponding sequence of words or phonemes. A key component in a speech recognition system is the acoustic model, which computes the conditional probability of the output sequence given the input sequence. However, directly computing this conditional probability is challenging due to many factors including the variable lengths of the input and output sequences. The hidden Markov model (HMM) converts this sequence-level classification task into a frame-level classification problem, where each acoustic frame is classified into one of the hidden states, and each output sequence corresponds to a sequence of hidden states. To make it computationally tractable, HMMs usually rely on the conditional independence assumption and the first-order Markov rule -the well-known weaknesses of HMMs <ref type="bibr" target="#b0">[1]</ref>. Furthermore, the HMM-based pipeline is composed of a few relatively independent modules, which makes the joint optimisation nontrivial.</p><p>There has been a consistent research effort to seek architectures to replace HMMs and overcome their limitation for acoustic modelling, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>; however these approaches have not yet improved speech recognition accuracy over HMMs. In the past few years, several neural network based approaches have been proposed and demonstrated promising results. In particular, the connectionist temporal classification (CTC) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features. However, CTC simplifies the sequence-level error function by a product of the frame-level error functions (i.e., independence assumption), which means it essentially still does frame-level classification. It also requires the lengths of the input and output sequence to be the same, which is inappropriate for speech recognition. CTC deals with this problem by replicating the output labels so that a consecutive frames may correspond to the same output label or a blank token.</p><p>Attention-based RNNs have been demonstrated to be a powerful alternative sequence-to-sequence transducer, e.g., in machine translation <ref type="bibr" target="#b9">[10]</ref>, and speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>A key difference of this model from HMMs and CTCs is that the attention-based approach does not apply the conditional independence assumption to the input sequence. Instead, it maps the variable-length input sequence into a fixed-size vector representation at each decoding step by an attention-based scheme (see <ref type="bibr" target="#b9">[10]</ref> for further explanation). It then generates the output sequence using an RNN conditioned on the vector representation from the source sequence. The attentive scheme suits the machine translation task well, because there may be no clear alignment between the source and target sequence for many language pairs. However, this approach does not naturally apply to the speech recognition task, as each output token only corresponds to a small size window of acoustic spectrum.</p><p>In this paper, we study segmental RNNs <ref type="bibr" target="#b13">[14]</ref> for acoustic modelling. This model is similar to CTC and attention-based RNN in the sense that an RNN encoder is also used for feature extraction, but it differs in the sense that the sequence-level conditional probability is defined using an segmental (semi-Markov) CRF <ref type="bibr" target="#b14">[15]</ref>, which is an extension on the standard CRF <ref type="bibr" target="#b15">[16]</ref>. There have been numerous works on CRFs and their variants for speech recognition, e.g, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17]</ref> (see <ref type="bibr" target="#b17">[18]</ref> for an overview). In particular, feed-forward neural networks have been used with segmental CRFs for speech recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, segmental RNNs are different in that they are endto-end models -they do not depend on external systems to provide segmentation boundaries and features, instead, they are trained by marginalising out all possible segmentations, while the features are derived from the encoder RNNs, which are trained jointly with the segmental CRFs. Our experiments were performed on the TIMIT dataset, and we achieved 17.3% PER from first-pass decoding with zeroth-order CRF and without using any language model -the best reported result using CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Segmental Recurrent Neural Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Segmental Conditional Random Fields</head><p>Given a sequence of acoustic frames X = {x1, ? ? ? , xT } and its corresponding sequence of output labels y = {y1, ? ? ? , yJ }, where T ? J, segmental (or semi-Markov) conditional random field defines the sequence-level conditional probability with the auxiliary segment labels E = {e1, ? ? ? , eJ } as</p><formula xml:id="formula_0">P (y, E | X) = 1 Z(X) J j=1 exp f (yj, ej, X) ,<label>(1)</label></formula><p>where ej = sj, nj is a tuple of the beginning (sj) and the end (nj) time tag for the segment of yj, and nj &gt; sj while nj, sj ? [1, T ]; yj ? Y and Y denotes the vocabulary set; Z(X) is the normaliser that that sums over all the possible (y, E) pairs, i.e.,</p><formula xml:id="formula_1">Z(X) = y,E J j=1 exp f (yj, ej, X) .<label>(2)</label></formula><p>Here, we only consider the zeroth-order CRF, while the extension to higher order models is straightforward. Similar to other CRF-based models, the function f (?) is defined as</p><formula xml:id="formula_2">f (yj, ej, X) = w ?(yj, ej, X),<label>(3)</label></formula><p>where ?(?) denotes the feature function, and w is the weight vector. Previous works on CRF-based acoustic models mainly use heuristically handcrafted feature function ?(?). They also usually rely on an external system to provide the segment labels. In this paper, we define ?(?) using neural networks, and the segmentation E is marginalised out during training, which makes our model self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Representations</head><p>We use neural networks to define the feature function ?(?), which maps the acoustic segment and its corresponding label into a joint feature space. More specifically, yj is firstly represented as a one-hot vector vj, and it is then mapped into a continuous space by a linear embedding matrix M as</p><formula xml:id="formula_3">uj = Mvj<label>(4)</label></formula><p>Given the segment label ej, we use an RNN to map the acoustic segment to a fixed-dimensional vector representation, i.e.,</p><formula xml:id="formula_4">h j 1 = r(h0, xs j ) (5) h j 2 = r(h j 1 , xs j +1)<label>(6)</label></formula><p>. . .</p><formula xml:id="formula_5">h j d j = r(h j d j ?1 , xn j )<label>(7)</label></formula><p>where h0 denotes the initial hidden state, dj = nj ? sj denotes the duration of the segment and r(?) is a non-linear function. We take the final hidden state h j d j as the segment embedding vector, then ?(?) can be represented as</p><formula xml:id="formula_6">?(yj, ej, X) = g(uj, h j d j ),<label>(8)</label></formula><p>where g(?) corresponds to one layer or multiple layers of linear or non-linear transformation. In fact, it is flexible to include other relevant features as additional inputs to the function g(?), e.g., the duration feature which can be obtained by converting dj into another embedding vector. In practice, multiple RNN layers can be used transform the acoustic signal X before extracting the segment embedding vector h j d j as <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_7">x 1 x 2 x 3 x 4 y 2 y 1</formula><p>x 5</p><p>x 6 y 3 <ref type="figure">Figure 1</ref>: Segmental RNN using a first-order CRF. The coloured circles denote the segment embedding vector h j d j in Eq. <ref type="bibr" target="#b6">(7)</ref>. Using bi-directional RNNs is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Conditional Maximum Likelihood Training</head><p>For speech recognition, the segmentation labels E are usually unknown, training the model by maximising the conditional probability as Eq. <ref type="formula" target="#formula_0">(1)</ref> is therefore not practical. The problem can be addressed by defining the loss function as the negative marginal log-likelihood as</p><formula xml:id="formula_8">L(?) = ? log P (y | X) = ? log E P (y, E | X) = ? log E j exp f (yj, ej, X) ?Z(X,y) + log Z(X),<label>(9)</label></formula><p>where ? denotes the set of model parameters, and Z(X, y) denotes the summation over all the possible segmentations when only y is observed. To simplify notations, the objective function L(?) is define with only one training utterance. However, the number of possible segmentations is exponential with the length of X, which makes the naive computation of both Z(X, y) and Z(X) impractical. Fortunately, this can be addressed by using the following dynamic programming algorithm as proposed in <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_9">?0 = 1 (10) ?t = 0&lt;k&lt;t ? k ? y?Y f (y, k, t , X)<label>(11)</label></formula><formula xml:id="formula_10">Z(X) = ?T<label>(12)</label></formula><p>In Eq. <ref type="formula" target="#formula_0">(11)</ref>, the first summation is over all the possible segmentation up to timestep t, and the second summation is over all the possible labels from the vocabulary. The computation cost of this algorithm is O(T 2 ? |Y|), where |Y| is the size of the vocabulary. The cost can be further reduced by introducing an upper bound of the segment length, in which case Eq. (11) can be rewritten as</p><formula xml:id="formula_11">?t = l&lt;k&lt;t ? k ? y?Y f (y, k, t , X)<label>(13)</label></formula><formula xml:id="formula_12">l = 0 if t ? L &lt; 0 t ? L otherwise<label>(14)</label></formula><p>where L denotes the maximum value of the segment length. The cost is then reduced to O(L?T ?|Y|), and for long sequences</p><formula xml:id="formula_13">x 1 x 2 x 3 x 4 ? ? ? x 1 x 2 x 3</formula><p>x 4 ? ? ? a) concatenate / add b) skip <ref type="figure">Figure 2</ref>: Hierarchical subsampling recurrent network <ref type="bibr" target="#b20">[21]</ref> . The size of the subsampling window is two in this example.</p><p>like speech signals where T L, the computational savings are substantial.</p><p>The term Z(X, y) can be computed similarly. In this case, since the label y is now observed, the summation over all the possible labels y ? Y in Eq. (11) is not necessary, i.e.,</p><formula xml:id="formula_14">?0,0 = 1 (15) ?t,j = 0&lt;k&lt;t ? k,j?1 ? f (yj, k, t , X)<label>(16)</label></formula><p>Z(X, y) = ?T,J</p><p>Again, we can limit the length of the possible segments as Eq. <ref type="bibr" target="#b12">(13)</ref>. Given Z(X) and Z(X, y), the loss function L(?) can be minimised using the stochastic gradient decent (SGD) algorithm similar to training other neural network models. Other losses, for example, hinge, can be considered in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Viterbi Decoding</head><p>During decoding, we need to search the target label sequence y that yields the highest posterior probability given X by marginalising out all the possible segmentations:</p><formula xml:id="formula_16">y * = arg max y log E P (y, E | X)<label>(18)</label></formula><p>This involves minor modification of the recursive algorithm in Eq. (11) that instead of summing over all the possible labels, the Viterbi path up to the timestep t is</p><formula xml:id="formula_17">? * t = 0&lt;k&lt;t ? * k ? max y?Y f (y, k, t , X)<label>(19)</label></formula><p>However, marginalising out all the possible segmentations is still expensive. The computational cost can be further reduced by greedy searching the most likely segmentation, i.e.,</p><formula xml:id="formula_18">? * t = max 0&lt;k&lt;t ? * k ? max y?Y f (y, k, t , X),<label>(20)</label></formula><p>which corresponds to the decoding objective as y * , E * = arg max y,E log P (y, E | X)  This joint maximization algorithm may yield high search error, because it only considers one segmentation. In the future, we shall investigate the beam search algorithm which may yield a lower search error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Further Speedup</head><p>It is computationally expensive for RNNs to model long sequences, and the number of possible segmentations is exponential with the length of the input sequence as mentioned before. The computational cost can be significantly reduced by using the hierarchical subsampling RNN <ref type="bibr" target="#b20">[21]</ref> to shorten the input sequences, where the subsampling layer takes a window of hidden states from the lower layer as input as shown in <ref type="figure">Figure 2</ref>. In this work, we consider three variants: a) concatenate -the hidden states in the subsampling window are concatenated before been fed into the next layer; b) add -the hidden states are added into one vector for the next layer; c) skip -only the last hidden state in the window is kept and all the others are skipped. The last two schemes are computationally cheaper as they do not introduce extra model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Setup</head><p>We used the TIMIT dataset to evaluate the segmental RNN acoustic models. This dataset was preferred for the rapid evaluation of different system settings, and for the comparison to other CRF and end-to-end systems. We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe <ref type="bibr" target="#b21">[22]</ref>. We used the core test set as our evaluation set, which has 192 utterances. We used 24 dimensional log fiterbanks (FBANKs) with delta and double-delta coefficients, yielding 72 dimensional feature vectors. Our models were trained with 48 phonemes, and their predictions were converted to 39 phonemes before scoring. The dimension of uj was fixed to be 32. For all our experiments, we used the long short-term memory (LSTM) networks <ref type="bibr" target="#b22">[23]</ref> as the implementation of RNNs, and the networks were always bi-directional. We set the initial SGD learning rate to be 0.1, and we exponentially decay the learning rate by a factor of 2 when the validation error stopped decreasing. Our models were trained with dropout  , using an specific implementation for recurrent networks <ref type="bibr" target="#b24">[25]</ref>. The dropout rate was 0.2 unless specified otherwise. Our models were randomly initialised with the same random seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results of Hierarchical Subsampling</head><p>We first demonstrate the results of the hierarchical subsampling recurrent network, which is the key to speed up our experiments. We set the size of the subsampling window to be 2, therefore each subsampling layer reduced the time resolution by a factor of 2. We set the maximum segment length L in Eq. <ref type="bibr" target="#b13">(14)</ref> to be 300 milliseconds, which corresponded to 30 frames of FBANKs (sampled at the rate of 10 milliseconds). With two layers of subsampling recurrent networks, the time resolution was reduced by a factor of 4, and the value of L was reduced to be 8, yielding around 10 times speedup as shown in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table" target="#tab_1">Table 2</ref> compares the three implementations of the recurrent subsampling network detailed in section 2.5. We observed that concatenating all the hidden states in the subsampling window did not yield lower phone error rate (PER) than using the simple skipping approach, which may be due to the fact that the TIMIT dataset is small and it prefers a smaller model. On the other hand, adding the hidden states in the subsampling window together worked even worse, possibly due to that the sequential information in the subsampling window was flattened. In the following experiments, we sticked to the skipping method, and using two subsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hyperparameters and Different Features</head><p>We then evaluated the model by tuning the hyperparameters, and the results are given in <ref type="table" target="#tab_2">Table 3</ref>. We tuned the number of LSTM layers, and the dimension of LSTM cells, as well as the dimensions of w and the segment vector h j d j . In general, larger models with dropout regularisation yielded higher recognition accuracy. Our best result was obtained using 6 layers of 250dimensional LSTMs. However, without the dropout regularisation, the model can be easily overfit due to the small size of training set. In the future, we shall evaluate this model with a large dataset. We then evaluated another two types of features using the same system configuration that achieved the best result in <ref type="table" target="#tab_2">Table 3</ref>. We increased the number of FBANKs from 24 to 40, which yielded slightly lower PER. We also evaluated the standard Kaldi features -39 dimensional MFCCs spliced by a context window of 7, followed by LDA and MLLT transform and with feature-space speaker-dependent MLLR, which were the same features used in the HMM-DNN baseline in <ref type="table" target="#tab_4">Table 5</ref>. The well-engineered features improved the accuracy of our system by more than 1% absolute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to Related Works</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we compare our result to other reported results using segmental CRFs as well as recent end-to-end systems. Previous state-of-the-art result using segmental CRFs on the TIMIT dataset is reported in <ref type="bibr" target="#b27">[28]</ref>, where the first-pass decoding was used to prune the search space, and the second-pass was used to re-score the hypothesis using various features including neural network features. Besides, the ground-truth segmentation was used in <ref type="bibr" target="#b27">[28]</ref>. We achieved considerably lower PER with firstpass decoding, despite the fact that our CRF was zeroth-order, and we did not use any language model. Furthermore, our results are also comparable to that from the CTC and attentionbased RNN end-to-end systems. The accuracy of segmental RNNs may be further improved by using higher-order CRFs or incorporating a language model into the decode step, and using beam search to reduce the search error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we present the segmental RNN -a novel acoustic model that combines the segmental CRF with an encoder RNN for end-to-end speech recognition. We discuss the practical training and decoding algorithms of this model for speech recognition, and the subsampling network to reduce the computational cost. Our experiments were performed on the TIMIT dataset, and we achieved strong recognition accuracy using zeroth-order CRF, and without using any language model. In the future, we shall investigate discriminative training criteria, and incorporating a language model into the decoding step. Future works also include implementing a weighted finite sate transducer (WFST) based decoder and scaling this model to large vocabulary datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Speedup by hierarchical subsampling networks.</figDesc><table><row><cell cols="3">subsampling L speedup</cell></row><row><cell>No</cell><cell>30</cell><cell>1</cell></row><row><cell>1 layer</cell><cell>15</cell><cell>?3x</cell></row><row><cell>2 layers</cell><cell>8</cell><cell>?10x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of hierarchical subsampling networks. d(w) and d(hj) denote the dimension of w and h j d j in Eqs.(3)and(7)respectively. layers denotes the number of LSTM layers and hidden is the dimension of the LSTM cells. We reduced the dimension of h j d j from the LSTM output for computational reasons. conc is short for the concatenating operation.</figDesc><table><row><cell cols="6">System d(w) d(h j d j ) layers hidden PER(%)</cell></row><row><cell>skip</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>128</cell><cell>21.2</cell></row><row><cell>conc</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>128</cell><cell>21.3</cell></row><row><cell>add</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>128</cell><cell>23.2</cell></row><row><cell>skip</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>250</cell><cell>20.1</cell></row><row><cell>conc</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>250</cell><cell>20.5</cell></row><row><cell>add</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>250</cell><cell>21.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of tuning the hyperparameters.</figDesc><table><row><cell cols="6">Dropout d(w) d(h j d j ) layers hidden PER</cell></row><row><cell></cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>128</cell><cell>21.2</cell></row><row><cell></cell><cell>64</cell><cell>32</cell><cell>3</cell><cell>128</cell><cell>21.6</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>3</cell><cell>128</cell><cell>21.4</cell></row><row><cell></cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>250</cell><cell>20.1</cell></row><row><cell>0.2</cell><cell>64</cell><cell>32</cell><cell>3</cell><cell>250</cell><cell>20.4</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>3</cell><cell>250</cell><cell>20.6</cell></row><row><cell></cell><cell>64</cell><cell>64</cell><cell>6</cell><cell>250</cell><cell>19.3</cell></row><row><cell></cell><cell>64</cell><cell>32</cell><cell>6</cell><cell>250</cell><cell>20.2</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>6</cell><cell>250</cell><cell>20.2</cell></row><row><cell></cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>128</cell><cell>21.3</cell></row><row><cell>0.1</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>250</cell><cell>20.9</cell></row><row><cell></cell><cell>64</cell><cell>64</cell><cell>6</cell><cell>250</cell><cell>20.4</cell></row><row><cell>?</cell><cell>64</cell><cell>64</cell><cell>6</cell><cell>250</cell><cell>21.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of three types of acoustic features.</figDesc><table><row><cell>Features 24-dim FBANK 40-dim FBANK</cell><cell cols="3">Deltas d(xt) PER ? 72 19.3 ? 120 18.9</cell></row><row><cell>Kaldi</cell><cell>?</cell><cell>40</cell><cell>17.3</cell></row><row><cell>regularisation [24]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison to Related Works. LM denotes the language model, and SD denotes speaker-dependent transform. The HMM-DNN baseline was trained with cross-entropy using the Kaldi recipe. Sequence training did not improve it due to the small amount of data. Note that RNN transducer and attention-based RNN are equipped with built-in RNNLMs.</figDesc><table><row><cell>System HMM-DNN first-pass SCRF [26]</cell><cell cols="3">LM SD PER ? ? 18.5 ? ? 33.1</cell></row><row><cell>Boundary-factored SCRF [27] Deep Segmental NN [19] Discriminative segmental cascade [28] + 2nd pass with various features</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>26.5 21.9 21.7 19.9</cell></row><row><cell>CTC [29]</cell><cell>?</cell><cell>?</cell><cell>18.4</cell></row><row><cell>RNN transducer [29]</cell><cell>-</cell><cell>?</cell><cell>17.7</cell></row><row><cell>Attention-based RNN [11]</cell><cell>-</cell><cell>?</cell><cell>17.6</cell></row><row><cell>Segmental RNN Segmental RNN</cell><cell>? ?</cell><cell>? ?</cell><cell>18.9 17.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wegmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
		<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From HMM&apos;s to segment models: A unified view of stochastic modeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Digalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="page" from="360" to="378" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition using SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1197" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for phone classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech recognition using augmented conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hifny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06018</idno>
		<title level="m">Segmental recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-markov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with segmental conditional random fields: A summary of the JHU CLSP 2010 summer workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Compernolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5044" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields in speech, audio, and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1054" to="1075" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep segmental neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1849" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical subsampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="109" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motl?cek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Semmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classification and recognition with direct segment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4161" to="4164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient segmental conditional random fields for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1898" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative segmental cascades for feature-rich phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
