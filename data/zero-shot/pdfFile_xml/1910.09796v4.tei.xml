<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Fact Verification with Kernel Graph Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Fact Verification with Kernel Graph Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more finegrained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernelbased attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT's effectiveness. All source codes of this work are available at https://github. com/thunlp/KernelGAT. Al Jardine is an American rhythm guitarist Claim Verification SUPPORTS REFUTES NOT ENOUGH INFO Evidence Reasoning Alan Charles Jardine (born September 3, 1942) is an American musician, singer and songwriter who cofounded the Beach Boys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>He is best known as the band's rhythm guitarist,</head><p>and for occasionally singing lead vocals on singles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online contents with false information, such as fake news, political deception, and online rumors, have been growing significantly and spread widely over the past several years. How to automatically "fact check" the integrity of textual contents, to prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society.</p><p>Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia <ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref>. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity <ref type="bibr" target="#b17">(Nie et al., 2019a;</ref><ref type="bibr" target="#b33">Yoneda et al., 2018;</ref><ref type="bibr" target="#b9">Hanselowski et al., 2018)</ref>.</p><p>There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements . This paper presents a new neural structural reasoning model, Kernel Graph Attention Network <ref type="bibr">(KGAT)</ref>, that provides more fine-grained evidence selection and reasoning capability for fact verification using neural matching kernels <ref type="bibr" target="#b29">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>. Given retrieved evidence pieces, KGAT first constructs an evidence graph, using claim and evidence as graph nodes and fullyconnected edges. It then utilizes two sets of kernels, one on the edges, which selectively summarize clues for a more fine-grained node representation and propagate clues among neighbor nodes through a multi-layer graph attention; and the other on the nodes, which performs more accurate evidence selection by better matching evidence with the claim. These signals are combined by KGAT, to jointly learn and reason on the evidence graph for more accurate fact verification.</p><p>In our experiments on FEVER <ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref>, a large-scale fact verification benchmark, KGAT achieves a 70.38% FEVER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches . Our experiments demonstrate KGAT's strong effectiveness especially on facts that require multiple evidence reasoning: our kernel-based attentions provide more sparse and focused attention patterns, which are the main source of KGAT's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The FEVER shared task <ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref> aims to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The recently launched FEVER shared task 1.0 is hosted as a competition on Codalab 1 with a blind test set and has drawn lots of attention from NLP community.</p><p>Existing fact verification models usually employ FEVER's official baseline <ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref> with a three-step pipeline system <ref type="bibr" target="#b0">(Chen et al., 2017a)</ref>: document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. <ref type="bibr" target="#b17">Nie et al. (2019a)</ref> concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label <ref type="bibr" target="#b15">(Luken et al., 2018;</ref><ref type="bibr" target="#b33">Yoneda et al., 2018;</ref><ref type="bibr" target="#b9">Hanselowski et al., 2018)</ref>. TwoWingOS <ref type="bibr" target="#b32">(Yin and Roth, 2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR  formulates claim verification as a graph reasoning task and provides two kinds of attentions. It conducts reasoning and aggregation over claim evidence pairs with a graph model <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b24">Scarselli et al., 2008;</ref><ref type="bibr" target="#b11">Kipf and Welling, 2017)</ref>. <ref type="bibr" target="#b35">Zhong et al. (2019)</ref> further employs XLNet  and establishes a semantic-level graph for reasoning for a better performance. These graph based models establish node interactions for joint reasoning over several evidence pieces.</p><p>Many fact verification systems leverage Natural Language Inference (NLI) techniques <ref type="bibr" target="#b1">(Chen et al., 2017b;</ref><ref type="bibr" target="#b7">Ghaeini et al., 2018;</ref><ref type="bibr" target="#b20">Parikh et al., 2016;</ref><ref type="bibr" target="#b23">Radford et al., 2018;</ref> to verify the claim. The NLI task aims to classify the relationship between a pair of premise and hypothesis as either entailment, contradiction or neutral, similar to the FEVER task, though the later requires systems to find the evidence pieces themselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b1">(Chen et al., 2017b)</ref>, which employs some forms of hard or soft alignment to associate the relevant sub-components between premise and hypothesis. BERT, the pre-trained deep bidirectional Transformer, has also been used for better text representation in FEVER and achieved better performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b25">Soleimani et al., 2019)</ref>.</p><p>The recent development of neural information retrieval models, especially the interaction based ones, have shown promising effectiveness in extracting soft match patterns from query-document interactions <ref type="bibr" target="#b10">(Hu et al., 2014;</ref><ref type="bibr" target="#b19">Pang et al., 2016;</ref><ref type="bibr" target="#b29">Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>. One of the effective ways to model text matches is to leverage matching kernels <ref type="bibr" target="#b29">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018)</ref>, which summarize word or phrase interactions in the learned embedding space between query and documents. The kernel extracts matching patterns which provide a variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type="bibr" target="#b3">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels can be integrated with contextualized representations, i.e., BERT, to better model the relevance between query and documents <ref type="bibr" target="#b16">(MacAvaney et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Kernel Graph Attention Network</head><p>This section describes our Kernel Graph Attention Network (KGAT) and its application in Fact Verification. Following previous research, KGAT first constructs an evidence graph using retrieved evidence sentences D = {e 1 , . . . , e p , . . . , e l } for claim c, and then uses the evidence graph to predict the claim label y (Sec. 3.1 and 3.2). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the reasoning model includes two main components: Evidence Propagation with Edge Kernels (Sec. 3.3) and Evidence Selection with Node Kernels (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reasoning with Evidence Graph</head><p>Similar to previous research , KGAT constructs the evidence graph G by using each claim-evidence pair as a node and connects all node pairs with edges, making it a fullyconnected evidence graph with l nodes: N = {n 1 , . . . , n p , . . . , n l }.</p><p>KGAT unifies both multiple and single evidence reasoning scenarios and produces a probability P (y|c, D) to predict claim label y. Different from previous work , we follow the standard graph label prediction setting in graph neural network <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref> and split the prediction into two components: 1) the label prediction in each node conditioned on the whole graph P (y|n p , G); 2) the evidence selection probability P (n p |G): P (y|c, D) = l p=1 P (y|c, e p , D)P (e p |c, D),</p><p>(1) or in the graph notation:</p><formula xml:id="formula_0">P (y|G) = l p=1 P (y|n p , G)P (n p |G).<label>(2)</label></formula><p>The joint reasoning probability P (y|n p , G) calculates node label prediction with multiple evidence. The readout module <ref type="bibr" target="#b12">(Knyazev et al., 2019)</ref> calculates the probability P (n p |G) and attentively combines per-node signals for prediction. The rest of this section describes the initialization of node representations (n p ) in Sec. 3.2, the calculation of per-node predictions P (y|n p , G) with Edge Kernels (Sec. 3.3), and the readout module P (n p |G) with Node Kernels (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Node Representations</head><p>The node representations are initialized by feeding the concatenated sequence of claim, document (Wiki) title, and evidence sentence, to pre-trained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. Specifically, in the node n p , the claim and evidence correspond to m tokens (with "[SEP]") and n tokens (with Wikipedia title and "[SEP]") . Using the BERT encoder, we get the token hidden states H p with the given node n p :</p><formula xml:id="formula_1">Joint Evidence Reasoning MLP MLP Claim Label Node Kernel ? " #(%|' 1 , *) #(%|' , , *) #(%|' -, *) Evidence Reasoning Evidence Selection Edge Kernel ' 1 ' - ' , #(%|G) ? / 0 #(' -|*) 1(' 0 ) 2 3 2 4 2 0 ? / 4 #(' , |*) 1(' 4 ) ? / 3 #(' 1 |*) 1(' 3 )</formula><formula xml:id="formula_2">H p = BERT(n p ).</formula><p>( <ref type="formula">3)</ref> The representation of the first token ("[CLS]") is denoted as the initial representation of node n p :</p><formula xml:id="formula_3">z p = H p 0 .<label>(4)</label></formula><p>The rest of the sequences H p 1:m+n are also used to represent the claim and evidence tokens: H p 1:m for the claim tokens and H p m+1:m+n for the evidence tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Edge Kernel for Evidence Propagation</head><p>The evidence propagation and per-node label prediction in KGAT are conducted by Edge Kernels, which attentively propagate information among nodes in the graph G along the edges with the kernel attention mechanism.</p><p>Specifically, KGAT calculates the node n p 's representation v p with the kernel attention mechanism, and uses it to produce the per-node claim prediction y: v p = Edge-Kernel(n p , G), P (y|n p , G) = softmaxy(Linear(v p )).</p><p>The edge kernel of KGAT conducts a hierarchical attention mechanism to propagate information between nodes. It uses token level attentions to produce node representations and sentence level attentions to propagate information along edges.</p><p>Token Level Attention. The token level attention uses kernels to get the fine-grained representation? q?p of neighbor node n q , according to node n p . The content propagation and the attention are controlled by kernels.</p><p>To get the attention weight ? q?p i for i-th token in n q , we first conduct a translation matrix M q?p between q-th node and p-th node. Each element of the translation matrix M q?p ij in M q?p is the cosine similarity of their corresponding tokens' BERT representations:</p><formula xml:id="formula_5">M q?p ij = cos(H q i , H p j ).<label>(6)</label></formula><p>Then we use K kernels to extract the matching feature K(M q?p i ) from the translation matrix M q?p <ref type="bibr" target="#b29">(Xiong et al., 2017;</ref><ref type="bibr" target="#b4">Dai et al., 2018;</ref><ref type="bibr" target="#b22">Qiao et al., 2019;</ref><ref type="bibr" target="#b16">MacAvaney et al., 2019)</ref>:</p><formula xml:id="formula_6">K(M q?p i ) = {K1(M q?p i ), ..., KK (M q?p i )}. (7)</formula><p>Each kernel K k utilizes a Gaussian kernel to extract features and summarizes the translation score to support multi-level interactions:</p><formula xml:id="formula_7">K k (M q?p i ) = log j exp(? (M q?p ij ? ? k ) 2 2? 2 k ),<label>(8)</label></formula><p>where ? k and ? k are the mean and width for the k-th kernel, which captures a certain level of interactions between the tokens <ref type="bibr" target="#b29">(Xiong et al., 2017)</ref>. Then each token's attention weight ? q?p i is calculated using a linear layer:</p><formula xml:id="formula_8">? q?p i = softmaxi(Linear( K(M q?p i ))).<label>(9)</label></formula><p>The attention weights are used to combine the token representations (? q?p ):</p><formula xml:id="formula_9">z q?p = m+n i=1 ? q?p i ? H q i ,<label>(10)</label></formula><p>which encodes the content signals to propagate from node n q to node n p . Sentence Level Attention. The sentence level attention combines neighbor node information to node representation v p . The aggregation is done by a graph attention mechanism, the same with previous work .</p><p>It first calculate the attention weight ? q?p of n q node according to the p-th node n p :</p><formula xml:id="formula_10">? q?p = softmaxq(MLP(z p ?? q?p )),<label>(11)</label></formula><p>where ? denotes the concatenate operator and z p is the initial representation of n p . Then the p-th node's representation is updated by combining the neighbor node representation? z q?p with the attention:</p><formula xml:id="formula_11">v p = ( l q=1 ? q?p ?? q?p ) ? z p .<label>(12)</label></formula><p>It updates the node representation with its neighbors, and the updated information are selected first by the token level attention (Eq. 9) and then the sentence level attention (Eq. 11). Sentence Level Claim Label Prediction. The updated p-th node representation v p is used to calculate the claim label probability P (y|n p ):</p><formula xml:id="formula_12">P (y|n p , G) = softmaxy(Linear(v p )).<label>(13)</label></formula><p>The prediction of the label probability for each node is also conditioned on the entire graph G, as the node representation is updated by gather information from its graph neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Node Kernel for Evidence Aggregation</head><p>The per-node predictions are combined by the "readout" function in graph neural networks , where KGAT uses node kernels to learn the importance of each evidence. It first uses node kernels to calculate the readout representation ?(n p ) for each node n p :</p><formula xml:id="formula_13">?(n p ) = Node-Kernel(n p ).<label>(14)</label></formula><p>Similar to the edge kernels, we first conduct a translation matrix M c?e p between the p-th claim and evidence, using their hidden state set H p 1:m and H p m+1:m+n . The kernel match features K(M c?e p i ) on the translation matrix are combined to produce the node selection representation ?(n p ):</p><formula xml:id="formula_14">?(n p ) = 1 m ? m i=1 K(M c?e p i ).<label>(15)</label></formula><p>This representation is used in the readout to calculate p-th evidence selection probability P (n p |G): P (n p |G) = softmaxp(Linear(?(n p ))).</p><p>KGAT leverages the kernels multi-level soft matching capability <ref type="bibr" target="#b29">(Xiong et al., 2017)</ref> to weight the node-level predictions in the evidence graph based on their relevance with the claim:</p><formula xml:id="formula_16">P (y|G) = l p=1 P (y|n p , G)P (n p |G).<label>(17)</label></formula><p>The whole model is trained end-to-end by minimizing the cross entropy loss:</p><formula xml:id="formula_17">L = CrossEntropy(y * , P (y|G)),<label>(18)</label></formula><p>using the ground truth verification label y * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Methodology</head><p>This section describes the dataset, evaluation metrics, baselines, and implementation details in our experiments. Dataset. A large scale public fact verification dataset FEVER <ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref> is used in our experiments. The FEVER consists of 185,455 annotated claims with 5,416,537 Wikipedia documents from the June 2017 Wikipedia dump. All claims are classified as SUPPORTS, REFUTES or NOT ENOUGH INFO by annotators. The dataset partition is kept the same with the FEVER Shared Task <ref type="bibr">(Thorne et al., 2018b)</ref> as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Evaluation Metrics. The official evaluation metrics 2 for claim verification include Label Accuracy (LA) and FEVER score. LA is a general evaluation metric, which calculates claim classification accuracy rate without considering retrieved evidence. The FEVER score considers whether one complete set of golden evidence is provided and better reflects the inference ability.</p><p>We also evaluate Golden FEVER (GFEVER) scores, which is the FEVER score but with golden evidence provided to the system, an easier setting. Precision, Recall and F1 are used to evaluate evidence sentence retrieval accuracy using the provided sentence level labels (whether the sentence is evidence or not to verify the claim).</p><p>Baselines. The baselines include top models during FEVER 1.0 task and BERT based models.</p><p>Three top models in FEVER 1.0 shared task are compared. Athene <ref type="bibr" target="#b9">(Hanselowski et al., 2018)</ref> and UNC NLP <ref type="bibr" target="#b17">(Nie et al., 2019a)</ref> utilize ESIM to encode claim evidence pairs. UCL MRG <ref type="bibr" target="#b33">(Yoneda et al., 2018)</ref> leverages Convolutional Neural Network (CNN) to encode claim and evidence. These three models aggregate evidence by attention mechanism or label aggregation component.</p><p>The BERT based models are our main baselines, they significantly outperform previous methods without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous 2 https://github.com/sheffieldnlp/ fever-scorer  work . BERT-pair and BERTconcat regard claim-evidence pair individually or concatenate all evidence together to predict claim label. GEAR utilizes a graph attention network to extract supplement information from other evidence and aggregate all evidence through an attention layer. <ref type="bibr" target="#b25">Soleimani et al. (2019)</ref>; <ref type="bibr" target="#b18">Nie et al. (2019b)</ref> are also compared in our experiments. They implement BERT sentence retrieval for a better performance. In addition, we replace kernel with dot product to implement our GAT version, which is similar to GEAR, to evaluate kernel's effectiveness. Implementation Details. The rest of this section describes our implementation details.</p><p>Document retrieval. The document retrieval step retrieves related Wikipedia pages and is kept the same with previous work <ref type="bibr" target="#b9">(Hanselowski et al., 2018;</ref><ref type="bibr" target="#b25">Soleimani et al., 2019)</ref>. For a given claim, it first utilizes the constituency parser in AllenNLP  to extract all phrases which potentially indicate entities. Then it uses these phrases as queries to find relevant Wikipedia pages through the online Me-diaWiki API 3 . Then the convinced article are reserved <ref type="bibr" target="#b9">(Hanselowski et al., 2018)</ref>.</p><p>Sentence retrieval. The sentence retrieval part focuses on selecting related sentences from retrieved pages. There are two sentence retrieval models in our experiments: ESIM based sentence retrieval and BERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work <ref type="bibr" target="#b9">(Hanselowski et al., 2018;</ref>. The base version of BERT is used to implement our BERT based sentence retrieval model. We use the "[CLS]" hidden state to represent claim and evidence sentence pair. Then a learning to rank layer is leveraged to project "[CLS]" hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work <ref type="bibr" target="#b34">(Zhao et al., 2020;</ref><ref type="bibr" target="#b31">Ye et al., 2020)</ref> also employs our BERT based sentence retrieval in their experiments.</p><p>Claim verification. During training, we set the  batch size to 4 and accumulate step to 8. All models are evaluated with LA on the development set and trained for two epochs. The training and development sets are built with golden evidence and higher ranked evidence with sentence retrieval. All claims are assigned with five pieces of evidence. The BERT (Base), BERT (Large) and RoBERTa  are evaluated in claim verification.</p><p>In our experiments, the max length is set to 130. All models are implemented with PyTorch. BERT inherits huggingface's implementation 4 . Adam optimizer is used with learning rate = 5e-5 and warm up proportion = 0.1. The kernel size is set to 21, the same as previous work <ref type="bibr" target="#b22">(Qiao et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Result</head><p>The experiments are conducted to study the performance of KGAT, its advantages on different reasoning scenarios, and the effectiveness of kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>The fact verification performances are shown in <ref type="table" target="#tab_3">Table 2</ref>. Several testing scenarios are conducted to compare KGAT effectiveness to BERT based baselines: BERT (Base) Encoder with ESIM retrieved sentences, with BERT retrieved sentences, and BERT (Large) Encoder with BERT retrieved sentences.</p><p>Compared with baseline models, KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type="bibr" target="#b9">Hanselowski et al., 2018)</ref>, KGAT outperforms the graph attention models GEAR and our GAT on both development and testing sets.  It illustrates the effectiveness of KGAT among graph based reasoning models. With BERT based sentence retrieval, our KGAT also outperforms BERT (Base) <ref type="bibr" target="#b25">(Soleimani et al., 2019)</ref> by almost 1% FEVER score, showing consistent effectiveness with different sentence retrieval models. When using BERT (Large) as the encoder, KGAT also outperforms the corresponding version of <ref type="bibr" target="#b25">Soleimani et al. (2019)</ref>. KGAT with RoBERTa performs the best compared with all previously published research on all evaluation metrics. CorefBERT <ref type="bibr" target="#b31">(Ye et al., 2020)</ref> extends our KGAT architecture and explicitly models co-referring relationship in context for better performance. The sentence retrieval performances of ESIM and BERT are compared in <ref type="table" target="#tab_5">Table 3</ref>. The BERT sentence retrieval outperforms ESIM sentence retrieval significantly, thus also helps improve KGAT's reasoning accuracy. Nevertheless, for more fair comparisons, our following experiments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on Different Scenarios</head><p>This experiment studies the effectiveness of kernel on multiple and single evidence reasoning scenarios, as well as the contribution of kernels.</p><p>The verifiable instances are separated (except instances with "NOT ENOUGH INFO" label ) into two groups according to the golden evidence labels. If more than one evidence pieces are required, the claim is considered as requiring multi-evidence reasoning. The single evidence reasoning set and the multiple evidence reasoning set contain 11,372 (85.3%) and 1,960 (14.7%) instances, respectively. We also evaluate two additional KGAT variations: KGAT-Node which only uses kernels on the node, with the edge kernels replaced by standard dotproduction attention, and KGAT-Edge which only uses kernels on the edge. The results of these systems on the two scenarios are shown in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>KGAT-Node outperforms GAT by more than 0.3% on both single and multiple reasoning sce-  narios. As expected, it does not help much on GFEVER, because the golden evidence is given and node selection is not required. It illustrates KGAT-Node mainly focuses on choosing appropriate evidence and assigning accurate combining weights in the readout. KGAT-Edge outperforms GAT by more than 0.8% and 0.1% on multiple and single evidence reasoning scenarios, respectively. Its effectiveness is mostly on combining the information from multiple evidence pieces.</p><p>The multiple and single evidence reasoning scenarios evaluate the reasoning ability from different aspects. The single evidence reasoning mainly focuses on selecting the most relevant evidence and inference with single evidence. It mainly evaluates model de-noising ability with the retrieved evidence. The multiple evidence reasoning is a harder and more complex scenario, requiring models to summarize necessary clues and reason over multiple evidence. It emphasizes to evaluate the evidence interactions for the joint reasoning. KGAT-Node shows consistent improvement on both two reasoning scenarios, which demonstrates the important role of evidence selection. KGAT-Edge, on the other hand, is more effective on multiple reasoning scenarios as the Edge Kernels help better propagate information along the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of Kernel in KGAT</head><p>This set of experiments further illustrate the influences of kernels in KGAT.</p><p>More Concentrated Attention. This experiment studies kernel attentions by their entropy, which reflects whether the learned attention weights are focused or scattered. The entropy of (a) Edge Attention.</p><p>(b) Node Attention.   <ref type="figure" target="#fig_3">Fig 4(a)</ref> shows the distribution of attention weights on evidence nodes p(n p ), sorted by their weights <ref type="figure" target="#fig_3">; Fig 4(b)</ref> evaluates the recall of selecting the golden standard evidence nodes at different depths.</p><p>the kernel attentions in KGAT, the dot-product attentions in GAT, and the uniform attentions are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The entropy of Edge attention is shown in <ref type="figure" target="#fig_2">Figure 3(a)</ref>. Both GAT and KGAT show a smaller entropy of the token attention than the uniform distribution. It illustrates that GAT and KGAT have the ability to assign more weight to some important tokens with both dot product based and kernel based attentions. Compared to the dot-product attentions in GAT, KGAT's Edge attention focuses on fewer tokens and has a smaller entropy.</p><p>The entropy of Node attentions are plotted in <ref type="figure" target="#fig_2">Figure 3</ref>(b). GAT's attentions distribute almost the same with the uniform distribution, while KGAT has concentrated Node attentions on a few evidence sentences. As shown in the next experiment, the kernel based node attentions focus on the correct evidence pieces and de-noises the retrieved sentences, which are useful for claim verification.</p><p>More Accurate Evidence Selection. This experiment evaluates the effectiveness of KGAT-Node through attention distribution and evidence recall. The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>We first obtain the node attention score in the  evidence graph from KGAT or GAT, and calculate the statistics of the maximum one for each claim, as most of which only require single evidence to verify. The attention score of the highest attended evidence node for each claim is plotted in <ref type="figure" target="#fig_3">Figure 4</ref>(a). As expected, KGAT concentrates its weight to select evidence nodes and provides a focused attention.</p><p>Then the evidence selection accuracy is evaluated by their evidence recall. We first rank all evidence pieces for each claim. Then the evidence recall with different ranking depths is plotted in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. KGAT achieves a much higher recall on top ranking positions-only the first ranked sentence covers nearly 80% of ground truth evidence, showing the node kernels' ability to select correct evidence. This also indicates the potential of the node kernels in the sentence retrieval stage, which we reserve for future work as this paper focuses on the reasoning stage.</p><p>Fine-Grained Evidence Propagation. The third analysis studies the distribution of KGAT-Edge's attention which is used to propagate the evidence clues in the evidence graph. <ref type="figure" target="#fig_5">Figure 5</ref> plots the attention weight distribution of the edge attention scores in KGAT and GAT, one from kernels and one from dot-products. The kernel attentions again are more concentrated: KGAT focuses fewer words while GAT's dot-product attentions are almost equally distributed among all words. This observation of the scattered dotproduct attention is consistent with previous research <ref type="bibr" target="#b2">(Clark et al., 2019)</ref>. As shown in the next case study, the edge kernels provide a fine-grained and intuitive attention pattern when combining evidence clues from multiple pieces.</p><p>Claim: Al Jardine is an American rhythm guitarist.</p><p>(1) [Al Jardine] Alan Charles Jardine <ref type="bibr">(born September 3, 1942)</ref> is an American musician, singer and songwriter who co-founded the Beach Boys.</p><p>(2) [Al Jardine] He is best known as the band's rhythm guitarist, and for occasionally singing lead vocals on singles such as "Help Me, Rhonda" (1965), "Then I Kissed Her" <ref type="bibr">(1965)</ref> and "Come Go with Me" <ref type="bibr">(1978)</ref>.</p><p>(3) [Al Jardine] In 2010, Jardine released his debut solo studio album, A Postcard from California. (4) [Al Jardine] In 1988, Jardine was inducted into the Rock and Roll Hall of Fame as a member of the Beach Boys. (5) [Jardine] Ray Jardine American rock climber, lightweight backpacker, inventor, author and global adventurer. Label: SUPPORT <ref type="table">Table 5</ref>: An example claim ) whose verification requires multiple pieces of evidence.</p><p>6 Case Study <ref type="table">Table 5</ref> shows the example claim used in GEAR  and the evidence sentences retrieved by ESIM, among which the first two are required evidence pieces. <ref type="figure" target="#fig_6">Figure 6</ref> presents the distribution of attentions from the first evidence to the tokens in the second evidence (? 2?1 i ) in KGAT (Edge Kernel) and GAT (dot-product).</p><p>The first evidence verifies that "Al Jardine is an American musician" but does not enough information about whether "Al Jardine is a rhythm guitarist". The edge kernels from KGAT accurately pick up the additional information evidence (1) required from evidence (2): "rhythm guitarist". It effectively fills the missing information and completes the reasoning chain. Interesting, "Al Jardine" also receives more attention, which helps to verify if the information in the second evidence is about the correct person. This kernel attention pattern is more intuitive and effective than the dot-product attention in GAT. The later one scatters almost uniformly across all tokens and hard to explain how the joint reasoning is conducted. This seems to be a common challenge of the dot-product attention in Transformers <ref type="bibr" target="#b2">(Clark et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presents KGAT, which uses kernels in Graph Neural Networks to conduct more accurate evidence selection and fine-grained joint reasoning. Our experiments show that kernels lead to the more accurate fact verification. Our studies illustrate the two kernels play different roles and contribute to different aspects crucial for fact verification. While the dot-product attentions are rather scattered and hard to explain, the kernel-based attentions show intuitive and effective attention patterns: the node kernels focus more on the correct evidence pieces; the edge kernels accurately gather the necessary information from one node to the other to complete the reasoning chain. In the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An Example of Fact Verification System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>KGAT Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Attention Weight Entropy on Evidence Graph, from KGAT and GAT, of graph edges and nodes. Uniform weights' entropy is also shown for comparison. Less entropy shows more concentrated attention.(a) Attention Distribution.(b) Evidence Recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Evidence Selection Effectiveness of KGAT and GAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The Attention Weight Distribution from GAT and KGAT on evidence sentence tokens. Top 10% tokens are presented. The rest follows standard long tail distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Edge Attention Weights on Evidence Tokens. Darker red indicates higher attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of FEVER Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Fact Verification Accuracy. The performances of top models during FEVER 1.0 shared task and BERT based models with different scenarios are presented.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evidence Sentence Retrieval Accuracy. Sentence level Precision, Recall and F1 are evaluated by official evaluation<ref type="bibr" target="#b26">(Thorne et al., 2018a)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Claim Verification Accuracy on Claims that</cell></row><row><cell>requires Multiple and Single evidence Pieces. Stan-</cell></row><row><cell>dard GAT with no kernel (GAT), with only node ker-</cell></row><row><cell>nel (KGAT-Node), with only edge kernel (KGAT-Edge)</cell></row><row><cell>and the full model (KGAT-Full) are compared.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/ competitions/18814</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.mediawiki.org/wiki/API: Main_page</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/huggingface/ pytorch-transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is jointly supported by the NSFC project under the grant no. 61661146007, the funds of Beijing Advanced Innovation Center for Language Resources (No. TYZ19005), and the NExT++ project, the National Research Foundation, Prime Minister's Office, Singapore under its IRC@Singapore Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3159652.3159659</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
		<meeting>Workshop for NLP Open Source Software (NLP-OSS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dr-bilstm: Dependent reading bidirectional LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983769</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UKP-athene: Multi-sentence textual entailment for claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Several experiments on investigating pretraining and knowledgeenhanced models for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12104</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">QED: A fact verification system for the fever shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Luken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanjiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="156" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CEDR: contextualized embeddings for document ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331317</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6859" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<title level="m">Understanding the behaviors of bert in ranking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Technical report</title>
		<meeting>Technical report</meeting>
		<imprint>
			<publisher>OpenAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">BERT for evidence retrieval and claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Soleimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Worring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02655</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Christos Christodoulopoulos, and Arpit Mittal. 2018b. The fact extraction and verification (FEVER) shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end neural adhoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080809</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06870</idno>
		<title level="m">Coreferential reasoning learning for language representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TwoWingOS: A two-wing optimization strategy for evidential claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UCL machine reading group: Four factor framework for fact finding (HexaF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Reasoning over semantic-level graph for fact checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03745</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GEAR: Graph-based evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
