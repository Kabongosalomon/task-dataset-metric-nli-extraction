<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojt?ch</forename><surname>Van?ura</surname></persName>
							<email>vancurv@fit.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kord?k</surname></persName>
							<email>pavel.kordik@fit.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology Czech Technical University in Prague Prague</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Variational Autoencoder with Shallow Parallel Path for Top-N Recommendation (VASP)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-recommender systems, variational autoencoders,</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently introduced EASE algorithm presents a simple and elegant way, how to solve the top-N recommendation task. In this paper, we introduce Neural EASE to further improve the performance of this algorithm by incorporating techniques for training modern neural networks. Also, there is a growing interest in the recsys community to utilize variational autoencoders (VAE) for this task. We introduce deep autoencoder FLVAE benefiting from multiple non-linear layers without an information bottleneck while not overfitting towards the identity. We show how to learn FLVAE in parallel with Neural EASE and achieve the state of the art performance on the MovieLens 20M dataset and competitive results on the Netflix Prize dataset.</p><p>Index Terms-recommender systems, variational autoencoders,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the increasing amount of information on the Web, Recommender Systems (RS) are an important way to overcome infobesity. On the other hand, companies like NetFlix, Youtube, Amazon, or Google are making significant revenues from recommendations <ref type="bibr" target="#b0">1</ref> . Thus, RS are gaining more attention over the past two decades.</p><p>As online companies grow, RS have to scale to millions of active users and millions of items. Speed of training and recall are also increasingly important as available content often change dynamically and RS need to be able to react in realtime.</p><p>Proper evaluation of RS is also increasingly important topic as offline evaluation is often biased predictor of the online performance <ref type="bibr" target="#b16">[17]</ref>. For offline evaluation, the recsys community shifted towards Top-N approaches <ref type="bibr" target="#b8">[9]</ref> as evaluating the performance based on root mean squared error on top of a predicted rating matrix can be very misleading.</p><p>In the Top-N recommendation scenario <ref type="bibr" target="#b2">[3]</ref>, RS is recommending N most relevant items for every user. This is the typical case in various domains, including media, news, or e-commerce.</p><p>Various approaches have been proposed for solving the Top-N recommendation task, including collaborative filtering with matrix factorization to give an example. Recently, sparsedata autoencoders <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref> gained <ref type="bibr" target="#b0">1</ref> According to <ref type="bibr" target="#b24">[25]</ref> 80% movies on NetFlix, 60% videos on Youtube are watched based on recommendations; recommendations are responsible of 35% sale revenues on Amazon <ref type="bibr" target="#b21">[22]</ref> much attention and were providing state-of-the-art results in solving this task. We examined various proposed models, including denoising autoencoders, variational autoencoders, and shallow autoencoder called the EASE <ref type="bibr" target="#b19">[20]</ref>, which despite being a simple linear model, is providing competitive and explainable results while addressing the biggest problem with sparse autoencoders: overfitting towards identity.</p><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, our motivation was to build a RS model, that is as elegant and explainable as EASE while leveraging the potential of deep autoencoders to model complex nonlinear patterns in the data.</p><p>In order to do this, we had to overcome several issues, most importantly, the overfitting towards identity.</p><p>Traditionally, overfitting towards identity is addressed by using dropout in the input layer <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, this approach is not effective enough and is not enabling the usage of really deep architectures.</p><p>In this work, we propose three major contributions to address the issues mentioned above. We propose:</p><p>? the usage of focal loss for training autoencoders for Top-N recommendation. ? a simple yet effective data augmentation technique to prevent Top-N recommending autoencoders from overfitting towards identity. ? a joint-learning technique based on the Hadamard product for training different combinations of various models. As a demonstration, we build the VASP, a Variational Autoencoder with a Shallow parallel Path. VASP combines deep Variational Autoencoder and a neural variant of shallow EASE jointly trained together to model both linear and nonlinear patterns in the data. VASP was able to achieve state of the art performance on the MovieLens 20M dataset and competitive results on the Netflix Prize dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Matrix Factorization (MF) has been the first choice model for many years since the team "BellKor's Pragmatic Chaos" won the Netflix Prize <ref type="bibr" target="#b11">[12]</ref>.</p><p>In 2016 Sedhain et al. proposed AutoRec <ref type="bibr" target="#b17">[18]</ref>, an autoencoder-based model for collaborative filtering with explicit ratings that outperforms all current baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2102.05774v1 [cs.LG] 10 Feb 2021</head><p>After emergence of variational autoencoders (VAE), the collaborative filtering model MultVAE was proposed in 2018 by Liang et al. <ref type="bibr" target="#b12">[13]</ref> This model uses multinomial log-likelihood for data distribution.</p><p>H. Steck proposed EASE <ref type="bibr" target="#b19">[20]</ref>, the Embarrasingelly Shallow Autoencoder with no hidden layers as opposed to deep architectures. This approach was able to beat SOTA models when introduced.</p><p>Several techniques for improving the MultVAE were proposed recently. RecVAE <ref type="bibr" target="#b18">[19]</ref> uses a separate regularization term in the form of the KL divergence between the actual parameter distribution and the distribution in previous training step preventing instability during training.</p><p>H+VAMP <ref type="bibr" target="#b9">[10]</ref> implements a variational autoencoder with a variational mixture of posteriors prior (Vamp Prior) with the goal to learn better latent representations of user-items interactions.</p><p>During the evolution of recommender systems, many simple, shallow (linear, wide), and complex deep architectures have been proposed. Cheng et al. proposed a combination of those two approaches into a single framework called Wide&amp;Deep Learning <ref type="bibr" target="#b1">[2]</ref> and introduced a technique called joint training. The authors also point out the distinction between joint training and ensembling. We are inspired by this work, but our approach is quite different. We do not process item attributes, just the interactions, therefore deep path is not design to encode items but to find nonlinear interaction patterns. Also, voting scheme is different.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, the authors propose ensembling of pre-trained recommender models by variational autoencoder. However, jointlearning of such model seems to be problematic from the perspective of scaling and practical usability.</p><p>Many other deep learning techniques originally developed for computer vision or natural language processing was later successfully used in other fields such as recommender systems. Residual networks <ref type="bibr" target="#b6">[7]</ref> are good example. Another example is using the approach from <ref type="bibr" target="#b7">[8]</ref> for dense layers in RecVAE <ref type="bibr" target="#b18">[19]</ref>.</p><p>We follow this trend by adopting Focal Loss (FL) from <ref type="bibr" target="#b13">[14]</ref> for recommendation systems. This novel approach is used for imbalanced classes in object detection addressing the imbalance between the background class and other classes. That means that the loss is higher for examples in the training set that are difficult to classify. This perfectly fits the situation in collaborative filtering, where some items are more popular than others. It is more difficult to recommend niche items as they do not have many interactions. Higher loss for these items push recommender system to focus more on cold start and niche items.</p><p>Another essential idea while training an autoencoder on sparse data is to prevent overfitting towards learning identity function between the input and the output layer of the autoencoder <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, the authors proposed the Split-Brain Autoencoder, which prevents learning identity by splitting input image into two separate channels: grayscale channel X 1 and color channels X 2 . Learning is then performed in a separate way by training two networks, F 1 to perform <ref type="figure">Fig. 1</ref>. VASP Architecture automatic colorization by learning X 2 by showing X 1 and F 2 to make a grayscale prediction by learning X 2 by showing X 1 . On the other hand, our approach uses only one neural network with automated data augmentation as a preprocessing step.</p><p>III. OUR APPROACH Following notation from <ref type="bibr" target="#b12">[13]</ref>, we index users as u ? {1, ..., U }, items as i ? {1, ..., I}, and user-item interaction matrix X ? N U ?I . Lowercase x u = [x u1 , x u2 , ..., x uI ] T ? N I denotes the interaction history andx u = [x u1 ,x u2 , ...,x uI ] T ? N I predicted ratings of the user u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural EASE (NEASE)</head><p>Following <ref type="bibr" target="#b19">[20]</ref>, the EASE model can be described as:</p><formula xml:id="formula_0">x u = W ? x u ,<label>(1)</label></formula><p>where W ? R |I|?|I| is the weight matrix. Diagonal of W is constrained to zero to prevent learning identity function between the input and the output. In <ref type="bibr" target="#b19">[20]</ref> authors proposed using square loss between the data x u and the predicted scoresx u because this training objective has a closed-form solution. The authors also suggest that using more complex loss functions may lead to better prediction accuracy with higher computational costs.</p><p>To enable running the model in parallel to a deep autoencoder, we interpret the EASE model (1) as a single-layer perceptron without bias nodes and with forced zeros on the diagonal, which can be trained with any suitable loss function using backpropagation.</p><p>Our experiments with several different loss functions are described in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MultVAE with focal loss (FLVAE)</head><p>Consistently with any other variational autoencoder <ref type="bibr" target="#b10">[11]</ref>, the MultVae model's generative process starts by sampling kdimensional latent representation z u from a standard Gaussian prior <ref type="bibr" target="#b12">[13]</ref>. Then, under an assumption that interaction history x u has been drawn from a multinomial distribution, a neural network f ? (?) is used to produce a probability distribution ?(z u ) over I items:</p><formula xml:id="formula_1">z u ? N (0, I k ), ?(z u ) ? exp{f ? (z u )}, x u ? M ult(N u , ?(z u ))</formula><p>Variational autoencoder then aims to maximize the average marginal likelihood p(z u |x u ) = p(x u |z u )p(z u )dz. Since f ? (?) is a neural network, p(z u |x u ) becomes intractable and it is approximated with evidence lower bound (ELBO):</p><formula xml:id="formula_2">log p ? E q [log p(x u |z u ) ? KL(q(z u |x u )||p(z u ))] (2)</formula><p>where q(x u ; ?) is a variational approximation of the posterior distribution, p(x u ; ?) is the prior distribution, ? and ? are parameters of p(x u |z u ), log p(x u |z u ) is the log-likelihood for user u and KL is the Kullback-Leibler divergence.</p><p>FL <ref type="bibr" target="#b13">[14]</ref> is defined as</p><formula xml:id="formula_3">F L(p t ) = ?? t (1 ? p t ) ? log(p t ),<label>(3)</label></formula><p>where p t is:</p><formula xml:id="formula_4">p t = x ui if x ui = 1 1 ?x ui otherwise<label>(4)</label></formula><p>and ? t , ? act as hyperparameters.</p><p>Since maximising log-likelihood is the same as minimising cross-entropy and focal loss can be understood as a form of weighted cross-entropy, ELBO (2) can be easily rewritten:</p><formula xml:id="formula_5">log p ?E q [? t (1 ? p(x u |z u )) ? log p(x u |z u )? ?KL(q(z u |x u )||p(z u ))]<label>(5)</label></formula><p>C. VASP</p><p>Recommender model m can be expressed as a function m(?) : x u ?x u . If m uses a sigmoid function on the output, it's obvious thatx uI ?&lt; 0, 1 &gt;. We propose joint-learning with Hadamard product <ref type="bibr" target="#b14">[15]</ref>, denoted for combining any number n, n ? N of recommender models m n as</p><formula xml:id="formula_6">m n (x u ) = n j=1 m j = m 1 (x u ) m 2 (x u ) ... m n (x u )<label>(6)</label></formula><p>since m n (x u ) =x nu and x u is the same for all models in combination, (6) can be directly rewritten as m n (x u ) =x nu = n j=1x ju <ref type="bibr" target="#b6">(7)</ref> whilex nu ? 0, 1 .</p><p>While in Wide &amp; Deep <ref type="bibr" target="#b1">[2]</ref>, networks are combined with the summation (logical OR), in VASP we use the Hadamard product (logical AND), meaning that both networks have to agree. In <ref type="bibr" target="#b1">[2]</ref>, activation of a single network is sufficient for positive output.</p><p>Proposed VASP architecture ( <ref type="figure">Fig. 1.)</ref> uses combination of two models, NEASE and FLVAE ensembled by element-wise multiplication <ref type="formula">(7)</ref>:</p><formula xml:id="formula_8">m V ASP (x u ) = m F LV AE (x u ) m EASE (x u ) =x F LV AEu x N EASEu<label>(9)</label></formula><p>To satisfy condition <ref type="bibr" target="#b7">(8)</ref> we add sigmoid function to <ref type="bibr" target="#b0">(1)</ref>:</p><formula xml:id="formula_9">x N EASEu = ?(W ? x u )<label>(10)</label></formula><p>Since m F LV AE and m EASE are both fully differentiable, m V ASP is also fully differentiable and backpropagation can be used to optimize the m V ASP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data augmentation to prevent learning identity</head><p>Inspired by <ref type="bibr" target="#b23">[24]</ref> we prevent learning identity by spliting the input interactions x u before every training epoch randomly into two parts, x Au and x Bu so:  </p><formula xml:id="formula_10">x Aui = 0 if x ui = 0 1 ? x Bui otherwise x Bui = 0 if x ui = 0 1 ? x Aui otherwise (11) while I i=1 x Aui ? I i=1 x Bui<label>(12)</label></formula><p>The autoencoder is learning x Bui by showing x Aui in one training step and than x Aui by showing x Bui in another. Thus autoencoder is still seeing all the data, but does not see the identity and cannot learn it (as demonstrated by <ref type="figure" target="#fig_0">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTERPRETING VASP</head><p>In order to analyze the effect of shallow and deep components of the proposed ensemble model, we have implemented a workflow to produce a visualization of movie embeddings learned by individual models and the ensemble.</p><p>The principle was the following. We performed a sensitivity analysis of models by putting one-hot vectors to the input and generating output probabilities (reconstructions). These probabilities were then transformed to distances by linear scaling and then projected into a t-SNE plot. See <ref type="figure" target="#fig_1">Figure 3</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETUP</head><p>To verify our assumtions by experiments, we implemented three models: neural variant of EASE, deep Variational Autoencoder, and then VASP: joint learning model consisting of NEASE and deep Variational Autoencoder ensembled by the Hadamard product as described in chapter III-C. We trained those models on two datasets: MovieLens20M and Netflix prize dataset, and compared results over various baselines, including current SOTA models.</p><p>A. Datasets 1) MovieLens20M <ref type="bibr" target="#b5">[6]</ref>: Dataset of 27000 movies rated by 138,000 users generating 20 million ratings in total. We preprocessed the dataset according to <ref type="bibr" target="#b12">[13]</ref>: Since this dataset contains explicit ratings, we converted the data to implicit interactions by considering valid interaction only rating of four or higher 2 . Only users with five or more interactions remain in the dataset after preprocessing. We randomly choose 10000 users as a test set and train our models on the rest.</p><p>2) Netflix Prize Dataset <ref type="bibr" target="#b0">[1]</ref>: Dataset from Netflix prize -over 100 million ratings from 480000 randomly-chosen, anonymous Netflix customers over 17000 movie titles. We converted explicit ratings to implicit interactions by the same method as we used on the MovieLens20M. We randomly choose 40000 users as a test set and train our models on the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>We evaluate our models in the same way as in <ref type="bibr" target="#b12">[13]</ref>. First, we sample 80% of the test user's interactions as input for the model, and then we measure Recall@k and N CDG@k for predicted interactions against the remaining 20% of the user's interactions.</p><p>N CDG for TOP-k recommended items, denoted N CDG@k is defined as</p><formula xml:id="formula_11">N DCG@k = DCG@k IDCG@k<label>(13)</label></formula><p>where</p><formula xml:id="formula_12">DCG@k = k i=1 2 reli ? 1 log 2 (i + 1)<label>(14)</label></formula><p>and <ref type="bibr" target="#b1">2</ref> Note that we use implicit interactions because it is prevalent case for practical recommendation tasks</p><formula xml:id="formula_13">IDCG@k = |R k | i=1 2 reli ? 1 log 2 (i + 1)<label>(15)</label></formula><p>rel i is relevance of the recommendation at position i and R k is the list of those 20% interactions acting as "true" user interactions.</p><p>Recall for TOP-k recommended items, denoted Recall@k is defined as</p><formula xml:id="formula_14">Recall@k = |R k ? R k | R k<label>(16)</label></formula><p>whereR k is top-k items recommended by evaluated model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines</head><p>We chose several autoencoder-based models for a topn recommendation, including MultDAE and MultVAE from <ref type="bibr" target="#b12">[13]</ref>, EASE from <ref type="bibr" target="#b19">[20]</ref> and current SOTA models, RecVAE <ref type="bibr" target="#b18">[19]</ref> and H+VAMP <ref type="bibr" target="#b9">[10]</ref> as baselines for performance evaluation.</p><p>D. Implemented models 1) Neural EASE: was implemented as a dense layer with forced zeros on diagonal by kernel constraint. We evaluate three different loss functions: mean squared error, cosine proximity loss and focal loss. Since EASE has forced zeros on diagonal in parametrs matrix to prevent learning identity, no data augmentation was used.</p><p>2) Variational Autoencoder: we implemented FLVAE with densely connected residual network both in encoder and decoder. Sigmoid activation was used on the output.</p><p>Data augmentation described in section III-D was used to prevent autoencoder in learning identity.</p><p>Default values of ? t = 0.25 and ? = 2.0 in (3) was used.</p><p>3) VASP: is build by connecting neural EASE and FLVAE by the Hadamard product as described in III-C. Hyperparameters was the same as for plain FLVAE. We evaluated three variants: pre-trained EASE and FLVAE joined together as ensemble, jointly training form the start and alternating approach where FLVAE and EASE was training in every step separatedly. We prevent our model to learn identity by using data augmentation described in III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hyperparameters</head><p>We used 2048 units for latent space and 4096 units for hidden layers. We used seven densely connected residual hidden layers for the encoder and five layers of the same architecture in the decoder. We trained our model for 50 epochs with a learning rate of 0.00005 and batches of 1024 samples. Then, we lower the learning rate to 0.00001 and trained for another 20 epochs. Then we performed finetuning with a learning rate of 0.000001 for another 20 epochs.</p><p>All models was implemented in Tensorflow <ref type="bibr" target="#b4">[5]</ref> and the source code with notes for reproducing the results is publicly available on our GitHub page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>Neural EASE: We evaluated three different variants of the model based on the loss function used -mean squared error, cosine loss, and focal loss (see <ref type="table" target="#tab_0">Table VI</ref>). We found that using cosine proximity loss leads to better performance of the model as authors of <ref type="bibr" target="#b19">[20]</ref> expected. FLVAE: Authors of MultVAE reject deeper architectures by stating that "going deeper does not improve performance." We investigated the matter and believed that overfitting towards identity was to blame. We address this issue by adopting data augmentation described in the section III-D. This approach successfully allowed us to build much bigger models than in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref> or <ref type="bibr" target="#b9">[10]</ref> where 200 units for latent space and 600 in hidden layers was used.</p><p>VASP: We evaluated three methods of training models connected by the Hadamard product (see <ref type="table" target="#tab_0">Table VI</ref>). First, we connected pre-trained models and evaluated them as an ensemble. Then we initialized the joint model and trained it from scratch. Lastly, we experimented with the alternating approach, where in one step was frozen weights of FLVAE, and in the next step, we weights of the EASE model were frozen instead. However, this approach did not perform better than joint-learning from the start.</p><p>Finally, we have compared our base models NEASE, FLVAE and jointy learned ensemble VASP to state of the art approaches (see <ref type="table" target="#tab_0">Table VI</ref>). Significantly best performing models are in bold. Our VASP outperformed other models and achieved the SOTA for MovieLens 20M dataset. It also performed quite well for the Netflix dataset (second highest ranking model).</p><p>In our future experiments, we will carefully analyse the H+Vamp Gated model that performed better on Netflix. We will try to put it in the ensemble with the Neural EASE or use the idea of Variational Mixture of Posteriors to improve performance of our deep FLVAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We proved EASE to be a compelling Top-N recommendation model that can still match current SOTA baselines. We proposed a data augmentation method to prevent overfitting to identity and experimentally proved that using this method leads to better performance of autoencoders used for top-n recommendation.</p><p>We proposed a novel joint-learning technique for training multiple models together. Using that we constructed VASP, Variational Autoencoder with parallel Shalow Path and experimentally proved, that variational autoencoder connected with parallel simple shallow linear model can match current sophisticated SOTA models and even outperform them in some cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Data augmentation to prevent overfitting towards identity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Explaining VASP on MovieLens20M Dataset: Output of the joint model (left) was linearly decomposed to EASE component (middle) and FLVAE component (right) to demonstrate that EASE is learning more apparent linear dependencies and FLVAE the non-linear ones. Red = Horrors, Blue = Children movies, Green = Western movies and Yellow = Noir.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>WITH DIFFERENT LOSS FUNCTIONS FOR THE EASE MODEL ON MOVIELENS20M DATASET.</figDesc><table><row><cell cols="4">Loss function used NCDG@100 Recall@20 Recall@50</cell></row><row><cell>MSE</cell><cell>0.425</cell><cell>0.393</cell><cell>0.523</cell></row><row><cell>Cosine proximity</cell><cell>0.431</cell><cell>0.403</cell><cell>0.532</cell></row><row><cell>Focal loss</cell><cell>0.377</cell><cell>0.343</cell><cell>0.426</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>WITH DIFFERENT TRAINING APPROACH FOR THE VASP MODEL ON MOVIELENS20M DATASET.</figDesc><table><row><cell>Training approach</cell><cell cols="3">NCDG@100 Recall@20 Recall@50</cell></row><row><cell>Pretrained ensemble</cell><cell>0.442</cell><cell>0.414</cell><cell>0.545</cell></row><row><cell>Alternating training</cell><cell>0.436</cell><cell>0.401</cell><cell>0.543</cell></row><row><cell>Joint learning</cell><cell>0.448</cell><cell>0.414</cell><cell>0.552</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">RESULTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MovieLens 20M</cell><cell></cell><cell></cell><cell>Netflix Prize Dataset</cell><cell></cell></row><row><cell></cell><cell cols="6">NCDG@100 Recall@20 Recall@50 NCDG@100 Recall@20 Recall@50</cell></row><row><cell>Mult-DAE</cell><cell>0.419</cell><cell>0.387</cell><cell>0.524</cell><cell>0.380</cell><cell>0.344</cell><cell>0.438</cell></row><row><cell>Mult-VAE</cell><cell>0.426</cell><cell>0.395</cell><cell>0.537</cell><cell>0.386</cell><cell>0.351</cell><cell>0.444</cell></row><row><cell>EASE</cell><cell>0.420</cell><cell>0.391</cell><cell>0.521</cell><cell>0.393</cell><cell>0.362</cell><cell>0.445</cell></row><row><cell>RecVAE</cell><cell>0.442</cell><cell>0.414</cell><cell>0.553</cell><cell>0.394</cell><cell>0.361</cell><cell>0.452</cell></row><row><cell>H+Vamp Gated</cell><cell>0.445</cell><cell>0.413</cell><cell>0.551</cell><cell>0.409</cell><cell>0.376</cell><cell>0.463</cell></row><row><cell>Neural EASE</cell><cell>0.431</cell><cell>0.403</cell><cell>0.532</cell><cell>0.395</cell><cell>0.363</cell><cell>0.447</cell></row><row><cell>FLVAE</cell><cell>0.445</cell><cell>0.409</cell><cell>0.547</cell><cell>0.398</cell><cell>0.363</cell><cell>0.450</cell></row><row><cell>VASP</cell><cell>0.448</cell><cell>0.414</cell><cell>0.552</cell><cell>0.406</cell><cell>0.372</cell><cell>0.457</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Our research has been supported by the Grant Agency of the Czech Technical University in Prague (SGS20/213/OHK3/3T/18), the Czech Science Foundation (GA?R 18-18080S), Recombee and VUSTE-APIS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
		<title level="m">The Netflix Prize. KDD Cup and Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys 2017 -Proceedings of the 11th ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="396" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Performance of recommender algorithms on top-N recommendation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Turrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys&apos;10 -Proceedings of the 4th ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensvae: Ensemble variational autoencoders for recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahlem</forename><surname>Drif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hocine</forename><surname>Houssem Eddine Zerrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cherifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="188335" to="188351" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12-770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation of item-based top-n recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing VAEs for collaborative filtering: Flexible priors &amp; gating mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys 2019 -13th ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2018 -Proceedings of the World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Tsung Yi Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Elizabeth Million. The hadamard product. Creative commons</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Sparse autoencoder. CS294A Lecture notes</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing offline and online evaluation results of recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Rehorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kordik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the REVEAL workshop at RecSyS conference (RecSyS&apos;18)</title>
		<meeting>the REVEAL workshop at RecSyS conference (RecSyS&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autorec</surname></persName>
		</author>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web -WWW &apos;15 Companion</title>
		<meeting>the 24th International Conference on World Wide Web -WWW &apos;15 Companion<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RecVAE: A new variational autoencoder for top-n recommendations with implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Shenbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Alekseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2020 -Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<imprint>
			<publisher>Association for Computing Machinery, Inc</publisher>
			<date type="published" when="2020-01" />
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Embarrassingly shallow autoencoders for sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2019 -Proceedings of the World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoders that don&apos;t overfit towards the identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Matrix and Tensor Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zioupos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
