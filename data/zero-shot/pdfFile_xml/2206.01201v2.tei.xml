<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanze</forename><surname>Lin</surname></persName>
							<email>yuanze@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Xie</surname></persName>
							<email>yujiaxie@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Dongdong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">?</forename></persName>
							<email>dochen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Chenguang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper revisits visual representation in knowledge-based visual question answering (VQA) and demonstrates that using regional information in a better way can significantly improve the performance. While visual representation is extensively studied in traditional VQA, it is under-explored in knowledge-based VQA even though these two tasks share the common spirit, i.e., rely on visual input to answer the question. Specifically, we observe that in most state-of-the-art knowledge-based VQA methods: 1) visual features are extracted either from the whole image or in a sliding window manner for retrieving knowledge, and the important relationship within/among object regions is neglected; 2) visual features are not well utilized in the final answering model, which is counter-intuitive to some extent. Based on these observations, we propose a new knowledge-based VQA method REVIVE, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. The key motivation is that object regions and inherent relationship are important for knowledge-based VQA. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous state-of-the-art method by a large margin (+3.6%). We also conduct detailed analysis and show the necessity of regional information in different framework components for knowledge-based VQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many vision-based decision making processes in our daily life go beyond perception and recognition. For example, if we see a salad bowl in the deli bar, our decision on whether to buy it does not only depend on what is in the bowl, but also the calories in each of the item. This motivates the knowledge-based Visual Question Answering (VQA) task <ref type="bibr" target="#b22">[23]</ref>, which extends traditional VQA task <ref type="bibr" target="#b1">[2]</ref> to solve more complex problems, i.e., where commonsense knowledge is required to answer the open-domain questions.</p><p>By definition, knowledge-based VQA takes three different information sources to predict the answer: input visual information (image), input question, and the external knowledge. While existing research on knowledge-based VQA mainly focuses on improving the incorporation of external knowledge, this paper focuses on improving the object-centric visual representation and presents a comprehensive empirical study to demonstrate that visual features matter in this task.</p><p>Intuitively, visual information should be well used for both knowledge retrieval and final answering. However, we find existing state-of-the-art (SOTA) methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8]</ref> in such domain have not fully utilized it. On the one hand, they simply use either the whole image or a sliding window on the image to retrieve the external knowledge. On the other hand, they ignore the essential visual information (i.e., object-centric representations) in the final answering model. In other words, they fuse only the retrieved knowledge and the question as a pure natural language processing (NLP) model to obtain the answer, a typical method <ref type="bibr" target="#b7">[8]</ref> is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (b).</p><p>In this paper, we revisit visual representation in knowledge-based VQA, and argue that the information of object regions and their relationship should be considered and used in a dedicated way. The underlying motivation is shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>, which demonstrates that understanding the objects and their relationship is necessary. To this end, we propose REVIVE to better utilize REgional VIsual Representation for knowledge-based Visual quEstion answering. It not only exploits the detailed regional information for better knowledge retrieval, but also fuses the regional visual representation into the final answering model. Specifically, we first use the object detector GLIP <ref type="bibr" target="#b16">[17]</ref> to locate the objects, and then use the cropped region proposals to retrieve different types of external knowledge. Finally, we integrate the knowledge together with the regional visual features into a unified transformer based answering model for final answer generation.</p><p>We perform extensive experiments on the OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>, and the proposed REVIVE achieves the SOTA performance of 58.0% accuracy, a 3.6% absolute improvement from the results of previous SOTA method <ref type="bibr" target="#b7">[8]</ref>.</p><p>We summarize our contribution as follows:</p><p>(a) We systematically explore how to better exploit the visual feature to retrieve knowledge.</p><p>The empirical results suggest the region-based approach performs the best, compared to whole image-based and sliding window-based approaches. (b) We integrate the regional visual representation, retrieved external and implicit knowledge into a transformer-based question answering model, which can effectively leverage the three information sources for solving knowledge-based VQA. (c) Our proposed REVIVE achieves the state-of-the-art performance on OK-VQA dataset, i.e., 58.0% accuracy, surpassing the previous methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge-Based VQA. Knowledge-based VQA <ref type="bibr" target="#b22">[23]</ref> aims to predict answers for general questions by leveraging external knowledge beyond image content. Early works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref> introduce external knowledge to solve visual question answering (VQA) tasks. OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref> is the first largescale dataset with questions that need be answered using external knowledge instead of a provided fixed knowledge base <ref type="bibr" target="#b34">[35]</ref>. Recent studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8]</ref> integrate different knowledge from various external knowledge resources, e.g., ConceptNet <ref type="bibr" target="#b29">[30]</ref>, Wikipedia <ref type="bibr" target="#b32">[33]</ref>, etc, for solving knowledge-based VQA. Later, PICa <ref type="bibr" target="#b38">[39]</ref> regards large language models, e.g., GPT-3 <ref type="bibr" target="#b2">[3]</ref> as an implicit knowledge source and employs it <ref type="bibr" target="#b2">[3]</ref> to get answer prediction based on textual prompts. Inspired by the recent success of knowledge-retrieved methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> that leverage <ref type="figure">Figure 2</ref>: The illustration of REVIVE. It exploits regional information (i.e., features, positions and tags), question and context to retrieve different types of knowledge. In addition, it also incorporates learned object-centric region features with retrieved knowledge for answer generation.</p><p>external knowledge retrieval with language generative models for open-domain question answering, KAT <ref type="bibr" target="#b7">[8]</ref> exploits the FiD reader <ref type="bibr" target="#b11">[12]</ref> to perform knowledge reasoning over retrieved implicit and explicit knowledge. Our work instead emphasizes revisiting the visual representation for knowledge retrieval, i.e., resorting to regional visual representation. In addition, we propose to incorporate objectcentric regional visual representation together with retrieved knowledge in the answer generative model. Several works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref> have incorporated visual embeddings or captions in predicting the final answers. However, these works target at different settings and they haven't fully explored how to better use regional representations to retrieve knowledge.</p><p>Vision-Language Models. Recent years have witnessed the rapid development of vision-language models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. Those works usually first pre-train a neural network on a large-scale image-text dataset and then finetune the models for solving specific vision-language tasks. Among them, VinVL <ref type="bibr" target="#b40">[41]</ref> aims to learn the object-centric representation. CLIP <ref type="bibr" target="#b25">[26]</ref> pre-trains the models with large-scale text-image pairs by contrastive learning. GLIP <ref type="bibr" target="#b39">[40]</ref> reformulates the pre-training process by unifying object detection and phrase grounding. Our method uses the three models as sub-modules to identify object-centric regions and retrieve knowledge for knowledge-based VQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Knowledge-based VQA task <ref type="bibr" target="#b22">[23]</ref> seeks to answer questions based on external knowledge beyond images. Specifically, let us denote a knowledge-based VQA dataset as</p><formula xml:id="formula_0">D = {(I i , Q i , A i )} N i=1</formula><p>, where I i , Q i and A i denote the input image, question and answer of the i-th sample respectively, and N is the number of total samples. Given the dataset, the goal is to train a model with parameter ? to generate the answer A i with input I i and Q i .</p><p>In this section, we introduce our method REVIVE. <ref type="figure">Figure 2</ref> shows an overview of the method. We leverage the detected regions of the input image to obtain the object-centric region features and retrieve explicit knowledge. Meanwhile, we prompt GPT-3 [3] by regional tags, question and context to retrieve implicit knowledge. After that, the regional visual features, retrieved knowledge, and the text prompt consists of regional tags, question and context will then be fused into a encoder-decoder module to generate the answer. We explain more details in Section 3.1, 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regional Feature Extraction Module</head><p>Given an image I, we first adopt a object detector to give us the positions of region proposals,</p><formula xml:id="formula_1">B = {b j } M j=1 = D(I),<label>(1)</label></formula><p>where B = {b j } M j=1 is the set of bouding boxes, M is the number of detected boxes, and D(?) is the object detector.</p><p>Here, we adopt D(?) as the visual grounding model GLIP <ref type="bibr" target="#b16">[17]</ref>. We use the text prompt "Detect: person, bicycle, car, ..., toothbrush", which contains all object categories of MSCOCO dataset <ref type="bibr" target="#b18">[19]</ref>. In this way, the model can provide us with all bounding boxes associated with those categories.</p><p>After we get the bounding boxes B of interested objects from GLIP, We crop the image I according to B to obtain region proposals R = {r j } M j=1 . We then extract the object-centric visual features from the proposals: v j = E(r j ), where v j ? R S is the visual embedding of the j-th proposal, S is the embedding dimension and E(?) represents the image encoder. Inspired by the strong transferring capability of recent contrastively trained vision-language models, we adopt the visual encoder of CLIP <ref type="bibr" target="#b25">[26]</ref> as our image encoder E(?). We use the encoding of [CLS] token as the final embedding.</p><p>To understand the relationship between/among the objects, we find it also important to introduce the position information B along with its regional visual features.</p><p>In addition to the embeddings, explicitly obtaining the description of each region proposal in the textual format is also helpful for knowledge retrieval. For the contrastively trained vision-language models, the training loss explicitly encourages inner product between the image embedding and the text embedding to be larger if the image and the text are well-aligned. Therefore, such a model is capable of selecting the tags that describe the image from a set of customized tagsT by computing the inner product. Denote the language encoder of CLIP as T (?). Given a set of tagsT = {t i } N1 i=1 , N 1 is the number of total tags, we compute the inner product between the region proposals and all tags, and adopt the tags with the top-P similarities as the description of the region proposals,</p><formula xml:id="formula_2">H = {h p } P p=1 = arg TopP ti?T E(r j ), T (t i ) , j = 1, ? ? ? , M,<label>(2)</label></formula><p>where ?, ? is the inner product, P denotes the number of the obtained regional tags and H means the retrieved regional tags.</p><p>In complement to the localized textual description H, we adopt a caption model to explicitly describe the relationships between the major objects and provide more context,</p><formula xml:id="formula_3">c = C(I),<label>(3)</label></formula><p>where C(?) is the caption model. For example, in <ref type="figure">Figure 2</ref>, the context "Two brown dogs fighting over a red frisbee" provides us with the essential relationships between the objects, e.g., fighting over a red frisbee. Here, we adopt Vinvl <ref type="bibr" target="#b40">[41]</ref> as the caption model C(?).</p><p>In summary, we extract regional visual and positional information as {v j } M j=1 and {b j } M j=1 , and textual descriptions for the objects and the relationship between the objects as H and c. In the next section, we will elaborate on how we use these regional information sources to retrieve external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object-Centric Knowledge Retrieval Module</head><p>Inspired by KAT <ref type="bibr" target="#b7">[8]</ref>, we consider both the explicit knowledge and implicit knowledge. But different from it, we utilize regional visual information to help boost the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Explicit Regional Knowledge</head><p>Since the questions from knowledge-based VQA <ref type="bibr" target="#b22">[23]</ref> are general and open-ended, introducing external knowledge is important for model to generate accurate answers by providing extra and complementary knowledge beyond visual contents of input images.</p><p>External Knowledge Base. We construct an external knowledge base Q by constructing a subset from Wikidata <ref type="bibr" target="#b32">[33]</ref> following KAT <ref type="bibr" target="#b7">[8]</ref>. Specifically, we extract 8 commonly appeared categories, i.e., Role, Point of interest, Tool, Vehicle, Animal, Clothing, Company, Sport, to form the subset Q. Each item in Q consists of an entity and a corresponding description, e.g., one entity and its description can be "pegboard" and "board wall covering with regularly-spaced holes for insertion of pegs or hooks" respectively.</p><p>Regional Knowledge Retrieval. As mentioned earlier, vision-language models like CLIP are capable of selecting the most relevant text from a set of texts. We reformat the entries in knowledge base Q as "{entity} is a {description}", and denote the reformatted text set as T . We retrieve the top-K most relevant knowledge entries among all the regional proposals as explicit knowledge E,</p><formula xml:id="formula_4">E = {e k } K k=1 = arg TopK di?T E(r j ), T (d i ) , j = 1, ? ? ? , M,<label>(4)</label></formula><p>where K denotes the number of retrieved explicit knowledge samples. In our implementation, we use FAISS <ref type="bibr" target="#b13">[14]</ref> to speed up the computation of Equation <ref type="formula" target="#formula_2">(2)</ref> and (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Implicit Knowledge with Regional Descriptions</head><p>Large language models, e.g., GPT-3 <ref type="bibr" target="#b2">[3]</ref>, not only excel in many language tasks, but also memorize lots of commonsense knowledge from its training corpus <ref type="bibr" target="#b38">[39]</ref>. Therefore, we exploit GPT-3 <ref type="bibr" target="#b2">[3]</ref> as our implicit knowledge base by reformulating the task as open-domain question answering.</p><p>Context-Aware Prompt with Regional Descriptions. We design the textual prompt based on question Q, caption c, and tags H. Different from PICa <ref type="bibr" target="#b38">[39]</ref> and KAT <ref type="bibr" target="#b7">[8]</ref> that use whole-image feature to get the tags, we utilize fine-grained regional features to extract regional tags. Specifically, we adopt the prompt X to be "context: {caption} + {tags}. question: {question}". In this way, the language model is also supplemented with regional visual information.</p><p>Implicit Knowledge Retrieval. Finally, we query GPT-3 model <ref type="bibr" target="#b2">[3]</ref> which takes the reformulated prompt X as input, and obtain predictive answer. Since some of the questions may have ambiguity, we follow the prompt tuning procedure of PICa <ref type="bibr" target="#b38">[39]</ref> and get answer candidates {o u } U u=1 .</p><p>In addition to answer prediction, we also aim for acquiring corresponding explanation e u from GPT-3 model to obtain more context information. To be more specific, the corresponding explanation is acquired by feeding the text prompt "{question} {answer candidate}. This is because" into GPT-3. Note that "{question}" and "{answer candidate} are input question Q and GPT-3's answer o u for image I respectively. The final retrieved implicit knowledge can be denoted as</p><formula xml:id="formula_5">I = {(o u , e u )} U u=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoder-Decoder Module with Object-Centric Visual Features</head><p>Once we've retrieved the explicit and implicit knowledge and the regional information, we utilize the FiD network structure <ref type="bibr" target="#b11">[12]</ref> to encode and decode retrieved knowledge and regional information.</p><p>Knowledge Encoder. For the explicit knowledge, we reformat the input text as "entity: {entity} description: {description}", where the entity and the description is from the entries in the retrieved explicit knowledge E. We denote this text as h k , where k = 1, ? ? ? , K.</p><p>For implicit knowledge, we adopt input format as "candidate: {answer} evidence: {explana-tion}", where answer is the retrieved answer o u and explanation is e u . Here, u = 1, ? ? ? , U , where U is the number of answers provided by GPT-3. We denote the input text as s u .</p><p>We then encode the knowledge in textual format by the FiD's encoder <ref type="bibr" target="#b31">[32]</ref>, which is denoted as F e ,</p><formula xml:id="formula_6">? k = F e (h k ), ? u = F e (s u ),<label>(5)</label></formula><p>in which ? k ? R D , ? u ? R D and D means the embedding dimension.</p><p>Regional Visual Encoder. We introduce a visual encoder for the regional visual embeddings {v j } M j=1 and positional coordinates {b j } M j=1 . We feed v j and b j into two different fully connected layers, stack the outputs into a sequence of vectors, and then feed them into a transformer encoder F v ,</p><formula xml:id="formula_7">f = F v (Concat(FC 1 (v 1 ), FC 2 (b 1 ), ? ? ? , FC 1 (v M ), FC 2 (b M ))),<label>(6)</label></formula><p>where f ? R (2M )?D , FC 1 (?) and FC 2 (?) are two different fully-connected layers, Concat(?) is the concatenation operation along a new dimension.</p><p>Context-aware Question Encoder. To better leverage the context information, we replace the input question Q by the context-aware prompt X, we then encode it by the same transformer encoder F e (?),</p><formula xml:id="formula_8">q = F e (X),<label>(7)</label></formula><p>where q ? R D and q means encoded context-aware question.</p><p>Generative Decoder. We have obtained the knowledge encoding {? k } K k=1 and {? u } U u=1 , visual encoding f , and context-aware question encoding q. Note that as the outputs of the encoder F e , they are all sequences of vectors. We then concatenate these vectors along the first dimension, and feed them into the FiD's decoder F d , y = F d (Concat(? 1 , ? ? ? , ? K , ? 1 , ? ? ? , ? U , f, q)), (8) where y means the generated answer. The cross entropy loss function is adopted to train the model,</p><formula xml:id="formula_9">L = ? L =1 log p ? (? |y &lt; ),<label>(9)</label></formula><p>in which L is the length of the ground truth answer text,? is ground truth text at the position and ? is the model parameters.</p><p>Model Ensemble. To generate more accurate answers, one promising method is to leverage multiple trained models, i.e., model ensemble. In our experiments, we just train three models whose initialized seeds are different, and then the most frequent result among the generated results from these three models is selected as final answer prediction for each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relationship to Existing Works</head><p>Inspired by KAT <ref type="bibr" target="#b7">[8]</ref>, REVIVE also retrieves two types of knowledge, i.e., implicit and explicit knowledge. Different from KAT, we explore how to better use visual features to retrieve knowledge. Motivated by the fact that the retrieved knowledge should also corresponds to individual concepts in the images in addition to the global theme, we use extracted regional features to retrieve external knowledge, and use regional descriptions to obtain the implicit knowledge. Moreover, we integrate the visual representation of object regions with retrieved knowledge in the answer generative model. The pipeline differences between KAT <ref type="bibr" target="#b7">[8]</ref> and our method can be explained in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>There're two works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b21">22]</ref> that leverage visual regions for knowledge-based VQA as well. However, MAVEx <ref type="bibr" target="#b37">[38]</ref> considers object regions as a kind of knowledge without using their visual representation to retrieve other knowledge, KRISP <ref type="bibr" target="#b21">[22]</ref> utilizes object regions to learn implicit knowledge by a transformer-based model and retrieve external knowledge by the text symbols of these regions, while our proposed REVIVE explores how to better leverage visual representation to retrieve knowledge and integrate their visual features with retrieved knowledge into the answering model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref> is selected for evaluation, which is currently the largest knowledgebased VQA dataset. OK-VQA dataset includes 14055 questions associated with 14031 images from MSCOCO dataset <ref type="bibr" target="#b18">[19]</ref>. Its questions cover a variety of knowledge categories, and are annotated by Amazon Mechanical Turkers. The training and testing split consist of 9009 and 5046 samples respectively. Each data sample is made up of one question, one corresponding image and 10 groundtruth answers. To construct the general domain tag setT , we collect the most frequently searched 400K queries in Bing Search as the tags.</p><p>Pre-processing. We utilize the pre-trained visual grounding model GLIP-T <ref type="bibr" target="#b16">[17]</ref> to detect objectcentric region proposals by using its default prompt "Detect: person, bicycle, car, ..., toothbrush", which contains all object categories of MS-COCO dataset <ref type="bibr" target="#b18">[19]</ref>. The captions of images are obtained by the pre-trained Vinvl-Large model <ref type="bibr" target="#b40">[41]</ref>. For explicit knowledge and regional tag retrieval, we choose CLIP model (ViT-B/16 variant) <ref type="bibr" target="#b25">[26]</ref>. In our experiments, we adopt U , K, M and P as 5, 40, 36 and 30 respectively. Note that the models of CLIP, GLIP, Vinvl and GPT-3 are all frozen during usage.</p><p>Implementation Details. We use 4 ? NVIDIA V100 32Gb to train models for 10K steps, with a batch size of 8. The learning rate is 8e ?5 and AdamW <ref type="bibr" target="#b19">[20]</ref> is chosen as optimizer. The warm-up steps are 1K and the trained models are evaluated every 500 steps. We initialize our model with the pre-trained T5 model <ref type="bibr" target="#b26">[27]</ref>, i.e., T5-large, following KAT <ref type="bibr" target="#b7">[8]</ref>. The encoder F v in Equation <ref type="formula" target="#formula_7">(6)</ref> consists of 9 transformer layers <ref type="bibr" target="#b31">[32]</ref>. Note that we evaluate the prediction results after normalization, and the normalization process mainly includes removing articles, punctuation and duplicated whitespace and lowercasing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Evaluation Metric. In our experiments, we choose the soft accuracy of VQAv2 <ref type="bibr" target="#b1">[2]</ref> as evaluation metric for comparison.  <ref type="bibr" target="#b38">[39]</ref> Frozen GPT-3 (175B) 43.3 PICa-Full <ref type="bibr" target="#b38">[39]</ref> Frozen GPT-3 (175B) 48.0 KAT (Single) <ref type="bibr" target="#b7">[8]</ref> Wikidata+Frozen GPT-3 (175B) 53.1 KAT (Ensemble) <ref type="bibr" target="#b7">[8]</ref> Wikidata+Frozen GPT-3 (175B) 54.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REVIVE (Single)</head><p>Wikidata+Frozen GPT-3 (175B) 56.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REVIVE (Ensemble)</head><p>Wikidata+Frozen GPT-3 (175B) 58.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art Methods</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can see that previous works (e.g., KRISP <ref type="bibr" target="#b21">[22]</ref>, Visual Retriever-Reader <ref type="bibr" target="#b20">[21]</ref> and MAVEx <ref type="bibr" target="#b37">[38]</ref>) achieve similar performances, about 38.4% to 39.4% accuracy. Until recently, PICa <ref type="bibr" target="#b38">[39]</ref> is the first one that exploits the pre-trained language model GPT-3 <ref type="bibr" target="#b2">[3]</ref> as knowledge base for knowledge-based VQA task and KAT <ref type="bibr" target="#b7">[8]</ref> further introduces Wikidata <ref type="bibr" target="#b32">[33]</ref> as an external knowledge resource, these two works obtain significant performances compared with previous ones.</p><p>The proposed REVIVE can outperform all existing methods by large margins. Specifically, even using the same knowledge resources (i.e., Wikidata <ref type="bibr" target="#b32">[33]</ref> and GPT-3 <ref type="bibr" target="#b2">[3]</ref>), our single model can achieve 56.6% accuracy versus previous state-of-the-art method KAT's 53.1% accuracy, when using model ensemble, our method can achieve 58.0% accuracy compared with KAT's 54.4% accuracy. These results demonstrate the effectiveness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Next, we conduct extensive ablation studies on the single model to figure out the influence of each component of REVIVE.</p><p>Effect of Region Proposal Number. We perform the ablation study to figure out the effect of using different region proposal numbers. The results are displayed in <ref type="table" target="#tab_1">Table 2</ref>. It can be observed that when the region proposal number is 36, the model achieves optimal performance. We conjecture that when the number of region proposals is too large, there are some meaningless and noisy region proposals, while if the number of region proposals is too small, many essential object-centric regions are ignored, which both hurt the model's performance.</p><p>Different Knowledge Retrieval Methods. The way of utilizing visual representation for retrieving knowledge plays an important role in knowledge-based VQA. We show the results of using three kinds of knowledge retrieval methods, i.e., image-based, sliding window-based and region-based, in <ref type="table" target="#tab_4">Table 5</ref>. Note that sliding window-based approach follows KAT <ref type="bibr" target="#b7">[8]</ref>. Specifically, we first resizes input images to 384 ? 384 and then crop the images with a sliding window whose size is 256 ? 256 and stride size is 128. We can observe that the proposed region-based approach achieves best performance and surpasses sliding window-based method by 1.8% points, which can validate the effectiveness of exploiting region-based visual representation for retrieving knowledge. Effect of Regional Tag Number. In order to introduce more semantics into contexts, we propose to add region-aware descriptions (i.e., regional tags) behind given contexts. We report the results of using different regional tag number for text prompt X in <ref type="table" target="#tab_2">Table 3</ref>. The results show that when the    number of regional tags is 30, it achieves optimal performances. In fact, when the number of regional tags is too large, we'll retrieve relatively irrelevant object tags, sacrificing the model's performance.</p><p>Effect of Positional Coordinates. In addition to incorporating visual representation of objectcentric region proposals into the model, we also adopt the position information (i.e., positional coordinates). The results of whether using positional coordinates are reported in <ref type="table" target="#tab_3">Table 4</ref>. Introducing regional coordinates can improve the performance by 0.8% points.</p><p>Effect of Each Component. Finally, we showcase the results of using different components of REVIVE in <ref type="table">Table 6</ref>. We can observe that the introduced components can consistently improve the model's performance. Especially for knowledge retrieval, using the regional descriptions can improve the performance of implicit knowledge by 1.2%, while adopting the regional features can boost the performance of explicit knowledge retrieval by 1.1%.</p><p>The object-centric region features can achieve 1.4% points improvement, feeding context-aware questions, which can be denoted as prompt "context: {caption}. question: {question}", into the answer generative model attain 0.5% points gain, further introducing regional descriptions (i.e.., regional tags) into contexts, i.e., prompt X, has 0.7% points improvement. These results can validate the efficiencies of our proposed components. <ref type="table">Table 6</ref>: Ablation study on different components of REVIVE. Note that "Imp." and "R-Imp." mean implicit knowledge retrieved without and with the proposed regional descriptions, "Exp." and "R-Exp." mean explicit knowledge retrieved without and with regional features, "Visual" represents object-centric region features, "Context" and "Tag" mean introducing the contexts and regional descriptions (i.e. tags) into the final answering model respectively. "Acc." means accuracy. Imp. R-Imp. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Result Analysis</head><p>Finally, we present the quantitative results and provide analysis for error cases, so that we can have a clear insight into the proposed approach.</p><p>Visualizing Results. The success cases of our approach are shown in <ref type="figure">Figure 3</ref>. We can observe that our approach can accurately retrieve implicit and explicit knowledge, which corresponds to the detected object regions, and deal with the relationship among these object areas. For example, in <ref type="figure">Figure 3</ref>: Representative success cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>. "Q", "C", "A" and "GT" denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. "Acc." means accuracy. <ref type="figure">Figure 4</ref>: Representative failure cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>.</p><p>the left example of <ref type="figure">Figure 3</ref>, our method can recognize the potential referred objects and retrieve useful knowledge (e.g., cheeseburger and cheese), thus generating the correct answer, while in the right example, our method can also retrieve important knowledge (e.g., brazilian terrier) to answer the breed of the referred dog.</p><p>Failure Cases Analysis. We showcase the failure examples in <ref type="figure">Figure 4</ref>. As shown in the left example, even though the prediction result Cabin doesn't appear in the ground truth answers, the generated answer of our approach is still reasonable for such scenario. For the right example, our predicted result is wrong due to the difficulty of answering such a general question. From <ref type="figure">figure 4</ref>, we can also observe that our method can generate useful object-centric regions and accurately retrieve corresponding knowledge, especially explicit knowledge, which can demonstrate the potential of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Broader Impact</head><p>The quality of constructed Wikidata subset and designed textual prompt can influence final retrieved knowledge. In addition, the detector for obtaining region proposals also affect retrieved knowledge and visual features, all these factors affect the models' performances.</p><p>This paper proposes a novel approach REVIVE for knowledge-based VQA. REVIVE can help models to efficiently use visual and language information sources to answer open-domain questions. It can also generalize to real-life products, e.g., dialogue robot. However, the failure cases of REVIVE will be negative to the society when using it as the educational technique. There may also exist certain forms of bias, i.e., the model may predict biased answers if the training data of knowledge-based VQA contain certain bias. For example, <ref type="bibr" target="#b0">[1]</ref> suggests the model may be driven by superficial correlations in the training data, and <ref type="bibr" target="#b9">[10]</ref> shows the VQA datasets may contain gender and racial bias that may cause the models to learn harmful stereotypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview</head><p>In the supplementary materials, we provide the following sections:</p><p>(a) Implementation details of implicit knowledge retrieval in Section B.</p><p>(b) Ablation study experiments in Section C.</p><p>(c) Visualization results in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details of Implicit Knowledge Retrieval</head><p>We first describe more implementation details of implicit knowledge retrieval of the proposed REVIVE. Specifically, we explain how we extract multiple answer candidates.</p><p>Multiple Candidates. We retrieve multiple implicit knowledge candidates for each sample during training and inference stages to improve the robustness of answer generation. Specifically, we follow PICa <ref type="bibr" target="#b38">[39]</ref>, which proposes to use multi-query ensemble, i.e., they prompt the GPT-3 <ref type="bibr" target="#b2">[3]</ref> for k 1 times and choose the one with the highest probability as final answer prediction. Compared with PICa's multi-query ensemble approach, we take all these k 1 predictions from GPT-3 <ref type="bibr" target="#b2">[3]</ref> as implicit knowledge candidates. Note that for each candidate, we also prompt the GPT-3 model to obtain its corresponding explanation. In our experiments, we just retrieve 5 (i.e., k 1 = 5) implicit knowledge candidates and corresponding explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Study</head><p>Next, we conduct more ablation study experiments to provide deeper insight into the components of our proposed REVIVE.</p><p>The effect of multiple implicit knowledge candidates. To validate the influence of the number of retrieved implicit knowledge candidate on the model's performance, we report the results in <ref type="table" target="#tab_6">Table  7</ref>. When using only one implicit knowledge candidate, the model can achieve 55.8% accuracy, after taking 5 implicit knowledge candidates, the performance can be improved to 56.6% accuracy. However, when the retrieved candidate number is 8, we can see that the performance isn't the best, we conjecture that it's enough to include essential candidates when k 1 = 5. Due to certain incorrect answer predictions by GPT-3, larger k 1 may introduce incorrect and unnecessary candidates, thus hurting the model's performance by using too much noisy and misleading knowledge.</p><p>The effect of explicit knowledge number. Since the number of retrieved explicit knowledge samples can have an effect on the model's performance, we conduct the experiments and show the results in <ref type="table" target="#tab_7">Table 8</ref>. We find the model can achieve optimal performance when k 2 = 40. It's reasonable to see that a too large k 2 (i.e., k 2 = 50) cannot let the model achieve optimal performance, since when k 2 increases, there will exist certain retrieved explicit knowledge samples which have relatively low confidences, thus introducing unreliable knowledge and hurting the model's performance.</p><p>The effect of using different detectors. To figure out the effect of choosing different object detectors on the final performances, we show the results of using Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> and GLIP <ref type="bibr" target="#b16">[17]</ref> in <ref type="table">Table 9</ref>. We can see that Faster R-CNN with ResNet-50 and ResNet-101 as the backbone can achieve 55.3% and 55.6% accuracy respectively, and using the GLIP as the object detector can achieve the optimal performance (i.e., 56.6%). These results demonstrate the accuracy of detecting object regions play an important role in the final performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization Results</head><p>Finally, we showcase more visualization cases in <ref type="figure">Figure 5</ref>, 6, 7, 8 and 9. In <ref type="figure">Figure 5</ref>, using the proposed regional descriptions/tags, we can retrieve more accurate implicit knowledge. Taking the top example of <ref type="figure">Figure 5</ref> for explanation, without introducing the informative regional descriptions (e.g., "sunlight" and "sun"), we cannot generate the correct implicit knowledge candidate "Sun", since the "Lamp" is also reasonable when given the question and context, which can demonstrate the effectiveness of using the regional descriptions for implicit knowledge retrieval.   <ref type="table">Table 9</ref>: Ablation study on using different object detectors. Note that Faster R-CNN (R50) and Faster R-CNN (R101) mean using ResNet-50 <ref type="bibr" target="#b8">[9]</ref> and ResNet-101 <ref type="bibr" target="#b8">[9]</ref> as backbones. Detector Accuracy (%)</p><p>Faster R-CNN (R50) 55.3 Faster R-CNN (R101) 55.6 GLIP 56.6 <ref type="figure">Figure 5</ref>: The implicit knowledge retrieval visualization results without and with the proposed regional descriptions/tags. Note that "Imp." and "R-Imp." mean the implicit knowledge retrieved without and with the regional descriptions/tags. "Regional Tags" represents the proposed regional descriptions. "Context" means the caption. We only use 10 regional tags for illustration.</p><p>In <ref type="figure">Figure 6</ref>, 7, 8 and 9, we can see that our proposed method can focus on important object-centric areas, and then retrieve relevant knowledge for corresponding regional areas, which can be used to generate accurate answers. These visualization results can demonstrate the effectiveness and potential of the proposed REVIVE. <ref type="figure">Figure 6</ref>: Representative visualization cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>. "Q", "C", "A" and "GT" denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. "Acc." means accuracy. <ref type="figure">Figure 7</ref>: Representative visualization cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>. "Q", "C", "A" and "GT" denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. "Acc." means accuracy. <ref type="figure">Figure 8</ref>: Representative visualization cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>. "Q", "C", "A" and "GT" denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. "Acc." means accuracy. <ref type="figure">Figure 9</ref>: Representative visualization cases of the proposed REVIVE on OK-VQA dataset <ref type="bibr" target="#b22">[23]</ref>. "Q", "C", "A" and "GT" denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. "Acc." means accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) An example from OK-VQA dataset, our method utilizes the retrieved knowledge and object-centric regions to solve the question. (b) The pipeline of previous state-of-the-art method KAT [8]. (c) The pipeline of our proposed REVIVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results comparison with existing methods on OK-VQA dataset<ref type="bibr" target="#b22">[23]</ref>, the evaluation metric (i.e., accuracy) is in %.</figDesc><table><row><cell>Method</cell><cell>Knowledge Resources</cell><cell>Accuracy (%)</cell></row><row><cell>Q only [23]</cell><cell>-</cell><cell>14.9</cell></row><row><cell>MLP [23]</cell><cell>-</cell><cell>20.7</cell></row><row><cell>BAN [23]</cell><cell>-</cell><cell>25.1</cell></row><row><cell>BAN+AN [23]</cell><cell>Wikipedia</cell><cell>25.6</cell></row><row><cell>MUTAN [23]</cell><cell>-</cell><cell>26.4</cell></row><row><cell>BAN+KG-AUG [16]</cell><cell>Wikipedia+ConceptNet</cell><cell>26.7</cell></row><row><cell>MUTAN+AN [23]</cell><cell>Wikipedia</cell><cell>27.8</cell></row><row><cell>ConceptBERT [7]</cell><cell>ConceptNet</cell><cell>33.7</cell></row><row><cell>KRISP [22]</cell><cell>Wikipedia + ConceptNet</cell><cell>38.4</cell></row><row><cell>Visual Retriever-Reader [21]</cell><cell>Google Search</cell><cell>39.2</cell></row><row><cell>MAVEx [38]</cell><cell>Wikipedia+ConceptNet+Google Images</cell><cell>39.4</cell></row><row><cell>PICa-Base</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on using different region proposal number.</figDesc><table><row><cell># of region proposals</cell><cell>Accuracy (%)</cell></row><row><cell>5</cell><cell>54.7</cell></row><row><cell>18</cell><cell>55.8</cell></row><row><cell>36</cell><cell>56.6</cell></row><row><cell>50</cell><cell>56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on using different regional tag number.</figDesc><table><row><cell># of regional tags</cell><cell>Accuracy (%)</cell></row><row><cell>8</cell><cell>56.2</cell></row><row><cell>24</cell><cell>56.4</cell></row><row><cell>30</cell><cell>56.6</cell></row><row><cell>50</cell><cell>56.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Ablation study on adopting bounding</cell></row><row><cell>box coordinates.</cell><cell></cell></row><row><cell>Positional coordinates</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell>55.8</cell></row><row><cell></cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Ablation study on adopting different meth-</cell></row><row><cell>ods for retrieving knowledge.</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Image-based</cell><cell>53.2</cell></row><row><cell>Sliding window-based</cell><cell>54.8</cell></row><row><cell>Region-based</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on using different implicit knowledge candidates. k 1 represents the number of retrieved implicit knowledge candidates.</figDesc><table><row><cell>k 1</cell><cell>Accuracy (%)</cell></row><row><cell>1</cell><cell>55.8</cell></row><row><cell>3</cell><cell>56.3</cell></row><row><cell>5</cell><cell>56.6</cell></row><row><cell>8</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on using different explicit knowledge numbers. k 2 represents the number of retrieved explicit knowledge samples.</figDesc><table><row><cell>k 2</cell><cell>Accuracy (%)</cell></row><row><cell>10</cell><cell>55.6</cell></row><row><cell>20</cell><cell>55.9</cell></row><row><cell>30</cell><cell>56.2</cell></row><row><cell>40</cell><cell>56.6</cell></row><row><cell>50</cell><cell>56.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-based video question answering with unsupervised scene descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowit vqa: Answering knowledge-based questions about videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10826" to="10834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conceptbert: Concept-aware representation for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Gard?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Ziaeefard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><surname>Baptiste Abeloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Kat: A knowledge augmented transformer for vision-and-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08614</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08148</idno>
		<title level="m">Gender and racial bias in visual question answering datasets</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pseudo-q: Generating pseudo language queries for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08481</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00300</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boosting visual question answering with contextaware knowledge aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1227" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Liunian Harold Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03857</idno>
		<title level="m">Jenq-Neng Hwang, et al. Grounded language-image pre-training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Weakly-supervised visual-retrieverreader for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04014</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14111" to="14121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ok-vqa: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Out of the box: Reasoning with graph convolution nets for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medhini</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Straight to the facts: Learning knowledge base retrieval for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medhini</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="451" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kvqa: Knowledgeaware visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanket</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naganand</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8876" to="8884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ufo: A unified transformer for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10023</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fvqa: Fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2413" to="2427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Explicit knowledge-based reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02570</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Anton van den Hengel, and Anthony Dick</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-modal answer validation for knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12248</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05014</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mucko: multi-layer cross-modal knowledge reasoning for fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09073</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
