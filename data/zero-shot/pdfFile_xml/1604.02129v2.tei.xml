<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Horizon Lines in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky Lexington</orgName>
								<address>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky Lexington</orgName>
								<address>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
							<email>jacobs@cs.uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky Lexington</orgName>
								<address>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Horizon Lines in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>WORKMAN, ZHAI, JACOBS: HORIZON LINES IN THE WILD 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The horizon line is an important contextual attribute for a wide variety of image understanding tasks. As such, many methods have been proposed to estimate its location from a single image. These methods typically require the image to contain specific cues, such as vanishing points, coplanar circles, and regular textures, thus limiting their realworld applicability. We introduce a large, realistic evaluation dataset, Horizon Lines in the Wild (HLW), containing natural images with labeled horizon lines. Using this dataset, we investigate the application of convolutional neural networks for directly estimating the horizon line, without requiring any explicit geometric constraints or other special cues. An extensive evaluation shows that using our CNNs, either in isolation or in conjunction with a previous geometric approach, we achieve state-of-the-art results on the challenging HLW dataset and two existing benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image horizon line estimation is one of the most fundamental geometric problems in computer vision. Knowledge of the horizon line enables a wide variety of applications, including: image metrology <ref type="bibr" target="#b7">[8]</ref>, geometrically biased pedestrian and vehicle detection <ref type="bibr" target="#b13">[14]</ref>, and perspective correction in consumer photographs <ref type="bibr" target="#b17">[18]</ref>. Despite this demonstrated importance, progress on this task has stagnated and nearly all recent methods that focus on this problem make assumptions about the presence of particular geometric objects in the scene, such as vanishing points <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>, repeated textures <ref type="bibr" target="#b6">[7]</ref>, and coplanar circles <ref type="bibr" target="#b4">[5]</ref>. Existing benchmark datasets for single image horizon line estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> were created to evaluate methods that use the orthogonal vanishing point cue, contributing to this stagnation.</p><p>We introduce a new benchmark dataset, Horizon Lines in the Wild (HLW), containing real-world images with labeled horizon lines. Our dataset is significantly larger and more diverse than existing benchmark datasets for horizon line detection. Instead of focusing on a particular geometric cue, we take a learning-based approach and propose to use a deep convolutional neural network (CNN) to directly estimate the horizon line. The resulting network implicitly combines both geometric and semantic cues, makes no explicit assumptions about the contents of the underlying scene, and is several orders of magnitude faster than current state-of-the-art methods which focus on vanishing points. c 2016. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1604.02129v2 [cs.CV] 16 Aug 2016</p><p>Recent work using learning-based methods for horizon line estimation has been limited, with three notable exceptions. Fefilatyev et al. <ref type="bibr" target="#b10">[11]</ref> proposed to segment the sky and then detect the horizon line in the resulting binary mask. This approach is limited to when the horizon line is visible, such as from a boat on the ocean on a clear day. Ahmad et al. <ref type="bibr" target="#b1">[2]</ref> proposed a segmentation approach to estimate the location of the skyline, a closely related, but distinct, problem. Zhai et al. <ref type="bibr" target="#b27">[28]</ref> use a CNN as a prior over likely horizon line locations, but they focus on the vanishing point cue. We propose to use a CNN to directly estimate the horizon line location. However, we show that by using our CNN as context for their method, replacing the one they proposed, significantly improves performance for vanishing point based horizon line estimation. Extensive experiments demonstrate that our CNN-based approach is fast, requiring only milliseconds per image, and accurate, achieving state-of-theart performance on two popular datasets designed to showcase purely geometric methods, and the challenging HLW dataset.</p><p>Our main contributions are: 1) a novel approach for using structure from motion to automatically label images with a horizon line, 2) a large evaluation dataset of images with labeled horizon lines, 3) a CNN-based approach for directly estimating the horizon line in a single image, and 4) an extensive evaluation of a variety of CNN design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Horizon Line: Geometric Definition</head><p>The image location of the horizon line is defined as the projection of the line at infinity for any plane which is orthogonal to the local gravity vector. The gravity vector often coincides with the local ground plane surface normal, but not always. This is distinct from the problem of detecting the skyline, which is the set of points where the sky and the ground meet.</p><p>A camera is defined by its extrinsic and intrinsic parameters. A point in the world, X i , is related to a pixel, p ci , in a camera, c, as follows:</p><formula xml:id="formula_0">[u ci , v ci , 1] T = p ci ? K c (R c X i + t c ),<label>(1)</label></formula><p>where R c is the camera orientation, t c is the camera translation, and K c is the intrinsic calibration. For our camera coordinates we assume that the positive x-direction is to the right, the positive y-direction is up, and the viewing direction is down the negative z-axis. Using this parameterization, the world viewing direction of our camera is R T c [0, 0, ?1] T . Assuming that the world vector [0, 1, 0] T points in the zenith direction, the horizon line in our image is defined as the set of pixels, p, where</p><formula xml:id="formula_1">p T K ?T c R c [0, 1, 0] T = 0.<label>(2)</label></formula><p>If the intrinsic calibration, K c , of the camera is known, then the horizon line provides a sufficient set of constraints to estimate the camera tilt and roll in world coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A New Dataset for Horizon Line Detection</head><p>We introduce Horizon Lines in the Wild (HLW), a large dataset of real-world images with labeled horizon lines, captured in a diverse set of environments. The dataset is available for download at our project website <ref type="bibr" target="#b0">[1]</ref>. We begin by characterizing limitations in existing datasets for evaluating horizon line detection methods and then describe our approach for leveraging structure from motion to automatically label images with horizon lines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Limitations of Existing Datasets</head><p>There are two main datasets that have been used in recent work on estimating horizon lines: the Eurasian Cities Dataset <ref type="bibr" target="#b2">[3]</ref> (ECD) and the older York Urban Dataset <ref type="bibr" target="#b8">[9]</ref> (YUD). We argue that these datasets have outlived their usefulness. They are too small and do not reflect the diversity of environments in which real-world horizon line detection methods must work. ECD is the predominant benchmark dataset used for evaluating automatic vanishing point detection algorithms. It consists of 103 outdoor images captured in large urban areas, many of which do not satisfy the Manhattan world assumption <ref type="bibr" target="#b5">[6]</ref>, i.e., that most lines correspond to one of three mutually orthogonal directions, one of which is up. Of these images, the first 25 are used for model fitting, with the remainder used for testing. Of the 78 testing images, a majority are considered quite easy. Due to a combination of the few number of testing images and the small number of challenging images, the difference in performance between various methods often depends on a single image. The older YUD dataset is similarly small (102 images, first 25 for model fitting) and is seen as too easy because the images are captured in a confined area with a single camera, there are relatively fewer outlier line segments, the scenes satisfy the Manhattan world assumption, and there is no camera roll.</p><p>To obtain ground truth horizon lines for ECD and YUD, a manual process akin to the following was used: identify families of parallel line segments, estimate a vanishing point for each, and compute the horizon line from the horizontal vanishing points using a least squares fit. This process is slow, error prone, and severely limits the diversity of scenes. As Lezama et al. <ref type="bibr" target="#b18">[19]</ref> note, there is even a duplicated testing image in ECD, with each instance having a different ground truth horizon line.</p><p>It is our belief that the limitations of these datasets have caused useful progress in this research area to stagnate. Recent state-of-the-art methods are quite slow, which is reasonable when you have a small testing dataset. For example, we find that the approach of Lezama et al. <ref type="bibr" target="#b18">[19]</ref> requires approximately 30 seconds per image on YUD and 1 minute per image on ECD (results obtained using code made available by the authors). These methods have also focused on a particular processing pipeline: detect line segments, find vanishing points, then globally optimize to find a consistent scene interpretation. The reliance on vanishing points limits these methods to regions with many man-made structures. There is clearly a need for a larger and more diverse dataset for evaluating horizon line estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Leveraging Structure from Motion</head><p>We introduce a novel technique for automatically labeling images with horizon lines using structure from motion (SfM), which we then employ to generate a large evaluation dataset. Kendall et al. <ref type="bibr" target="#b16">[17]</ref> used a similar strategy to generate a dataset to evaluate a CNN-based method for camera relocalization. Their work focused on learning a scene-specific CNN, whereas our goal is a scene-agnostic CNN that does not require scene-specific training data. The output of SfM is the extrinsic and intrinsic camera parameters for a subset of the input images. Typically these images are downloaded from photo-sharing websites, such as Flickr, around major landmarks. The extrinsic coordinates output by SfM algorithms typically have an unknown global orientation and translation. Since our focus is the horizon line, we just need to estimate the global up direction (the yaw of the reconstruction is irrelevant to our needs). A commonly used approach to estimate the global orientation is to average the image 'up' directions in world coordinates. The implicit assumption of this approach is that the expected tilt and roll of a camera is zero. While this works well in many cases, it fails in scenes with a single dominant landmark that is viewed from one direction (e.g., Notre Dame in Paris). In practice, we found that we get more reliable world zenith direction estimates if we instead only assume that the expected roll of a camera is zero. For a given set of images, we solve for the world direction of the points at infinity in the left, [?1, 0, 0], and right, [1, 0, 0], directions. Given a set of these points, we use singular value decomposition to estimate a basis for the horizon plane ( <ref type="figure" target="#fig_1">Figure 2</ref>), ignoring images that are rotated by 90 degrees (using reconstruction error).</p><p>Starting from 185 high-quality SfM models in the 1DSfM <ref type="bibr" target="#b23">[24]</ref>, Landmarks <ref type="bibr" target="#b19">[20]</ref>, and YFCC100M <ref type="bibr" target="#b12">[13]</ref> datasets, we filtered out anomalous images, fit and manually validated a global horizon line for each model, and then projected the horizon line back into each image. The resulting dataset, HLW, contains 100 553 images. From each 1DSfM model, we hold out 100 images at random, including holding out two models completely, resulting in 2 018 images to be used for evaluation. We hold out 525 training images for validation (approximately 3 from each model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Augmenting using Street-Side Imagery</head><p>The SfM models are mostly of tourist landmarks, which are usually in urban areas. Images of more natural areas, such as Mount Rushmore, Stonehenge, and the Grand Canyon, are included. However, the dataset contains few, if any, images of many scene types, including: forests, crop fields, industrial parks, and residential streets. To reduce this bias, we aug- ment our training dataset with rectilinear cutouts extracted from equirectangular street-side imagery panoramas (via Google Street View). We first use the SfM models to learn a plausible distribution of camera focal length (equivalently field of view), tilt, and roll. We model focal length using a normal distribution. We find that the camera roll is well modeled by Student's t-distribution (v = 2.43). For camera tilt, we use a kernel density estimate (Epanechnikov kernel, ? = .003). Camera yaw is sampled uniformly at random. Starting from 50 000 panoramas, sampled from the continental US and 93 metropolitan areas around the world, we generate 500 000 training images by randomly sampling square cutouts based on the learned distributions.  Even when considering this small set of images, there is clearly much greater diversity of scene types in HLW (e.g., zoomed in view of a statue, elevated view of a city). The scenes in ECD consist primarily of urban images with large buildings in the background. HLW also has a wider and much more densely sampled distribution of horizon line locations than ECD or YUD. We represent the horizon line as ? = x cos ? + y sin ? , where ? is the perpendicular distance from the origin to the horizon line and ? is the angle the horizon line makes with the horizontal axis. <ref type="figure" target="#fig_2">Figure 3</ref> shows the joint distribution over ? (x-axis) and ? (y-axis) for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Comparisons with Existing Datasets</head><p>We evaluated the recent state-ofthe-art method by Lezama et al. <ref type="bibr" target="#b18">[19]</ref> on HLW. The standard error metric used for horizon line detection is the maximum distance from the detection to the ground truth in image space, normalized by the height of the image, which we refer to as horizon detection error. This is often reported for a set of images as the area under the curve of the cumulative histogram of errors (AUC). Barinova et al. <ref type="bibr" target="#b2">[3]</ref> motivate the use of horizon detection error as the standard accuracy measure for automatic vanishing point detection algorithms. <ref type="figure" target="#fig_4">Figure 4</ref> visualizes the result. The large relative performance difference compared to other benchmarks highlights the challenging nature of the HLW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Direct Horizon Line Estimation</head><p>We propose to use convolutional neural networks (CNNs) to estimate the location of the horizon line from raw pixel intensities. This approach is fast and does not require extensive manual tuning of parameters. Importantly, the computational cost only depends on the size of the image, not the content of the scene, such as the number of line segments. Our work explores design and implementation choices which have a significant impact on the accuracy of the resulting model, including: target label space, weight initialization, and objective function.</p><p>For all experiments we use the GoogleNet architecture <ref type="bibr" target="#b20">[21]</ref> because it achieves similar accuracy to other architectures we tested, but with many fewer parameters. Our CNNs expect the input images to have a fixed size and a square aspect ratio. For non-square images, we extract a maximal square center crop and, optionally, a dense grid if using an aggregation strategy (Section 3.3). We experimented with reshaping the image to be square, but the resulting networks were far less accurate. This result is in line with previous work <ref type="bibr" target="#b24">[25]</ref> showing that maintaining aspect ratio is important when estimating camera focal length, which is a closely related geometric task.</p><p>We consider two parameterizations of the horizon line: 1) slope/offset, (? , ?), where ? is the perpendicular distance from the origin to the horizon line and ? is the angle the horizon line makes with the x-axis of the image and 2) left/right, (l, r), where l is the vertical offset at which the horizon line intersects the left side of the image, r is similarly defined. We represent ?, l, and r in units of image heights. The remainder of this section describes two CNN variants for predicting the horizon line location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Approach</head><p>As most existing work has applied CNNs for classification tasks, we initially frame horizon line estimation as a classification problem. The primary benefit of such a formulation is that the output of a CNN trained for a one-of-many classification task is a probability distribution over the categories; in our case a distribution over possible horizon lines in the image. For each parameter we generate N = 100 bins by linearly interpolating the cumulative distribution function of that parameter over the training data. Additionally for slope, ? , we force the bin edges to be symmetric.</p><p>Our process for adapting the GoogleNet architecture is as follows: 1) duplicate each softmax classifier (a fully connected layer followed by a multinomial logistic loss, where real-valued predictions are first passed through a softmax function to get a probability distribution over classes) to occur once for each parameter and then 2) modify the fully connected layer for each softmax classifier to output a N-dimensional vector corresponding to the N bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regression Approach</head><p>Regression using deep CNNs is widely seen as more challenging than classification due to difficulties in controlling the optimization process and handling outliers. Despite this, recent work has proposed to use deep CNNs for regression tasks, including: pose estimation <ref type="bibr" target="#b22">[23]</ref>, camera relocalization <ref type="bibr" target="#b16">[17]</ref>, and depth estimation <ref type="bibr" target="#b9">[10]</ref>. As discussed by Belagiannis et al. <ref type="bibr" target="#b3">[4]</ref>, optimization is typically performed using the L 2 loss, but outliers reduce the generalization ability of the network and increase the convergence time. Girshick <ref type="bibr" target="#b11">[12]</ref> note that if the regression targets are unbounded, training with the L 2 loss can require careful parameter tuning to prevent exploding gradients.</p><p>For our regression networks we minimize the Huber loss <ref type="bibr" target="#b14">[15]</ref>, a robust loss function that is less sensitive to outliers:</p><formula xml:id="formula_2">L(x) = 1 2 x 2 for |x| ? ? , ? (|x| ? 1 2 ? ) otherwise.<label>(3)</label></formula><p>For this work, we set ? = 1. To adapt the GoogleNet architecture for regressing the horizon line, we replace each softmax classifier with a regressor (once for each parameter) and modify the corresponding fully connected layer to output a scalar.</p><p>Our results show that optimization using the Huber loss results in more accurate predictions than using the L 2 loss. However, using only a regression objective did not perform as well as a classification objective. To address this, we investigated two initialization strategies: 1) initializing from the weights of a previously trained classification network, and 2) jointly optimizing a classification and regression network, with shared weights, where the softmax classifiers act as a form of regularization. We find that using both strategies, we can significantly improve performance and reduce convergence time, even when using the L 2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregating Estimates Across Subwindows</head><p>When applied to classification problems, the standard procedure for processing an image through a CNN is to extract multiple subwindows, feed each through the network separately, and average the predictions. This strategy is applicable to the problem of object recognition, where the target label is shared across subwindows. For horizon line estimation, each subwindow has a unique target label (as the horizon line position changes). Therefore this strategy is insufficient.</p><p>We propose two strategies for aggregating estimates: 1) projecting the horizon line from the subwindow to the full-size image and averaging in image space (weighted by the confidence in each estimate), and 2) optimizing for the horizon line in the full image that is maximally likely in all subwindows. For the latter, we assume that each subwindow is independent and minimize the negative log-likelihood,</p><formula xml:id="formula_3">E = ? 1 N N ? i=1 log(W (I i ; ?)),<label>(4)</label></formula><p>where W is a function that maps the global horizon line, ?, into the coordinate frame for subwindow I i , and extracts the probability. Our results show that both strategies improve accuracy relative to using only a center crop, but the averaging strategy is faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted an extensive evaluation of our proposed techniques, which use convolutional neural networks for horizon line estimation, on the HLW, YUD, and ECD datasets. By using our networks, either in isolation or in conjunction with a previous method, we achieve state-of-the-art results on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We implemented the proposed networks using the Caffe <ref type="bibr" target="#b15">[16]</ref> deep learning toolbox. Sample code, including models and solver settings, is available on the project website <ref type="bibr" target="#b0">[1]</ref>. We trained each network using stochastic gradient descent with a step learning rate policy, a mini-batch size of 40, for 125 000 iterations (approximately 35 epochs). We set the base learning rates to 10 ?3 and 10 ?5 for classification and regression, respectively, decreasing by an order of magnitude every 25 000 iterations (when training from scratch, we use the GoogleNet quick solver <ref type="bibr" target="#b15">[16]</ref>). We kept a snapshot every 1 000 iterations, selecting the snapshot that minimizes horizon error on the HLW validation set. The input image size for all of our networks is 224 ? 224. We combined the HLW and street-side imagery to form a training set. For the HLW imagery, we performed data augmentation by randomly mirroring the image horizontally with 50% probability and sampling a square crop (minimum side length 85% of the smallest image dimension). We extracted ten crops from each image, adjusting the horizon line for each. Since the street-side imagery was already square with randomly sampled camera orientations, we just scaled to the input size of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>When training a deep CNN, it is common practice to start optimization from the weights of a previously trained network <ref type="bibr" target="#b26">[27]</ref> and "fine-tune" (updating the weights of randomly initialized layers more). We apply this strategy, in conjunction with our methods outlined in Section 3, starting from a large number of pretrained CNNs. In all cases, we take advantage of models made publicly available by the authors. The accuracy of each network on several datasets can be seen in in <ref type="table" target="#tab_0">Table 1</ref>, where the leftmost column represents which network was used as initialization. We consider several different initializations: a network trained for object recognition (ImageNet <ref type="bibr" target="#b15">[16]</ref>), a network trained for scene categorization (Places <ref type="bibr" target="#b29">[30]</ref>), a network trained for camera relocalization (PoseNet-Street <ref type="bibr" target="#b16">[17]</ref>), and a network trained for salient object detection (Salient <ref type="bibr" target="#b28">[29]</ref>).</p><p>As in Section 2.4, we compute horizon detection error and report the area under the curve of the cumulative histogram of errors. For classification, all networks have competitive performance on HLW, but the choice of initialization is significant and we found the (? , ?) parameterization to be superior. Our best network on HLW achieves 69.97% AUC using this parameterization and was initialized using Places (we refer to this network as 'Best' in the remainder of the table). Overall performance is lower on the test imagery from the held out models (held), compared to the full set (all). This result is consistent with recent work on scene-specific camera relocalization <ref type="bibr" target="#b16">[17]</ref> demonstrating the capability of a CNN to preserve pose information.</p><p>It proved more challenging to obtain good results for the regression task. Fine-tuning performed much worse than classification, for both loss functions, likely requiring further  <ref type="bibr" target="#b18">[19]</ref> 51.32% 52.59% 89.57% Zhai et al. <ref type="bibr" target="#b27">[28]</ref> 57.33% 58.24% 90.80% manual parameter tweaking. Despite this, we found the (l, r) parameterization to be superior, and the Huber loss to be significantly better than the L 2 loss. Applying the strategies outlined in Section 3.2, namely initializing from the weights of the best classification network and regularizing training with softmax classifiers, significantly improves the performance of our networks, making them competitive with classification. Qualitative results from our approach are shown in <ref type="figure" target="#fig_5">Figure 5</ref> for four ECD images. Finally, using our best classification network we evaluate the subwindow aggregation methods from Section 3.3. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. In addition to a standard center crop, we extract a 3 ? 3 grid of crops (each 99% of the minimum dimension), chosen empirically using the HLW validation set. We saw no benefit from using smaller crop sizes, as are commonly used for semantic image classification. Both averaging in image space (average) and optimizing across subwindows (optimize) significantly improve performance  To highlight the ability of our networks, we update the recent state-of-the-art method by Zhai et al. <ref type="bibr" target="#b27">[28]</ref>, which uses a CNN to provide global context for vanishing point estimation, to use our best classification network (using code provided by the authors). With this change, we improve performance on HLW and advance the state-of-the-art results on both the ECD and YUD datasets ( <ref type="table" target="#tab_1">Table 2</ref>). For ECD, our relative improvement in AUC is 5.3%. For YUD, our relative improvement is 13.0%, where Zhai et al. <ref type="bibr" target="#b27">[28]</ref> previously reported a relative improvement of 5.0%. Despite the limitations of these two benchmark datasets, these are significant performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced Horizon Lines in the Wild (HLW), a new dataset for single image horizon line estimation, to address the limitations of existing horizon line detection datasets. HLW is several orders of magnitude larger than any existing dataset for horizon line detection, has a much wider variety of scenes and camera perspectives, and wasn't constructed to highlight the value of any particular geometric cue. Our hope is that it will continue to drive advances on this important problem in the future.</p><p>Using HLW, we investigated methods for directly estimating the horizon line using convolutional neural networks, including both classification and regression formulations. Our methods are appealing because there is no need to make explicit geometric assumptions on the contents of the underlying scene, unlike virtually all existing methods, and we can simultaneously take advantage of both geometric and semantic cues that are present in the image. Despite this generality, the performance of our methods is competitive, achieving state-of-the-art results on two existing benchmark datasets designed for geometric methods, and outperforming all existing methods on the challenging real-world imagery contained in HLW. Our method is fast, works in natural environments, and can provide a prior over horizon line location that can be used as input to other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Montages highlighting the diversity of perspectives and scenes in HLW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Using a SfM model to estimate the horizon line. (a) Each point represents the left/right direction of an image in world coordinates (blue = outlier). Two vectors represent the estimated horizon plane. (b) The horizon line projected into one image from the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distribution of horizon lines for images in HLW versus other benchmark datasets (red = higher likelihood). The x-axis is slope and the y-axis is vertical offset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label></label><figDesc>Lezama et al. (YUD): 94.07% Lezama et al. (ECD): 89.57% Lezama et al. (HLW): 52.59%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Evaluating the recent state-of-the-art method by Lezama et al. [19] on HLW. The fraction of images (y-axis) with a horizon error less than a threshold (x-axis). The AUC is shown in the legend. A montage of sample images from HLW and ECD are shown in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Example results showing the estimated distribution over horizon lines. For each image, the ground truth horizon line (dash green) and the predicted horizon line (magenta) are shown. A false-color overlay (red = more likely, transparent = less likely) shows the estimated distribution over the point on the horizon line closest to the image center. over a network evaluated on the center crop alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of our networks on HLW and ECD. Classification ImageNet Softmax 64.49% 62.10% 69.02% 67.08% 82.28% 82.99% Places Softmax 65.73% 59.54% 69.97% 67.38% 83.96% 80.45% PoseNet Softmax 60.49% 61.35% 61.65% 63.56% 78.36% 77.77% Salient Softmax 64.65% 62.10% 67.60% 67.25% 82.62% 80.11% Random Softmax 62.27% 56.64% 67.58% 62.75% 78.63% 77.17%</figDesc><table><row><cell></cell><cell>Loss</cell><cell cols="2">HLW (held)</cell><cell cols="2">HLW (all)</cell><cell>ECD</cell></row><row><cell></cell><cell></cell><cell>(? , ?)</cell><cell>(l, r)</cell><cell>(? , ?)</cell><cell>(l, r)</cell><cell>(? , ?)</cell><cell>(l, r)</cell></row><row><cell>Regression</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Places</cell><cell>L 2</cell><cell cols="5">44.54% 45.86% 46.84% 49.10% 71.43% 69.70%</cell></row><row><cell>Best</cell><cell>L 2</cell><cell cols="5">55.54% 56.55% 60.78% 62.16% 76.65% 76.59%</cell></row><row><cell>Places</cell><cell>Huber</cell><cell cols="5">53.11% 53.85% 57.79% 58.78% 76.72% 76.72%</cell></row><row><cell>Best</cell><cell>Huber</cell><cell cols="5">62.86% 63.23% 67.19% 67.27% 81.19% 81.85%</cell></row><row><cell cols="4">Regression (regularized w/ classification)</cell><cell></cell><cell></cell></row><row><cell>Best</cell><cell>L 2</cell><cell cols="5">57.29% 58.48% 63.92% 64.41% 79.24% 82.89%</cell></row><row><cell>Best</cell><cell>Huber</cell><cell cols="5">60.38% 60.51% 67.18% 66.66% 81.79% 82.55%</cell></row><row><cell>Other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Lezama et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of post-processing strategies. HLW ECD YUD Ours 69.97% 83.96% 85.33% Ours (average) 71.16% 83.60% 86.41% Ours (optimize) 70.66% 86.05% 86.11% [28] (CNN = Orig.) 58.24% 90.80% 94.78% [28] (CNN = Ours) 65.50% 91.29% 95.46%</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Jan-Michael Frahm, Jared Heinly, Yunpeng Li, Torsten Sattler, Noah Snavely, and Kyle Wilson for making SfM models available to us. This research was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory, contract FA8650-12-C-7212. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Horizon Lines in The Wild project website</title>
		<ptr target="http://hlw.csr.uky.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A machine learning approach to horizon line detection using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touqeer</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><forename type="middle">E</forename><surname>Regentova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ara</forename><surname>Nefian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric image parsing in man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Tretiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust optimization for deep regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera calibration with two arbitrary coplanar circles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape from texture: homogeneity revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Horizon detection using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Fefilatyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volha</forename><surname>Smarodzinava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldgof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconstructing the world* in six days *(as captured by the yahoo 100 million image dataset)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks for realtime 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic upright adjustment of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding vanishing points via point alignments in image primal and dual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Grompone Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Worldwide pose estimation using 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust global translations with 1dsfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepFocal: A method for direct focal length estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Greenwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Baltenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A minimum error vanishing point detection approach for uncalibrated monocular images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a non-manhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unconstrained salient object detection via proposal subset optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radom?r</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
