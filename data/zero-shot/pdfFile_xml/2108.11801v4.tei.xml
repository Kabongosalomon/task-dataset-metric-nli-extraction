<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinkle</forename><surname>Srivastav</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Gangi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Domain Adaptation</term>
					<term>Human Pose Estimation</term>
					<term>Person Instance Segmentation</term>
					<term>Operating Room</term>
					<term>Low resolution Images</term>
					<term>Semi-supervised Learning</term>
					<term>Self-training</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fine-grained localization of clinicians in the operating room (OR) is a key component to design the new generation of OR support systems. Computer vision models for person pixel-based segmentation and body-keypoints detection are needed to better understand the clinical activities and the spatial layout of the OR. This is challenging, not only because OR images are very different from traditional vision datasets, but also because data and annotations are hard to collect and generate in the OR due to privacy concerns. To address these concerns, we first study how joint person pose estimation and instance segmentation can be performed on low resolutions images with downsampling factors from 1x to 12x. Second, to address the domain shift and the lack of annotations, we propose a novel unsupervised domain adaptation method, called AdaptOR, to adapt a model from an in-the-wild labeled source domain to a statistically different unlabeled target domain. We propose to exploit explicit geometric constraints on the different augmentations of the unlabeled target domain image to generate accurate pseudo labels and use these pseudo labels to train the model on high-and low-resolution OR images in a self-training framework. Furthermore, we propose disentangled feature normalization to handle the statistically different source and target domain data. Extensive experimental results with detailed ablation studies on the two OR datasets MVOR+ and TUM-OR-test show the effectiveness of our approach against strongly constructed baselines, especially on the low-resolution privacy-preserving OR images. Finally, we show the generality of our method as a semi-supervised learning (SSL) method on the large-scale COCO dataset, where we achieve comparable results with as few as 1% of labeled supervision against a model trained with 100% labeled supervision. Code is available at https://github.com/CAMMA-public/HPE-AdaptOR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The significant rise in the supervised deep-learning methods has paved the way for the visual understanding of persons in challenging environments. Recent progress has pushed its boundaries from coarse bounding box detection to more finegrained pose estimation, providing keypoint-level understanding, and instance segmentation, providing pixel-level understanding. Joint person pose-estimation and instance segmentation aim to localize the body keypoints and estimate segmentation masks for all persons in a given image using a single model. It can support a variety of computer vision applications ranging from virtual try-on <ref type="bibr" target="#b42">(Han et al., 2018)</ref>, smart video synthesis <ref type="bibr" target="#b9">(Chan et al., 2019)</ref>, human activity recognition <ref type="bibr" target="#b113">(Song et al., 2021)</ref> to self-driving cars <ref type="bibr" target="#b71">(Liang et al., 2020)</ref>. The healthcare sector, especially the modern operating room (OR), could hugely benefit from such models to enable novel context-aware computer-assisted systems.</p><p>Novel context-aware systems in sensor-enhanced and visually complex modern ORs have the potential to stream-line clinical workflow processes, detect adverse events, and support real-time decision making by automatically analyzing clinical activities <ref type="bibr" target="#b92">(Padoy, 2019;</ref><ref type="bibr" target="#b127">Vercauteren et al., 2019;</ref><ref type="bibr" target="#b79">Maier-Hein et al., 2020;</ref><ref type="bibr" target="#b81">Mascagni and Padoy, 2021)</ref>. This has been illustrated by the recent development of new OR applications such as activity analysis in robot-assisted surgery <ref type="bibr" target="#b107">(Sharghi et al., 2020)</ref>, semantic scene understanding of OR <ref type="bibr">(Li et al., 2020c)</ref>, surgical workflow recognition in the OR <ref type="bibr" target="#b56">(Kadkhodamohammadi et al., 2020;</ref><ref type="bibr" target="#b145">Zhang et al., 2021)</ref>, and radiation risk monitoring during hybrid surgery <ref type="bibr" target="#b100">(Rodas et al., 2017)</ref>. As clinicians are the main dynamic actors in the OR, models for joint person pose estimation and instance segmentation are key components in building various smart assistance applications. The radiation risk monitoring <ref type="bibr" target="#b100">(Rodas et al., 2017)</ref>, for example, needs such models to understand harmful exposure of radiations to the clinicians at pixel-and keypointlevel. Team activity analysis as another example <ref type="bibr" target="#b28">(Dias et al., 2019;</ref><ref type="bibr" target="#b110">Soenens et al., 2021)</ref> needs such models to understand interactions, non-verbal communications, and cognitive load, especially during critical phases of the surgery.</p><p>These systems, with immense promise to improve patient safety and care, however face hindrance due to the privacysensitive nature of the OR environment. Continuous mon- When a model trained on the source domain is applied to the unseen target domain, we see a substantial decrease in the localization accuracy and an increase in the missed detections. Our unsupervised domain adaptation method significantly improves the results on high and low-resolution OR images. The separate clusters of the source domain and the target domain images are obtained by running a dimension reduction technique: Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) <ref type="bibr" target="#b82">(McInnes et al., 2018;</ref><ref type="bibr" target="#b33">Duhaime et al.)</ref>. The source and the target domain images are a subset of the COCO <ref type="bibr" target="#b74">(Lin et al., 2014)</ref> and the MVOR <ref type="bibr">(Srivastav et al., 2018) datasets, respectively.</ref> itoring by the ceiling-mounted cameras raises potential privacy concerns for the patients and clinicians. Therefore, the data from the cameras is often recorded at low-resolution to improve privacy, as suggested in the literature <ref type="bibr" target="#b20">(Chou et al., 2018;</ref><ref type="bibr" target="#b114">Srivastav et al., 2019</ref><ref type="bibr" target="#b115">Srivastav et al., , 2020</ref>. Developing person localization approaches for these spatially-degraded but privacypreserving low-resolution images is consequently an important challenge that we introduce and tackle in this paper. While person pose estimation and instance segmentation models have improved substantially in the unconstrained environments, they fail remarkably on the unseen target domains due to the visual differences <ref type="bibr" target="#b98">(Recht et al., 2018;</ref><ref type="bibr" target="#b116">Srivastav et al., 2018)</ref>. The OR as a target domain also presents many challenges due to notable changes in the visual appearance at a global and instance level. Indeed, the OR has particular lighting conditions, and the clinicians wear loose clothes and surgical masks and occlude one another due to close proximity and instrument clutter. <ref type="figure" target="#fig_0">Fig. 1</ref> shows such global and instance-level visual differences between natural and OR images. One way to overcome such domain differences is to fine-tune a model on the manually labeled data from the target domain. For example, the authors in <ref type="bibr" target="#b55">(Kadkhodamohammadi et al., 2017b;</ref><ref type="bibr" target="#b2">Belagiannis et al., 2016;</ref><ref type="bibr">Li et al., 2020c)</ref> developed fullysupervised approaches for multi-view 3D pose estimation and semantic segmentation for the OR. However, collecting the labels for the data is not only time-consuming and expensiveannotating a single image with pixel-level segmentation can take up to 90 minutes <ref type="bibr" target="#b21">(Cordts et al., 2016</ref>) -but also particu-larly unscalable for the OR due to privacy concerns. The scalable and successful crowd-sourcing platforms, for example, Amazon Turk, can not be easily used for the privacy-sensitive OR environment to provide large-scale manually labeled data. Approaches that can adapt a model to the unseen and unlabeled target domain are therefore very promising.</p><p>In this work, we propose a novel unsupervised domain adaptation (UDA) approach, called AdaptOR, for joint person pose estimation and instance segmentation. We aim to adapt a model from a labeled source domain, i.e., unconstrained natural images from COCO <ref type="bibr" target="#b74">(Lin et al., 2014)</ref> to an unlabeled target domain, i.e., constrained low-resolution OR images with downsampling factors from 1x to 12x. The UDA methods have been extensively studied for various computer vision tasks ranging from image classification <ref type="bibr" target="#b154">(Zhuang et al., 2020)</ref>, object detection  to semantic segmentation <ref type="bibr" target="#b123">(Toldo et al., 2020)</ref>. Unlike the existing UDA approaches that have primarily been applied to general object classes, we aim to study the UDA for a single but highly challenging "person" class inside the visually complex OR environment while simultaneously exploiting articulated "person" class properties for effective domain adaptation.</p><p>We choose Mask R-CNN ) as our backbone model for joint person pose estimation and instance segmentation, which is primarily designed for a single domain fully supervised training. Inspired from UDA for image classification <ref type="bibr" target="#b10">(Chang et al., 2019)</ref>, we propose disentangled feature normalization (DFN) for our backbone model to train it on two statis-tically different domains. DFN replaces every feature normalization layer in the feature extractor of the backbone model with two feature normalization layers: one for the source domain and another for the target domain. With the improved design, the backbone model expects an input batch containing half the images from the source domain and another half from the target domain. DFN therefore modifies the multi-task loss function to compute and weigh the loss differently for the two domains. The use of separate feature normalization layers for the two domains effectively disentangle the feature learning and stabilizes the training. Given a backbone model with the ability to train on two statistically distinct domains, we build our approach based on a self-training framework <ref type="bibr" target="#b111">(Sohn et al., 2020a;</ref><ref type="bibr" target="#b76">Liu et al., 2021;</ref><ref type="bibr" target="#b26">Deng et al., 2021)</ref>, where we aim to predict similar predictions from a model under different augmentations of the same image, thereby taking the confident predictions from one augmented image -called weakly augmented image -as pseudo labels for the other augmented image -called strongly augmented image. Unlike image classification tasks <ref type="bibr" target="#b4">(Berthelot et al., 2019a;</ref><ref type="bibr" target="#b111">Sohn et al., 2020a)</ref> where the model predictions need to be invariant to the different augmentations applied to the input image. The spatial localization tasks such as pose estimation or instance segmentation however can change the model predictions under certain geometric augmentations, e.g., random-flip or random-resize. Thankfully, these changes in the predictions need to satisfy transformation equivariant constraints i.e., prediction labels also need to be transformed according to the applied geometric augmentations. We therefore use the transformation equivariant constraints to add explicit geometric constraints on the weakly and the strongly augmented unlabeled images to generate high-quality pseudo labels; for example, the random-flip operation has to exploit the chirality property <ref type="bibr" target="#b142">(Yeh et al., 2019)</ref> for pose estimation to map the keypoints to the horizontally flipped image.</p><p>To improve the performance of the model on low-resolution OR images as needed to improve the privacy, we also propose to extend the data augmentation pipeline with a strong-resize augmentation for the strongly augmented image by applying two resize operations on the input image: a down-sampling and an up-sampling operation with a scaling factor randomly chosen between 1x to 12x. It generates heavily blurred images (see example downsampled images in figure 2) that naturally extend our approach to the privacy-preserving low-resolution images. Training the model using the two sets of weak and strong augmentations also enforces consistency regularization <ref type="bibr" target="#b120">(Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b106">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b111">Sohn et al., 2020a)</ref>: a popular regularization technique utilized in a semisupervised learning (SSL). The SSL is closely related to the UDA and aims to generalize a model to the same domain with limited labeled and large-scale unlabeled data.</p><p>We further extend our approach with mean-teacher for stable training <ref type="bibr" target="#b120">(Tarvainen and Valpola, 2017)</ref>, where instead of using a single model to generate and consume the pseudo labels, we create two copies of a given source domain trained model: a teacher and a student model. The teacher model generates the pseudo labels on the weakly augmented image that is used by the student model to train on the corresponding strongly augmented image. The weights of the teacher model are updated using temporal ensembling of the weights of the student model, thereby helping it to improve its predictions due to ensembling while simultaneously generating better pseudo labels to improve the student model. <ref type="figure">Fig. 3</ref> illustrate the complete architecture of our approach.</p><p>We evaluate our approach on the two OR datasets: MVOR+ <ref type="bibr" target="#b116">(Srivastav et al., 2018</ref><ref type="bibr" target="#b115">(Srivastav et al., , 2020</ref> and TUM-OR-test <ref type="bibr" target="#b2">(Belagiannis et al., 2016)</ref>. The MVOR+ dataset is extended from the public MVOR <ref type="bibr" target="#b116">(Srivastav et al., 2018)</ref> with full-body keypoints in COCO format for all the persons. The default annotation in the TUM-OR-test contains only the six common COCO keypoints in the upper body bounding box. Therefore, we reannotate the TUM-OR-test using a semi-automatic approach 1 . Both MVOR+ and TUM-OR-test do not contain ground-truth for the person instance masks. We therefore evaluate the mask segmentation results by computing tight bounding boxes around the prediction masks and comparing them with the corresponding ground-truth bounding boxes, along with qualitative results 2 . We show that our approach performs significantly better after domain adaption and against strongly constructed baselines, especially on low-resolution OR images even downsampled up to 12x. As our backbone model based on Mask R-CNN performs person bounding box detection by design, we use the model to evaluate for the person bounding boxes and show significant improvements in the bounding box detection results. We also conduct extensive ablation studies to shed light on the different components of our approach and their contributions to the results. The <ref type="figure" target="#fig_0">figure 1</ref> shows a comparative qualitative result before and after the domain adaptation.</p><p>Finally, without bells and whistles, our UDA approach can be easily used as an SSL approach on the same domain dataset -by using regular feature normalization instead of DFN. We show the generality of our approach as an SSL method on the same domain COCO dataset with different percentages of supervision. With as few as 1% of labeled supervision, we obtain 57.7% (38.2 keypoint AP) in the pose estimation and 72.3% (36.1 mask AP) in instance segmentation, a strong improvement against the model trained with 100% of labeled supervision (66.2 keypoint AP and 49.9 mask AP). These initial valuable baselines for the joint person pose estimation and instance segmentation could help foster SSL research on largescale public datasets. We can summarize the contributions of our work mainly in the following five aspects:</p><p>1. We propose to study joint pose estimation and instance segmentation on OR images at different low resolutions to address the privacy concerns in the OR. 2. We propose a novel UDA approach to adapt the model to the unlabeled OR target domain by exploiting advanced data augmentations, explicit geometric con-straints, disentangled feature normalization (DFN), and mean-teacher training. 3. We show the generality of our approach as a novel SSL approach by using regular feature normalization instead of DFN. 4. We extend two challenging OR datasets with person bounding box and 2D keypoint annotations for the evaluation. 5. We achieve significantly better results against strongly constructed baselines on the high-and low-resolution OR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human pose estimation</head><p>Human pose estimation (HPE) has been mainly studied either using bottom-up (keypoint-first) or top-down (personfirst) approaches. The bottom-up approaches first detect all the keypoints for all the persons and then use a group postprocessing method to associate keypoints to person instances; conversely, the top-down approaches first obtain the bounding box for each person instance using an off-the-shelf object detector and then employ a single-person pose estimation method to get the keypoints. The group post-processing methods in bottom-up approaches include Part Affinity Fields in CMU-Pose <ref type="bibr" target="#b8">(Cao et al., 2017)</ref>, Part Association Field in Pif-Paf <ref type="bibr" target="#b61">(Kreiss et al., 2019)</ref>, and Associative Embedding (AE) in <ref type="bibr" target="#b88">(Newell et al., 2017;</ref><ref type="bibr" target="#b18">Cheng et al., 2020)</ref>. The leading methods for single-person pose estimation in the top-down approaches include Simple-Baseline , Alpha-Pose <ref type="bibr" target="#b34">(Fang et al., 2017)</ref>, Cascaded-Pyramid-Network <ref type="bibr" target="#b16">(Chen et al., 2018b)</ref>, HRNet , and EvoPose2D <ref type="bibr">(Mc-Nally et al., 2020)</ref>. The bottom-up approaches are computationally faster due to their person-agnostic keypoint localization but yield inferior accuracy compared to the top-down approaches. The two-stage design in the top-down approaches helps them achieve significantly better accuracy but at a more computational cost. Built on top of anchor-free detector <ref type="bibr" target="#b122">(Tian et al., 2019b)</ref>, some recent approaches such as DirectPose <ref type="bibr" target="#b121">(Tian et al., 2019a)</ref> and FCPose <ref type="bibr" target="#b80">(Mao et al., 2021)</ref> consider the keypoints as a special bounding-box with more than two corners and propose to regress the keypoint coordinates directly.</p><p>HPE in the OR is a relatively new field with approaches applied to either single or multi-view images and on color (RGB), depth (D), or both color and depth (RGB-D) images. The initial work <ref type="bibr" target="#b52">(Kadkhodamohammadi et al., 2014)</ref> propose a method to consistently track the upper body poses by offline optimization using discrete Markov Random Field (MRF) on the short RGB-D video sequences. The authors further propose an approach using the pictorial structure model <ref type="bibr" target="#b36">(Fischler and Elschlager, 1973;</ref><ref type="bibr" target="#b35">Felzenszwalb and Huttenlocher, 2005)</ref> initially designed for the RGB images to the RGB-D images with a handcrafted histogram of depth difference (HDD) features <ref type="bibr" target="#b53">(Kadkhodamohammadi et al., 2015)</ref>. Subsequent work use the multi-view RGB images <ref type="bibr" target="#b2">(Belagiannis et al., 2016)</ref> and multi-view RGB-D images <ref type="bibr">(Kadkhodamohammadi et al., 2017b,a)</ref> for 3D HPE along with the corresponding multiview RGB and multi-view RGB-D extensions to the pictorial structure model. Some recent work utilizes multi-view depth data for 3D HPE in the OR either using a voxel-based model <ref type="bibr" target="#b43">(Hansen et al., 2019)</ref> or point R-CNN model <ref type="bibr" target="#b1">(Bekhtaoui et al., 2020)</ref>. Previous work from the authors <ref type="bibr" target="#b114">(Srivastav et al., 2019</ref><ref type="bibr" target="#b115">(Srivastav et al., , 2020</ref> has also studied unsupervised domain adaptation for the OR. The authors in <ref type="bibr" target="#b114">(Srivastav et al., 2019)</ref> adapt the RT-Pose RGB model <ref type="bibr" target="#b8">(Cao et al., 2017)</ref> to the low-resolution OR depth images for 2D HPE, and the authors in <ref type="bibr" target="#b115">(Srivastav et al., 2020)</ref> adapt the Mask R-CNN model  to the OR RGB images for joint 2D/3D HPE. Both these approaches use a two-stage approach. In the first stage, a complex multi-stage teacher model is used to generate pseudo labels on the target domain, and in the second stage, a student model is trained using these pseudo labels. Conversely, our approach uses a single-stage approach to generate and consume the pseudo labels on the fly using the same given model as a teacher and a student. The visual shift of the source and the target domain data, in our single-stage design, is handled by improving the model with disentangled feature normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance segmentation</head><p>Instance segmentation has been extensively studied in the context of multi-class object detection. Like human pose estimation, the instance segmentation approaches can also be categorized into the bottom-up and top-down approaches. The top-down method also uses a two-stage design first to detect the bounding box and then either classify mask proposals or estimate segmentation masks from the bounding box proposals <ref type="bibr" target="#b13">Chen et al., 2019b;</ref><ref type="bibr" target="#b0">Bai and Urtasun, 2017;</ref><ref type="bibr" target="#b75">Liu et al., 2017;</ref><ref type="bibr" target="#b63">Lee and Park, 2020)</ref>. Similarly, the bottom-up methods associate pixel-level semantic segmentation output to the object instance <ref type="bibr" target="#b72">Liang et al., 2017;</ref><ref type="bibr" target="#b60">Kirillov et al., 2017)</ref>. Inside the OR, the only related work <ref type="bibr">(Li et al., 2020c</ref>) addresses a 3D scene semantic segmentation from multi-view depth images; however, the data is obtained from simulated clinical activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint person pose estimation and instance segmentation</head><p>A few notable works address the joint person pose estimation and instance segmentation <ref type="bibr" target="#b93">(Papandreou et al., 2018;</ref><ref type="bibr" target="#b46">He et al., 2017;</ref><ref type="bibr" target="#b144">Zhang et al., 2019b;</ref><ref type="bibr" target="#b152">Zhou and He, 2020)</ref>. The authors in <ref type="bibr" target="#b144">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b152">Zhou and He, 2020)</ref> use pose estimation as a strong prior for the person instance segmentation. The PersonLab <ref type="bibr" target="#b93">(Papandreou et al., 2018)</ref> as a bottomup method and Mask R-CNN  as a top-down method are designed for the joint person pose estimation and instance segmentation. We extend the top-down Mask R-CNN to build the backbone model in this work. Unlike the top-down approaches for HPE, which use separate networks for the two stages and do not share the features and computations, Mask R-CNN uses different heads for the end task that share common features across the heads, making it computationally faster and more easily configurable. This configurability property can be utilized to <ref type="figure">Fig. 2</ref>: Sample image from the OR downsampled at different resolutions. The downsampled images contain little information to identify clinicians and the patients, making them more suitable for activity analysis in privacy-sensitive OR environments. either use the model for a particular task -for example, only for instance segmentation or pose estimation -or extend it further -for example, for dense pose estimation <ref type="bibr" target="#b41">(G?ler et al., 2018)</ref>.</p><formula xml:id="formula_0">(a) 640x480 (1x) (b) 80x60 (8x) (c) 64x48 (10x) (d) 53x40 (12x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Privacy-preserving low-resolution image recognition</head><p>The privacy-sensitive OR environment poses challenges in bringing the AI inside the OR. The recent controversies <ref type="bibr" target="#b96">(Powles and Hodson, 2017)</ref> have raised public awareness regarding how personal data should be collected and controlled, along with how AI algorithms should use personal data in a privacy-safe way <ref type="bibr" target="#b118">(Symons and Bass, 2017)</ref>. One way to address these challenges is by using the federated learning <ref type="bibr" target="#b83">(McMahan et al., 2017)</ref> framework that allows training the model in a decentralized manner without explicitly sharing data. The federated learning has been recently used in medical imaging for segmenting the brain tumor <ref type="bibr" target="#b108">(Sheller et al., 2018)</ref> and detecting COVID-19 lung abnormalities in CT . Unlike medical imaging data, where privacy-sensitive information essentially lies in the metadata, direct video recording of OR using ceiling cameras contains the private information in the data itself. Adapting a model to very low-resolution images has been suggested in the literature to improve privacy <ref type="bibr" target="#b20">(Chou et al., 2018</ref>) that can further be incorporated inside the federated learning setup to improve multi-centric generalization. Indeed, as low-resolution images significantly degrade the spatial details, it could provide a viable means to improve privacy.</p><p>The low-resolution image recognition has been studied for various computer vision tasks ranging from 2D human pose estimation on RGB <ref type="bibr" target="#b87">(Neumann and Vedaldi, 2018</ref>) and depth <ref type="bibr" target="#b114">(Srivastav et al., 2019)</ref> images, face recognition <ref type="bibr" target="#b37">(Ge et al., 2018)</ref>, image classification , image retrieval , object detection <ref type="bibr" target="#b44">(Haris et al., 2018;</ref><ref type="bibr" target="#b65">Li et al., 2017)</ref>, to activity recognition <ref type="bibr" target="#b20">(Chou et al., 2018;</ref><ref type="bibr" target="#b104">Ryoo et al., 2017)</ref>. The low-resolution images as a means for privacy preservation have primarily been studied for the activity recognition in the hospital <ref type="bibr" target="#b20">(Chou et al., 2018)</ref>, indoor posture recognition <ref type="bibr" target="#b38">(Gochoo et al., 2020)</ref>, and 2D human pose estimation on depth images inside the OR <ref type="bibr" target="#b114">(Srivastav et al., 2019)</ref>. These approaches address the spatial degradation of the lowresolution image either at the image space <ref type="bibr" target="#b20">(Chou et al., 2018)</ref> by using off-the-shelf super-resolution model to enhance the spatial details of the low-resolution image or at the feature space <ref type="bibr" target="#b114">(Srivastav et al., 2019;</ref><ref type="bibr" target="#b119">Tan et al., 2018)</ref> by directly optimizing the features suitable for the end task. Our approach falls into the latter, where we utilize advanced data augmentations to enforce consistency constraints between the high-and the low-resolution image derived from the pseudo labels, consequently enhancing the features for the low-resolution image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Unsupervised domain adaptation</head><p>Unsupervised domain adaptation (UDA) methods assume the availability of a labeled source domain and an unlabeled target domain sharing a common label space. The UDA approaches for the different end tasks can be broadly classified in two main areas: adversarial domain alignment and selftraining.</p><p>The main idea in adversarial domain alignment based UDA approaches is to update either the feature, input, or output space from the target domain such that they are distributed in the same way as the source domain. At the feature space, for example, the domain invariant feature space is achieved using an additional neural network called domain classifier which essentially plays a min-max game with the feature extractor using adversarial learning <ref type="bibr" target="#b39">(Goodfellow et al., 2014)</ref>. I.e., the domain classifier tries to fool the feature extractor by accurately distinguishing the source and the target domain features using a binary classification loss on the domain labels; the feature extractor, in turn, tries to fool the domain classifier by producing domain invariant feature such that the domain classifier would result in poor domain discrimination accuracy. The adversarial domain alignment has been studied at the feature space in <ref type="bibr" target="#b3">(Ben-David et al., 2010;</ref><ref type="bibr" target="#b49">Hoffman et al., 2016;</ref><ref type="bibr" target="#b15">Chen et al., 2018a</ref><ref type="bibr" target="#b11">Chen et al., , 2019a</ref><ref type="bibr" target="#b32">Du et al., 2019;</ref><ref type="bibr" target="#b105">Saito et al., 2019;</ref><ref type="bibr" target="#b124">Tran et al., 2019;</ref><ref type="bibr" target="#b50">Hsu et al., 2020;</ref><ref type="bibr" target="#b109">Sindagi et al., 2020;</ref><ref type="bibr" target="#b128">VS et al., 2021)</ref>, at the input space in <ref type="bibr" target="#b153">(Zhu et al., 2017;</ref><ref type="bibr">Chen et al., 2019c,d;</ref><ref type="bibr" target="#b19">Choi et al., 2019;</ref><ref type="bibr" target="#b68">Li et al., 2019)</ref>, and at the output space in <ref type="bibr" target="#b125">(Tsai et al., 2018;</ref><ref type="bibr" target="#b77">Luo et al., 2019;</ref><ref type="bibr" target="#b126">Tsai et al., 2019;</ref><ref type="bibr" target="#b58">Kim and Byun, 2020)</ref>. Although these methods have made significant progress, stable training in the adversarial setup requires complicated training routines with careful adjustment to training parameters. Moreover, aligning the two domains using the domain classifier may not guarantee a required discriminative capability for a given end task.</p><p>The self-training based UDA methods have emerged as promising alternatives to adversarial domain alignment as they follow a simple approach to learn the domain invariant representations. The main idea in the self-training is to generate pseudo labels on the unlabeled target domain by refining the predictions -generated from a given source domain trained model -using domain/task-specific heuristics, for example, confidence score in object detection <ref type="bibr" target="#b26">(Deng et al., 2021)</ref> or uncertainty in semantic segmentation <ref type="bibr" target="#b70">(Liang et al., 2019;</ref><ref type="bibr" target="#b151">Zheng and Yang, 2021)</ref>. These pseudo labels are then used to train a model on the target domain jointly with the labeled source domain. The self-training has been extensively studied for object detection and semantic segmentation tasks <ref type="bibr" target="#b51">(Inoue et al., 2018;</ref><ref type="bibr" target="#b155">Zou et al., 2018;</ref><ref type="bibr" target="#b102">RoyChowdhury et al., 2019;</ref><ref type="bibr" target="#b57">Khodabandeh et al., 2019;</ref><ref type="bibr" target="#b59">Kim et al., 2019;</ref><ref type="bibr" target="#b156">Zou et al., 2019;</ref><ref type="bibr" target="#b148">Zhao et al., 2020a;</ref><ref type="bibr" target="#b130">Wang and Breckon, 2020;</ref><ref type="bibr" target="#b151">Zheng and Yang, 2021)</ref>. The self-training methods could further be improved in a mean-teacher framework to tackle noise in the pseudo labels <ref type="bibr" target="#b70">Liang et al., 2019)</ref>. The mean-teacher and the self-training based UDA approaches have predominantly been inspired by the advances in the SSL <ref type="bibr" target="#b120">(Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b5">Berthelot et al., 2019b;</ref><ref type="bibr" target="#b111">Sohn et al., 2020a;</ref><ref type="bibr" target="#b76">Liu et al., 2021)</ref>. In fact, the UDA can be posed inside an SSL framework with the source domain data as the labeled and the target domain data as unlabeled along with additional complexity of the visual shift of the two domains.</p><p>Some recent works aim to learn domain-specific feature representation instead of domain invariant using disentangled feature normalization. These approaches modify the feature normalization layers -as these control the feature distribution statistics -with two separate layers to disentangle the features from the two domains <ref type="bibr" target="#b10">(Chang et al., 2019)</ref>. The domainspecific features learning has been studied in the UDA for image classification <ref type="bibr" target="#b10">(Chang et al., 2019;</ref><ref type="bibr" target="#b131">Wang et al., 2019)</ref>, and federated learning on medical imaging . It has also been used to boost performance in the supervised learning , and adversarial robustness <ref type="bibr" target="#b140">(Xie and Yuille, 2019)</ref>. The authors in <ref type="bibr">(Wu and Johnson, 2021)</ref> comprehensively discuss the feature normalization under various visual recognition tasks. There also exist several survey papers that extensively discuss UDA for the end task of image classification <ref type="bibr" target="#b94">(Patel et al., 2015;</ref><ref type="bibr" target="#b129">Wang and Deng, 2018;</ref><ref type="bibr" target="#b154">Zhuang et al., 2020)</ref>, semantic segmentation <ref type="bibr" target="#b123">(Toldo et al., 2020;</ref><ref type="bibr" target="#b149">Zhao et al., 2020b)</ref>, and object detection .</p><p>A few notable works propose to use the UDA on the medical domain for cross-domain segmentation task <ref type="bibr" target="#b90">Ouyang et al., 2019;</ref><ref type="bibr" target="#b89">Orbes-Arteainst et al., 2019;</ref><ref type="bibr" target="#b11">Chen et al., 2019a)</ref>, and image classification . The authors in <ref type="bibr" target="#b30">(Dong et al., 2020)</ref> also study the UDA to identify domain invariant transferable features for endoscopic lesions segmentation. The authors in (DiPietro and Hager, 2019) study the surgical workflow recognition with as few as one labeled sequence. <ref type="bibr" target="#b150">Zheng et al., 2020)</ref> papers on medical imaging.</p><p>Our UDA approach for the joint person pose estimation and instance segmentation builds on the notable contributions from the related domains. We propose disentangled feature normalization that uses separate normalization layers in the feature extractor and modifies the multi-task loss function for our joint person pose estimation and instance segmentation model. We use a generic self-training framework along with extended data augmentation pipeline and mean-teacher training to add explicit geometric constraints on the different augmentations of input images, thereby generating accurate pseudo labels that are especially useful to adapt the model to the low-resolution OR images. We show the effectiveness of our approach on the two OR datasets at varying downsampling scales. We further demonstrate the generality of our approach as a novel SSL method on the large-scale COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detailed methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem overview</head><p>Given an end-to-end model for joint person pose estimation and instance segmentation trained on the source domain</p><formula xml:id="formula_1">labeled dataset X = {x i |y i } N l i=1 , we aim to adapt it to the un- labeled target domain dataset U = u j N u i=1</formula><p>. The source domain images are natural in the wild images, whereas the target domain images are the high-resolution and low-resolution (downsampled up to 12x) images from the OR. N l and N u are the number of labeled and unlabeled images, respectively. The source domain's labeled dataset consists of images x i with the corresponding ground-truth labels y i . The ground-truth labels y i consist of bounding boxes P bbox ? R m?4 , keypoints P kp ? R m?n?2 , and masks P mask ? R m?p?2 , where m is the number of persons, n is the number of 2D keypoints for each pose, and p is the number of contour points on the groundtruth binary mask. The unlabeled data from the target domain consists of only the images u j .</p><p>We first explain the backbone models chosen for this work and the proposed UDA method, which we call AdaptOR. Briefly, we first extend Mask R-CNN  with disentangled feature normalization (DFN) to handle the statistically different datasets from the two domains. Then we develop our approach by designing geometrically constrained data augmentations to generate and use the pseudo labels for adapting the model to the unlabeled target domain consisting of high-and low-resolution images from the OR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone models</head><p>We choose the Mask R-CNN ) model, where the mask and the keypoint head are designed to use a single person class. We refer to this model as km-rcnn tailored to joint person pose estimation and instance segmentation. It can also perform person bounding box detection by design. kmrcnn works as follows: it first extracts the image features using a feature pyramid network (FPN)  with a Resnet-50 backbone <ref type="bibr" target="#b47">(He et al., 2016)</ref>. The extracted features pass through a region proposal network (RPN) to generate the bounding-box proposals. The RoiAlign layer  uses these proposals to extract the fixed-size feature maps. The fixed-size feature maps pass through three heads: bounding box head, keypoint head, and mask head. The bounding box head classifies and regresses for the person bounding box, the keypoint head generates the spatial heat-maps corresponding to each body keypoint, and the mask head generates segmentation masks. We use the same multitask losses as described in <ref type="formula">(He et</ref>   <ref type="figure">Fig. 3</ref>: Overview of our approach for unsupervised domain adaptation. We generate two types of augmentations on the given unlabeled target domain images: weak and strong. The weakly augmented images pass through a frozen teacher model and a thresholding function to generate the pseudo labels. These pseudo labels are then geometrically transformed into the strongly augmented image space. A student model uses these transformed pseudo labels to train on the strongly augmented unlabeled images jointly with the labeled source domain images. The weights of the frozen teacher model are updated using the exponential moving average (EMA) of the student model's weights. We also replace every group normalization (GN) layer in the feature extractor with two GN layers (GN(S) and GN(T)) to normalize features of two domains separately, as needed to handle statistically different source and target domains.</p><p>Doll?r, 2017) instead of cross-entropy loss for the better handling of foreground-background class imbalance in our UDA framework . Overall, the supervised loss term L s consists of six losses: binary cross-entropy loss for RPN proposal classification L rpn cls , L1 loss for RPN proposal regression L rpn reg , focal loss <ref type="bibr" target="#b101">(Ross and Doll?r, 2017)</ref> for bounding box classification L bbox cls , smooth L1 loss for bounding box regression L bbox reg , cross-entropy loss for the keypoint head L kps ce , and the binary cross-entropy for the mask head L bce .</p><p>(1)</p><formula xml:id="formula_2">L s = i L rpn cls ( f l i , y l i ) + L rpn reg ( f l i , y l i ) + L bbox cls ( f l i , y l i ) + L bbox reg ( f l i , f l i ) + L kp ce ( f l i , y l i ) + L mask bce ( f l i , y l i ).</formula><p>Here, f l i and y l i correspond to the features and the ground-truth labels for the labeled input image x l i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Initialization</head><p>The state-of-the-art approaches for downstream tasks such as object detection <ref type="bibr" target="#b99">(Ren et al., 2015)</ref> and instance segmentation  initialize the backbone network from the supervised ImageNet <ref type="bibr" target="#b25">(Deng et al., 2009)</ref> weights. The feature normalization during the training is performed using frozen batch normalization (BN) in all the feature extraction layers. It, in turn, uses statistics (mean and variance) derived from the ImageNet training set and freezes its affine parameters (weights and biases).</p><p>The current advancements in self-supervised methods to learn generic feature representations exploiting large-scale unlabeled data have started to surpass the supervised ImageNet baselines on the downstream tasks <ref type="bibr" target="#b45">He et al., 2020;</ref><ref type="bibr" target="#b86">Misra and Maaten, 2020)</ref>. However, the backbone feature extractor weights from the self-supervised methods may not have the same distribution as supervised ImageNet methods. The use of frozen BN during the training therefore could lead to unstable training. Authors in  suggest training the BN layers using Cross-GPU BN <ref type="bibr" target="#b95">(Peng et al., 2018)</ref> to circumvent the issue. We find in our experiments that group normalization (GN) <ref type="bibr" target="#b133">(Wu and He, 2018)</ref> works equally well without the overhead of communicating the batch statistics over all the GPUs resulting in an increased training speed. We follow the network design from <ref type="bibr" target="#b133">(Wu and He, 2018;</ref><ref type="bibr" target="#b137">Wu et al., 2019c)</ref> to change the BN layers of km-rcnn with the GN layers. The updated model, called km-rcnn+, is initialized from the self-supervised method MoCo-v2 (Chen et al., 2020; He et al., 2020) and trained on the source domain dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Disentangled feature normalization</head><p>Given the model, km-rcnn+, trained on the labeled source domain dataset, we aim to adapt it to the unlabeled target domain. We observe in our experiments that feature normalization plays a vital role in training the model on different domains as suggested in the literature <ref type="bibr" target="#b10">Chang et al., 2019;</ref><ref type="bibr">Wu and Johnson, 2021)</ref>. We propose disentangled feature normalization (DFN) to effectively disentangle the feature learning for the datasets of different domains by replacing every group normalization (GN) layer in the feature extractor with two GN layers: one for the source domain images, GN(S), and another for the target domain images, GN(T). The updated model, called km-rcnn++, uses separate affine parameters at every normalization stage in the feature extractor for the source and the target domain images, efficiently normalizing the features of the two domains, see <ref type="figure">figure 3</ref>. The GN parameters for the target domain, GN(T), are initialized from the source domain GN parameters, GN(S), before the domain adaptation training.</p><p>The UDA approaches require weighing the losses differently for unlabeled and labeled images, usually to weigh more the unlabeled losses than the labeled ones to overcome the over-fitting to the labeled set. It can be easily performed if the underlying model is the same for the two domains: the usual case of the existing UDA approaches. However, with our improved design, the km-rcnn++ model expects an input batch containing the first half of images from the source domain and the second half of images from the target domain. DFN therefore modifies the loss function described in equation 1 to compute and weigh the losses on the source and the target domain images differently. The input batch passes through the feature extractor, and the obtained features are divided into two halves corresponding to the source and the target domains. Each half then passes through the RPN network and the three heads to compute the separate RPN, bounding box, keypoint, and mask losses for source and the target domain images as given below.</p><p>(2)</p><formula xml:id="formula_3">L s = i L rpn cls ( f l i , y l i ) + L rpn reg ( f l i , y l i ) + L bbox cls ( f l i , y l i ) + L bbox reg ( f l i , f l i ) + L kp ce ( f l i , y l i ) + L mask bce ( f l i , y l i ) (3) L u = i L rpn cls ( f u i , y u i ) + L rpn reg ( f u i , y u i ) + L bbox cls ( f u i , y u i ) + L bbox reg ( f u i , f u i ) + L kp ce ( f u i , y u i ) + L mask bce ( f u i , y u i ).</formula><p>Here, f l i and f u i correspond to the features of the labeled and unlabeled domain, respectively. The y l i corresponds to the source domain labeled ground-truth labels, and y u i correspond to the target domain pseudo labels. The following section explains the automatic generation of target domain pseudo labels y u i . The km-rcnn++ in the inference mode uses only the GN layers corresponding to the target domain, thereby maintaining the same number of parameters and inference cost compared to km-rcnn+.</p><p>Algorithm 1 : AdaptOR algorithm to adapt a model trained on the labeled source domain dataset to the unlabeled target domain (operating room) Inputs:</p><formula xml:id="formula_4">? Labeled dataset from the source domain X = {x i |y i } N l i=1 , unlabeled dataset from target domain U = u j N u i=1</formula><p>, y i = (P bbox , P kp , P mask ): ground-truth labels for the bounding box, keypoints, and mask for each person in the given labeled image.</p><p>? p t (y|x;?): teacher model, p s (y|x; ?): student model,?, ?:</p><p>weights of the teacher and the student model respectively  </p><formula xml:id="formula_5">X w , y w , U w = T w (X b , y b , U b ) //</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AdaptOR</head><p>Given a model, km-rcnn++, that can handle the datasets of different domains, we explain AdaptOR, our proposed method for unsupervised domain adaptation. We first explain transformation equivariance constraints, as needed to add explicit geometric constraints, and then the data augmentation pipeline, followed by the complete algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Transformation equivariance constraints</head><p>The state-of-the-art UDA or SSL approaches for image classification exploit the transformation invariance property on the unlabeled data, i.e., the classification labels remain unchanged irrespective of the transformation applied to the input image. However, the invariance property does not hold for the spatial localization tasks, and labels get changed with the viewpoint changes of the image due to geometric transforms, for example, resize and horizontal flip. But, these changes in the labels are equivariant to the applied transformations. Mathematically, if F (.) is a model that outputs the spatial localization labels for the input image I under transformation T , we can minimize F (T (I)) ? T (F (I)) under transformation equivariance constraints, i.e., the transformation T can be to used map the localization labels to the transformed image space. We use this property to provide the explicit geometric constraints on the unlabeled images. Additionally, specific to the human pose estimation under horizontal flipping transformation, we exploit the chirality transform <ref type="bibr" target="#b142">(Yeh et al., 2019)</ref> for the mapping of the human pose to the horizontally flipped image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Data augmentations</head><p>Data Augmentations construct novel and realistic samples by computing stochastic transforms on the input data. The recent advancements in data augmentations have been the key to the performance boost in the supervised as well as SSL approaches <ref type="bibr" target="#b22">(Cubuk et al., 2019</ref><ref type="bibr" target="#b23">(Cubuk et al., , 2020</ref><ref type="bibr" target="#b27">DeVries and Taylor, 2017)</ref>. We use two types of augmentations: weak and strong. The weak augmentations, T w , consist of random-flip and random-resize whereas strong augmentations, T s , consist of spatial augmentations from rand-augment <ref type="bibr" target="#b23">(Cubuk et al., 2020)</ref>, random cut-out (DeVries and Taylor, 2017), randomflip, and random-resize, along with strong-resize augmentation to generate privacy-preserving low-resolution images. The strong-resize data augmentation down-sample and upsample the input image with a random scaling factor chosen between 1x to 12x. <ref type="figure">Fig. 2</ref> shows sample images from the OR at different downsampling scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Algorithm</head><p>Given the weakly augmented image, constructed using transformation T w , and the strongly augmented image, constructed using the transformation T s , our idea is to geometrically transform the pseudo labels -obtained from the model's predictions -of the weakly augmented image to the corresponding strongly augmented image. As the weakly and the strongly augmented images are generated using different geometric transformations with the pseudo labels being in the weakly augmented image coordinate system, we exploit transformation equivariance constraints to transform the pseudo labels by applying a transformation, T s T ?1 w , to go from weakly augmented image space to the strongly augmented image space. The model is trained on the strongly augmented images with the transformed pseudo labels.</p><p>However, training the same model to generate and consume the pseudo labels may lead to unstable training. The meanteacher (Tarvainen and Valpola, 2017) from semi-supervised learning has been proposed to stabilize the training using closely coupled teacher and a student model. We therefore adapt mean-teacher in our approach, where we use the teacher model to generate the pseudo labels on the weakly augmented image, and the student model to train on the corresponding strongly augmented image using the pseudo labels. As the source domain GN parameters, GN(S), are trained under the direct supervision, we use GN(S) layers in the teacher model for the inference on the unlabeled target domain. The weights of the teacher and the student models are initialized from the same model, kmrcnn++. The weights of the student model are updated using the stochastic gradient descent based backpropagation, whereas the weights of the teacher model are updated using the exponential moving average (EMA) of the weights of the student model:</p><formula xml:id="formula_6">? = ?? + (1 ? ?)?,</formula><p>where? and ? are the weights of the teacher model and the student models, respectively, and ? is a decay parameter. The EMA helps the teacher model to generate better predictions due to its temporal ensembled weights from the student model, in turn improving the student model for better training. The detailed algorithm is explained in algorithm 1 and illustrated in <ref type="figure">figure 3</ref>.</p><p>Furthermore, we also test AdaptOR as an SSL approach, called AdaptOR-SSL, on a source domain dataset by making minimal changes. We use the kmrcnn+ model, without disentangled feature normalization, as the images are coming from the same domain and do not concatenate the labeled and the unlabeled batches. The labeled and the unlabeled batches pass separately through the kmrcnn+ model to calculate the separate losses on the labeled and the unlabeled data. The AdaptOR-SSL uses x%(x=1,2,5,10) of images from the source domain as the labeled dataset and the rest of the images as the unlabeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Baselines</head><p>We first introduce several self-training based baselines that we have constructed for our joint person pose estimation and instance segmentation task by extending representative approaches. We extend pseudo-label <ref type="bibr" target="#b62">(Lee et al., 2013;</ref><ref type="bibr" target="#b112">Sohn et al., 2020b)</ref>, data-distillation <ref type="bibr" target="#b97">(Radosavovic et al., 2018)</ref>, and ORPose <ref type="bibr" target="#b115">(Srivastav et al., 2020)</ref> as our baselines approaches. We refer to the extended version of pseudo-label, data-distillation, and ORPose as KM-PL, KM-DDS, and KM-ORPose, respectively. The KM as a prefix signifies that these approaches have been extended for the joint pose (keypoint) estimation and instance (mask) segmentation tasks. The baselines approaches are two-stage approaches where the first stage generates the pseudo labels on the unlabeled data. The second stage jointly trains the model using the pseudo and the ground truth labels. AdaptOR on the other hand generates the pseudo labels on the unlabeled data on-the-fly during the training. For a fair comparison, we train all the baseline methods with the same training strategy, data augmentation pipeline, and kmrcnn++ model. We give a brief overview of extended baseline approaches as follows.  <ref type="figure" target="#fig_0">(1x, 8x, 10x, and 12x</ref>) and nine target resolution <ref type="bibr">(480, 520, 560, 600, 640, 680, 720, 760, and 800)</ref> corresponding to the shorter side of the image for MVOR+ and TUM-OR-test datasets. We see an increase in the accuracy with the increase in target resolution for the TUM-OR-test dataset. We also observe an increase in accuracy for the MVOR+ dataset but only up to around 680 pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KM-PL</head><p>We modify the pseudo-labeling <ref type="bibr" target="#b62">(Lee et al., 2013)</ref> approach to generate the pseudo labels on a single-scale image on the unlabeled target domain data. The authors in <ref type="bibr" target="#b112">(Sohn et al., 2020b)</ref> recently use a similar approach with advanced data augmentations for the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KM-DDS</head><p>KM-DDS <ref type="bibr" target="#b97">(Radosavovic et al., 2018)</ref> is also a pseudolabeling approach, but instead of generating pseudo labels on a single scale, it aggregates the labels from multiple scales with random horizontal flipping transformations. Authors use the approach for multi-class object detection and human pose estimation. We further extend it to generate pseudo labels for the masks. Similar to the authors, we use scaling and random horizontal flipping transformations on nine predefined image sizes ranging from 400 to 1200 pixels with a step size of 100. Here, the image size corresponds to the shorter side of the image; the size of the longer side of the image is computed by maintaining the same aspect ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">KM-ORPose</head><p>KM-ORPose <ref type="bibr" target="#b115">(Srivastav et al., 2020)</ref> uses the teacherstudent learning paradigm for the domain adaptation in the OR for joint person detection and 2D/3D human pose estimation. It combines the knowledge-distillation <ref type="bibr" target="#b48">(Hinton et al., 2015;</ref><ref type="bibr" target="#b143">Zhang et al., 2019a</ref>) -using complex three-stage models -along with data-distillation <ref type="bibr" target="#b97">(Radosavovic et al., 2018)</ref> to generate accurate pseudo labels. In the first stage, it uses cascademask-rcnn <ref type="bibr" target="#b7">(Cai and Vasconcelos, 2019)</ref> with the deformable convolution <ref type="bibr" target="#b24">(Dai et al., 2017)</ref> based resnext-152 backbone  to generate the person bounding boxes. We use the same network to get the pseudo masks as well. In the second stage, it uses the HRNet-w48 model (384x288 input size)  to get the pseudo labels for the poses. KM-ORPose is a strong baseline as it uses a complex multistage teacher model to generate accurate pseudo labels for the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and evaluation metrics</head><p>We use COCO <ref type="bibr" target="#b74">(Lin et al., 2014)</ref> as source domain dataset. It contains 57k images, and the ground truth labels have 150k instances of person bounding box, segmentation mask, and 17 body keypoints. The test dataset of COCO, called COCO-val, contain 5k images with 10777 person instances.</p><p>We train and evaluate our approach on the two target domain OR datasets: MVOR <ref type="bibr" target="#b116">(Srivastav et al., 2018</ref><ref type="bibr" target="#b115">(Srivastav et al., , 2020</ref> and TUM-OR <ref type="bibr" target="#b2">(Belagiannis et al., 2016)</ref>. MVOR contains data captured during real surgical interventions, whereas TUM-OR contains OR images from simulated surgical activities. The unlabelled training datasets of MVOR and TUM-OR contain 80k and 1.5k images, respectively. The testing dataset of MVOR, called MVOR+, and TUM-OR, called TUM-OR-test, contain 2196 images with 5091 person instances and 2400 images with 11611 person instances, respectively. The MVOR+ dataset is extended from the public MVOR dataset <ref type="bibr" target="#b116">(Srivastav et al., 2018</ref><ref type="bibr" target="#b115">(Srivastav et al., , 2020</ref>. Before the extension, it consists of 4699 person bounding boxes, 2926 2D upper body poses with 10 keypoints, (and 1061 3D upper body poses). The fullyannotated extension called MVOR+ consists of 5091 person bounding boxes, and 5091 body poses with 17 keypoints in the COCO format. The original TUM-OR-test consists of only the upper-body bounding boxes with six common COCO keypoints. These annotations are not suitable for our evaluation purpose; hence we annotate the TUM-OR-test using a semiautomatic approach. We first use a state-of-the-art person detector <ref type="bibr" target="#b7">(Cai and Vasconcelos, 2019)</ref> to get the person bounding boxes and manually correct all the bounding boxes. We then run the HRNet model  on all the corrected bounding boxes to get the poses. The predicted poses are corrected using the keypoint annotation tool 3 . An overview of the datasets used in this work is shown in the Table 1.</p><p>The image sizes of MVOR+ and TUM-OR-test datasets are 640x480 and 1280x720, respectively. We also conduct experiments with downsampled images using the scaling factors 8x, 10x, and 12x, yielding images of size 80x64, 64x48, and 53x40 for the MVOR+ dataset and 160x90, 128x72, and 107x60 for the TUM-OR-test dataset.</p><p>We use the Average Precision AP 0.5:0.95 metric from COCO <ref type="bibr" target="#b74">(Lin et al., 2014)</ref> for the evaluation. The bounding box evaluation metric AP bb person uses intersection over union (IoU) over boxes, and the pose estimation evaluation metric AP kp person uses the object keypoint similarity (OKS) over person keypoints to compare the ground-truth and the predictions. Both MVOR+ and TUM-OR-test do not have a ground-truth for the person instance segmentation masks. Hence, we evaluate the mask predictions by computing a tight bounding box on the prediction masks and comparing them with ground-truth bounding boxes called AP bb person (from mask). We also show extensive qualitative results for the instance segmentation and pose estimation in the supplementary video. The instance segmentation on the source domain COCO images is evaluated using the AP mask person which uses IoU over masks to compare the ground-truth and the predictions.  <ref type="bibr" target="#b133">(Wu and He, 2018)</ref> and initialized using self-supervised MoCo-v2 approach <ref type="bibr" target="#b45">He et al., 2020)</ref> perform equally well with the model using Cross-GPU BN <ref type="bibr" target="#b95">(Peng et al., 2018)</ref> but using less training time. The first row result for the kmrcnn model is obtained from the paper . Rest of the results correspond to the models that we train. Inference is performed on a single-scale of 800 pixels following . Automatic mixed precision (AMP) uses single-and half-precision (32 bits and 16 bits) floating operation to speed up the training while trying to maintain single-precision (32 bits) model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Initialization  <ref type="bibr" target="#b47">(He et al., 2016)</ref>, self-supervised MOCO-v2 initialization <ref type="bibr" target="#b45">He et al., 2020)</ref> with Cross-GPU BN <ref type="bibr" target="#b95">(Peng et al., 2018)</ref>, and self-supervised MOCO-v2 initialization <ref type="bibr" target="#b45">He et al., 2020)</ref> with group normalization (GN) <ref type="bibr" target="#b133">(Wu and He, 2018)</ref>. The goal of these experiments is to obtain one suitable source-only baseline as an initialization model for the UDA experiments. The last model with self-supervised MOCO-v2 initialization and GN, called kmr-cnn+, is further used in the SSL experiments and extended in UDA experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">AdaptOR: unsupervised domain adaptation (UDA) on target domains</head><p>The UDA experiments on source domain COCO datasets and target domains MVOR and TUM-OR datasets are conducted to train the kmrcnn++ model for eight sets of experiments. The first four experiments are for the target domain MVOR and the last four for TUM-OR. For each target domain, the first three experiments train the kmrcnn++ model on three constructed baseline methods: KM-PL, KM-DDS, and KM-ORPose, respectively, and the fourth experiment trains the kmrcnn++ model on our AdaptOR method. Eleven ablation experiments are conducted with the source domain COCO dataset and the target domain MVOR dataset: the first experiment evaluates the contribution of disentangled feature normalization, the next five different types of strong augmentations, and the last five different unsupervised loss weights loss values ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">AdaptOR-SSL: semi-supervised learning (SSL) on</head><p>source-domain The SSL experiments on the source domain COCO dataset are conducted for four experiments where we train the kmr-cnn+ model using 1%, 2%, 5%, and 10% of COCO dataset as the labeled set and the rest of the data as the unlabeled set. The kmrcnn+ model uses the regular GN layers instead of disentangled feature normalization layers. We use the same labeled and unlabeled images and training iterations as used by Unbiased-teacher , the current state-of-the-art in SSL for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Domain adaptation on AdaptOR-SSL model</head><p>AdaptOR assumes it has access to all the source-domain labels in the previous experiments. We conduct a final experiment to see how AdaptOR performs when initialized from a source-domain model trained with less source domain data. We take a AdaptOR-SSL model trained using 10% labeled and 90% unlabeled source domain data and use it to initialize AdaptOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation details</head><p>The source domain fully supervised training experiments, explained in section 5.2.1, are conducted with batch size 16 and learning rate 0.02 for 270k iterations with multi-step (210k and 250k) learning rate decay on eight V100 GPUs.</p><p>The AdaptOR and the AdaptOR-on-AdaptOR-SSL experiments explained in section 5.2.2, 5.2.4, respectively, are conducted on four V100 GPUs with a labeled and unlabeled batch size of eight (four images/GPU) and a learning rate of 0.001. The experiments are conducted for 65k iterations for the MVOR dataset and 10k iterations for the TUM-OR dataset. Finally, the AdaptOR-SSL experiments explained in 5.2.3 are conducted on four V100 GPUs following the linear learning rate scaling rule <ref type="bibr" target="#b40">(Goyal et al., 2017)</ref>.</p><p>The spatial augmentations from rand-augment <ref type="bibr" target="#b23">(Cubuk et al., 2020)</ref> consist of "inversion", "auto-contrast", "posterize", "equalize", "solarize", "contrast-variation", "colorjittering", "sharpness-variations", and "brightness-variations" implemented using a python image library 4 . The random cutout (DeVries and Taylor, 2017) augmentation places square boxes of random sizes chosen between 40 to 80 pixels at random locations in the image. The random-resize operation for the weakly and strongly augmented images resize the image to a size randomly sampled from 600 to 800 pixels for SSL experiments following . For the UDA experiments, we choose the random-resize range from 480 to 800 pixels to provide more size variability in the data augmentation and match the original size of the MVOR dataset <ref type="table">Table 3</ref>: Results for the baseline approaches and AdaptOR. We see improvements in all three metrics on both the target domain datasets, especially on the low-resolution images making the proposed approach suitable for the deployment inside the privacy-sensitive OR environment. The source-only results correspond to the model trained on the labeled source domain without any training on the target domain images. The KM-PL, KM-DDS, and KM-ORPose are strong baselines proposed in this work. (640x480). The image size corresponds to the shorter side of the image. We use a detectron2 framework <ref type="bibr" target="#b135">(Wu et al., 2019a)</ref> to run all the experiments with automatic mixed precision (AMP) <ref type="bibr" target="#b85">(Micikevicius et al., 2017)</ref>. We use bounding box threshold ? bbox = 0.7, keypoint threshold ? kp = 0.1, mask threshold ? mask = 0.5, EMA decay rate ? = 0.9996, unsupervised loss weight ? = 3.0 for AdaptOR, and ? = 2.0 for AdaptOR-SSL. <ref type="table" target="#tab_2">Table 2</ref> shows the results of kmrcnn and kmrcnn+ models trained on the source domain COCO dataset. The kmrcnn trained using self-supervised MoCo-v2 weights with Cross-GPU BN <ref type="bibr" target="#b95">(Peng et al., 2018)</ref> obtains improvement of approximately 1% in all the three metrics compared to supervised ImageNet weights using frozen BN. The kmrcnn+ using GN performs equally well but with less training time. The kmr-cnn+ model is therefore further used in the SSL experiments and extended in UDA experiments. 5 https://github.com/matteorr/coco-analyze 6.2. AdaptOR: unsupervised domain adaptation (UDA) on target domains <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_3">figure 4</ref> show the result of our unsupervised domain adaptation experiments using AdaptOR. The first and the second half in table 3 show the results for MVOR+ and TUM-OR-test datasets, respectively. We evaluate the models at four downsampling scales (1x, 8x, 10x, and 12x). As the model is trained on unlabeled image sizes from 480 to 800 pixels (shorter side), we evaluate the model on nine target resolutions <ref type="bibr">(480, 520, 560, 600, 640, 680, 720, 760, and 800)</ref>, i.e., for a given downsampling scale, we down-sample the image with the scale and up-sample it to the given target resolution. The target resolution also corresponds to the shorter size of the image to maintain the aspect ratio. We use bilinear interpolation for the downsampling and up-sampling. The results in <ref type="table">Table  3</ref> show the mean and standard deviation of the results computed on all the target resolutions for bounding box detection AP bb person , pose estimation AP kp person , and instance segmentation AP bb person (from mask) on a given downsampling scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVOR+</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Source domain fully supervised training</head><p>The first row shows the source-only results for the kmrcnn+ model trained on source domain images and evaluated on the target domain. The significant decrease in the low-resolution results of the kmrcnn+ is likely because such heavily downsampled images are not present in the source domain. The improved result for the KM-DDS approach compared to KM-PL shows the effects of generating pseudo labels using the multiscale and flipping transformation. The bounding box and seg- mentation results for the KM-ORPose are slightly worse than the KM-PL and KM-DDS. It may be because KM-ORPose uses a state-of-the-art object detector trained on all the 80 class categories from COCO whereas, KM-PL and KM-DDS use the model trained specifically for the person class. The AdaptOR performs significantly better compared to baseline approaches, especially on the low-resolution at different target resolutions, see figure 4, suggesting the potential of our approach for low-resolution images in the privacy-sensitive OR environment. We observe a slight decrease in the accuracy for AP kp person metric on original size, likely due to the use of the multi-stage complex teacher model to generate the pseudo poses. Instead, our approach improves the given model in a model agnostic way without relying on an external teacher model to generate the pseudo labels. We also plot the results at individual scales in the figure 4. The figure 5 and 6 show qualitative results comparing our approach with the baseline approaches.</p><p>We further analyze the impact of different localization errors at the keypoint level before and after the domain adaptation using an approach described in <ref type="bibr" target="#b103">(Ruggero Ronchi and Perona, 2017)</ref>. As shown in <ref type="figure">Fig. 7, after</ref>  (b) AdaptOR <ref type="figure">Fig. 7</ref>: Localization errors at individual keypoint level for the pose estimation task before and after the domain adaptation. "Jitter", "Inversion", "Swap", and "Miss" are various localization errors defined in <ref type="bibr" target="#b103">(Ruggero Ronchi and Perona, 2017)</ref>: "Jitter" error is the error in predicted keypoint w.r.t close proximity of the correct ground truth, "Inversion" error is due to the right-left swap of the body part, "Swap" is the error in assigning predicted keypoint to a wrong person, and "Miss" error is due to completely missing the correct ground truth location. We use the author's code repository <ref type="bibr" target="#b103">(Ruggero Ronchi and Perona, 2017)</ref>   <ref type="figure">Fig. 8</ref> shows t-sne feature visualization (Van der Maaten and Hinton, 2008) of the layer5 resnet features of the backbone model illustrating the appropriate segregation of the features after the domain adaptation. We also conduct experiments to quantify the use of two separate GN layers, GN(S) and  <ref type="bibr">(480, 520, 560, 600, 640, 680, 720, 760, and 800)</ref>.</p><p>GN(T), in the feature extractor for domain-specific normalization compared to either a single GN layer or a single frozen BN layer. The first row in <ref type="table" target="#tab_7">Table 4</ref> shows the results for the krcnn <ref type="bibr" target="#b136">Wu et al., 2019b</ref>) model using frozen BN <ref type="bibr" target="#b47">(He et al., 2016)</ref> layers for joint bounding box detection and pose estimation. We take the source domain COCO trained weights from detectron2 <ref type="bibr" target="#b135">(Wu et al., 2019a)</ref> library and train it on the MVOR dataset. The second row shows the results for the kmrcnn+ model using a single GN layer for both domains.</p><p>We also evaluate kmrcnn++ where we use the GN layers corresponding to source domain GN(S) to evaluate on the target domain (kmrcnn++ GN(S)). We obtain significantly better results by using our design of the two separate GN layers for feature normalization. <ref type="table" target="#tab_8">Table 5</ref> shows the ablation experiments to see the effect of using different types of augmentations on the strongly transformed images used by the student model during training. The results show that the strong-resize augmentations are needed to adapt the model to the low-resolution OR images. The geometric transform exploiting the transformation equivariance constraints significantly improves the results, especially for the pose estimation task, where we also utilize the chirality transforms to map the flipped keypoints to the horizontally flipped image. The results are further improved using the random-augment and random-cut augmentations. 6.2.1.3 Effect of unsupervised loss weight (?) values Unsupervised loss weight (?) controls the proportion of the total loss attributed to the unsupervised loss for the target domain. As the aim is to adapt the model to the target domain, higher value of ? generally leads to better performance. <ref type="figure">Fig. 9</ref> shows the ablation results for different values of unsupervised loss weight (?). We observe that the increase in the ? increases the accuracy; however, it starts to decrease after the ? value of 4.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.2">Components of AdaptOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">AdaptOR-SSL: semi-supervised learning (SSL) on</head><p>source-domain <ref type="table">Table 6</ref> shows the results of SSL experiments using AdaptOR-SSL on the COCO dataset with 1%, 2%, 5%, and 10% labeled supervision. The results with 100% labeled supervision are presented in <ref type="table" target="#tab_2">Table 2</ref>. The first two rows in <ref type="table">Table 6</ref> show the results of two fully supervised baselines: supervised and supervised++. The supervised baseline uses 1% supervision 2% supervision 5% supervision 100% supervision <ref type="figure" target="#fig_0">Fig. 10</ref>: Qualitative results on a sample image from COCO-val dataset with x%(x=1,2,5,100) of labeled supervision. We use the AdaptOR-SSL for 1%, and 5% labeled supervision with the rest of the data as the unlabeled data. We see comparable qualitative results with 1% of labeled supervision to 100% of labeled supervision. The red arrows show either missed detections or localization errors. <ref type="table">Table 6</ref>: Results for AdaptOR-SSL on COCO-val dataset under the semi-supervised learning setting with x%(x=1,2,5,10) of labeled supervision. We compare it with the fully supervised baselines trained on the same labeled data without using any unlabeled data. The supervised baseline uses only the random resize and random-flip data augmentations as used in  whereas supervised++ uses the same data augmentation pipeline as in AdaptOR-SSL containing weakly and strongly augmented labeled images. We also compare it with the current state-of-the-art SSL object detector Unbiased-Teacher  for the person bounding box detection task. The inference is performed on a single scale of 800 pixels (shorter side) following the same settings as used in <ref type="bibr" target="#b76">Liu et al., 2021</ref> random-resize and random-flip augmentations as used , whereas the supervised++ uses the our data augmentation pipeline containing weakly and strongly augmented labeled images. We observe significant improvement in the results by utilizing our data augmentation pipeline. We also compare our bounding box detection results with the current state-of-the-art SSL approach for object detection, Unbiasedteacher : a multi-class object bounding box detection approach using self-training and mean-teacher based SSL approach. Different from ours, it uses fully supervised ImageNet weights for initialization and does not exploit the transformation equivariance constraints using geometric augmentations. As the Unbiased-teacher performs bounding box detection on 80 COCO classes, we compare our results with their person category results AP bb person from the model obtained from their GitHub repository 7 . We observe significant improvement in results attributed to our initialization using the self-supervised method, feature normalization using GN, exploitation of the geometric constraints on the unlabeled data, <ref type="table">Table 7</ref>: Performance comparison when applying AdaptOR-SSL models trained with 1%, 2%, 5%, and 10% source domain labels to the target domain of MVOR+ (see "Before UDA" results). When we apply the AdaptOR approach on the AdaptOR-SSL model (trained using 10% source domain labels), we observe an improvement in the performance (see "After UDA" results). Results corresponding to 100% source domain labeled supervision in "Before UDA" and "After UDA" are obtained from <ref type="table" target="#tab_2">Table 2</ref>  and single class training for person category exploiting mask and the keypoint annotations. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the qualitative results from the models trained with 1%, 2%, 5% and 100% labels. We also show the qualitative results in the supplementary video on some YouTube videos and observe comparable qualitative results with 1% of labeled supervision w.r.t 100% of labeled supervision. <ref type="table">Table 7</ref> shows results when we evaluate AdaptOR-SSL models trained using 1%, 2%, 5%, and 10% source domain labels to our MVOR+ target domain. We observe a significant decrease in the results (see "Before UDA" results in the <ref type="table">Table  7)</ref>. As a final experiment, we initialize our AdaptOR approach with AdaptOR-SSL model trained using 10% source domain labels. We observe an increase in the performance after the domain adaptation. However, there still exists a gap of around 4% for AP bb person and AP bb ( f rom mask) person , and 12% for AP kp person . These results show the need to develop effective domain adaptation approaches in the presence of limited source domain labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Domain adaptation on AdaptOR-SSL model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Manual annotations, especially for spatial localization tasks, are considered the main bottleneck in the design of AI systems. With advances in digital technology providing a wide variety of visual signals, the modern OR has started to use AI to develop next-generation smart assistance systems. However, the progress is hindered due to the cost and privacy concerns for obtaining manual annotations. In this work, we tackle the joint person pose estimation and instance segmentation task needed to analyze OR activities and propose an unsupervised domain adaptation approach to adapt a model trained on a labeled source domain to an unlabeled target domain. We propose a new self-training based framework with advanced data augmentations to generate pseudo labels for the unlabeled target domain. The high-quality effectiveness in the pseudo labels is ensured by applying explicit geometric constraints of the different augmentations on the unlabeled input image. We also introduce disentangled feature normalization for the statistically different source and the target domains and use the mean-teacher paradigm to stabilize the training. Evaluation of the method on the two target domain datasets, MVOR+ and TUM-OR-test, with extensive ablation studies, show the effectiveness of our approach. We further demonstrate that the proposed approach can effectively be adapted to the lowresolution images of the target domain, as needed to ensure OR privacy, even up to a downsampling factor of 12x. Finally, we illustrate the generality of our approach as the SSL method on the large-scale COCO dataset, where we obtain better results with as few as 1% of labeled annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Global and instance-level visual differences between source domain natural images and target domain OR images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>?(p, ? = ? bbox , ? kp , ? mask ): function to convert predictions (p) to pseudo labels using thresholds (?) consisting of bounding box threshold ? bbox , keypoint threshold ? kp , and mask threshold ? mask ? T w (.): weak transform, T s (.): strong transform ? L: modified multi-task loss function as described in section 3.2.2 and equations 2 and 3, ?: EMA decay rate, ?: unsupervised weight loss value, ?: learning rate Outputs:?: Final teacher model weights 1: for all (X b , y b , U b ) ? (X, U) do // sample a batch from the labeled and unlabeled dataset 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>apply weak transform to the labeled and unlabeled batch to construct weakly augmented labeled (Xw , yw ) and unlabeled (Uw ) batch 3: X s , y s , U s = T s (X b , y b , U b ) // apply strong transform to the labeled and unlabeled batch to construct strongly augmented labeled (Xs , ys ) and unlabeled (Us ) batch 4:? s = ?(p t (U w ;?), ?) // run the teacher model pt (y|x;?) on the weakly augmented unlabeled batch Uw, and convert the predictions into the pseudo labels?s using the thresholding function ?(p, ?) 5:? s = T s (T ?1 w (? s )) // apply the transform to convert the pseudo labels?s into the coordinates of strongly augmented unlabeled batch (Us ) 6: X, y = concat(X w , X s , U s ), concat(y w , y s ,? s ) // concatenate the strongly augmented unlabeled batch with the weakly and strongly augmented labeled batch 7: L s , L u = L(p s (X; ?), y) // compute the loss using the multi-task loss function on the student model 8: loss = L s + ?L u // add the supervised and the unsupervised losses 9: ? = S GD(?, ?, ? ? (loss)) // update the parameters of the student model ? using stochastic gradient descent with momentum 10:? = ?? + (1 ? ?)? // update the parameters of teacher model? using the exponential moving average 11: end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Bounding box detection AP bb person , pose estimation AP kp person , and instance segmentation AP bb person (from mask) results for unsupervised domain adaptation experiments on four downsampling scales</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Qualitative results for bounding box detection, pose estimation, and instance segmentation on a sample MVOR+ image for the baseline approaches and AdaptOR. Results are displayed on the for original image and corresponding downsampled images with downsampling factor 8 and 12. The red arrows show either missed detections or localization errors. Localization errors are noticeable on the low-resolution images Qualitative results for bounding box detection, pose estimation, and instance segmentation on a sample TUM-OR-test image for the baseline approaches and AdaptOR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Head Keypoint Head 2D Boxes 2D Key points Unlabeled Dataset Labeled Dataset {X | y}</head><label></label><figDesc>al., 2017) except for bounding box classification loss where we use focal loss(Ross and    </figDesc><table><row><cell></cell><cell></cell><cell cols="4">Box Head Mask Random subsampling {U b } Source Domain {X b | y b } Target Domain {U} RPN ROI Align R-50+FPN Teacher ? GN(S)</cell><cell>Pseudo labeling + transformation (? s ? w -1 )</cell><cell>2D Masks</cell></row><row><cell>Weak and Strong</cell><cell>Transform</cell><cell>Weak Transform Strong Transform</cell><cell>(? w ) (? s )</cell><cell>R-50+FPN GN(S) GN(T)</cell><cell cols="3">Keypoint Head EMA ( ? = ? ? + (1-?) ) ROI Align RPN Student Mask Head</cell><cell>Pseudo Supervision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Box Head Box Head</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Labeled Supervision</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>An overview of the source and the target domain datasets used in this work.</figDesc><table><row><cell>Dataset</cell><cell cols="3">type # images # instances</cell></row><row><cell>Source domain labeled dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>COCO</cell><cell>train</cell><cell>57,000</cell><cell>150,000</cell></row><row><cell>COCO-val</cell><cell>test</cell><cell>5,000</cell><cell>10,777</cell></row><row><cell>Target domain unlabelled datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MVOR</cell><cell>train</cell><cell>80,000</cell><cell>-</cell></row><row><cell>MVOR+</cell><cell>test</cell><cell>2,196</cell><cell>5,091</cell></row><row><cell>TUM-OR</cell><cell>train</cell><cell>1,500</cell><cell>-</cell></row><row><cell>TUM-OR-test</cell><cell>test</cell><cell>2,400</cell><cell>11,611</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the source domain COCO-val dataset with 100% labeled supervision. The kmrcnn+ model using GN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>61?0.34 40.42?2.17 34.87?2.47 29.61?2.69 68.61?1.54 41.84?2.33 31.08?2.83 24.00?2.90 KM-PL 60.21?0.51 57.14?0.34 55.88?0.39 54.26?0.41 72.28?1.51 65.44?1.45 62.84?1.02 62.42?1.55 KM-DDS 60.79?0.47 57.88?0.39 56.74?0.37 55.12?0.45 72.51?1.45 65.98?1.18 63.87?0.99 62.68?1.32 KM-ORPose 58.88?0.69 55.14?0.56 53.81?0.52 51.96?0.47 69.73?1.22 63.46?0.93 60.71?0.73 60.14?0.94 AdaptOR 61.41?0.40 59.48?0.35 58.55?0.36 57.33?0.43 72.75?0.88 67.33?0.78 65.53?0.57 65.65?0.66 83?0.40 55.60?0.49 53.16?0.48 50.02?0.46 78.39?1.76 69.24?1.07 65.29?0.93 60.56?1.21 KM-ORPose 62.50?0.53 57.18?0.60 54.59?0.59 51.24?0.47 80.49?1.74 69.90?1.03 65.64?0.94 60.67?0.73 AdaptOR 60.86?0.38 57.35?0.61 55.42?0.66 52.60?0.60 77.84?1.24 70.65?1.04 67.36?0.96 63.27?1.21 95?0.37 37.98?2.21 32.58?2.37 27.56?2.48 69.33?1.46 40.38?2.30 30.11?2.79 22.97?2.93 KM-PL 56.50?0.60 54.06?0.44 52.90?0.48 51.33?0.46 71.93?1.34 65.43?1.46 63.16?0.89 62.67?1.11 KM-DDS 57.12?0.47 54.76?0.50 53.78?0.49 52.06?0.67 71.99?1.18 65.96?1.07 64.02?0.70 63.01?1.02 KM-ORPose 55.46?0.76 52.37?0.62 51.23?0.55 49.34?0.46 68.05?1.13 61.15?1.09 58.53?0.86 57.89?1.00 AdaptOR 59.34?0.40 57.44?0.42 56.62?0.41 55.39?0.51 72.13?0.91 66.55?0.80 65.04?0.52 65.15?0.65</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TUM-OR-test</cell></row><row><cell></cell><cell>1x</cell><cell>8x</cell><cell>10x</cell><cell>12x</cell><cell>1x</cell><cell>8x</cell><cell>10x</cell><cell>12x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AP bb person (mean?std)</cell><cell></cell><cell></cell></row><row><cell>source-only</cell><cell cols="5">56.AP kp person (mean?std)</cell><cell></cell><cell></cell></row><row><cell>source-only</cell><cell cols="8">50.55?0.39 23.99?2.25 16.86?2.16 11.31?1.91 65.60?4.55 27.21?1.49 19.41?1.86 13.18?1.81</cell></row><row><cell>KM-PL</cell><cell cols="8">58.72?0.44 55.19?0.43 52.81?0.55 49.53?0.46 77.49?1.87 67.57?1.03 63.46?0.89 58.24?1.05</cell></row><row><cell>KM-DDS</cell><cell cols="4">59.AP bb (from mask) person</cell><cell>(mean?std)</cell><cell></cell><cell></cell></row><row><cell>source-only</cell><cell>54.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>domain adaptation, our approach correctly detects more keypoints while reducing the impact of different localization errors. Additional qualitative results for the UDA experiments on MVOR+ and TUM-OR-</figDesc><table><row><cell></cell><cell>Miss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Jitter</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Nose</cell><cell>: 14.0</cell><cell cols="2">Wrists : 5.7</cell><cell></cell><cell>Nose</cell><cell>: 12.6</cell><cell>Wrists : 5.5</cell></row><row><cell></cell><cell></cell><cell>Eyes</cell><cell>: 18.7</cell><cell>Hips</cell><cell>: 11.9</cell><cell></cell><cell>Eyes</cell><cell>: 18.5</cell><cell>Hips</cell><cell>: 17.3</cell></row><row><cell></cell><cell></cell><cell>Ears</cell><cell>: 10.5</cell><cell cols="2">Knees : 11.1</cell><cell></cell><cell>Ears</cell><cell>: 13.0</cell><cell>Knees : 9.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Should.: 11.7</cell><cell cols="2">Ankles : 7.1</cell><cell></cell><cell cols="2">Should.: 10.8</cell><cell>Ankles : 2.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Elbows : 9.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Elbows : 10.4</cell></row><row><cell>Good : 63.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jit. : 13.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inv. : 6.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Miss : 15.8 Swap : 2.0</cell><cell>Inversion</cell><cell>Nose</cell><cell>: 0.0</cell><cell cols="2">Wrists : 13.2</cell><cell>Swap</cell><cell>Nose</cell><cell>: 2.2</cell><cell>Wrists : 15.9</cell></row><row><cell></cell><cell></cell><cell>Eyes</cell><cell>: 9.2</cell><cell>Hips</cell><cell>: 26.0</cell><cell></cell><cell>Eyes</cell><cell>: 5.0</cell><cell>Hips</cell><cell>: 18.7</cell></row><row><cell></cell><cell></cell><cell>Ears</cell><cell>: 3.5</cell><cell cols="2">Knees : 12.4</cell><cell></cell><cell>Ears</cell><cell>: 9.0</cell><cell>Knees : 2.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Should.: 15.4</cell><cell cols="2">Ankles : 9.1</cell><cell></cell><cell cols="2">Should.: 26.9</cell><cell>Ankles : 1.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Elbows : 11.0</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Elbows : 17.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) source-only</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Miss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Jitter</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Nose</cell><cell>: 10.8</cell><cell cols="2">Wrists : 6.7</cell><cell></cell><cell>Nose</cell><cell>: 11.5</cell><cell>Wrists : 6.6</cell></row><row><cell></cell><cell></cell><cell>Eyes</cell><cell>: 14.9</cell><cell>Hips</cell><cell>: 14.3</cell><cell></cell><cell>Eyes</cell><cell>: 14.8</cell><cell>Hips</cell><cell>: 18.1</cell></row><row><cell></cell><cell></cell><cell>Ears</cell><cell>: 9.8</cell><cell cols="2">Knees : 13.6</cell><cell></cell><cell>Ears</cell><cell>: 13.3</cell><cell>Knees : 12.1</cell></row><row><cell></cell><cell></cell><cell cols="2">Should.: 10.8</cell><cell cols="2">Ankles : 8.6</cell><cell></cell><cell cols="2">Should.: 11.3</cell><cell>Ankles : 1.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Elbows : 10.5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Elbows : 10.4</cell></row><row><cell>Good : 72.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jit. : 11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inv. : 4.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Miss : 9.9 Swap : 2.1</cell><cell>Inversion</cell><cell>Nose</cell><cell>: 0.0</cell><cell cols="2">Wrists : 14.8</cell><cell>Swap</cell><cell>Nose</cell><cell>: 2.8</cell><cell>Wrists : 17.2</cell></row><row><cell></cell><cell></cell><cell>Eyes</cell><cell>: 6.7</cell><cell>Hips</cell><cell>: 25.6</cell><cell></cell><cell>Eyes</cell><cell>: 5.8</cell><cell>Hips</cell><cell>: 17.8</cell></row><row><cell></cell><cell></cell><cell>Ears</cell><cell>: 3.3</cell><cell cols="2">Knees : 12.3</cell><cell></cell><cell>Ears</cell><cell>: 9.0</cell><cell>Knees : 3.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Should.: 17.2</cell><cell cols="2">Ankles : 9.9</cell><cell></cell><cell cols="2">Should.: 25.8</cell><cell>Ankles : 1.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Elbows : 10.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Elbows : 16.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>5 for plotting the results.<ref type="bibr" target="#b78">Van der Maaten and Hinton, 2008)</ref> of the layer5 resnet features of the backbone model on random 200 images of the source and the target domain test datasets. The source-only model uses only the GN(S) layers whereas the AdapOR uses separate GN(S) and GN(T) layers for the source and the target domain images, respectively. The AdapOR model appropriately segregates the source and the target domain image features from the two domains helping in improving the domain adaptation for the downstream heads.</figDesc><table><row><cell>source-only</cell><cell>AdaptOR</cell><cell>source-only</cell><cell>AdaptOR</cell></row><row><cell>MVOR+</cell><cell>COCO-val</cell><cell>TUM-OR-test</cell><cell>COCO-val</cell></row><row><cell cols="2">Fig. 8: t-sne feature visualization (test are presented in the supplementary video 6</cell><cell>6.2.1. Ablation experiments</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">6.2.1.1 Disentangled feature normalization</cell></row><row><cell>6 https://youtu.be/gqwPu9-nfGs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study comparing the kmrcnn++ model using the two GN layer-based design for feature normalization with the kmrcnn+ that uses only a single layer. We also compare it with a krcnn model using single frozen BN, and kmrcnn++ GN(S), the same kmrcnn++ model but using the GN layers corresponding to the source domain.00?0.35 56.78?0.37 55.87?0.34 54.43?0.36 kmrcnn+ 60.71?0.16 58.75?0.33 58.03?0.31 56.97?0.39 kmrcnn++ GN(S) 59.64?0.46 55.86?0.48 53.84?0.64 51.61?0.74 kmrcnn++ 61.41?0.40 59.48?0.35 58.55?0.36 57.33?0.43</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MVOR+</cell><cell></cell></row><row><cell>Models</cell><cell>1x</cell><cell>8x</cell><cell>10x</cell><cell>12x</cell></row><row><cell></cell><cell></cell><cell cols="2">AP bb person (mean?std)</cell><cell></cell></row><row><cell>krcnn</cell><cell cols="3">59.AP kp person (mean?std)</cell><cell></cell></row><row><cell>krcnn</cell><cell cols="4">57.96?0.32 55.48?0.62 53.34?0.55 50.50?0.44</cell></row><row><cell>kmrcnn+</cell><cell cols="4">47.15?0.30 45.27?0.44 43.89?0.44 42.01?0.44</cell></row><row><cell cols="5">kmrcnn++ GN(S) 58.64?0.40 52.37?0.41 49.51?0.46 46.08?0.51</cell></row><row><cell>kmrcnn++</cell><cell cols="4">60.86?0.38 57.35?0.61 55.42?0.66 52.60?0.60</cell></row><row><cell></cell><cell></cell><cell>AP bb (from mask) person</cell><cell>(mean?std)</cell><cell></cell></row><row><cell>krcnn</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>kmrcnn+</cell><cell cols="4">55.18?0.25 53.85?0.42 53.28?0.5 52.44?0.58</cell></row><row><cell cols="5">kmrcnn++ GN(S) 58.22?0.46 54.77?0.62 53.06?0.67 50.70?0.72</cell></row><row><cell>kmrcnn++</cell><cell cols="4">59.34?0.40 57.44?0.42 56.62?0.41 55.39?0.51</cell></row><row><cell cols="5">Fig. 9: Results for different values of unsupervised loss weight (?) on</cell></row><row><cell cols="5">the MVOR+ dataset. Results show the mean and confidence interval</cell></row><row><cell cols="5">computed using different downsampling scales (1x, 8x, 10x, and 12x)</cell></row><row><cell cols="2">and target resolutions</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study quantifying the different augmentations on the strongly transformed image used by the student model for the training. Here, sr: strong-resize, ra: random-augment, rc: random-cut, and geom: geometric transformations consisting of random-resize and random-flip.61?0.34 40.42?2.17 34.87?2.47 29.61?2.69 58.06?0.28 45.14?1.70 40.19?2.09 35.45?2.28 58.34?0.34 58.03?0.31 57.25?0.33 55.97?0.33 59.64?0.34 58.74?0.30 58.01?0.36 56.80?0.32 58.43?0.31 57.72?0.31 56.99?0.33 55.58?0.29 59.79?0.54 58.38?0.44 57.48?0.45 56.21?0.46 61.41?0.40 59.48?0.35 58.55?0.36 57.33?0.43 95?0.37 37.98?2.21 32.58?2.37 27.56?2.48 56.08?0.32 42.12?1.78 37.19?2.13 32.56?2.27 55.81?0.38 55.66?0.46 54.94?0.43 53.73?0.51 57.14?0.35 56.52?0.38 55.84?0.42 54.62?0.41 56.06?0.32 55.50?0.33 54.70?0.41 53.30?0.40 57.58?0.50 56.34?0.45 55.48?0.50 54.21?0.62 59.34?0.40 57.44?0.42 56.62?0.41 55.39?0.51</figDesc><table><row><cell>MVOR+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>28.43 34.52 37.88 15.91 23.58 30.96 37.77 18.13 23.93 29.34 33.47 supervised++ 28.59 34.27 41.18 43.60 25.78 32.14 41.45 46.51 24.18 29.14 35.40 37.83 AdaptOR-SSL 42.57 45.37 49.90 52.70 38.22 44.08 49.79 56.65 36.06 38.96 43.10 45.46</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">AP bb person</cell><cell></cell><cell></cell><cell cols="2">AP kp person</cell><cell></cell><cell></cell><cell cols="2">AP mask person</cell><cell></cell></row><row><cell>1%</cell><cell>2%</cell><cell>5%</cell><cell>10%</cell><cell>1%</cell><cell>2%</cell><cell>5%</cell><cell>10%</cell><cell>1%</cell><cell>2%</cell><cell>5%</cell><cell>10%</cell></row><row><cell cols="4">supervised 22.09 Unbiased-Teacher 39.18 40.76 43.72 46.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and 3, respectively. 33?0.67 39.89?1.97 34.46?2.44 28.63?3.25 2% 48.28?0.64 41.12?2.00 35.93?2.16 30.51?2.54 5% 51.27?0.48 43.11?2.08 37.95?2.14 31.75?2.72 10% 53.95?0.65 44.74?1.92 39.83?2.03 34.13?2.60 100% 56.61?0.34 40.42?2.17 34.87?2.47 29.61?2.69 After UDA 10% 57.58?0.56 55.80?0.60 54.70?0.51 53.44?0.38 100% 61.41?0.40 59.48?0.35 58.55?0.36 57.33?0.43 28?1.06 16.64?1.17 12.72?1.90 08.34?1.94 2% 30.16?0.58 21.44?1.91 16.28?2.22 11.35?2.39 5% 37.09?0.30 25.93?2.22 20.12?2.38 13.84?2.50 10% 41.51?0.58 28.57?1.88 22.57?2.15 16.17?2.38 100% 50.55?0.39 23.99?2.25 16.86?2.16 11.31?1.91 After UDA 10% 48.52?0.50 45.73?0.56 43.74?0.47 40.90?0.44 100% 60.86?0.38 57.35?0.61 55.42?0.66 52.60?0.60 54?0.78 38.37?2.32 32.44?2.73 26.32?3.42 2% 47.96?0.90 39.32?2.29 33.54?2.30 27.87?2.44 5% 50.55?0.74 41.09?2.30 35.68?2.16 29.45?2.69 10% 52.79?0.69 42.63?2.17 37.18?2.10 31.41?2.60 100% 54.95?0.37 37.98?2.21 32.58?2.37 27.56?2.48 After UDA 10% 55.60?0.52 54.07?0.58 53.00?0.49 51.55?0.35 100% 59.34?0.40 57.44?0.42 56.62?0.41 55.39?0.51</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MVOR+</cell><cell></cell></row><row><cell>models</cell><cell>1x</cell><cell>8x</cell><cell>10x</cell><cell>12x</cell></row><row><cell></cell><cell></cell><cell cols="2">AP bb person (mean?std)</cell><cell></cell></row><row><cell>Before UDA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1%</cell><cell cols="3">48.AP kp person (mean?std)</cell><cell></cell></row><row><cell>Before UDA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1%</cell><cell cols="2">25.AP bb (from mask) person</cell><cell>(mean?std)</cell><cell></cell></row><row><cell>Before UDA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1%</cell><cell>47.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will release MVOR+ dataset and the new TUM-OR-test annotations along with the source code at https://github.com/CAMMA-public/ HPE-AdaptOR.2 https://youtu.be/gqwPu9-nfGs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/visipedia/annotation tools</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/jizongFox/pytorch-randaugment</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/facebookresearch/unbiased-teacher</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by French state funds managed by the ANR within the Investissements d'Avenir program under reference ANR-16-CE33-0009 (DeepSurg) and ANR-10-IAHU-02 (IHU Strasbourg). This work was granted access to the HPC resources of IDRIS under the allocation 20XX-[AD011011631R1] made by GENCI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5221" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">View invariant human body detection and pose estimation from multiple depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bekhtaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04258</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing human skeletons in an operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B B</forename><surname>Shitrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring object relation in mean teacher for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11457" to="11466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: high quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5933" to="5942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations, in: International conference on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crdoco: Pixellevel domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Privacy-preserving action recognition for smart hospitals using lowresolution depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Machine Learning for Health</title>
		<imprint>
			<biblScope unit="issue">ML4H</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased mean teacher for crossdomain object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4091" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Physiological synchronization and entropy as measures of team cognitive load</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gabany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">103250</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated surgical activity recognition with one labeled sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="458" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What can be transferred: Unsupervised domain adaptation for endoscopic lesions segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4023" to="4032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Federated deep learning for detecting covid-19 lung abnormalities in ct: a privacy-preserving multinational validation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duhaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eskildsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sorba</surname></persName>
		</author>
		<ptr target="https://github.com/YaleDHLab/pix-plot" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition in the wild via selective knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2051" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lownet: Privacy preserved ultra-low resolution posture image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gochoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alnajjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="663" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fusing information from multiple 2d depth cameras for 3d human pose estimation in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1871" to="1879" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Task-driven super resolution: Object detection in low-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11316</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Every pixel matters: Center-aware feature alignment for domain adaptive object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="733" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-domain weaklysupervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporally consistent 3d pose estimation in the interventional room using discrete mrf optimization over rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Computer-Assisted Interventions</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pictorial structures on rgb-d images for human pose estimation in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Articulated clinician detection using 3d pictorial structures on rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A multi-view rgb-d approach for human pose estimation in operating rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards videobased surgical workflow understanding in open orthopaedic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sivanesan Uthraraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oussedik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A robust learning approach to domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodabandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="480" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6092" to="6101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Instancecut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5008" to="5017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Domain adaptive medical image segmentation via adversarial learning of disease-specific spatial patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Loehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09313</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07623</idno>
		<title level="m">Fedbn: Federated learning on non-iid features via local batch normalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Transformation-consistent self-ensembling model for semisupervised medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rabindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09487</idno>
		<title level="m">2020c. A robotic 3d perception system for operating room environment awareness</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exploring uncertainty in pseudo-label guided unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106996</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9131" to="9140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Proposalfree network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Surgical data science-from concepts to clinical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?rz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fallert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<idno>arXiv-2011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9034" to="9043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Or black box and surgical control tower: Recording and streaming data and analytics to improve surgical care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visceral Surgery</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcphee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08446</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Tiny people pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="558" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2278" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Knowledge distillation for semi-supervised domain adaptation, in: OR 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orbes-Arteainst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>S?rensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Data efficient unsupervised domain adaptation for cross-modality image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaption of object detectors: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13502</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Machine and deep learning for workflow recognition during surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minimally Invasive Therapy &amp; Allied Technologies</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Google deepmind and healthcare in an age of algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Powles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health and technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="351" to="367" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Do cifar-10 classifiers generalize to cifar-10?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00451</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">See it with your own eyes: markerless mobile augmented reality for radiation awareness in the hybrid room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Rodas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="429" to="440" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Automatic adaptation of object detectors to new domains using self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="780" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Privacy-preserving human activity recognition from extreme low resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6956" to="6965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04586</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Automatic operating room surgical activity recognition for robot-assisted surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haugerud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Multiinstitutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Prior-based domain adaptive object detection for hazy and rainy conditions, in: European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="763" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Assessment of endovascular team performances using a comprehensive data capture platform in the hybrid room: A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soenens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Doyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vlerick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vermassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grantcharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Herzeele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Vascular and Endovascular Surgery</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1028" to="1029" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Human pose estimation and its application to action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="page">103055</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Human pose estimation on privacypreserving low-resolution depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI, Springer</publisher>
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Self-supervision on unlabelled or data for multi-person 2d/3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Mvor: A multi-view rgb-d operating room dataset for 2d and 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdolrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="MICCAI" to="LABELS workshop" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Me, my data and i: The future of the personal data economy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Symons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Feature super-resolution: Make machine see more clearly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3994" to="4002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Directpose: Direct end-to-end multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation in semantic segmentation: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maracani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technologies 8</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Gotta adapt&apos;em all: Joint pixel and feature-level domain adaptation for recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2672" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Cai4cai: The rise of contextual artificial intelligence in computer-assisted interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4516" to="4526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structured prediction based selective pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6243" to="6250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Transferable normalization: towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1953" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Studying very low resolution recognition using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4792" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07576</idno>
		<title level="m">2021. Rethinking&quot; batch&quot; in batchnorm</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Detectron2-keypoint-rcnn-baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Keypoints/keypointrcnnR50FPN3x.yaml" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="Detectron2-maskcnn-gn-baseline" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Intriguing properties of adversarial training at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8163" to="8173" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Pose2seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Real-time medical phase recognition using long-term video understanding and progress gate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Burd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102224</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Collaborative unsupervised domain adaptation for medical image diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7834" to="7844" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Collaborative training between region proposal localization and classification for domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A review of single-source deep unsupervised visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">An annotation sparsification strategy for 3d medical image segmentation via representative selection and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6925" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Poseg: Pose-aware refinement network for human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="15007" to="15016" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="43" to="76" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
