<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
							<email>stanislav.fort@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><forename type="middle">Pascanu</forename><surname>Deepmind</surname></persName>
							<email>deepmindajbrock@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De Deepmind</surname></persName>
							<email>sohamde@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
							<email>slsmith@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In computer vision, it is standard practice to draw a single sample from the data augmentation procedure for each unique image in the mini-batch. However recent work has suggested drawing multiple samples can achieve higher test accuracies. In this work, we provide a detailed empirical evaluation of how the number of augmentation samples per unique image influences model performance on held out data when training deep ResNets. We demonstrate drawing multiple samples per image consistently enhances the test accuracy achieved for both small and large batch training. Crucially, this benefit arises even if different numbers of augmentations per image perform the same number of parameter updates and gradient evaluations (requiring the same total compute). Although prior work has found variance in the gradient estimate arising from subsampling the dataset has an implicit regularization benefit, our experiments suggest variance which arises from the data augmentation process harms generalization. We apply these insights to the highly performant NFNet-F5, achieving 86.8% top-1 w/o extra data on ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data augmentation plays a crucial role in computer vision, and it is currently essential to achieve competitive performance on held-out data <ref type="bibr" target="#b23">[Shorten and Khoshgoftaar, 2019]</ref>. However the origin of the benefits of data augmentation are not fully understood. In addition, a number of authors have identified that SGD has an implicit regularization benefit, whereby large learning rates and small batch sizes achieve higher accuracy on held-out data <ref type="bibr" target="#b15">[Keskar et al., 2017</ref><ref type="bibr" target="#b20">, Mandt et al., 2017</ref><ref type="bibr" target="#b24">, Smith and Le, 2018</ref><ref type="bibr" target="#b14">, Jastrz?bski et al., 2018</ref><ref type="bibr" target="#b3">, Chaudhari and Soatto, 2018</ref><ref type="bibr" target="#b17">, Li et al., 2019</ref><ref type="bibr" target="#b26">, Smith et al., 2021</ref>, yet most authors have not considered how data augmentation and large learning rates interact during training.</p><p>In this work, we note that data augmentation has two distinct influences on the gradient of a single training example. First, augmentation operations like left-right flips, random crops or RandAugment <ref type="bibr" target="#b5">[Cubuk et al., 2020]</ref> change the expected value of the gradient of an example, introducing bias. Second, since we draw a finite number of samples from the augmentation procedure in each minibatch (typically one per unique image), data augmentation also introduces variance. A key goal of this work is to establish whether the benefits of data augmentation arise solely from bias, or whether the variance introduced by the augmentation procedure is also beneficial. We were inspired to study this question by a recent study of Dropout <ref type="bibr" target="#b33">[Wei et al., 2020]</ref>, which claimed that both the bias and variance introduced by Dropout contribute to generalization. In addition, we wish to understand how the variance introduced by the augmentation procedure interacts with the variance introduced when we estimate the gradient on a subset of the training set.</p><p>To distinguish between the roles of bias and variance, we exploit a simple modification to standard training pipelines, which we call augmentation multiplicity (see <ref type="figure" target="#fig_0">Figure 1</ref>). This technique was first proposed by <ref type="bibr" target="#b12">Hoffer et al. [2019]</ref> as a promising strategy for large-batch training, and has recently gained popularity when training Vision Transformers <ref type="bibr" target="#b31">[Touvron et al., 2020</ref><ref type="bibr" target="#b32">[Touvron et al., , 2021</ref>. The key insight of augmentation multiplicity is that we can preserve the bias in the per-example gradients introduced by data augmentation, while simultaneously suppressing the variance, by drawing multiple augmentation samples of each unique image in the batch. One can achieve this either by allowing the batch size to grow as the augmentation multiplicity increases, or by holding the batch size fixed and reducing the number of unique examples in each batch. Note that in the latter case the variance arising from data augmentation falls but the variance arising from sub-sampling the dataset increases. We study both schemes in this work. Since SGD exhibits different behaviour at different batch sizes <ref type="bibr" target="#b9">[Goyal et al., 2017</ref><ref type="bibr" target="#b19">, Ma et al., 2018</ref><ref type="bibr">, Zhang et al., 2019</ref>, we also study augmentation multiplicity in both the small and large batch regimes. Our key findings are as follows:</p><p>1. If the number of unique images per batch is fixed (batch size grows as augmentation multiplicity increases), then test accuracy rises as augmentation multiplicity rises. This phenomenon arises even after we tune the epoch budget, implying the benefits of data augmentation arise from the bias in the gradient, not the variance. Note, by contrast, performance degrades for very large batch sizes in standard pipelines .</p><p>2. Augmentation multiplicities greater than 1 also achieve higher test accuracy if we hold the batch size fixed (implying that the number of unique images per batch decreases). This benefit is particularly pronounced for large batch training, but it also arises with small batch sizes. However in this setting, despite achieving higher accuracy on the test set, large augmentation multiplicities achieve slower convergence on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>To confirm our insights scale to high-performance networks, we provide an empirical evaluation of augmentation multiplicity on NFNets <ref type="bibr" target="#b2">[Brock et al., 2021b]</ref>. Using an NFNet-F5 with SAM regularization and augmentation multiplicity 16, we achieve 86.8% top-1 w/o extra data on ImageNet. We achieve this result after training for 34 epochs (168900 steps), while training the same model w/o augmentation multiplicity requires significantly more epochs and reaches lower accuracy. 3 4. These observations are counter-intuitive, as when the batch size is fixed, large augmentation multiplicities increase the overall variance in the minibatch gradient estimate (since the number of unique images in each batch is reduced). However because we draw multiple samples of the augmentation procedure for each image, the variance in the per-example gradients of specific examples is reduced. We conclude that the variance in the gradient estimate arising from sub-sampling the dataset has an implicit regularization benefit enhancing generalization, but that the variance which arises from the data augmentation procedure harms test accuracy.</p><p>As stated above, augmentation multiplicity was first proposed by <ref type="bibr" target="#b12">Hoffer et al. [2019]</ref> as a promising strategy for large batch training (i.e. by increasing the batch size as the augmentation multiplicity increases), while <ref type="bibr" target="#b0">Berman et al. [2019]</ref> found it can also enhance performance when the batch size is fixed. In addition, <ref type="bibr" target="#b4">Choi et al. [2019]</ref> explored using augmentation multiplicity to reduce communication overheads on device, and <ref type="bibr" target="#b11">Hendrycks et al. [2019]</ref> showed that it can improve robustness to distributional shift. Finally, augmentation multiplicity was recently used by <ref type="bibr" target="#b31">Touvron et al. [2020</ref><ref type="bibr" target="#b32">Touvron et al. [ , 2021</ref> to enhance the performance of Vision Transformers <ref type="bibr" target="#b7">[Dosovitskiy et al., 2020]</ref>. However, although augmentation multiplicity has been explored in a number of works, there has been no conclusive study demonstrating that its benefits are robust after careful tuning of the compute budget and the learning rate. In addition, prior work has not proposed that the benefits of augmentation multiplicity are connected to the implicit regularization benefit of SGD <ref type="bibr" target="#b15">[Keskar et al., 2017</ref><ref type="bibr" target="#b17">, Li et al., 2019</ref><ref type="bibr" target="#b26">, Smith et al., 2021</ref>.</p><p>We provide the first systematic study of how augmentation multiplicity influences deep networks trained with SGD, and we seek to explain why large augmentation multiplicities enhance generalization. We find that large augmentation multiplicities achieve higher test accuracy for both small and large batch sizes, and crucially they often achieve higher test accuracy without requiring larger compute budgets. We therefore believe large augmentation multiplicities should become the default setting in many vision applications.</p><p>This paper is structured as follows. In Section 2, we evaluate the performance of augmentation multiplicity when the number of unique images per batch is fixed. This section demonstrates that the variance introduced by the augmentation procedure harms test accuracy. In Section 3 we evaluate the performance of augmentation multiplicity when the total batch size is fixed, in order to demonstrate that augmentation multiplicity is a practical method for enhancing model performance in standard pipelines. Next, we explore why augmentation multiplicity enhances generalization in Section 4. Finally, we apply augmentation multiplicity to the NFNet model family <ref type="bibr" target="#b2">[Brock et al., 2021b]</ref> in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The benefits of data augmentation arise from bias, not variance</head><p>In this section, we show that the benefits of data augmentation in ResNets arise from the bias in the augmentation procedure, not the variance. We first consider a 16-4 Wide-ResNet [Zagoruyko and <ref type="bibr">Komodakis, 2016]</ref>, trained on CIFAR-100 <ref type="bibr" target="#b16">[Krizhevsky et al., 2009]</ref> with padding, left-right flips and random crops. The data augmentation process introduces a bias in the evaluated gradients. More specifically, if (x) denotes the loss for input x and F (x, ?) denotes the augmentation function whose output is the augmented image, with ? being the collection of random variables denoting the noise in the augmentation process, then data augmentation changes the expected value of the loss since:</p><formula xml:id="formula_0">E ? [ (F (x, ?))] = (x).<label>(1)</label></formula><p>However, typically practitioners only augment each image in a minibatch once by sampling a single ? 1 and computing (F (x, ? 1 )). This introduces variance into the training process. To disentangle the effects of bias and variance on final model performance, we train with a range of augmentation multiplicities n, meaning that for each unique input x, we produce n augmented inputs by drawing n random samples of the noise ?, and average the loss over these n samples:</p><formula xml:id="formula_1">1 n n i=1 (F (x, ? i )).<label>(2)</label></formula><p>Large augmentation multiplicities reduce the gradient covariance, helping to isolate the effect of bias.</p><p>In the CIFAR-100 experiments in this section, we draw 64 unique inputs in each minibatch, such that the total batch size B = 64n grows with augmentation multiplicity n. Note that we keep the number of unique examples in the minibatch fixed in this section, allowing the batch size to grow as the augmentation multiplicity rises, in order to isolate the role of the variance which arises from the augmentation procedure, without increasing the variance from sub-sampling the dataset.</p><p>The performance of batch-normalized networks depends strongly on the examples used to evaluate the batch statistics. Therefore to simplify our analysis, we train our Wide-ResNets without normalization using the SkipInit initialization scheme . We use SGD with a momentum coefficient of 0.9 and weight decay with a coefficient of 5 learning rate is constant for the first m/2 epochs, and then decays by a factor of 2 every remaining m 20 epochs. We provide the mean accuracy of the best 5 out of 7 training runs.</p><p>In <ref type="figure">Figure 2</ref>(a), we plot the test accuracy for a range of augmentation multiplicities, across a range of epoch budgets. We independently tune the learning rate on a logarithmic grid spaced by factors of 2 for each combination of epoch budget and augmentation multiplicity. After tuning the epoch budget, we find that large augmentation multiplicities achieve slightly higher test accuracy. For instance, augmentation multiplicity 16 achieves a peak test accuracy of 74.3 ? 0.1% after 128 epochs, while augmentation multiplicity 1 (normal training) achieves a peak of 74.1 ? 0.01% after 256 epochs. Since higher augmentation multiplicities reduce the variance from data augmentation without changing the bias, this confirms that the variance arising from data augmentation reduces the test accuracy. Large augmentation multiplicities also require fewer training epochs/parameter updates to reach optimal performance, indicating that the variance from data augmentation slows down training. For completeness, we note that training without data augmentation at batch size 64 achieves a substantially lower test accuracy of 61.4% after tuning the learning rate and compute budget, verifying that, as expected, the bias introduced by data augmentation significantly enhances generalization. Note that a similar experiment appeared in <ref type="bibr" target="#b12">Hoffer et al. [2019]</ref>, however our study is the first to retune the learning rate and epoch budget for each augmentation multiplicity. This is essential to establish whether the variance from data augmentation is detrimental by eliminating the possibility that the original hyper-parameters were sub-optimal.</p><p>In <ref type="figure">Figure 2</ref>(b), we demonstrate that large augmentation multiplicities also achieve faster convergence on the training set (per epoch/per parameter update) in this "growing batch" setting. This is intuitive, since large multiplicities perform more gradient evaluations per minibatch, reducing the variance of the gradient estimate. For clarity, we plot the training loss at the learning rate in our grid which minimizes training loss. We evaluate the training loss on the full training set using the raw images (without data augmentation). <ref type="figure">Figure 2</ref>(c), we plot how the test accuracy for a budget of 128 epochs depends on the learning rate. Note that 128 epochs is the optimal epoch budget for large augmentation multiplicities. We find that different multiplicities achieve similar test accuracy when the learning rate is small, but large augmentation multiplicities achieve higher test accuracy when the learning rate is large, and this enables large augmentation multiplicities to achieve higher overall test accuracy after tuning. These results suggest that the variance in the data augmentation operation reduces the stability of training, which impairs training with large learning rates and consequently reduces test performance. We discuss how large learning rates benefit generalization in more detail in Section 4 <ref type="bibr" target="#b17">[Li et al., 2019</ref><ref type="bibr" target="#b26">, Smith et al., 2021</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finally in</head><p>We provide additional results in <ref type="figure">Figure 3</ref>, for a ResNet-50 <ref type="bibr" target="#b10">[He et al., 2016</ref>] trained on ImageNet <ref type="bibr" target="#b22">[Russakovsky et al., 2015]</ref> with 256 unique images per batch using the Normalizer-Free (NF-) strategy of <ref type="bibr" target="#b1">Brock et al. [2021a]</ref>. We use SGD with Momentum coefficient 0.9, cosine learning rate decay <ref type="bibr" target="#b18">[Loshchilov and Hutter, 2016]</ref> and baseline pre-processing including left-right flips and random crops <ref type="bibr" target="#b30">[Szegedy et al., 2017]</ref>  <ref type="figure">Figure 3</ref>: Results with an NF-ResNet-50 on ImageNet. The number of unique images per batch is fixed, such that the batch size grows as the augmentation multiplicity n increases. (a) Top-1 accuracy at optimal learning rates for a range of epoch budgets. Larger augmentation multiplicities achieve higher test accuracies and also require fewer training epochs. (b) The training cross entropy for a range of epoch budgets. Note that we evaluate the cross entropy on a single minibatch, not the full dataset. (c) The test accuracy at a range of learning rates for a compute budget of 128 epochs. 5 ? 10 ?5 , label smoothing with a coefficient of 0.1 <ref type="bibr" target="#b29">[Szegedy et al., 2016]</ref>, stochastic depth with a drop rate of 0.1 <ref type="bibr" target="#b13">[Huang et al., 2016]</ref> and dropout before the final linear layer with a drop probability of 0.25 <ref type="bibr" target="#b27">[Srivastava et al., 2014]</ref>. We tune the learning rate on a logarithmic grid spaced by factors of 2, performing a single training run at each learning rate. Once again, we observe that large augmentation multiplicities achieve higher top-1 accuracies while also requiring fewer training epochs. For instance, augmentation multiplicity 1 (normal training) achieves a peak top-1 accuracy of 77.9% after 512 epochs, while augmentation multiplicity 8 achieves a peak top-1 accuracy of 78.4% after 128 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An empirical evaluation of augmentation multiplicity for fixed batch sizes</head><p>In Section 2, we established that the generalization benefit of data augmentation arises from the bias it introduces into the gradient estimate, while the variance from data augmentation both slows down optimization and impedes generalization. We also showed that large augmentation multiplicities can achieve higher test accuracy even after tuning the compute budget. However we allowed the minibatch size to grow as the augmentation multiplicity rose. This is impractical, since the batch size is usually determined by the hardware available for training. In this section, we confirm that large augmentation multiplicities continue to achieve superior test accuracy if we maintain a constant batch size B, such that the number of unique training examples per minibatch declines as the augmentation multiplicity increases. The behaviour of SGD differs substantially in the small batch, 'noise dominated' regime and the large batch, 'curvature dominated' regime <ref type="bibr" target="#b19">[Ma et al., 2018</ref><ref type="bibr">, Zhang et al., 2019</ref>. We consider both regimes here.</p><p>As before, we consider two models; A 16-4 Wide-ResNet trained on CIFAR-100 at batch sizes 64 (small) and 1024 (large), and an NF-ResNet-50 <ref type="bibr" target="#b1">[Brock et al., 2021a]</ref> trained on ImageNet at batch sizes 256 (small) and 4096 (large). We follow the same training pipelines as described in Section 2. In <ref type="figure">Figure 4</ref>, we plot the performance of both models in the large batch limit for a range of compute budgets. Note that since the number of unique images per batch now decreases as the augmentation multiplicity increases, larger multiplicities perform more parameter updates per epoch of training. We therefore provide the test accuracy achieved for a given epoch budget in figures a and c, as well as the test accuracy achieved within a given number of parameter updates in figures b and d. For clarity, since the batch size is fixed in this section, the computational cost of training is proportional to the number of parameter updates. Considering first figures a and c, for both models large augmentation multiplicities achieve significantly higher test accuracy while requiring a smaller epoch budget. For instance on the ResNet-50, augmentation multiplicity 8 achieves 78.6% top-1 accuracy after 128 epochs, while augmentation multiplicity 1 achieves a peak accuracy of 77.3% after 512 epochs.</p><p>In figures b and d, we observe that moderately large augmentation multiplicities also achieve higher test accuracy than normal training (n=1) without requiring more parameter updates (and therefore without requiring more compute) <ref type="bibr" target="#b0">[Berman et al., 2019]</ref>. For instance on the Wide-ResNet, augmentation multiplicity 1 achieves 72.1 ? 0.1% after 4 ? 10 5 updates, while multiplicity 4 achieves 73.7 ? 0.04%.</p><p>In <ref type="figure">Figure 5</ref>, we provide a similar panel of figures when training the same two models in the small batch limit. Considering first figures 5(a) and 5(c), on both CIFAR-100 and ImageNet, augmentation  <ref type="figure">Figure 4</ref>: Large augmentation multiplicities achieve significantly higher test accuracy after large batch training. Here the batch size is independent of the augmentation multiplicity n, such that the number of unique images per batch declines as augmentation multiplicity increases. We emphasize that large multiplicities perform more parameter updates per epoch here. We show the test accuracy achieved for a given epoch budget (a/c) and for a given number of parameter updates <ref type="bibr">(b/d</ref> multiplicities larger than 1 continue to achieve higher test accuracy while also requiring fewer training epochs. On CIFAR-100, the benefits of augmentation multiplicity saturate for multiplicities n 4, while on ImageNet the peak test accuracy continues to rise for all multiplicities considered. Note however that in the small batch regime large augmentation multiplicities typically require more parameter updates (figures 5(b) and 5(d)).</p><p>Based on the experiments presented in sections 2 and 3, we conclude that augmentation multiplicity is not only beneficial during large batch training <ref type="bibr" target="#b12">[Hoffer et al., 2019]</ref>, but that it also enhances generalization when the batch size is fixed. This phenomenon was first observed by <ref type="bibr" target="#b0">Berman et al. [2019]</ref>, however our study is the first to consider multiple batch sizes and to verify that the phenomenon is robust to retuning both the compute budget and the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why do large augmentation multiplicities enhance generalization?</head><p>In Section 3, we demonstrated that augmentation multiplicities larger than 1 achieve higher test accuracy for both small and large batch training, despite containing fewer unique training examples in each minibatch. This phenomenon is highly surprising since, as observed by <ref type="bibr" target="#b12">Hoffer et al. [2019]</ref>, gradients evaluated on different augmentations of the same image are correlated, while gradients evaluated on independent training examples are not. If we maintain a fixed batch size but reduce the number of unique images in the minibatch, the overall variance in our estimate of the gradient will increase, which we intuitively expect to lead to slower convergence on the training set (we evaluate the gradient variance at initialization for a range of augmentation multiplicities in appendix A). To verify this intuition, we plot the cross entropy achieved on the training set after a given number of parameter updates when training the Wide-ResNet on CIFAR-100 at batch size 1024 in <ref type="figure">Figure 6</ref>(a) and batch size 64 in <ref type="figure">Figure 6(b)</ref>. As expected, in both cases smaller augmentation multiplicities achieve faster convergence on the training set. We therefore conclude that the benefits  <ref type="figure">Figure 6</ref>: A 16-4 Wide-ResNet trained on CIFAR-100 at batch size 1024 and 64. Small augmentation multiplicities achieve lower training loss after a given number of parameter updates (a/b). For large batch sizes, large augmentation multiplicities achieve higher test accuracy without changing the optimal learning rate (c). For small batch sizes, large augmentation multiplicities achieve higher test accuracy but must be trained with smaller learning rates (d).</p><p>of large augmentation multiplicities arise only when evaluating our model on held-out data, i.e., large augmentation multiplicities impede optimization but benefit generalization.</p><p>Indeed, <ref type="bibr" target="#b12">Hoffer et al. [2019]</ref> suggested that large augmentation multiplicities may enable better generalization when the batch size is large because, since gradients evaluated on different augmentations of the same image are correlated, the variance in the final minibatch gradient estimate is larger. They argued that this preserves the generalization benefits of noise observed during small batch training <ref type="bibr" target="#b15">[Keskar et al., 2017]</ref>. However this argument is incomplete, since it does not give any explanation for why large batch sizes (with augmentation multiplicity) achieve superior test accuracy to smaller batch sizes (without augmentation multiplicity).</p><p>As an alternative explanation, we propose the benefits of augmentation multiplicity arise from the interaction between data augmentation and the use of finite learning rates. <ref type="bibr">4</ref> Recall that when training with growing batches in Section 2, large augmentation multiplicities were stable at larger learning rates, which appeared to account for their superior test accuracy. To investigate whether a similar phenomenon arises when the batch size is fixed, we plot the test accuracy achieved at a range of learning rates when training the Wide-ResNet on CIFAR-100 for 128 epochs at batch size 1024 in <ref type="figure">Figure 6</ref>(c) and at batch size 64 in <ref type="figure">Figure 6(d)</ref>. We observe different behaviour in the small and large batch regimes. For large batch training, large augmentation multiplicities achieve higher test accuracy at similar learning rates, while for small batch sizes large augmentation multiplicities achieve higher test accuracy but require smaller learning rates. In both cases, large augmentation multiplicities do not enable stable training at larger learning rates.</p><p>To explain these observations, we note that a large body of work has found that large learning rates have an implicit regularization benefit which enhances the test accuracy, and the strength of this regularization benefit is proportional to the ratio of the learning rate to the batch size <ref type="bibr" target="#b14">[Jastrz?bski et al., 2018</ref><ref type="bibr" target="#b17">, Li et al., 2019</ref><ref type="bibr" target="#b26">, Smith et al., 2021</ref>. Reducing the batch size increases the variance in the gradient, which enhances the implicit regularization effect. However prior empirical work has not distinguished whether reducing the batch size enhances the implicit regularization effect by reducing the number of unique examples in the minibatch, or whether it enhances the implicit regularization effect because it reduces the number of samples from the data augmentation process, which increases the variance in the minibatch gradient arising from data augmentation.</p><p>We observed in Section 2 that reducing the variance arising from the data augmentation procedure increases the test accuracy. We therefore conclude that the implicit regularization benefit of SGD arises from the variance introduced by minibatching, not the variance introduced by data augmentation. On this basis, we predict that when using augmentation multiplicity, the generalization benefit will be governed by the ratio of the learning rate to the number of unique training examples in the batch, not the ratio of the learning rate to the total batch size. When the number of unique examples in each minibatch is fixed (Section 2), this ratio is proportional to the learning rate. However, when the batch size is fixed, this ratio is proportional to the product of the learning rate and the augmentation multiplicity, which we define for clarity as the 'temperature' <ref type="bibr" target="#b20">[Mandt et al., 2017</ref><ref type="bibr" target="#b24">, Smith and Le, 2018</ref><ref type="bibr" target="#b21">, Park et al., 2019</ref>. In <ref type="figure">Figure 7</ref>, we plot the test accuracy achieved after 128 epochs for both the Wide-ResNet/CIFAR-100 and NF-ResNet50/ImageNet in both the small and large batch limits 2 5 2 4 2 3 2 2 2 1 2 0 2 1 2 2 Temperature (n )  <ref type="figure">Figure 7</ref>: We plot the test accuracy accuracy at a range of 'temperatures' for a 16-4 Wide-ResNet on CIFAR-100 (a/b) and an NF-ResNet50 on ImageNet (c/d) after 128 epochs of training with both large and small batch sizes. We define the temperature as the product of the learning rate and the augmentation multiplicity n. In all four cases different augmentation multiplicities achieve similar performance when the temperature is small, however larger augmentation multiplicities are stable at higher temperatures, which enables them to achieve higher test accuracies overall.</p><p>at a range of temperatures. Remarkably, we observe very similar behaviour in each plot. Different augmentation multiplicities achieve similar test accuracy when the temperature is small, but large augmentation multiplicities are stable at larger temperatures, and this enables them to achieve higher test accuracy overall.</p><p>To summarize, large augmentation multiplicities enhance generalization because they suppress a detrimental source of variance in the gradient estimate (the variance arising from data augmentation itself). This enables us to train at larger 'temperatures' (either larger learning rates or fewer unique images per minibatch), which in turn enhances the generalization benefit of finite learning rates. This generalization benefit arises from a beneficial source of variance in the gradient estimate (the variance arising from mini-batching). Note that although there is a large body of work studying the implicit regularization benefit of SGD, we believe our paper is the first to distinguish between the different roles of separate sources of variance in the gradient estimate.</p><p>We note however that, as observed in both Section 2 and Section 3, while large augmentation multiplicities achieve higher test accuracy after tuning the epoch budget, they do achieve lower test accuracy for very large epoch budgets (larger than the optimal epoch budget). Although this regime is not important in practice, this observation suggests that the variance in the gradient arising from data augmentation can help reduce over-fitting when models are over-trained. For completeness, we show in Appendix B that different augmentation multiplicities do not achieve similar test accuracy at the same temperature for large epoch budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">An empirical evaluation of augmentation multiplicity for NFNets</head><p>In Section 3, we showed that large augmentation multiplicities enhance generalization for both small and large batch training. In this section, we verify that these benefits continue to arise for highly performant, strongly regularized models. To achieve this goal, we evaluate the performance of augmentation multiplicity on the NFNet model family, recently proposed by <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>. NFNets comprise a family of models, denoted by NFNet-Fx, where x ? {1, 2, 3, 4, 5, 6}. These models were designed such that it takes roughly twice as long to evaluate a gradient for F2 as for F1 on similar hardware, and similarly twice as long to evaluate a gradient for F3 as for F2, and so on. NFNets are highly expressive, and consequently they benefit from extremely strong regularization and data augmentation, incorporating Dropout <ref type="bibr" target="#b27">[Srivastava et al., 2014]</ref>, Stochastic Depth <ref type="bibr" target="#b13">[Huang et al., 2016]</ref>, <ref type="bibr">RandAugment [Cubuk et al., 2020]</ref>, <ref type="bibr">Mixup [Zhang et al., 2018]</ref> and Cutmix <ref type="bibr" target="#b34">[Yun et al., 2019]</ref>. They are therefore ideally suited to evaluating the performance of large augmentation multiplicities at scale.</p><p>Following the implementation of <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>, we train each model variant with a batch size of 4096 using SGD with a momentum coefficient of 0.9, cosine annealing <ref type="bibr" target="#b18">[Loshchilov and Hutter, 2016]</ref>, and Adaptive Gradient Clipping (AGC). A moving average of the weights is stored during training and used to make predictions during inference <ref type="bibr" target="#b28">[Szegedy et al., 2015]</ref>. We provide results both with and without Sharpness-Aware Minimization (SAM) <ref type="bibr" target="#b8">[Foret et al., 2021]</ref>, an optimization technique which enhances generalization but roughly doubles the time required to compute a gradient. <ref type="bibr" target="#b2">Brock et al. [2021b]</ref> use learning rate = 1.6, however we found = 0.8 achieves higher top-1 accuracy for large augmentation multiplicities, which we adopt for all multiplicities n &gt; 1. In the  original study, all models were trained for 112,600 parameter updates (corresponding to 360 epochs at augmentation multiplicity 1). For all other details, we refer the reader to <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>.</p><p>In <ref type="table" target="#tab_6">Table 1</ref>, we provide a detailed evaluation of how augmentation multiplicity influences the performance of NFNet-F2, trained without SAM. Note that when applying augmentation multiplicities larger than 1, we take care to ensure that Mixup and CutMix are applied to different training inputs, not different augmentations of the same image. Larger augmentation multiplicities achieve higher test accuracy, even under a fixed compute budget of 112,600 parameter updates, while augmentation multiplicity 16 continues to improve when the compute budget is increased to 225,200 updates. As we show in <ref type="figure">Figure 8(a)</ref>, an intriguing implication of these results is that, despite achieving higher test accuracy, large augmentation multiplicities require an order of magnitude fewer training epochs. For clarity, we emphasize that we use 'epoch' to denote a full pass through the training set, and that large augmentation multiplicities still require similar numbers of parameter updates. However, this observation demonstrates that we could dramatically reduce the compute cost of training large vision models if we could reduce the variance arising from the data augmentation procedure.</p><p>In <ref type="table" target="#tab_7">Table 2</ref> we report the performance of NFNet-F3 and NFNet-F5 when trained with SAM <ref type="bibr" target="#b8">[Foret et al., 2021]</ref> at augmentation multiplicity 16. Our NFNet-F3 achieves 86.45% top-1 accuracy after 225,200 updates. This is within 0.05% of the 86.5% achieved by an NFNet-F6 with SAM after 112,600 updates without augmentation multiplicity in the original paper of <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>. However since NFNet-F3 requires roughly 8x less time to evaluate a gradient, it achieves this result with roughly 4x less compute, while also containing significantly fewer parameters and being significantly faster to evaluate at inference. After applying augmentation multiplicity, our NFNet-F5 with SAM achieves 86.8% after 168,900 updates. For clarity, we plot a range of results with and without SAM in <ref type="figure">Figure 8(b)</ref>. Large augmentation multiplicities consistently outperform the baseline accuracies reported by <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our study was inspired by a recent study of the origin of the regularization benefits of dropout <ref type="bibr" target="#b33">[Wei et al., 2020]</ref>. Like data augmentation, dropout introduces both bias and variance into minibatch gradients. However <ref type="bibr" target="#b33">Wei et al. [2020]</ref> found in LSTMs both the bias and the variance introduced by dropout contribute to generalization. For completeness, we investigate the roles of bias and variance in dropout for Wide-ResNets in appendix D. We do not observe a generalization benefit from variance in these experiments.</p><p>We have shown that large augmentation multiplicities achieve higher test accuracy, both when the batch size is proportional to the augmentation multiplicity, and when the batch size is fixed (such that the number of unique images in each minibatch decreases as the augmentation multiplicity increases).  <ref type="figure">Figure 8</ref>: An evaluation of augmentation multiplicity on ImageNet using NFNets. (a) We consider a range of augmentation multiplicities on NFNet-F2 and NFNet-F3, trained at batch size 4096 without the SAM optimizer <ref type="bibr" target="#b8">[Foret et al., 2021]</ref>. Larger augmentation multiplicities achieve higher top-1 accuracy and require fewer training epochs. For clarity, we emphasize that we define an epoch as a single pass through the training set. (b) We compare the performance achieved at augmentation multiplicity 16 to the original performance reported at augmentation multiplicity 1 by <ref type="bibr" target="#b2">Brock et al. [2021b]</ref>. Circles denote models trained with SAM, while triangles denote models trained without SAM. Black points denote the original NFNet results reported by <ref type="bibr" target="#b2">Brock et al. [2021b]</ref> (without augmentation multiplicity). Our models reach substantially higher top-1 accuracy both with and without SAM.</p><p>A natural question raised by this observation is whether the benefits of augmentation multiplicity arise because we sample multiple augmentations of the same image in the same minibatch, or whether it is sufficient to resample different augmentations of the same unique images in neighbouring minibatches. We briefly compare these two schemes empirically in appendix C, where we find that, while both implementations achieve higher test accuracy than standard training, the benefits of augmentation multiplicity are most significant when we sample multiple augmentations per unique image inside the same minibatch (the scheme studied in the main text). In particular, this approach achieves superior performance for large augmentation multiplicities.</p><p>Finally, we note that augmentation multiplicity has appeared under a range of names in the literature ('batch augmentation' <ref type="bibr" target="#b12">[Hoffer et al., 2019]</ref>, 'data echoing' <ref type="bibr" target="#b4">[Choi et al., 2019]</ref>, 'repeated augmentation' <ref type="bibr" target="#b0">[Berman et al., 2019]</ref>). We chose the terminology 'augmentation multiplicity' in this work as it enabled us to describe our experiments more succinctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We provide an empirical study of how augmentation multiplicity, the number of data augmentation samples drawn per unique image in each minibatch, influences test accuracy when training deep residual networks. We find that augmentation multiplicities greater than 1 consistently achieve higher test accuracy, despite achieving slower convergence on the training set (when the overall batch size is fixed). These benefits are particularly significant during large batch training but also arise when the batch size is small. We argue that this phenomenon arises from the interaction between the variance data augmentation introduces into the gradient estimate and the role of finite learning rates during training. Our study suggests practitioners should consider choosing large augmentation multiplicities as the default when training neural networks for computer vision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Variance</head><p>Fixed Batch Growing Batch <ref type="figure">Figure 9</ref>: We evaluate the mean variance in the minibatch gradient estimate, across all model parameters for a 16-4 Wide-ResNet evaluated on CIFAR-100 at initialization without weight decay. For "fixed batch" the batch size B = 16 is independent of the augmentation multiplicity, while for "growing batch" the batch size B = 16n grows as the augmentation multiplicity rises to ensure the number of unique examples in each minibatch is fixed. In the "growing batch" scheme, the variance across minibatches does not depend strongly on the augmentation multiplicity, however in the "fixed batch" scheme, the variance across minibatches increases as the augmentation multiplicity increases.</p><p>To verify that increasing the augmentation multiplicity increases the variance in the minibatch estimate of the gradient when the batch size is fixed, we plot the variance between minibatch gradients for the 16-4 Wide-ResNet at initialization in figure 9. Since the gradient at initialization is otherwise dominated by the L 2 loss, we set the L 2 coefficient to zero. The model is otherwise unchanged. We first evaluate the variance for all model parameters, then average the variance across all parameters within a particular layer, before averaging again across all model layers to obtain a single scalar value. When the number of unique examples in each minibatch is fixed ("growing batch"), the variance between minibatches does not depend strongly on the augmentation multiplicity, however when the batch size is fixed ("fixed batch"), the variance increases as the augmentation multiplicity rises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The dependence on the temperature for large epoch budgets</head><p>We showed in Section 4 that when the batch size is fixed, different augmentation multiplicities achieve similar test accuracy for moderate epoch budgets at small 'temperatures', which we define as the product of the augmentation multiplicity n and the learning rate , while large augmentation multiplicities achieve higher test accuracy for large temperatures. However we also observed that for large epoch budgets (when models are over-trained), small augmentation multiplicities can reduce overfitting. To explore this further, in <ref type="figure" target="#fig_0">Figure 10</ref> we show how the test accuracy depends on the temperature for both small and large epoch budgets on our 16-4 Wide-ResNet/CIFAR-100. We find that at small epoch budgets, different augmentation multiplicities achieve similar test accuracy when the temperature is small, as observed for moderate epoch budgets in Section 4, while for large epoch budgets, small augmentation multiplicities achieve higher test accuracy at a given temperature, verifying that small augmentation multiplicities can help prevent overfitting in over-trained models.  <ref type="figure" target="#fig_0">Figure 11</ref>: A 16-4 Wide-ResNet trained on CIFAR-100 with a fixed batch size of 1024. Solid lines ("within") denote the default implementation of augmentation multiplicity, whereby we sample n augmentations per image within each minibatch (reducing the number of unique images per batch). Meanwhile dashed lines ("neighbouring") denote an alternative scheme, whereby we only sample a single augmentation per image in each minibatch, but we resample the same training examples in n neighbouring minibatches (with different data augmentations). (a) We find that the neighbouring scheme can also enhance generalization for moderate multiplicities, however it performs poorly when the multiplicity is large. (b) The "neighbouring" scheme also achieves slightly lower training losses.</p><p>C Do multiple augmentations need to occur within the same minibatch?</p><p>In the main text, we analysed a single version of augmentation multiplicity. In this default implementation, we draw n augmentations of each unique image within the same minibatch. When the batch size is fixed, this implies the number of unique images per batch decreases as the augmentation multiplicity increases. In <ref type="figure" target="#fig_0">Figure 11</ref> we compare this default implementation (which we refer to as "within") to an alternative scheme, whereby we only sample a single augmentation per unique image in any single minibatch, but we sample the same set of unique images in n neighbouring minibatches. For clarity, these minibatches contain the same unique images but we draw different augmentations of each image in each adjacent minibatch. We refer to this alternative scheme as "neighbouring".</p><p>Normal training (augmentation multiplicity 1) is shown in blue, the default implementation of augmentation multiplicity ("within") is shown with full lines, and the alternative implementation ("neighbouring") is shown with dashed lines. For moderate multiplicities, both implementations enhance the test accuracy, as shown in <ref type="figure" target="#fig_0">Figure 11(a)</ref>. However for large augmentation multiplicities, the default implementation ("within") continues to enhance generalization, while the performance of the alternative scheme ("neighbouring") begins to degrade. We therefore conclude that to maximize the benefits of augmentation multiplicity, one should adopt the default implementation ("within"), placing multiple augmentations of the same image inside the same minibatch.  <ref type="figure" target="#fig_0">Figure 12</ref>: Results with a 16-4 Wide-ResNet on CIFAR-100, trained using dropout with a drop probability of 0.4. The number of unique images in each batch is fixed at 64, and we evaluate the performance at augmentation multiplicity 1. However we average each gradient across n models generated from n different samples of the dropout mask. In <ref type="figure">figure (a)</ref>, we find that the test accuracy does not depend strongly on the number of samples of the dropout mask, which suggests that the benefits of dropout in this network arise from the bias introduced in the gradient, not the variance. In <ref type="figure">figure (b)</ref> we show that large numbers of samples minimize the training loss more quickly. Finally in figure (c) we provide the test accuracy achieved for a range of learning rates after 512 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Bias and variance in dropout</head><p>Dropout <ref type="bibr" target="#b27">[Srivastava et al., 2014]</ref> is a widely used regularization technique in deep learning. For each minibatch gradient evaluation, a random boolean mask is sampled which covers the output activations of a chosen layer of the network, such that any output unit (and its connections) is dropped with a specified drop probability. The purpose of dropout is to force the network to learn redundant representations, thus discouraging overfitting and enhancing generalization.</p><p>Like data augmentation, dropout has two influences on the gradient distribution. First, it changes the expected value of the gradient, introducing bias. Second, since we typically only sample a single dropout mask, dropout also introduces a source of variance. The roles of these two effects was studied by <ref type="bibr" target="#b33">Wei et al. [2020]</ref>, who refer to the bias and variance as the implicit and explicit regularization effects respectively. To disentangle the role of bias and variance, they take an average of n gradients evaluated for n different random dropout masks before taking each optimization step. The larger the n, the lower the variance arising from the stochasticity of sampling a dropout mask, while the bias remains unchanged. The authors found that both the bias and the variance arising from dropout enhance generalization in LSTMs. Note that by contrast, we found that the variance arising from the data augmentation procedure harms generalization for ResNets trained on CIFAR-100 and ImageNet.</p><p>We replicate the empirical analysis of <ref type="bibr" target="#b33">Wei et al. [2020]</ref> for a 16-4 Wide-ResNet trained on CIFAR-100 in <ref type="figure" target="#fig_0">Figure 12</ref>. We apply dropout on the final linear layer of the network with drop probability 0.4, averaging each gradient over n samples of the dropout mask. Note that we average the gradient across different dropout masks but with the same augmentations of the input (i.e., the augmentation multiplicity is set to 1 throughout). We find that networks trained with dropout achieve higher test accuracy, while also requiring larger epoch budgets (see <ref type="figure">Figure 5</ref>(a) for an equivalent network trained without dropout at a range of augmentation multiplicities). However reducing the dropout variance by averaging the gradient across multiple masks does not appear to substantially enhance test accuracy, suggesting that the benefits of dropout in this network primarily arise from the bias in the gradient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two forms of augmentation multiplicity. In the top row we draw 2 augmentation samples per image, while preserving the number of unique examples in the batch (batch size grows as augmentation multiplicity rises). In the bottom row we draw 2 augmentation samples per image, but hold the batch size fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Wide-ResNet trained on CIFAR-100 at batch size 1024, while figures c and d consider an NF-ResNet50 trained on ImageNet at batch size 4096. In both cases, large augmentation multiplicities achieve substantially higher test accuracy, while also requiring fewer training epochs and without requiring more parameter updates. Large augmentation multiplicities achieve higher test accuracy for small batch training. We show the test accuracy achieved for a given epoch budget (a/c) and a given number of parameter updates (b/d).Figures a and b consider a 16-4Wide-ResNet trained on CIFAR-100 with a batch size of 64, while figures c and d consider an NF-ResNet50 trained on ImageNet at batch size 256. In both cases augmentation multiplicities greater than 1 achieve higher test accuracy in fewer training epochs while requiring similar or slightly larger numbers of parameter updates.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>). Figures a</cell></row><row><cell>and b consider a 16-4 32 128 512 Epoch Budget 70 71 72 73 74 Test Accuracy n=1 n=4 n=16 n=64</cell><cell>Test Accuracy</cell><cell>70 71 72 73 74</cell><cell>10 5</cell><cell>10 6 Parameter Updates</cell><cell>10 7 n=1 n=4 n=16 n=64</cell><cell>Top-1 Accuracy</cell><cell>75 76 77 78</cell><cell>8</cell><cell>32 Epoch Budget 128</cell><cell>512 n=1 n=2 n=4 n=8</cell><cell>Top-1 Accuracy</cell><cell>Parameter Updates 2 18 2 19 2 20 2 21 2 22 75 76 77 78 n=1 n=2 n=4 n=8</cell></row><row><cell>(a) CIFAR-100</cell><cell></cell><cell></cell><cell cols="2">(b) CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) ImageNet</cell><cell></cell><cell></cell><cell>(d) ImageNet</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>NFNet-F2 trained on ImageNet for a range of augmentation multiplicities, without the SAM optimizer, at batch size 4096. Large augmentation multiplicities achieve higher top-1 accuracy for the same number of parameter updates, and their performance improves further if we increase the compute budget. We also report the performance of NFNet-F3 trained for 225,200 updates at augmentation multiplicity 16. Results marked with an * are taken from the original publication of<ref type="bibr" target="#b2">Brock et al. [2021b]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NFNet-F2 w/out SAM</cell><cell></cell><cell cols="2">NFNet-F3 w/out SAM</cell></row><row><cell></cell><cell></cell><cell cols="3">Augmentation Multiplicity:</cell><cell></cell><cell cols="2">Augmentation Multiplicity:</cell></row><row><cell>Parameter updates:</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>1</cell><cell>16</cell></row><row><cell cols="6">112,600 85.20 85.25 85.35 85.47 85.49</cell><cell>85.7  *</cell><cell>-</cell></row><row><cell>225,200</cell><cell>-</cell><cell>-</cell><cell cols="3">84.53 85.25 85.71</cell><cell>-</cell><cell>86.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>NFNet-F3 and F5, trained on ImageNet at augmentation multiplicity 16 with SAM at batch size 4096. We significantly exceed the original performance reported by<ref type="bibr" target="#b2">Brock et al. [2021b]</ref>.</figDesc><table><row><cell>Augmentation Multiplicity 16</cell><cell cols="2">NFNet-F3 w/ SAM</cell><cell cols="2">NFNet-F5 w/ SAM</cell></row><row><cell>Parameter Updates:</cell><cell>112,600</cell><cell>225,200</cell><cell>112,600</cell><cell>168,900</cell></row><row><cell></cell><cell>85.76</cell><cell>86.45</cell><cell>86.54</cell><cell>86.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), pages 87.1-87.12, September 2016. Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J. Shallue, and Roger B. Grosse. Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model. In Advances in Neural Information Processing Systems, 2019.</figDesc><table><row><cell cols="4">Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical</cell></row><row><cell cols="4">risk minimization. In International Conference on Learning Representations, 2018. URL https:</cell></row><row><cell cols="3">//openreview.net/forum?id=r1Ddp1-Rb.</cell><cell></cell></row><row><cell cols="3">A Augmentation multiplicity and gradient variance</cell><cell></cell></row><row><cell>10 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 4</cell><cell>1</cell><cell>2 Augmentation Multiplicity 4 8</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>) we exhibit how the test accuracy depends on the temperature, defined as the product of the augmentation multiplicity n and the learning rate , for a budget of 32 training epochs. For both batch sizes, we find that the test accuracy is determined by the temperature for small epoch budgets when the temperature is also small, while large augmentation multiplicities achieve higher test accuracy for large temperatures. Meanwhile in (c)/(d), we show that small augmentation multiplicities achieve higher test accuracy at a given temperature when the epoch budget is large (512 training epochs).</figDesc><table><row><cell>Test Accuracy</cell><cell>2 5 2 4 2 3 2 2 2 1 2 0 2 1 2 2 Temperature (n ) 20 40 60 n=1 n=2 n=4 n=8 n=16</cell><cell>Test Accuracy</cell><cell cols="2">2 9 2 8 2 7 2 6 2 5 2 4 2 3 2 2 Temperature (n ) 55 60 65 70 n=1 n=4 n=16 n=64</cell><cell cols="2">Test Accuracy</cell><cell cols="2">2 5 2 4 2 3 2 2 2 1 2 0 2 1 2 2 Temperature (n ) 66 68 70 72 n=1 n=2 n=4 n=8 n=16</cell><cell>Test Accuracy</cell><cell>Temperature (n ) 2 9 2 8 2 7 2 6 2 5 2 4 2 3 2 2 66 68 70 72 74 n=1 n=4 n=16 n=64</cell></row><row><cell cols="2">(a) Large batch/32 epochs</cell><cell cols="4">(b) Small batch/32 epochs</cell><cell cols="3">(c) Large batch/512 epochs</cell><cell>(d) Small batch/512 epochs</cell></row><row><cell cols="9">Figure 10: A 16-4 Wide-ResNet trained on CIFAR-100 at two batch sizes, 1024 (large) and 64 (small).</cell></row><row><cell cols="4">In (a)/(b10 4 Parameter Updates 10 5 64 66 68 70 72 74 Test Accuracy n=1 n=4 n=16 n=64 Within Neighbouring</cell><cell cols="4">Train Cross Entropy</cell><cell>10 3 10 2 10 1 10 0 10 1</cell><cell>10 4 Parameter Updates 10 5 n=1 n=4 n=16 n=64 Within Neighbouring</cell></row><row><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For clarity, we define an epoch as a single pass through the entire training set. This requires more gradient evaluations as the augmentation multiplicity increases. However we also verify that NFNets trained with large augmentation multiplicities achieve higher validation accuracy while requiring less overall compute.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In support of this claim, we note that in the limit of vanishing learning rates, stochastic gradient descent follows the path of gradient flow for any batch size and any augmentation multiplicity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yee Whye Teh, Karen Simonyan, Zahra Ahmed and Hyunjik Kim for helpful advice, and Matthias Bauer for feedback on an earlier draft of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IX3Nnir2omJ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Information Theory and Applications Workshop (ITA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05550</idno>
		<title level="m">Faster neural network training with data echoing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6Tm1mposlrM" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training Imagenet in 1 Hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09335</idno>
		<title level="m">Augment your batch: better training with larger batches</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Three Factors Influencing Minima in SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning, ICANN</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards explaining the regularization effect of initial large learning rate in training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11669" to="11680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raef</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML</title>
		<meeting>the 35th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent as approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4873" to="4907" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The effect of network width on stochastic gradient descent and generalization: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5042" to="5051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bayesian perspective on generalization and stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the generalization benefit of noise in stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the origin of implicit regularization in stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dherin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rq_Qr0c1Hyo" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The implicit and explicit regularization effects of dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10181" to="10192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
