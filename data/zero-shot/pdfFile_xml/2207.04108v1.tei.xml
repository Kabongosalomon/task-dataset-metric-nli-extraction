<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ayoola</surname></persName>
							<email>tayoola@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhi</forename><surname>Tyagi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Fisher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pierleoni</surname></persName>
							<email>apierleo@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce ReFinED, an efficient end-toend entity linking model which uses finegrained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed. Our code and pre-trained models are available at https://github.com/alexa/ReFinED.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity linking (EL) is the task of recognising mentions of entities in unstructured text documents and linking them to the corresponding entities in a Knowledge Base (KB), such as Wikidata. EL is commonly a first stage in systems for question answering <ref type="bibr" target="#b21">(Wang et al., 2021)</ref>, automated KB population <ref type="bibr">(Hoffmann et al., 2011)</ref>, and relation extraction <ref type="bibr" target="#b0">(Baldini Soares et al., 2019)</ref>.</p><p>Currently, EL systems use deep learning methods to learn representations for entities and mentions <ref type="bibr">(Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b4">Le and Titov, 2018)</ref>. Initial techniques learned representations from text alone, which relied on entities appearing in similar contexts in the training data and meant models were only able to link mentions to entities that appeared in the training data. This is problematic both as KBs are continuously growing, and as it is infeasible to build an EL dataset containing all entities in a large KB (such as Wikidata with over 90 million entities). The largest public EL dataset is Wikipedia (using internal hyperlinks as labels), which covers just 3% of the entities in Wikidata.</p><p>Recent models addressed this problem by producing entity representations from a subset of KB information, e.g., entity descriptions <ref type="bibr" target="#b24">(Wu et al., 2020;</ref><ref type="bibr" target="#b7">Logeswaran et al., 2019)</ref> or fine-grained entity types <ref type="bibr" target="#b11">(Onoe and Durrett, 2020;</ref><ref type="bibr" target="#b15">Raiman and Raiman, 2018)</ref>, allowing linking to entities not present in the training data or added to the KB after training; termed "zero-shot" in the EL literature. <ref type="bibr">1</ref> However, existing zero-shot-capable EL approaches are an order of magnitude more computationally expensive than non-zero-shot models <ref type="bibr" target="#b20">(van Hulst et al., 2020)</ref> as they either require numerous entity types <ref type="bibr" target="#b11">(Onoe and Durrett, 2020)</ref>, multiple forward passes of a large-scale model to encode mentions and descriptions <ref type="bibr" target="#b24">(Wu et al., 2020)</ref>, or regeneration of the input text autoregressively <ref type="bibr" target="#b2">(Cao et al., 2020)</ref>. This makes large-scale processing expensive and thus makes it difficult to benefit from many advantages of zero-shot EL, e.g. the ability to keep up-to-date with new or updated KBs.</p><p>In this paper, we propose an efficient end-toend zero-shot-capable EL model, ReFinED 2 , which uses fine-grained entity types and entity descriptions to perform entity linking or entity disambiguation (ED; where entity mentions are given). We show that combining information from entity types and descriptions in a simple transformer-based encoder yields performance which is stronger than more complex architectures, surpassing state-ofthe-art (SOTA) on 4 ED datasets and 5 EL datasets, and improving overall EL performance by 3.7 F1 points on average across 8 datasets. Importantly, ReFinED performs mention detection, fine-grained entity typing, and entity disambiguation for all men-tions within a document in a single forward pass, making it comparable in terms of inference speed to non-zero-shot models. It is 6 times faster than the most efficient zero-shot-capable baseline (which has 9 F1 points lower performance), and more than 60 times faster than more accurate systems (which come within 3 F1 points of ReFinED's average ED performance).</p><p>As opposed to previous EL models which primarily use Wikipedia as the target KB, ReFinED targets Wikidata, which enables it to link to 15 times more entities. This is because prior work uses information (e.g. titles, categories, first sentences) from Wikipedia to perform linking. It is unclear whether prior work could be expanded to Wikidata without a drop in performance because entity descriptions are less informative and there are fewer types per entity <ref type="bibr" target="#b22">(Weikum et al., 2021)</ref>.</p><p>The combination of high accuracy, scalability (with respect to KB size) and fast inference speed makes ReFinED a strong choice for a "web-scale" 3 EL system, in which cost scales approximately linearly with inference speed. We have successfully deployed ReFinED to production in a real-world application and share the lessons learned in Section 6.</p><p>Our contributions are as follows:</p><p>1. We build a simple and efficient zero-shot capable end-to-end EL model using entity descriptions and entity typing, which outperforms previous approaches on standard-EL datasets by 3.7 F1 points on average.</p><p>2. We demonstrate our model is more than 6 times faster than existing low-accuracy zeroshot capable systems, and 60 times faster than higher-accuracy systems, whilst also being capable of disambiguating against Wikidatascale entity sets. The combination of accuracy, speed and scale makes the model suitable for web-scale information extraction.</p><p>3. We release our code and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Single architecture for entity linking EL consists of two main tasks, mention detection (MD) and ED. MD involves recognising mentions of entities in text, and ED assigns a KB entity to each mention. We follow <ref type="bibr" target="#b3">(Kolitsas et al., 2018)</ref> in training a joint model for MD and ED.</p><p>Entity disambiguation with fine-grained entity typing In <ref type="bibr" target="#b11">Onoe and Durrett (2020)</ref> and <ref type="bibr" target="#b15">Raiman and Raiman (2018)</ref> ED is formulated as an entity typing problem. A fine-grained entity typing model is trained on a distantly-supervised dataset consisting of over 10k types derived from Wikipedia categories (e.g. movies released in a specific year). The entity typing model is then used to link entities. We extend their approach to Wikidata, by using a subset of Wikidata triples for providing types instead of Wikipedia categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity disambiguation with entity descriptions</head><p>Several recent works have used entity descriptions for ED <ref type="bibr" target="#b24">(Wu et al., 2020;</ref><ref type="bibr" target="#b7">Logeswaran et al., 2019)</ref>. Typically, descriptions are sourced from Wikipedia by joining the entity's title with the first sentence of the Wikipedia article. Entities are ranked by concatenating mention context and entity description, then passing each mention-entity pair to a crossencoder. <ref type="bibr" target="#b24">Wu et al. (2020)</ref> shows a cross-encoder outperforms a bi-encoder, with the latter missing many fine-grained interactions between context and description. In our work, we find that a bi-encoder is sufficient to achieve SOTA performance when combined with fine-grained entity typing, and generalise the approach from Wikipedia (6M entities) to Wikidata (90M entities). 4</p><p>3 Proposed method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>Given a KB 5 with a set of entities E = {e 1 , e 2 , . . . , e |E| }, let X = [x 1 , x 2 , . . . , x |X| ] be a sequence of tokens in the document, and M = {m 1 , m 2 , ...m |M | } be a set of entity mentions. The goal of ED is to create a function M : M ? E which assigns each mention the correct entity label. In EL, both the mention spans and entity labels need to be predicted. We only consider mentions with a valid gold entity in the KB during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>We propose an end-to-end EL model which is jointly optimised for mention detection, finegrained entity typing, and entity disambiguation for all mentions within a document in a single forward pass. In this section, we describe the components of our model, depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Context representation</head><p>We encode the tokens x i in the input text document using a Transformer model. We use the contextualised token embeddings from the final layer, denoted as h i for the token x i . 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mention detection</head><p>Entity linking requires entity mentions to be predicted. We encode mentions using the BIO tagging format <ref type="bibr" target="#b16">(Ramshaw and Marcus, 1995)</ref> with 3 labels which indicate whether a token is at the beginning, inside of, or outside of a mention. We train a linear layer to perform token classification from the contextualised token embeddings h i using crossentropy loss L m with respect to the gold token labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mention representation</head><p>A fixed-length embedding m i for each mention m i is obtained by average pooling the contextualised tokens embeddings of the mention. All mentions M in a document X are encoded in a single forward pass, which improves efficiency relative to previous work that require a forward-pass for each mention <ref type="bibr" target="#b24">(Wu et al., 2020;</ref><ref type="bibr" target="#b12">Orr et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Entity typing score ?</head><p>Given a fixed set of types t ? T from a KB, where t is a relation-object pair (r, o) (e.g. (instance of, song)), we predict an independent probability for each type t for each mention by applying a linear layer f 1 followed by a sigmoid activation to the mention embedding m i . To score mention-entity pairs using predicted types, we calculate the Euclidean distance (L2 norm) between predicted types and the candidate entity's types c j binary vector 7 :</p><formula xml:id="formula_0">?(e j , m i ) = ?(f 1 (m i )) ? c j 2<label>(1)</label></formula><p>We follow <ref type="bibr" target="#b11">Onoe and Durrett (2020)</ref> by training the entity typing module on distantly-supervised type labels from the gold entity using binary crossentropy loss L t . See Appendix A for details on the choice of types T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Entity description score ?</head><p>We use a bi-encoder architecture similar to the work of <ref type="bibr" target="#b24">Wu et al. (2020)</ref> but modified to encode all mentions m i in a document simultaneously (as explained in Section 3.5). We represent KB entities as:</p><p>[CLS] label [SEP] description [SEP] where label and description are the tokens of the entity label and entity description in the KB. We use a separate Transformer model (trained jointly with our mention transformer) to encode the representation of KB entities e j into fixed dimension vectors (description embeddings) d j by taking final layer embedding for the [CLS] token. We apply linear layers f 2 and f 3 to the mention embeddings m i and entity description embeddings d j respectively to project them to a shared vector space. We calculate the dot product between the two projected embeddings to compute the entity scores:</p><formula xml:id="formula_1">?(e j , m i ) = f 2 (m i ) ? f 3 (d j )<label>(2)</label></formula><p>We train this module using cross-entropy loss L d , with respect to gold entity label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Combined score ?</head><p>We compute a combined score ? by applying a linear layer (with output dimension 1) f 4 on top of the concatenation of entity typing score, entity description score, and a global entity priorP (e|m).</p><p>The global entity prior is obtained from a corpus <ref type="bibr">(Hoffart et al., 2011)</ref> or a popularity metric (Diefenbach and Thalhammer, 2018). We includeP (e|m) to improve results for cases where context is limited (e.g. short question text). In addition, we add a special candidate for the NIL entity with an unnormalised score of 0, which indicates none of the candidate entities are correct.</p><p>?(e j , m i ) = f 4 (?(e j , m i ); ?(e j , m i );P (e j |m i )) (3) We train this module using cross-entropy loss L c with respect to the gold entity label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Optimisation and inference</head><p>We optimise the model using a weighted sum of the module-specific losses with fixed weights, which are tunable hyperparameters. At training time, we use the provided mention spans instead of the predicted mention spans and train mention detection alongside the other tasks:</p><formula xml:id="formula_2">L = ? 1 L m + ? 2 L t + ? 3 L d + ? 4 L c<label>(4)</label></formula><p>For EL inference, we use the predicted mention spans and take the KB entity (or NIL) with the highest combined score. For ED inference, we use the provided gold mention spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Zero-shot ED</head><p>Our proposed method is able to link to zero-shot (unseen during training) entities because it scores entities based on types and descriptions. New entities can be introduced by updating entity lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity disambiguation</head><p>Non-zero-shot ED We evaluate our model on the ED task using the same experimental setting as previous work (Ganea and Hofmann, 2017; <ref type="bibr" target="#b4">Le and Titov, 2018;</ref><ref type="bibr" target="#b2">Cao et al., 2020)</ref>. We pretrain on Wikipedia, then use the AIDA-CoNLL dataset <ref type="bibr">(Hoffart et al., 2011)</ref> to fine-tune and evaluate. We measure out-of-domain performance on the datasets MSNBC (Cucerzan, 2007), AQUAINT <ref type="bibr" target="#b8">(Milne and Witten, 2008)</ref>, ACE2004 <ref type="bibr" target="#b17">(Ratinov et al., 2011)</ref>, WNED-CWEB (CWEB) <ref type="bibr">(Gabrilovich et al., 2013)</ref> and WNED-WIKI (WIKI) (Guo and <ref type="bibr">Barbosa, 2018)</ref>. We report InKB micro-F1 <ref type="bibr" target="#b19">(R?der et al., 2018)</ref>. We also evaluate on AIDA-CoNLL using the candidate list generated by <ref type="bibr" target="#b13">Pershina et al. (2015)</ref>, known as PPRforNED, for the sake of comparison with previous SOTA results.</p><p>Zero-shot ED To compare our method to previous work, we measure zero-shot ED performance using the WikiLinksNED Unseen Mentions dataset <ref type="bibr">(Eshel et al., 2017;</ref><ref type="bibr" target="#b11">Onoe and Durrett, 2020)</ref>. The dataset contains a diverse set of ambiguous entities spanning multiple domains. We train our model on the provided training data and evaluate accuracy on the test set for seen and unseen (zero-shot) entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity linking</head><p>Non-zero-shot EL Following previous work <ref type="bibr" target="#b3">(Kolitsas et al., 2018;</ref><ref type="bibr" target="#b2">Cao et al., 2020)</ref>, we use the GERBIL platform <ref type="bibr" target="#b19">(R?der et al., 2018)</ref> to evaluate EL. We evaluate InKB micro-F1 with strong matching (predictions must match exactly the gold mention boundaries). Similarly to the non-zero-shot ED experiment, we pretrain on Wikipedia, then use the AIDA-CoNLL dataset for fine-tuning and evaluation. For out-of-domain performance evaluation we use MSNBC <ref type="bibr">(Cucerzan, 2007)</ref>, <ref type="bibr">OKE-2015</ref><ref type="bibr">, OKE-2016</ref><ref type="bibr" target="#b10">(Nuzzolese et al., 2015</ref>, N3-Reuters-128 (R128), N3-RSS-500 <ref type="bibr" target="#b18">(R?der et al., 2014)</ref>, <ref type="bibr">Derczynski (Derczynski et al., 2015)</ref>, KORE50 (Hoffart et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference speed</head><p>We compare the computational efficiency of our model to three high-performing EL systems <ref type="bibr" target="#b24">(Wu et al., 2020;</ref><ref type="bibr" target="#b2">Cao et al., 2020;</ref><ref type="bibr" target="#b12">Orr et al., 2021)</ref> for which code is available. We benchmark both modes of BLINK <ref type="bibr" target="#b24">(Wu et al., 2020)</ref>; the bi-encoder (encodes mention and entities independently) and the more accurate cross-encoder (encodes mention and entities jointly). <ref type="bibr">8</ref> We measure the time to perform end-to-end EL inference on the AIDA-CoNLL test dataset using a single V100 GPU. The dataset consists of 231 documents and 4464 mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training details</head><p>Candidate generation We follow <ref type="bibr" target="#b4">Le and Titov (2018)</ref> by selecting the top-30 candidate entities using entity priors. 9 For training, we only keep 5 candidates, 1 gold candidate, 2 candidates with the highestp(e j |m i ) and 2 random candidates. When the gold entity is not in the candidate list during training, we use NIL as the correct label.  Wikipedia pretraining We use Wikidata as our KB (i.e. for entity types and descriptions). To make comparisons reliable, we restrict to the set of entities in English Wikipedia (total of 6.2M). We build a training dataset from the 2021-02-01 dump of Wikipedia and Wikidata and use hyperlinks as entity labels. To increase entity label coverage, we add weak labels to mentions of the article entity <ref type="bibr" target="#b12">(Orr et al., 2021;</ref><ref type="bibr" target="#b1">Broscheit, 2019;</ref><ref type="bibr" target="#b2">Cao et al., 2020)</ref>. <ref type="bibr">10</ref> The dataset consists of approximately 100M mention-entity pairs. We use entity labels to generate entity type labels, as in <ref type="bibr" target="#b11">Onoe and Durrett (2020)</ref>. In addition, we follow F?vry et al. <ref type="formula" target="#formula_1">(2020)</ref> by adding mention labels to unlinked mentions using a named entity recogniser to provide additional mention detection signal.</p><p>Model details We divide the documents into chunks of 300 tokens and subsample 40 mentions per chunk during pretraining. The model is trained for 2 epochs on Wikipedia and the transformers are initialised with RoBERTa <ref type="bibr" target="#b6">(Liu et al., 2019)</ref> base weights. The description transformer has 2 layers. BERT-style masking (Devlin et al., 2019) is applied to mentions during pretraining. During fine-tuning and evaluation, we increase the sequence length to 512 and set the maximum candidate entities to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Entity disambiguation</head><p>Non-zero-shot ED We report InKB micro-F1 (with and without fine-tuning on AIDA) and compare it with SOTA ED models in <ref type="table">Table 1</ref>. Our model performs strongly across all datasets, surpassing the previous average F1 across the 6 <ref type="bibr">10</ref> We add weak labels by using simple heuristics such as matching mentions to the page's title. datasets by 0.7 F1 points. We observe the model achieves SOTA performance on 4 out of the 6 datasets without fine-tuning, suggesting it is able to learn patterns from Wikipedia that transfer well to other domains. Nonetheless, fine-tuning on the AIDA-CoNLL dataset leads to a substantial improvement (+6.4 F1 points) which can be attributed to the model learning peculiarities of the dataset (e.g. cricket score tables).</p><p>The ablations in <ref type="table">Table 1</ref> show entity types and entity descriptions are complementary (+2.0 F1 points when combined). This is explained by increased robustness to partially missing entity information (e.g. KB entities without descriptions) and different knowledge being expressed. Entity priors are useful but contribute less than other components of our combined score (Section 3.8). Without priors, F1 falls by 5.0 points on AQUAINT and increases by 1.2 points on ACE2004, which is expected as AQUAINT contains a high proportion of popular entities, and ACE2004 more rare entities. Pretraining has the largest impact on ED performance, particularly on datasets such as WIKI (+12.0 F1) derived from encyclopedia text.    Zero-shot ED In <ref type="table" target="#tab_6">Table 4</ref>, we report ED accuracy on the WikiLinksNED Unseen Mentions test set for seen and unseen entities. Our model outperforms the baseline by 3.0 F1, with, surprisingly, 6.6% higher accuracy for unseen than for seen entities. We find this is partly due to higher top 30 candidate recall for the unseen entity subset (95.0% compared to 91.1% for the seen entity subset) and also because our mention masking strategy reduces the reliance of entities appearing in the training data with similar surface forms. Moreover, Re-FinED uses entity types and descriptions to link entities instead of relying on entity memorisation, which means the number of training examples for a given entity will not necessarily correlate with performance. The number of similar entities in the training dataset and the ambiguity of the test examples <ref type="bibr" target="#b14">(Provatorova et al., 2021)</ref> will likely have more significant influence on performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entity linking</head><p>EL results are shown in <ref type="table" target="#tab_4">Table 3</ref>. ReFinED outperforms other models on all but 3 datasets, often by a considerable F1 point margin (e.g. 7.8 on N3-Reuters-128 and 4.0 on KORE50) and improves the average across all 8 datasets by 3.7 F1 points. EL improves as ED and mention detection can generalise to different datasets due to the model being pretrained on Wikipedia hyperlinks as opposed to only AIDA-CoNLL. We also report results on the ISTEX and WebQSP datasets in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Inference speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Deployment Details</head><p>We have successfully deployed the ReFinED EL model in a real-world application, the aim of which is to populate a KB by extracting facts from unstructured text found on web pages with high precision. The application requires running ReFinED on a billion web pages (in which we link 25 billion mentions) multiple times per year. The scale of this deployment highlighted a number of learning points. Firstly, the entity linking model must be computationally efficient. The inference speed of Re-FinED allows the processing of the billion web pages in 27k machine hours (2 days using 500 instances), on machines with a single T4 GPU. Given availability of cloud compute, the cost of processing the same documents with the models evaluated in Section 5.3 would scale approximately linearly with their inference speeds. That is, the BLINK bi-encoder would require 3000 instances for 2 days, or 500 instances for 12 days, implying a roughly 6-fold increase in cost.</p><p>Secondly, the scale of the number of pages also brings with it diversity of domains, meaning the model benefits from linking to a large catalogue of entities (over 90 million) -including zero-shot entities.</p><p>Thirdly, we observed that deploying an end-toend self-contained EL model is easier to horizontally scale and has a lower operational cost than deploying multiple systems for each subcomponent (such as candidate generation).</p><p>Finally, in real-world data, unlike in ED datasets, there are a large number of cases where the correct entity does not exist in the KB. This meant that we had to train the model on examples where the correct entity was not in the candidate list to reduce overprediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a scalable end-to-end EL model which uses entity types and entity descriptions to perform linking. Our model achieves SOTA results for both ED (+0.7 F1 points on average across 6 datasets) and EL (+3.7 F1 points on average across 8 datasets) while being 60 times faster than comparatively accurate baselines. We demonstrate our approach scales well to a KB (Wikidata) 15 times larger than Wikipedia while maintaining competitive performance. The combination of accuracy, speed and scale means the system is capable of being deployed to extract entities from web-scale datasets with higher accuracy and an order of magnitude lower cost than existing approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Entity Type Selection</head><p>Our entity types are formed from Wikidata relationobject pairs and relation-object pairs inferred from the Wikidata subclass hierarchy (for example, (instance of, organisation) can be inferred from (instance of, business)). We only consider types with the following relations: instance of, occupation, country, sport. We select types by iteratively adding types that separate (assuming an oracle type classifier) the gold entity from negative candidates for the most examples in our Wikipedia training dataset. Type information stored in KBs often varies in granularity between entities (e.g. some capital city entities have the type capital city and others only city), adversely affecting training signal. To mitigate this, we use the class hierarchy to add parent types to entities. This injects class hierarchy information into the model, enabling type granularity to depend on context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>We use the Hugging Face implementation of RoBERTa <ref type="bibr" target="#b23">(Wolf et al., 2019</ref>) and optimise our model using Adam (Kingma and Ba, 2015) with a linear learning rate schedule. We ignore the loss from mentions where the gold entity is not in the candidate set. The named-entity recogniser, used to preprocess our Wikipedia training dataset, is a RoBERTa token classification model trained on the AIDA-CoNLL dataset mention boundaries. We add weak entity labels for mentions that match the page's title (or surname for Wikipedia pages about people). We present our main hyperparameters in <ref type="table" target="#tab_11">Table 6</ref>. Due to the high computational cost of training the model, we did not conduct an extensive hyperparameter search. Training on Wikipedia took approximately 48 hours on a single machine with 4 V100 GPUs. The model has approximately 154M parameters (123 million in the roberta-base architecture, and 31M for the additional description encoder and output layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional results</head><p>Wikidata ED experimental setup To measure ED performance on non-Wikipedia entities, we expand our entity set to Wikidata (which has over 90M entities) and evaluate our model on the IS-TEX test dataset (Delpeuch, 2020). We add labels and aliases from Wikidata for candidate generation and remove entity priors from our entity scoring (Section 3.8). roberta-base # mention encoder layers 12 description transformer init. roberta-base # description encoder layers 2 # description tokens 32 ? 1 , ? 2 , ? 3 , ? 4 (0.01, 1, 0.01, 1) mention mask prob. 0.7 Entity Linking performance on questions We report results on the WebQSP dataset in <ref type="table" target="#tab_13">Table 7</ref>, which shows EL performance on questions. Our model has similar performance to ELQ, which is SOTA on WebQSP and is optimised for questions. Our model is faster than all baselines which can be attributed to using an end-to-end EL model, restricting ED predictions to the predicted mentions only, and using a smaller model (compared to ELQ which uses <ref type="bibr">BERT-large (Devlin et al., 2019)</ref>   <ref type="bibr" target="#b5">(Li et al., 2020)</ref>. #Q/s is number of questions per second for a single CPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our model architecture shown for a document with two mentions, England and FIFA World Cup. The model performs mention detection, entity typing, and entity disambiguation for all mentions in a single pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>MethodAIDA MSNBC* AQUAINT* ACE2004* CWEB* WIKI* Avg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: ED InKB micro F1 scores on in-domain and out-of-domain test sets. The best value in bold and second best is underlined.</figDesc><table><row><cell>Yang et al. (2018)</cell><cell>93.0</cell><cell>92.6</cell><cell>89.9</cell><cell>88.5</cell><cell>81.8</cell><cell>79.2</cell><cell>87.5</cell></row><row><cell>Yang et al. (2019)</cell><cell>93.7</cell><cell>93.8</cell><cell>88.3</cell><cell>90.1</cell><cell>75.6</cell><cell>78.8</cell><cell>86.7</cell></row><row><cell>Fang et al. (2019)</cell><cell>94.3</cell><cell>92.8</cell><cell>87.5</cell><cell>91.2</cell><cell>78.5</cell><cell>82.8</cell><cell>87.9</cell></row><row><cell>Wu et al. (2020)  ?</cell><cell>86.7</cell><cell>90.3</cell><cell>88.9</cell><cell>88.7</cell><cell>82.6</cell><cell>86.1</cell><cell>87.2</cell></row><row><cell>Cao et al. (2020)</cell><cell>93.3</cell><cell>94.3</cell><cell>89.9</cell><cell>90.1</cell><cell>77.3</cell><cell>87.4</cell><cell>88.7</cell></row><row><cell>Orr et al. (2021)  *  *</cell><cell>80.9</cell><cell>80.5</cell><cell>74.2</cell><cell>83.6</cell><cell>70.2</cell><cell>76.2</cell><cell>77.6</cell></row><row><cell>ReFinED (Wikipedia)</cell><cell>87.5</cell><cell>94.4</cell><cell>91.8</cell><cell>91.6</cell><cell>77.8</cell><cell>88.7</cell><cell>88.6</cell></row><row><cell>ReFinED (fine-tuned)</cell><cell>93.9</cell><cell>94.1</cell><cell>90.8</cell><cell>90.8</cell><cell>79.4</cell><cell>87.4</cell><cell>89.4</cell></row><row><cell cols="2">Ablations w/o entity priors (Wikipedia) 86.3</cell><cell>93.7</cell><cell>86.0</cell><cell>92.8</cell><cell>76.0</cell><cell>88.3</cell><cell>87.2</cell></row><row><cell>w/o entity types (Wikipedia)</cell><cell>82.2</cell><cell>92.6</cell><cell>91.1</cell><cell>90.1</cell><cell>76.5</cell><cell>87.0</cell><cell>86.6</cell></row><row><cell>w/o descriptions (Wikipedia)</cell><cell>85.7</cell><cell>93.9</cell><cell>89.5</cell><cell>91.2</cell><cell>76.1</cell><cell>84.3</cell><cell>86.8</cell></row><row><cell>w/o pretraining (fine-tuned)</cell><cell>88.2</cell><cell>92.3</cell><cell>86.8</cell><cell>90.6</cell><cell>75.1</cell><cell>74.5</cell><cell>84.6</cell></row></table><note>? Normalised accuracy is reported. *Out-of-domain datasets.* * Result obtained using code released by authors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>ED accuracy on AIDA-CoNLL using PPRForNED candidates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows accuracy on the AIDA-CoNLL</cell></row><row><cell>dataset when we use PPRforNED candidates. Re-</cell></row><row><cell>FinED outperforms purely entity typing approaches</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>EL InKB micro F1 scores on in-domain and out-of-domain test sets reported by Gerbil. The best value in bold and second best is underlined. *Out-of-domain datasets.</figDesc><table><row><cell>(Raiman and Raiman, 2018; Onoe and Durrett,</cell></row><row><cell>2020) by a margin of +2.2% accuracy, due to the</cell></row><row><cell>addition of entity descriptions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>ED accuracy on WikiLinksNED Unseen Mentions test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">shows the time taken to run inference on the</cell></row><row><cell cols="3">AIDA-CoNLL test dataset, alongside the average</cell></row><row><cell cols="3">ED performance. ReFinED is 6 times faster than</cell></row><row><cell cols="3">the BLINK (Wu et al., 2020) bi-encoder, which also</cell></row><row><cell cols="3">has an average F1 which is 9 points lower. Com-</cell></row><row><cell cols="3">pared to the higher accuracy systems, ReFinED is</cell></row><row><cell cols="3">60 times faster than the BLINK cross-encoder, and</cell></row><row><cell cols="3">140 times faster than the autoregressive approach</cell></row><row><cell cols="3">of Cao et al. (2020). This is because ReFinED uses</cell></row><row><cell cols="3">a single forward pass to jointly encode all mentions</cell></row><row><cell cols="3">and candidate KB entities in the document (512</cell></row><row><cell cols="3">token chunk), and hence requires ? 231 forward</cell></row><row><cell cols="3">passes for the full dataset. The bi-encoder model</cell></row><row><cell cols="3">requires ? 4464 forward passes as mentions are en-</cell></row><row><cell cols="3">coded individually, and the cross-encoder baseline</cell></row><row><cell cols="3">requires ? 90k forward passes as each mention is</cell></row><row><cell cols="3">encoded with each candidate. The autoregressive</cell></row><row><cell cols="3">approach suffers from high computational cost due</cell></row><row><cell cols="3">to the deep decoder, which generates a single token</cell></row><row><cell cols="3">at a time. Also, all baselines require a separate</cell></row><row><cell cols="3">model for MD whereas ReFinED performs end-</cell></row><row><cell cols="3">to-end EL using a single model, which improves</cell></row><row><cell cols="3">efficiency and simplifies model deployment.</cell></row><row><cell>Method</cell><cell cols="2">Time taken (s) Avg. ED F1</cell></row><row><cell>Cao et al. (2020)</cell><cell>2100</cell><cell>88.7</cell></row><row><cell>Wu et al. (2020) bi-encoder</cell><cell>93</cell><cell>80.4</cell></row><row><cell>Wu et al. (2020) cross-encoder</cell><cell>917</cell><cell>87.2</cell></row><row><cell>Orr et al. (2021)</cell><cell>438</cell><cell>77.6</cell></row><row><cell>ReFinED</cell><cell>15</cell><cell>89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Time taken in seconds for EL inference on AIDA-CoNLL test dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Our model hyperparameters Wikidata ED results We evaluate ED performance on the ISTEX dataset (which targets Wikidata). Our model outperforms Delpeuch (2020) (92.1 vs 87.0 micro F1) which uses hand-crafted features specifically designed for linking Wikidata entities. This shows that our approach scales to Wikidata and generalises well when there is increased mention ambiguity. Our model performs 0.5 F1 points below the SOTA Mulang' et al. (2020) (92.6 vs 92.1 micro F1) which is likely due to differing candidate entity generation methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Method</cell><cell cols="2">WebQSP #Q/s</cell></row><row><cell>TAGME (Ferragina and Scaiella, 2012)</cell><cell>36.1</cell><cell>2.39</cell></row><row><cell>BLINK (Wu et al., 2020) (Wikipedia)</cell><cell>80.8</cell><cell>0.80</cell></row><row><cell>ELQ (Li et al., 2020) (Wikipedia)</cell><cell>83.9</cell><cell>1.56</cell></row><row><cell>ELQ (Li et al., 2020) (fine-tuned)</cell><cell>89.0</cell><cell>1.56</cell></row><row><cell>ReFinED (Wikipedia)</cell><cell>84.1</cell><cell>2.78</cell></row><row><cell>ReFinED (fine-tuned)</cell><cell>89.1</cell><cell>2.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Entity linking weak matching InKB micro F1 scores on WebQSP EL dataset</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We refer to corpora with more than 1 billion documents as "web-scale".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We replace Wikipedia titles with Wikidata labels, and Wikipedia sentences with Wikidata entity descriptions.5  We assume entities in the KB have a textual description and a collection of facts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use bold letters for vectors throughout our paper, and treat mi and m i as different terms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use 1 to indicate the presence of an entity type and 0 the absence of an entity type for our binary vector. Note that a single entity can have multiple entity types.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We use a max context length of 128 tokens and precomputed entity embeddings for the bi-encoder. For the crossencoder, we use max context length of 32 tokens.9  Derived from Wikipedia hyperlink count statistics, YAGO, a large Web corpus and Wikidata aliases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Clara Vania, Grace Lee, and Amir Saffari for helpful discussions and feedback. We also thank the anonymous reviewers for valuable comments that improved the quality of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset statistics</head><p>We present the topic, number of documents and number of mentions for each dataset used for evaluation. The datasets used cover a variety of sources including wikipedia text, news articles, web text and tweets. Note that the performance of the model outside these domains may be significantly different.</p><p>Note also that all datasets used are for English only, allowing comparison to previous work. Our method is extendable to any language for which there is an language-specific version of Wikipedia on which the model could be trained. However, we cannot guarantee the accuracy of the model across these languages without further experimentation.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Investigating entity knowledge in BERT with simple neural end-to-end entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<idno>abs/2010.00904</idno>
	</analytic>
	<monogr>
		<title level="j">Autoregressive entity retrieval. ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end neural entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient one-pass end-to-end entity linking for questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6433" to="6441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3449" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating the impact of knowledge graph context on entity disambiguation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><surname>Onando Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitali</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open knowledge extraction challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Giovanni Nuzzolese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Lisa</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Presutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar?o</forename><surname>Garigliotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Web Evaluation Challenges</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8576" to="8583" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bootleg: Chasing the tail with self-supervised named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno>abs/2010.10363</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robustness evaluation of entity disambiguation using prior probes: the case of entity overshadowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Provatorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.820</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10501" to="10510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeptype: Multilingual entity linking by neural type system evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N? -a collection of datasets for named entity recognition and disambiguation in the nlp interchange format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sebastian Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Both</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-170286</idno>
		<title level="m">GERBILbenchmarking named entity recognition and linking consistently. Semantic Web</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="605" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rel: An entity linker standing on the shoulders of giants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">M</forename><surname>Van Hulst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2197" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Retrieval, re-ranking and multi-task learning for knowledge-base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="347" to="357" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Machine knowledge: Creation and curation of comprehensive knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning dynamic context augmentation for global entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective entity disambiguation with structured gradient tree boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
