<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
							<email>junjie.huang@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
							<email>guan.huang@phigent.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phigent</forename><surname>Robotics</surname></persName>
						</author>
						<title level="a" type="main">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single frame data contains finite information which limits the performance of the existing vision-based multicamera 3D object detection paradigms. For fundamentally pushing the performance boundary in this area, a novel paradigm dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the spatial-only 3D working space into the spatial-temporal 4D working space. We upgrade the naive BEVDet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. In this way, with negligible additional computing budget, we enable BEVDet4D to access the temporal cues by querying and comparing the two candidate features. Beyond this, we simplify the task of velocity prediction by degenerating it into the positional offset prediction in the two adjacent features. As a result, BEVDet4D with robust generalization performance reduces the velocity error by up to -62.9%. This makes the vision-based methods, for the first time, become comparable with those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes, we report a new record of 54.5% NDS with the high-performance configuration dubbed BEVDet4D-Base. At the same inference speed, this notably surpasses the previous leading method BEVDet-Base by +7.3% NDS. The source code is publicly available for further research 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, autonomous driving draws great attention in both the research and the industry community. The visionbased perception tasks in this scene include 3D object detection, BEV semantic segmentation, motion prediction, and so on. Most of them can be partly solved in the spatialonly 3D working space with a single frame of data. However, with respect to the time-relevant targets like velocity, current vision-based paradigms with merely a single frame of data perform far poorer than those with sensors like LiDAR or radar. For example, the velocity error of the recently leading method BEVDet <ref type="bibr" target="#b14">[15]</ref> in the vision-based 3D object detection is 3 times that of the LiDAR-based method CenterPoint <ref type="bibr" target="#b46">[46]</ref> and 2 times that of the radar-based method CenterFusion <ref type="bibr" target="#b27">[27]</ref>. To close this gap, we propose a novel paradigm dubbed BEVDet4D in this paper and pioneer the exploitation of vision-based autonomous driving in the spatial-temporal 4D space. As illustrated in <ref type="figure">Fig. 2</ref>, BEVDet4D makes the first attempt at accessing the rich information in the temporal domain. It simply extends the naive BEVDet <ref type="bibr" target="#b14">[15]</ref> by retaining the intermediate BEV features in the previous frames. Then it fuses the retained feature with the corresponding one in the current frame just by a spatial alignment operation and a concatenation operation. Other than that, we kept most other details of the framework unchanged. In this way, we place just a negligible extra computational budget on the inference process while enabling the paradigm to access the temporal cues by querying and comparing the two candidate features. Though simple in constructing the framework of BEVDet4D, it is nontrivial to build its robust performance. The spatial alignment operation and the learning targets should be carefully designed to cooperate with the elegant framework so that the velocity prediction task can be simplified and superior generalization performance can be achieved with BEVDet4D.</p><p>We conduct comprehensive experiments on the chal-  <ref type="figure">Figure 2</ref>: The framework of the proposed BEVDet4D paradigm. BEVDet4D retains the intermediate BEV feature of the previous frame and concatenates it with the ones generated by the current frame. Before that spatial alignment in the flat plane is conducted to partially simplify the velocity prediction task. lenge benchmark nuScenes <ref type="bibr" target="#b1">[2]</ref> to verify the feasibility of BEVDet4D and study its characteristics. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the trade-off between inference speed and performance of different paradigms. Without bells and whistles, the BEVDet4D-Tiny configuration reduces the velocity error by 62.9% from 0.909 mAVE to 0.337 mAVE. Besides, the proposed paradigm also has significant improvement in the other indicators like detection score (+2.6% mAP), orientation error (-12.0% mAOE), and attribute error (-25.1% mAAE). As a result, BEVDet4D-Tiny exceeds the baseline by +8.4% on the composite indicator NDS. The highperformance configuration dubbed BEVDet4D-Base scores high as 42.1% mAP and 54.5% NDS, which has surpassed all published results in vision-based 3D object detection <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref>. Last but not least, BEVDet4D achieves the aforementioned superiority just at a negligible cost in inference latency, which is meaningful in the scenario of autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision-based 3D object detection</head><p>Vision-based 3D object detection is a promising perception task in autonomous driving. In the last few years, fueled by the KITTI <ref type="bibr" target="#b10">[11]</ref> benchmark monocular 3D object detection has witness a rapid development <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b15">16]</ref>. However, the limited data and the single view disable it in developing more complicated tasks.</p><p>Recently, some large-scale benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35]</ref> have been proposed with sufficient data and surrounding views, offering new perspectives toward the paradigm development in the field of 3D object detection. Based on these bench-marks, some multi-camera 3D object detection paradigms have been developed with competitive performance. For example, inspired by the success of FCOS <ref type="bibr" target="#b36">[36]</ref> in 2D detection, FCOS3D <ref type="bibr" target="#b40">[40]</ref> treats the 3D object detection problem as a 2D object detection problem and conducts perception just in image view. Benefitting from the strong spatial correlation of the targets' attribute with the image appearance, it works well in predicting this but is relatively poor in perceiving the targets' translation, velocity, and orientation. PGD <ref type="bibr" target="#b41">[41]</ref> further develops the FCOS3D paradigm by searching and resolving the outstanding shortcoming (i.e. the prediction of the targets' depth). This offers a remarkable accuracy improvement on the baseline but at the cost of more computational budget and additional inference latency. Following DETR <ref type="bibr" target="#b2">[3]</ref>, DETR3D <ref type="bibr" target="#b42">[42]</ref> proposes to detect 3D objects in an attention pattern, which has similar accuracy as FCOS3D. Although DETR3D requires just half the computational budget, the complex calculation pipeline slows down its inference speed to the same level as FCOS3D. PETR <ref type="bibr" target="#b21">[22]</ref> further develops the performance of this paradigm by introducing the 3D coordinate generation and position encoding. Besides, they also exploit the strong data augmentation strategies just as BEVDet <ref type="bibr" target="#b14">[15]</ref>. Another concurrent work dubbed Graph-DETR3D <ref type="bibr" target="#b5">[6]</ref> also extends the DETR3D from two expects. Analogous to the second stage in CenterPoint <ref type="bibr" target="#b46">[46]</ref>, Graph-DETR3D samples multiple points in the 3D space instead of a single point when generating the features of the object queries. Another modification is making the multi-scale training become feasible for DETR3D paradigm by dynamically adjusting the depth target according to the scaling factor. As a novel paradigm, BEVDet <ref type="bibr" target="#b14">[15]</ref> makes the first attempt at apply-ing a strong data augmentation strategy in vision-based 3D object detection. As BEVDet explicitly encodes features in the BEV space, it is scalable in multiple aspects including multi-tasks learning, multi-sensors fusion, and temporal fusion. BEVDet4D is the temporal extension of BEVDet <ref type="bibr" target="#b14">[15]</ref>.</p><p>So far, few works have exploited the temporal cues in vision-based 3D object detection. Thus, the existing paradigms <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref> perform relatively poorly in predicting the time-relevant targets like velocity than the LiDAR-based <ref type="bibr" target="#b46">[46]</ref> or radar-based <ref type="bibr" target="#b27">[27]</ref> methods. To the best of our knowledge, <ref type="bibr" target="#b0">[1]</ref> is the only one pioneer in this perspective. However, they predict the results based on a single frame and exploit the 3D Kalman filter to update the results for the temporal consistency of results between image sequences. The temporal cues are exploited in the postprocessing phase instead of the end-to-end learning framework. Differently, we make the first attempt in exploiting the temporal cues in the end-to-end learning framework BEVDet4D, which is elegant, powerful, and still scalable. BEVFormer <ref type="bibr" target="#b17">[18]</ref> is a concurrent work of BEVDet4D. Analogous to those <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> in the VID literature, they mainly focus on the feature fusion in the spatial-temporal 4D working space with the attention mechanism <ref type="bibr" target="#b37">[37]</ref>. The comparable velocity precision of BEVFormer is achieved by fusing features from multiple adjacent frames (i.e. 4 frames in total), which is analogous to most LiDAR-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">46]</ref> with points from multiple sweeps. This is fundamentally different from the proposed BEVDet4D, which uses merely two adjacent frames and achieved a higher velocity precision in a more elegant pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection in Video</head><p>Video object detection mainly fueled by the ImageNet VID dataset <ref type="bibr" target="#b33">[33]</ref> is analogous to the well-known tasks of common object detection <ref type="bibr" target="#b18">[19]</ref> which performs and evaluates the object detection task in the image-view space. The difference is that detecting objects in video can access the temporal cues for improving detection accuracy. The methods in this area access the temporal cues mainly according to two kinds of mediums: the predicting results or the intermediate features. The former <ref type="bibr" target="#b45">[45]</ref> is analogous to <ref type="bibr" target="#b0">[1]</ref> in vision-based 3D object detection, who optimizes the prediction results in a tracking pattern. The latter reutilizes the features from the previous frame based on some special architectures like LSTM <ref type="bibr" target="#b13">[14]</ref> for feature distillation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">25]</ref>, attention mechanism <ref type="bibr" target="#b37">[37]</ref> for feature querying <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, and optical flow <ref type="bibr" target="#b9">[10]</ref> for feature alignment <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b51">51]</ref>. Specific for the scene of autonomous driving, BEVDet4D is analogous to the flow-based methods in mechanism but accesses the spatial correlation according to the ego-motion and conducts feature aggregation in the 3D space. Besides, BEVDet4D mainly focuses on the prediction of the velocity targets which is not in the scope of the common video object detection literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Structure</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, the overall framework of BEVDet4D is built upon the BEVDet <ref type="bibr" target="#b14">[15]</ref> baseline which is consists of four kinds of modules: an image-view encoder, a view transformer, a BEV encoder, and a task-specific head. All implementation details of these modules are kept unchanged. To exploit the temporal cues, BEVDet4D extends the baseline by retaining the BEV features generated by the view transformer in the previous frame. Then the retained feature is merged with the one in the current frame. Before that, an alignment operation is conducted to simplify the learning targets which will be detailed in the following subsection. We apply a simple concatenation operation to merge the features for verifying the BEVDet4D paradigm. More complicated fusing strategies have not been exploited in this paper.</p><p>Besides, the feature generated by the view transformer is sparse, which is too coarse for the subsequential modules to exploit the temporal cues. Therefore, an extra BEV encoder is applied to adjust the candidate features before the temporal fusion. In practice, the extra BEV encoder consists of two naive residual units <ref type="bibr" target="#b12">[13]</ref>, whose channel number is set the same as the input feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simplify the Velocity Learning Task</head><p>Symbol Definition Following nuScense <ref type="bibr" target="#b1">[2]</ref>, we denote the global coordinate system as O g ? XY Z, the ego coordinate system as O e(T ) ? XY Z, and the targets coordinate system as O t(T ) ? XY Z. As illustrated in <ref type="figure">Fig. 3</ref>, we construct a virtual scene with a moving ego vehicle and two target vehicles. One of the targets is static (i.e., O s ? XY Z painted green) in the global coordinate system, while the other one is moving (i.e., O m ? XY Z painted blue). The objects in two adjacent frames (i.e., frame T ? 1 and frame T ) are distinguished with different transparentness. The position of the objects is formulated as P x (t).</p><p>x ? {g, e(T ), e(T ? 1)} denotes the coordinate system where the position is defined in. t ? {T, T ? 1} denotes the time when the position is recorded. We use T dst src to denote the transformation from the source coordinate system into the target coordinate system.</p><p>Instead of directly predicting the velocity of the targets, we tend to predict the translation of the targets in the two adjacent frames. In this way, the learning task can be simplified as the time factor is removed and the positional shifting can be measured just according to the difference between the two BEV features. Besides, we tend to learn the position shifting that is irrelevant to the ego-motion. In this way, the learning task can also be simplified as the ego-motion <ref type="figure">Figure 3</ref>: Illustrating the effect of the feature alignment operation. Without the alignment operation (i.e. the first row), the following modules are required to study a more complicated distribution of the object motion, which is relevant to the ego-motion. By applying alignment operation in the second row, the learning targets can be simplified.</p><formula xml:id="formula_0">Ego Vehicle Static Vehicle Moving Vehicle C A Concatenate Align A C Receptive Field(T-1) Receptive Field(T) ( ) ( ) ( ) s( ? 1) ( ? 1) ( ? 1) Receptive Field(T-1) Receptive Field(T) ( ) ( ) ( ) s( ? 1) ( ? 1) ( ? 1) Receptive Field(T) ( ) ( ) ( ? 1) s( ? 1) Receptive Field(T) ( ) ( ) s( ? 1) ( ? 1) C</formula><p>will make the distribution of the targets' positional shifting more complicated.</p><p>For example, due to the ego-motion, a static object (i.e., the green box in <ref type="figure">Fig. 3</ref>) in the global coordinate system will be changed into a moving object in the ego coordinate system. More specifically, the receptive field of the BEV features is symmetrically defined around the ego. Considering the two features generated by the view transformer in the two adjacent frames, their receptive fields in the global coordinate system are diverse due to the ego-motion. Given a static object, its position in the global coordinate system is denoted as P g s (T ) and P g s (T ?1) in the two adjacent frames. The positional shifting in the two features should be formulated as:</p><formula xml:id="formula_1">P e(T ) s (T ) ? P e(T ?1) s (T ? 1) =T e(T ) g P g s (T ) ? T e(T ?1) g P g s (T ? 1) =T e(T ) g P g s (T ) ? T e(T ?1) e(T ) T e(T ) g P g s (T ? 1)<label>(1)</label></formula><p>According to Eq. 1, if we directly concatenate the two features, the learning target (i.e., the positional shifting of the target in the two features) of the following modules is relevant to the ego motion (i.e., T</p><formula xml:id="formula_2">e(T ?1) e(T )</formula><p>). To avoid this, we shift the target in the adjacent frame by T e(T ) e(T ?1) to remove the fraction of ego-motion.</p><formula xml:id="formula_3">P e(T ) s (T ) ? T e(T ) e(T ?1) P e(T ?1) s (T ? 1) =T e(T ) g P g s (T ) ? T e(T ) e(T ?1) T e(T ?1) e(T ) T e(T ) g P g s (T ? 1) =T e(T ) g P g s (T ) ? T e(T ) g P g s (T ? 1) =P e(T ) s (T ) ? P e(T ) s (T ? 1)<label>(2)</label></formula><p>According to Eq. 2, the learning target is set as the object's motion in the current frame's ego coordinate system, which is irrelevant to the ego-motion.</p><p>In practice, the alignment operation in Eq. 2 is achieved by feature alignment. Given the candidate features of the previous frame F(T ? 1, P e(T ?1) ) and the current frame F(T, P e(T ) ), the aligned feature can be obtained by:</p><formula xml:id="formula_4">F (T ? 1, P e(T ) ) = F(T ? 1, T e(T ?1) e(T ) P e(T ) ) (3) Alone with Eq. 3, bilinear interpolation is applied as T e(T ?1) e(T )</formula><p>P e(T ) may not be a valid position in the sparse feature of F(T ? 1, P e(T ?1) ). The interpolation is a suboptimal method that will lead to precision degeneration. The magnitude of the precision degeneration is negatively correlated with the resolution of the BEV features. A more precise method is to adjust the coordinates of the point cloud generated by the lifting operation in the view transformer <ref type="bibr" target="#b30">[30]</ref>. However, it is deprecated in this paper as it will destroy the precondition of the acceleration method proposed in the naive BEVDet <ref type="bibr" target="#b14">[15]</ref>. The magnitude of the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset We conduct comprehensive experiments on a large-scale dataset, nuScenes <ref type="bibr" target="#b1">[2]</ref>. nuScenes dataset includes 1000 scenes with images from 6 cameras with surrounding views, points from 5 Radars and 1 LiDAR. It is the up-todate popular benchmark for 3D object detection <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b29">29]</ref> and BEV semantic segmentation <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b44">44]</ref>. The scenes are officially split into 700/150/150 scenes for training/validation/testing. There are up to 1.4M annotated 3D bounding boxes for 10 classes: car, truck, bus, trailer, construction vehicle, pedestrian, motorcycle, bicycle, barrier, and traffic cone. Following CenterPoint <ref type="bibr" target="#b46">[46]</ref>, we define the region of interest (ROI) within 51.2 meters in the ground plane with a resolution of 0.8 meters by default.</p><p>Evaluation Metrics For 3D object detection, we report the official predefined metrics: mean Average Precision (mAP), Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), Average Attribute Error (AAE), and NuScenes Detection Score (NDS). The mAP is analogous to that in 2D object detection <ref type="bibr" target="#b18">[19]</ref> for measuring the precision and recall, but defined based on the match by 2D center distance on the ground plane instead of the Intersection over Union (IOU) <ref type="bibr" target="#b1">[2]</ref>. NDS is the composite of the other indicators for comprehensively judging the detection capacity.</p><p>The remaining metrics are designed for calculating the positive results' precision on the corresponding aspects (e.g., translation, scale, orientation, velocity, and attribute).</p><p>Training Parameters Following BEVDet <ref type="bibr" target="#b14">[15]</ref>, models are trained with AdamW <ref type="bibr" target="#b24">[24]</ref> optimizer, in which gradient clip is exploited with learning rate 2e-4, a total batch size of 64 on 8 NVIDIA GeForce RTX 3090 GPUs. Sublinear memory cost <ref type="bibr" target="#b3">[4]</ref> is used for GPU memory management. We apply a cyclic policy <ref type="bibr" target="#b43">[43]</ref>, which linearly increases the learning rate from 2e-4 to 1e-3 in the first 40% schedule and linearly decreases the learning rate from 1e-3 to 0 in the remainder epochs. By default, the total schedule is terminated within 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Processing</head><p>We keep all data processing settings the same as BEVDet <ref type="bibr" target="#b14">[15]</ref>. Specifically, we use W in ? H in to denote the width and height of the input image. By default in the training process, the source images with 1600?900 resolution <ref type="bibr" target="#b1">[2]</ref>   </p><formula xml:id="formula_5">) = (0.5 * (s * 1600 ? W in ), x 1 + W in , s * 900 ? H in , y 1 + H in ).</formula><p>Inference Speed We conduct all experiments based on MMDetection3D <ref type="bibr" target="#b6">[7]</ref>. The inference speed is the average upon 6019 validation samples <ref type="bibr" target="#b1">[2]</ref>. For monocular paradigms like FCOS3D <ref type="bibr" target="#b40">[40]</ref> and PGD <ref type="bibr" target="#b41">[41]</ref>, the inference speeds are divided by a factor of 6 (i.e. the number of images in a single sample <ref type="bibr" target="#b1">[2]</ref>), as they take each image as an independent sample. By default, the inference acceleration method proposed in BEVDet <ref type="bibr" target="#b14">[15]</ref> is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">nuScenes val set</head><p>We comprehensively compare the proposed BEVDet4D with the baseline method BEVDet <ref type="bibr" target="#b14">[15]</ref> and other paradigms like FCOS3D <ref type="bibr" target="#b40">[40]</ref>, PGD <ref type="bibr" target="#b41">[41]</ref>, DETR3D <ref type="bibr" target="#b42">[42]</ref>, PETR <ref type="bibr" target="#b21">[22]</ref> Graph-DETR3D <ref type="bibr" target="#b5">[6]</ref> and BEVFormer <ref type="bibr" target="#b17">[18]</ref>. Their numbers of parameters, computational budget, inference speed, and accuracy on the nuScenes val set are all listed in Tab. 1. Some state-of-the-art methods with other sensors are also listed for comparison like LiDAR-based method Center-Point <ref type="bibr" target="#b46">[46]</ref> and radar-based method CenterFusion <ref type="bibr" target="#b27">[27]</ref>. The high-speed version dubbed BEVDet4D-Tiny scores 47.6% NDS on nuScenes val set, which exceeds the baseline (i.e. BEVDet-Tiny <ref type="bibr" target="#b14">[15]</ref> with 39.2% NDS) by a large margin of +8.4% NDS. The improvement in the composite indicator NDS mainly derives from the reduction of the orientation error, the velocity error, and the attribute error. Specifically, benefitting from the well-designed BEVDet4D paradigm, the velocity error is significantly decreased by -62.9% from BEVDet-Tiny 0.909 mAVE to BEVDet4D-Tiny 0.337 mAVE. For the first time, the precision of the velocity prediction in the camera-based methods notably exceeds the CenterFusion <ref type="bibr" target="#b27">[27]</ref> 0.540 mAVE, who relies on the multi-sensor fusion with camera and radar for high precision in this aspect. Besides, at a similar inference speed, velocity precision of BEVDet4D-Tiny is also comparable with the state-of-the-art LiDAR-based method PointPillar <ref type="bibr" target="#b16">[17]</ref> (i.e. 17.9 FPS and 0.323 mAVE) implemented in <ref type="bibr" target="#b46">[46]</ref>. With respect to the orientation prediction, the proposed method also reduces the error in this aspect by -12.0% from BEVDet-Tiny 0.523 mAOE to BEVDet4D-Tiny 0.460 mAOE. This is because the orientation and velocity of the targets are strong-coupled. Analogously, the attribute error is reduced by -25.1% from BEVDet-Tiny 0.247 mAAE to BEVDet4D-Tiny 0.185 mAAE.</p><p>While upgrading the paradigm to BEVDet4D-Base analogous to BEVDet-Base <ref type="bibr" target="#b14">[15]</ref>, the promotion on the baseline is slightly narrowed to +7.3% on the composite indicator NDS from BEVDet-Base 47.2% NDS to BEVDet4D-Base 54.5% NDS. This surpasses the concurrent work of BEV-Former <ref type="bibr" target="#b17">[18]</ref> by +2.8% NDS (i.e. 54.5% NDS v.s. 51.7% NDS), while running faster than it in test time (i.e. 1.9 FPS v.s. 1.7 FPS). With test time augmentation, we further push the performance boundary to 55.2% NDS. It is worth noting that, thanks to the few framework adjustments, BEVDet4D achieves the aforementioned performance improvement at the cost of negligible extra inference latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">nuScenes test set</head><p>For the nuScenes test set, we train the BEVDet4D-Base configuration on the train and val sets. A single model with test time augmentation is adopted. As listed in Tab. 2, BEVDet4D ranks first on the nuScenes vision-based 3D object detection leader board with a score of 56.9% NDS, substantially surpassing the previous leading method BEVDet <ref type="bibr" target="#b14">[15]</ref> by +8.7% NDS. It also exceeds the concurrent work of BEVFormer <ref type="bibr" target="#b17">[18]</ref> by +3.4% NDS and significantly ex- ceeds those relied on additional data for pre-training like DD3D <ref type="bibr" target="#b29">[29]</ref>, DETR3D <ref type="bibr" target="#b42">[42]</ref>, and PETR <ref type="bibr" target="#b21">[22]</ref>. Besides the composite indicator, BEVDet4D has leading performance in most other indicators like mAP, mATE, mAOE, mAVE and mATE. With respect to the ability of generalization, the previous leading method BEVDet <ref type="bibr" target="#b14">[15]</ref> has merely +0.5% performance growth from val set 47.7% NDS to test set 48.2% NDS. However, with the same configuration, the performance boosting of BEVDet4D is +1.7% NDS from val set 55.2% NDS to test set 56.9% NDS. This indicates that exploiting temporal cues in BEVDet4D can also help improve the models' generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Road Map of Building BEVDet4D</head><p>In this subsection, we empirically show how the robust performance of BEVDet4D is built. BEVDet-Tiny <ref type="bibr" target="#b14">[15]</ref> without acceleration is adopted as a baseline. In other words, the spatial alignment operation in BEVDet4D is conducted within the view transformer by adjusting the pseudo point cloud <ref type="bibr" target="#b30">[30]</ref>. The results of different configurations are listed in Tab. 3. Some key factors are discussed one by one in the following. Directly concatenate the current frame feature with the previous one in configuration Tab. 3 (A), the overall performance drops from 39.2% NDS to 37.6% NDS by -1.6%. This modification degrades the models' performance, especially on the translation and the velocity aspects. We conjecture that, due to the ego-motion, the positional shift of the same static object between the two candidate features will confuse the following modules' judgment on the object position. With respect to the moving object, it is more complicated for the modules to judge out the velocity target defined in the current frame's ego coordinate system <ref type="bibr" target="#b14">[15]</ref> from the positional shift between the two candidate features which is described in Eq. 1. As to this end, the module needs to remove the fraction of ego-motion from this positional shift and consider the time factor.</p><p>By conducting translation-only align operation in configuration Tab. 3 (B), we enable the modules to utilize the position-aligned candidate features to make better percep- tions of the static targets. Besides, the velocity predicting task is simplified by removing the fraction of the egomotion. As result, the translation error is reduced by -5.4% to 0.672, which has surpassed the baseline configuration with a translation error of 0.691. Moreover, the velocity error is also reduced by -23.2% from configuration Tab. 3 (A) 1.544 mAVE to configuration Tab. 3 (B) 1.186 mAVE. However, this velocity error is still larger than that of the baseline configuration. We conjecture that the distribution of the positional shift is far from that of the velocity due to the inconsistent time duration between the two adjacent frames. Further removing the time factor in configuration Tab. 3 (C), we let the module directly predict the targets' positional shift in two candidate features. This modification successfully simplifies the learning targets and makes the trained module more robust on the validation set. The velocity error is thus further reduced by a large margin of -59.6% to 0.479 mAVE, which is just 52.7% of the naive BEDVet <ref type="bibr" target="#b14">[15]</ref>.</p><p>In configuration Tab. 3 (D), we apply an extra BEV encoder before concatenating the two candidate features. This slightly enlarges the computational budget by 2.8%. The change of inference speed is negligible. However, this modification offers comprehensive improvement on the baseline  (i.e., Tab. 3 (C)). The overall performance is improved by +0.9% NDS from 44.0% to 44.9%. By adjusting the loss weight of velocity prediction in the training process, configuration Tab. 3 (E) reduces the velocity error to 0.435. By considering the rotation variance of the ego pose in the align operation, configuration Tab. 3 (F) further reduces the velocity error by 13.6% from 0.435 (i.e., Tab. 3 (E)) to 0.376. This indicates that a precise align operation can help increase the precision of velocity prediction.</p><p>To search for the optimal test time interval between the current frame and the reference one, we use the unlabeled camera sweeps with 12Hz instead of the annotated camera frames (2Hz) in configuration Tab. 3 (G). The time interval between two camera sweeps is denoted as T ? 0.083s. We select three different time intervals in each training configuration and judge the adjusting direction by comparing them in test time. In this way, we can avoid the training disturbance in searching for this hyper-parameter. According to <ref type="figure" target="#fig_2">Fig. 4</ref>, The optimal interval is around 15T which is set as the test time interval by default in this paper. During the training process, we conduct data augmentation by randomly sampling time intervals within [3T, 27T ]. As a result, configuration Tab. 3 (G) further reduces the velocity error by 12.8% from 0.376 to 0.328.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Precision Degeneration of the Interpolation</head><p>We use configuration Tab. 3 (C) to exploit the precision degeneration of the interpolation operation. Several ablation configurations are constructed in Tab. 4 to study the factors like the BEV resolution and the interpolation operation. When a low BEV resolution of 0.8m?0.8m is applied, we observed a slight drop in velocity precision from configuration Tab. 4 (A) 0.479 mAVE to (B) 0.499 mAVE. This indicates that aligning the feature map after the view transformation with interpolation operation will introduce systematic error. However, the precondition of acceleration in BEVDet <ref type="bibr" target="#b14">[15]</ref> can be maintained in configuration Tab. 4 (B). Benefitting from the acceleration method, the inference speed can be scaled up to 15.6 FPS, which is twice that of the configuration Tab. 4 (A).</p><p>When a high BEV resolution of 0.4m?0.4m is applied, the performance difference between aligning within the view transformation and aligning after view transformation with interpolation operation is negligible (i.e. Tab. 4 (C) with 45.2 NDS v.s. Tab. 4 (D) with 45.3 NDS). High BEV resolution can help reduce the precision degeneration caused by the interpolation operation. Besides, from the perspective of inference acceleration, conducting aligning operations within the view transformation is deprecated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">The Position of the Temporal Fusion</head><p>It is not trivial to select the position of the temporal fusion in the BEVDet4D framework. We compare some typical positions in Tab. 5 to study this problem. Among all configurations, conducting temporal fusion after the extra BEV encoder in configuration Tab. 5 (B) is the most applicable one with the lowest velocity error of 0.429 mAVE. When bringing forward the temporal fusion in configuration Tab. 5 (A), the velocity error is increased by +11.9% to 0.480 mAVE. This indicates that the BEV feature generated by the view transformer is too coarse to be directly applied. An extra BEV encoder before temporal fusion can help alleviate this problem. When we postpone the temporal fusion to the back of the BEV encoder in configuration Tab. 5 (C), the overall performance degenerates to 39.4% NDS which is close to the baseline BEVDet <ref type="bibr" target="#b14">[15]</ref> with 39.2% NDS. More precisely, the feature from the previous frame helps slightly reduce the velocity error from 0.909 mAVE to 0.838 but increases the translation error from 0.691 mATE to 0.720 mATE. This indicates that the BEV encoder plays an important role in effectuating the proposed BEVDet4D paradigm by resisting the positional misleading from the previous frame feature and estimating the velocity according to the difference between the two candidate features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We pioneer the exploitation of vision-based autonomous driving in the spatial-temporal 4D space by proposing BEVDet4D to lift the scalable BEVDet <ref type="bibr" target="#b14">[15]</ref> from spatialonly 3D working space into spatial-temporal 4D working space. BEVDet4D retains the elegance of BEVDet while substantially pushing the performance in multi-camera 3D object detection, particularly in the velocity prediction aspect. Future works will focus on the design of framework and paradigm for actively mining the temporal cues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The inference speed and performance of different paradigms on the nuScenes val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Ablation on the time interval between the current frame and the reference one. Points drawn in the same color are in the same training configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different paradigms on the nuScenes val set. ? initialized from a FCOS3D backbone. ? with test-time augmentation. # with model ensemble.</figDesc><table><row><cell>Methods</cell><cell cols="3">Image Size #param. GFLOPs</cell><cell>Modality</cell><cell cols="8">mAP? mATE? mASE? mAOE? mAVE? mAAE? NDS? FPS</cell></row><row><cell>CenterFusion [27]</cell><cell>800?450</cell><cell>20.4M</cell><cell cols="3">-Camera &amp; Radar 0.332</cell><cell>0.649</cell><cell>0.263</cell><cell>0.535</cell><cell>0.540</cell><cell>0.142</cell><cell>0.453</cell><cell>-</cell></row><row><cell>VoxelNet [46]</cell><cell>-</cell><cell>8.8M</cell><cell>-</cell><cell>LiDAR</cell><cell>0.563</cell><cell>0.292</cell><cell>0.253</cell><cell>0.316</cell><cell>0.287</cell><cell>0.191</cell><cell cols="2">0.648 14.1</cell></row><row><cell>PointPillar [46]</cell><cell>-</cell><cell>6.0M</cell><cell>-</cell><cell>LiDAR</cell><cell>0.487</cell><cell>0.315</cell><cell>0.260</cell><cell>0.368</cell><cell>0.323</cell><cell>0.203</cell><cell cols="2">0.597 17.9</cell></row><row><cell>CenterNet[48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.306</cell><cell>0.716</cell><cell>0.264</cell><cell>0.609</cell><cell>1.426</cell><cell>0.658</cell><cell>0.328</cell><cell>-</cell></row><row><cell>FCOS3D [40]</cell><cell>1600?900</cell><cell>52.5M</cell><cell>2,008.2</cell><cell>Camera</cell><cell>0.295</cell><cell>0.806</cell><cell>0.268</cell><cell>0.511</cell><cell>1.315</cell><cell>0.170</cell><cell>0.372</cell><cell>1.7</cell></row><row><cell>DETR3D [42]</cell><cell>1600?900</cell><cell>51.3M</cell><cell>1,016.8</cell><cell>Camera</cell><cell>0.303</cell><cell>0.860</cell><cell>0.278</cell><cell>0.437</cell><cell>0.967</cell><cell>0.235</cell><cell>0.374</cell><cell>2.0</cell></row><row><cell>PGD [41]</cell><cell>1600?900</cell><cell>53.6M</cell><cell>2,223.0</cell><cell>Camera</cell><cell>0.335</cell><cell>0.732</cell><cell>0.263</cell><cell>0.423</cell><cell>1.285</cell><cell>0.172</cell><cell>0.409</cell><cell>1.4</cell></row><row><cell>PETR-R50 [22]</cell><cell>1056?384</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.313</cell><cell>0.768</cell><cell>0.278</cell><cell>0.564</cell><cell>0.923</cell><cell>0.225</cell><cell cols="2">0.381 10.7</cell></row><row><cell>PETR-R101 [22]</cell><cell>1408?512</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.357</cell><cell>0.710</cell><cell>0.270</cell><cell>0.490</cell><cell>0.885</cell><cell>0.224</cell><cell>0.421</cell><cell>3.4</cell></row><row><cell>PETR-Tiny [22]</cell><cell>1408?512</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.361</cell><cell>0.732</cell><cell>0.273</cell><cell>0.497</cell><cell>0.808</cell><cell>0.185</cell><cell>0.431</cell><cell>-</cell></row><row><cell>BEVDet-Tiny[15]</cell><cell>704?256</cell><cell>52.6M</cell><cell>215.3</cell><cell>Camera</cell><cell>0.312</cell><cell>0.691</cell><cell>0.272</cell><cell>0.523</cell><cell>0.909</cell><cell>0.247</cell><cell cols="2">0.392 15.6</cell></row><row><cell>BEVDet-Base[15]</cell><cell cols="2">1600?640 126.6M</cell><cell>2,962.6</cell><cell>Camera</cell><cell>0.393</cell><cell>0.608</cell><cell>0.259</cell><cell>0.366</cell><cell>0.822</cell><cell>0.191</cell><cell>0.472</cell><cell>1.9</cell></row><row><cell>BEVFormer [18]</cell><cell>1600?900</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.416</cell><cell>0.673</cell><cell>0.274</cell><cell>0.372</cell><cell>0.394</cell><cell>0.198</cell><cell>0.517</cell><cell>1.7</cell></row><row><cell>BEVDet4D-Tiny</cell><cell>704?256</cell><cell>53.6M</cell><cell>222.0</cell><cell>Camera</cell><cell>0.338</cell><cell>0.672</cell><cell>0.274</cell><cell>0.460</cell><cell>0.337</cell><cell>0.185</cell><cell cols="2">0.476 15.5</cell></row><row><cell>BEVDet4D-Base</cell><cell cols="2">1600?640 127.6M</cell><cell>2,989.2</cell><cell>Camera</cell><cell>0.421</cell><cell>0.579</cell><cell>0.258</cell><cell>0.329</cell><cell>0.301</cell><cell>0.191</cell><cell>0.545</cell><cell>1.9</cell></row><row><cell>FCOS3D ? ?# [40]</cell><cell>1600?900</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.343</cell><cell>0.725</cell><cell>0.263</cell><cell>0.422</cell><cell>1.292</cell><cell>0.153</cell><cell>0.415</cell><cell>-</cell></row><row><cell>DETR3D ? [42]</cell><cell>1600?900</cell><cell>51.3M</cell><cell>-</cell><cell>Camera</cell><cell>0.349</cell><cell>0.716</cell><cell>0.268</cell><cell>0.379</cell><cell>0.842</cell><cell>0.200</cell><cell>0.434</cell><cell>-</cell></row><row><cell>PGD ? ? [41]</cell><cell>1600?900</cell><cell>53.6M</cell><cell>-</cell><cell>Camera</cell><cell>0.369</cell><cell>0.683</cell><cell>0.260</cell><cell>0.439</cell><cell>1.268</cell><cell>0.185</cell><cell>0.428</cell><cell>-</cell></row><row><cell>PETR-R101 ? [22]</cell><cell>1600?900</cell><cell>-</cell><cell>-</cell><cell>Camera</cell><cell>0.370</cell><cell>0.711</cell><cell>0.267</cell><cell>0.383</cell><cell>0.865</cell><cell>0.201</cell><cell>0.442</cell><cell>-</cell></row><row><cell cols="3">BEVDet-Base ?[15] 1600?640 126.6M</cell><cell>-</cell><cell>Camera</cell><cell>0.397</cell><cell>0.595</cell><cell>0.257</cell><cell>0.355</cell><cell>0.818</cell><cell>0.188</cell><cell>0.477</cell><cell>-</cell></row><row><cell>BEVDet4D-Base ?</cell><cell cols="2">1600?640 126.6M</cell><cell>-</cell><cell>Camera</cell><cell>0.426</cell><cell>0.560</cell><cell>0.254</cell><cell>0.317</cell><cell>0.289</cell><cell>0.186</cell><cell>0.552</cell><cell>-</cell></row></table><note>precision degeneration will be quantitatively estimated in the ablation study Section. 4.3.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>are processed by random flipping, random scaling with a range of s ? [W in /1600?0.06, W in /1600+ 0.11], random rotating with a range of r ? [?5.4 ? , 5.4 ? ], and finally cropping to a size of W in ? H in . The cropping is conducted randomly in the horizon direction but is fixed in the vertical direction (i.e., (y 1 , y 2 ) = (max(0, s * 900 ? H in ), y 1 + H in ), where y 1 and y 2 are the upper bound and the lower bound of the target region.</figDesc><table /><note>) In the BEV space, the input feature and 3D object detection tar- gets are augmented by random flipping, random rotating with a range of [?22.5 ? , 22.5 ? ], and random scaling with a range of [0.95, 1.05]. Following CenterPoint [46], all mod-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art methods on the nuScenes test set. ? pre-train on DDAD<ref type="bibr" target="#b11">[12]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell cols="7">mAP? mATE? mASE? mAOE? mAVE? mAAE? NDS?</cell></row><row><cell>PointPillars(Light) [17]</cell><cell>LiDAR</cell><cell>0.305</cell><cell>0.517</cell><cell>0.290</cell><cell>0.500</cell><cell>0.316</cell><cell>0.368</cell><cell>0.453</cell></row><row><cell>CenterFusion [27]</cell><cell>Camera &amp; Radar</cell><cell>0.326</cell><cell>0.631</cell><cell>0.261</cell><cell>0.516</cell><cell>0.614</cell><cell>0.115</cell><cell>0.449</cell></row><row><cell>CenterPoint [46]</cell><cell cols="2">Camera &amp; LiDAR &amp; Radar 0.671</cell><cell>0.249</cell><cell>0.236</cell><cell>0.350</cell><cell>0.250</cell><cell>0.136</cell><cell>0.714</cell></row><row><cell>MonoDIS [34]</cell><cell>Camera</cell><cell>0.304</cell><cell>0.738</cell><cell>0.263</cell><cell>0.546</cell><cell>1.553</cell><cell>0.134</cell><cell>0.384</cell></row><row><cell>CenterNet [48]</cell><cell>Camera</cell><cell>0.338</cell><cell>0.658</cell><cell>0.255</cell><cell>0.629</cell><cell>1.629</cell><cell>0.142</cell><cell>0.400</cell></row><row><cell>FCOS3D [40]</cell><cell>Camera</cell><cell>0.358</cell><cell>0.690</cell><cell>0.249</cell><cell>0.452</cell><cell>1.434</cell><cell>0.124</cell><cell>0.428</cell></row><row><cell>PGD [41]</cell><cell>Camera</cell><cell>0.386</cell><cell>0.626</cell><cell>0.245</cell><cell>0.451</cell><cell>1.509</cell><cell>0.127</cell><cell>0.448</cell></row><row><cell>PETR [22]</cell><cell>Camera</cell><cell>0.434</cell><cell>0.641</cell><cell>0.248</cell><cell>0.437</cell><cell>0.894</cell><cell>0.143</cell><cell>0.481</cell></row><row><cell>BEVDet [15]</cell><cell>Camera</cell><cell>0.422</cell><cell>0.529</cell><cell>0.236</cell><cell>0.395</cell><cell>0.979</cell><cell>0.152</cell><cell>0.482</cell></row><row><cell>BEVFormer [18]</cell><cell>Camera</cell><cell>0.445</cell><cell>0.631</cell><cell>0.257</cell><cell>0.405</cell><cell>0.435</cell><cell>0.143</cell><cell>0.535</cell></row><row><cell>BEVDet4D</cell><cell>Camera</cell><cell>0.451</cell><cell>0.511</cell><cell>0.241</cell><cell>0.386</cell><cell>0.301</cell><cell>0.121</cell><cell>0.569</cell></row><row><cell>DD3D ? [29]</cell><cell>Camera</cell><cell>0.418</cell><cell>0.572</cell><cell>0.249</cell><cell>0.368</cell><cell>1.014</cell><cell>0.124</cell><cell>0.477</cell></row><row><cell>DETR3D ? [42]</cell><cell>Camera</cell><cell>0.386</cell><cell>0.626</cell><cell>0.245</cell><cell>0.394</cell><cell>0.845</cell><cell>0.133</cell><cell>0.479</cell></row><row><cell>Graph-DETR3D ? [6]</cell><cell>Camera</cell><cell>0.425</cell><cell>0.621</cell><cell>0.251</cell><cell>0.386</cell><cell>0.790</cell><cell>0.128</cell><cell>0.495</cell></row><row><cell>PETR ? [22]</cell><cell>Camera</cell><cell>0.441</cell><cell>0.593</cell><cell>0.249</cell><cell>0.383</cell><cell>0.808</cell><cell>0.132</cell><cell>0.504</cell></row></table><note>els are trained with CBGS [50]. In testing time, the input images are scaled by a factor of s = W in /1600 + 0.04 and cropped to W in ? H in resolution with a region defined as (x 1 , x 2 , y 1 , y 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of ablation study on the nuScenes val set. The align operation includes rotation (R) and translation (T). Extra denotes the extra BEV encoder. Aug. denotes the augmentation in time dimension when selecting the adjacent frame.Methods Align Target Extra Weight Aug. mAP? NDS? mATE? mASE? mAOE? mAVE? mAAE? #param. GFLOPs FPS</figDesc><table><row><cell>BEVDet</cell><cell></cell><cell>Speed</cell><cell>0.2</cell><cell>0.312 0.392</cell><cell>0.691</cell><cell>0.272</cell><cell>0.523</cell><cell>0.909</cell><cell>0.247</cell><cell>52.5M</cell><cell>215.3 7.8</cell></row><row><cell>A</cell><cell></cell><cell>Speed</cell><cell>0.2</cell><cell>0.296 0.376</cell><cell>0.711</cell><cell>0.274</cell><cell>0.501</cell><cell>1.544</cell><cell>0.234</cell><cell>52.6M</cell><cell>215.9 7.8</cell></row><row><cell>B</cell><cell>T</cell><cell>Speed</cell><cell>0.2</cell><cell>0.321 0.393</cell><cell>0.672</cell><cell>0.272</cell><cell>0.525</cell><cell>1.186</cell><cell>0.215</cell><cell>52.6M</cell><cell>215.9 7.8</cell></row><row><cell>C</cell><cell>T</cell><cell>Offset</cell><cell>0.2</cell><cell>0.320 0.440</cell><cell>0.697</cell><cell>0.274</cell><cell>0.514</cell><cell>0.479</cell><cell>0.229</cell><cell>52.6M</cell><cell>215.9 7.8</cell></row><row><cell>D</cell><cell>T</cell><cell>Offset</cell><cell>0.2</cell><cell>0.323 0.449</cell><cell>0.680</cell><cell>0.274</cell><cell>0.480</cell><cell>0.479</cell><cell>0.209</cell><cell>53.6M</cell><cell>222.0 7.7</cell></row><row><cell>E</cell><cell>T</cell><cell>Offset</cell><cell>1.0</cell><cell>0.322 0.452</cell><cell>0.685</cell><cell>0.271</cell><cell>0.496</cell><cell>0.435</cell><cell>0.201</cell><cell>53.6M</cell><cell>222.0 7.7</cell></row><row><cell>F</cell><cell cols="2">R&amp;T Offset</cell><cell>1.0</cell><cell>0.321 0.461</cell><cell>0.681</cell><cell>0.272</cell><cell>0.461</cell><cell>0.376</cell><cell>0.203</cell><cell>53.6M</cell><cell>222.0 7.7</cell></row><row><cell>G</cell><cell cols="2">R&amp;T Offset</cell><cell>1.0</cell><cell>0.340 0.481</cell><cell>0.660</cell><cell>0.270</cell><cell>0.453</cell><cell>0.328</cell><cell>0.176</cell><cell>53.6M</cell><cell>222.0 7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for the precision degeneration of the interpolation operation on the nuScenes val set.Configuration BEV Resolution Align mAP? NDS? mATE? mASE? mAOE? mAVE? mAAE? #param.</figDesc><table><row><cell>GFLOPs FPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for the position of temporal fusion in BEVDet4D on the nuScenes val set.</figDesc><table><row><cell>Methods</cell><cell>Configuration</cell><cell>Position</cell><cell cols="6">mAP? NDS? mATE? mASE? mAOE? mAVE? mAAE?</cell></row><row><cell>BEVDet[15]</cell><cell>-</cell><cell>-</cell><cell>0.312 0.392</cell><cell>0.691</cell><cell>0.272</cell><cell>0.523</cell><cell>0.909</cell><cell>0.247</cell></row><row><cell>BEVDet4D</cell><cell>A</cell><cell cols="2">Before Extra BEV Encoder 0.320 0.442</cell><cell>0.687</cell><cell>0.277</cell><cell>0.519</cell><cell>0.480</cell><cell>0.214</cell></row><row><cell>BEVDet4D</cell><cell>B</cell><cell>After Extra BEV Encoder</cell><cell>0.323 0.453</cell><cell>0.674</cell><cell>0.272</cell><cell>0.503</cell><cell>0.429</cell><cell>0.208</cell></row><row><cell>BEVDet4D</cell><cell>C</cell><cell>After BEV Encoder</cell><cell>0.311 0.394</cell><cell>0.720</cell><cell>0.274</cell><cell>0.536</cell><cell>0.838</cell><cell>0.245</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kinematic 3D Object Detection in Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Training Deep Nets with Sublinear Memory Cost. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memory Enhanced Global-Local Aggregation for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11582</idno>
		<title level="m">Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MMDetection3D: Open-MMLab next-generation platform for general 3D object detection</title>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
	</analytic>
	<monogr>
		<title level="m">MMDetection3D Contributors</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object Guided External Memory Network for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongpu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6678" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation Distillation Networks for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3D Packing for Self-Supervised Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11790</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8973" to="8983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">BEV-Former: Learning Bird&apos;s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonghao</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17270</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mobile Video Object Detection with Temporally-Aware Feature Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5686" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Looking Fast and Slow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Memory-Guided Mobile Video Object Detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">PETR: Position Embedding Transformation for Multi-View 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05625</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongdai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Jin Fang, and Liangjun Zhang. AutoShape: Real-Time Shape-Aware</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3D Object Detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15641" to="15650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DECOUPLED WEIGHT DECAY REGULARIZATION</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online Video Object Detection using Association LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2344" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometry Uncertainty Projection Network for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3111" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-View Semantic Segmentation for Sensing Surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankai</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho Yin Tiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4867" to="4873" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is Pseudo-Lidar needed for Monocular 3D Object detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3142" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="194" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Categorical Depth Distribution Network for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8555" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11138" to="11147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Disentangling Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalability in Perception for Autonomous Driving: Waymo Open Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FCOS: Fully Convolutional One-Stage Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Depthconditioned Dynamic Message Propagation for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive Coordinate Transforms for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10956</idno>
		<title level="m">Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Probabilistic and Geometric Depth: Detecting Objects in Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14160</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06922</idno>
		<title level="m">DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-View Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15536" to="15545" />
		</imprint>
	</monogr>
	<note>Shengfeng He, and Jia Pan</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking Assisted Faster Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1750" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Centerbased 3D Object Detection and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11784" to="11793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Objects are Different: Flexible Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3289" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as Points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7556" to="7566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards High Performance Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Flow-Guided Feature Aggregation for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Devil Is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2713" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
