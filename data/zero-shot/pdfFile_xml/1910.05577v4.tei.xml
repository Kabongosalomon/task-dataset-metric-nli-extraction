<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Gated Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
							<email>xudong.lin@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<email>shih.fu.chang@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Gated Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Context-Gated Convolution</term>
					<term>Global Context Information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the basic building block of Convolutional Neural Networks (CNNs), the convolutional layer is designed to extract local patterns and lacks the ability to model global context in its nature. Many efforts have been recently devoted to complementing CNNs with the global modeling ability, especially by a family of works on global feature interaction. In these works, the global context information is incorporated into local features before they are fed into convolutional layers. However, research on neuroscience reveals that the neurons' ability of modifying their functions dynamically according to context is essential for the perceptual tasks, which has been overlooked in most of CNNs. Motivated by this, we propose one novel Context-Gated Convolution (CGC) to explicitly modify the weights of convolutional layers adaptively under the guidance of global context. As such, being aware of the global context, the modulated convolution kernel of our proposed CGC can better extract representative local patterns and compose discriminative features. Moreover, our proposed CGC is lightweight and applicable with modern CNN architectures, and consistently improves the performance of CNNs according to extensive experiments on image classification, action recognition, and machine translation. Our code of this paper is available at https://github.com/XudongLinthu/context-gated-convolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have achieved remarkable successes on various tasks, e.g., image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44]</ref>, image translation <ref type="bibr" target="#b58">[59]</ref>, action recognition <ref type="bibr" target="#b4">[5]</ref>, sentence/text classification <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b29">30]</ref>, machine translation <ref type="bibr" target="#b13">[14]</ref>, etc. However, the sliding window mechanism of convolution makes it only capable of capturing local patterns, limiting its ability of utilizing global context. Taking the 2D convolution on the image as one example, as <ref type="figure" target="#fig_1">Fig. 1(a)</ref> shows, the traditional convolution only operates on the local image patch and thereby composes local features.  According to the recent research on neuroscience <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref>, neurons' awareness of global context is important for us to better interpret visual scenes, stably perceive objects, and effectively process complex perceptual tasks. Many methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref> have been recently proposed to introduce global context modeling modules into CNN architectures. As <ref type="figure" target="#fig_1">Fig. 1(b)</ref> shows, these methods, which are named as global feature interaction methods in this paper, modulate intermediate feature maps by incorporating the global context into the local feature representation.</p><p>However, as stated in <ref type="bibr" target="#b14">[15]</ref>, "rather than having a fixed functional role, neurons should be thought of as adaptive processors, changing their function according to the behavioural context". Therefore, the context information should be utilized to explicitly modulate the convolution kernels for "changing the structure of correlations over neuronal ensembles" <ref type="bibr" target="#b14">[15]</ref>. However, to the best of our knowledge, such a modulating mechanism has not been exploited in CNNs yet, even though it is one efficient and intuitive way. Motivated by this, we will model convolutional layers as "adaptive processors" and explore how to leverage global context to guide the composition of local features in convolution operations.</p><p>In this paper, we propose Context-Gated Convolution (CGC), as shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>, a new perspective of complementing CNNs with the awareness of the global context. Specifically, our proposed CGC learns a series of mappings to generate gates from the global context feature representations to modulate convolution kernels accordingly. With the modulated kernels, the traditional convolution is performed on input feature maps, which enables convolutional layers to dynamically capture representative local patterns and compose local features of interest under the guidance of global context. Our contributions lie in three-fold.</p><p>-To the best of our knowledge, we make the first attempt of introducing the context-awareness to convolutional layers by modulating their weights according to the global context. -We propose a novel lightweight Context-Gated Convolution (CGC) to effectively generate gates for convolution kernels to modify the weights with the guidance of global context. Our CGC consists of a Context Encoding Module that encodes context information into latent representations, a Channel Interacting Module that projects them into the space of output dimension, and a Gate Decoding Module that decodes the latent representations to produce the gate. -Our proposed CGC can better capture local patterns and compose discriminative features, and consistently improve the generalization of traditional convolution with a negligible complexity increment on various tasks including image classification, action recognition, and machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There have been many efforts in augmenting CNNs with context information. They can be roughly categorized into three types: first, adding backward connections in CNNs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54]</ref> to model the top-down influence <ref type="bibr" target="#b14">[15]</ref> like humans' visual processing system; second, modifying intermediate feature representations in CNNs according to the attention mechanism <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>; third, dynamically generating the parameters of convolutional layers according to local or global information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>For the first category of works, it is still unclear how the feedback mechanism can be effectively and efficiently modeled in CNNs. For example, Yang et al. <ref type="bibr" target="#b53">[54]</ref> proposed an Alternately Updated Clique to introduce feedback mechanisms into CNNs. However, compared to traditional CNNs, the complex updating strategy increases the difficulty for training them as well as the latency at the inference time. The second category of works is the global feature interaction methods. They <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> were proposed recently to modify local features according to global context information, usually by a global correspondence, i.e., the self-attention mechanism. There are also works on reducing the complexity of the self-attention mechanism <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10]</ref>. However, this family of works only considers changing the input feature maps.</p><p>The third type of works is more related to our work. Zhu et al. <ref type="bibr" target="#b59">[60]</ref> proposed to adaptively set the offset of each element in a convolution kernel and the gate value for each element in the input local feature patch. However, the mechanism only changes the input to the convolutional layer. The weight tensor of the convolutional layer is not considered. Wu et al. <ref type="bibr" target="#b52">[53]</ref> proposed to dynamically generate the weights of convolution kernels. However, it is specialized for Lightweight Convolution <ref type="bibr" target="#b52">[53]</ref> and only takes local segments as inputs. Another family of works on dynamic filters <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> also belongs to this type. They generate weights of convolution kernels using features extracted from input images by another CNN feature extractor. The expensive feature extraction process makes it more suitable for generating a few filters, e.g., in the case of low-level image processing. It is impractical to generate weights for all the layers in a deep CNN model in this manner.  <ref type="bibr" target="#b0">(1)</ref> and G <ref type="bibr" target="#b1">(2)</ref> from C and O to construct the gate G. and denote convolution and element-wise multiplication operations, respectively. ? is shown in Eq. (1). The dimension transformed in each linear layer is underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Context-Gated Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Without loss of generality, we consider one sample of 2D case. The input to a convolutional layer is a feature map X ? R c?h?w , where c is the number of channels, and h, w are respectively the height and width of the feature map. In each convolution operation, a local patch of size c ? k 1 ? k 2 is collected by the sliding window to multiply with the kernel W ? R o?c?k1?k2 of this convolutional layer, where o is the number of output channels, and k 1 , k 2 are respectively the height and width of the kernel. Therefore, only local information within each patch is extracted in one convolution operation. Although in the training process, the convolution kernels are learned from all the patches of all the images in the training set, the kernels are not adaptive to the current context during the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Module Design</head><p>In order to handle the aforementioned drawback of traditional convolution, we propose to incorporate the global context information during the convolution process. Different from the existing approaches that modify the input features according to the context, e.g., a global correspondence of feature representations, we attempt to directly modulate the convolution kernel under the guidance of the global context information.</p><p>One simple and straightforward way of modulating the convolution kernel W with global context information is to directly generate a gate G ? R o?c?k1?k2 of the same size as W according to the global context. Assuming that we generate the gate from a context vector v ? R l using a linear layer without the bias term, the number of parameters is l ? o ? c ? k 1 ? k 2 , which is extremely catastrophic when we modulate the convolution kernel of every convolutional layer. For modern CNNs, o and c can be easily greater than 100 or even 1,000, which makes o ? c the dominant term in the complexity. Inspired by previous works on convolution kernel decomposition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>, we propose to decompose the gate G into two tensors G (1) ? R c?k1?k2 and G (2) ? R o?k1?k2 , so that the complexity of o ? c can thereby significantly be broken down.</p><p>However, directly generating these two tensors is still impractical. Supposing that we generate them with two linear layers, the number of parameters is l ? (o + c) ? k 1 ? k 2 , which is of the same scale as the number of parameters of the convolution kernel itself. The bottleneck now is jointly modeling channel-wise and spatial interactions, namely l and (o + c) ? k 1 ? k 2 , considering that v ? R l is encoded from the input feature map X ? R c?h?w . Inspired by depth-wise separable convolutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>, we propose to model the spatial interaction and the channel-wise interaction separately to further reduce the complexity.</p><p>In this paper, we propose one novel Context-Gated Convolution (CGC) to incorporate the global context information during the convolution process. Specifically, our proposed CGC consists of three modules: the Context Encoding Module, the Channel Interacting Module, and the Gate Decoding Module. As shown in <ref type="figure">Fig. 2</ref>, the Context Encoding Module encodes global context information in each channel into a latent representation C via spatial interaction; the Channel Interacting Module projects the latent representation to the space of output dimension o via channel-wise interaction; the Gate Decoding Module produces G <ref type="bibr" target="#b0">(1)</ref> and G (2) from the latent representation C and the projected representation O to construct the gate G via spatial interaction. The detailed information is described in the following.</p><p>Context Encoding Module. To extract contextual information, we first use a pooling layer to reduce the spatial resolution to h ? w and then feed the resized feature map to the Context Encoding Module. It encodes information from all the spatial positions for each channel, and extracts a latent representation of the global context. We use a linear layer with weight E ? R h ?w ?d to project the resized feature map in each channel to a latent vector of size d. Inspired by the bottleneck structure from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b47">48]</ref>, we set d = k1?k2 2 to extract informative context, when not specified. The weight E is shared across different channels. A normalization layer and an activation function come after the linear layer. There are c channels, so the output of the Context Encoding Module is C ? R c?d . Since the output is fed into two different modules, we accordingly apply two individual Normalization layers so that different information can be conveyed if needed.</p><p>Channel Interacting Module. It projects the feature representations C ? R c?d to the space of output dimension o. Inspired by <ref type="bibr" target="#b18">[19]</ref>, we use a grouped linear layer I ? R Gate Decoding Module. It takes both C and O as inputs, and decodes the latent representations to the spatial size of convolution kernels. We use two linear layers whose weights D c ? R d?k1?k2 and D o ? R d?k1?k2 are respectively shared across different channels in C and O. Then each element in the gate G is produced by:</p><formula xml:id="formula_0">G h,i,j,k = ?(G (1) i,j,k + G (2) h,j,k ) = ?((CD c ) i,j,k + (OD o ) h,j,k ),<label>(1)</label></formula><p>where ?(?) denotes the sigmoid function. Now we have G with the same size of the convolution kernel W, which is generated from the global context by our lightweight modules. Then we can modulate the weight of a convolutional layer by element-wise multiplication to incorporate rich context information:</p><formula xml:id="formula_1">W = W G.<label>(2)</label></formula><p>With the modulated kernel, a traditional convolution process is performed on the input feature maps, where the context information can help the kernel capture more representative patterns and also compose features of interest.</p><p>Complexity. The computational complexity of our three modules is</p><formula xml:id="formula_2">O(c ? d ? h ? w + c ? o/g + c ? d ? k 1 ? k 2 + o ? d ? k 1 ? k 2 + o ? c ? k 1 ? k 2 ), where h , w can be set independent of h, w. It is negligible compared to convolution's O(o ? c ? k 1 ? k 2 ? h ? w)</formula><p>. Except the linear time of pooling, the complexity of these three modules is independent of the input's spatial size. The total number of parameters is</p><formula xml:id="formula_3">O(d ? h ? w + c ? o/g 2 + d ? k 1 ? k 2 ), which is negligible compared to traditional convolution's O(o ? c ? k 1 ? k 2 )</formula><p>. Therefore, we can easily replace the traditional convolution with our proposed CGC with a very limited computation and parameter increment, and enable convolutional layers to be adaptive to global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>We are aware of the previous works on dynamically modifying the convolution operation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>. As discussed before, <ref type="bibr" target="#b59">[60]</ref> essentially changes the input to the convolutional layer but not the weight tensor of the convolutional layer. Dynamic Convolution <ref type="bibr" target="#b52">[53]</ref> is specialized for the Lightweight convolution <ref type="bibr" target="#b52">[53]</ref> and only adaptive to local inputs. The family of work on dynamic filters <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> generates weights of convolution kernels using features extracted from input images by another CNN feature extractor. It is too expensive to generate weights for all the layers in a deep CNN model in this manner. In contrast, our CGC takes feature maps of a convolutional layer as input and makes it possible to dynamically modulate the weight of each convolutional layer, which systematically improves CNNs' global context modeling ability.</p><p>Both global feature interaction methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> modifying feature maps and our proposed CGC modulating kernels can incorporate the global context information into CNN architectures, which can boost the performance of CNNs. However, 1) with our CGC, the complexity of modulating kernels does not depend on input size, but global feature interaction methods, e.g., Non-local, may suffer from a quadratic computational complexity w.r.t. the input size; 2) our CGC can be easily trained from scratch and improve the training stability of CNNs according to our experiments (Sections 4.2 and 4.3); 3) by modulating kernels, our CGC can dynamically create kernels with specialized functions according to context (Section 4.2) and thus enable CNNs to accordingly capture discriminative information as adaptive processors, which cannot be realized by modifying feature maps. Moreover, our CGC is also somewhat complementary to global feature interaction methods (Sections 4.2 and 4.3) and we can further improve CNN's performance by applying both CGC and global feature interaction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the effectiveness of our proposed CGC in incorporating 1D, 2D, and 3D context information in 1D, 2D, and (2+1)D convolutions. We conduct extensive experiments on image classification, action recognition, and machine translation, and observe that our CGC consistently improves the performance of modern CNNs with a negligible parameter increment on six benchmark datasets: ImageNet <ref type="bibr" target="#b44">[45]</ref>, CIFAR-10 <ref type="bibr" target="#b30">[31]</ref>, ObjectNet <ref type="bibr" target="#b1">[2]</ref>, Something-Something (v1) <ref type="bibr" target="#b17">[18]</ref>, Kinetics <ref type="bibr" target="#b4">[5]</ref>, and IWSLT'14 De-En <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>All of the experiments are based on PyTorch <ref type="bibr" target="#b40">[41]</ref>. All the linear layers are without bias terms. We follow common practice to use Batch Normalization <ref type="bibr" target="#b26">[27]</ref> for computer vision tasks, and Layer Normalization <ref type="bibr" target="#b0">[1]</ref> for natural language processing tasks, respectively. We use ReLU <ref type="bibr" target="#b35">[36]</ref> as the activation function for all the experiments in this paper. We use average pooling with h = k 1 and w = k 2 , when not specified. Note that we only replace the convolution kernels with a spatial size larger than 1. For those point-wise convolutions, we take them as linear layers and do not modulate them. To reduce the size of I, we fix c/g = 16 when not specified. We initialize all these layers as what <ref type="bibr" target="#b19">[20]</ref> did for computer vision tasks and as what <ref type="bibr" target="#b16">[17]</ref> did for natural language processing tasks, when not specified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Classification</head><p>Experimental Setting. Following previous works <ref type="bibr" target="#b20">[21]</ref> on ImageNet <ref type="bibr" target="#b44">[45]</ref>, we train models on the ImageNet 2012 training set, which contains about 1.28 million images from 1,000 categories, and report the results on its validation set, which contains 50,000 images. We replace all the convolutions that are not 1 ? 1 in ResNet-50 <ref type="bibr" target="#b20">[21]</ref> with our CGC and train the network from scratch. Note that for the first convolutional layer, we use I ? R 3?64 for the Channel Interacting Module. We conduct experiments in two settings: Default and Advanced. For the Default setting, we follow common practice <ref type="bibr" target="#b20">[21]</ref> and apply minimum training tricks. For the Advanced setting, we borrow training tricks from <ref type="bibr" target="#b22">[23]</ref> to validate that our CGC can still improve the performance, even under a strong baseline. CIFAR-10 contains 50K training images and 10K testing images in 10 classes. We follow common practice <ref type="bibr" target="#b21">[22]</ref> to train and evaluate the models. We take ResNet-110 <ref type="bibr" target="#b21">[22]</ref> (with plain blocks) as the baseline model. All the compared methods are trained based on the same training protocol. The details are provided in the supplementary material. For evaluation, we report Top-1 and Top-5 accuracies of a single crop with the size 224 ? 224 for ImageNet and 32 ? 32 for CIFAR-10, respectively. ObjectNet <ref type="bibr" target="#b1">[2]</ref> is a new challenging evaluation dataset for image classification. There are 113 classes out of 313 ObjectNet classes, which overlap with ImageNet classes. We follow <ref type="bibr" target="#b1">[2]</ref> to evaluate models trained on ImageNet on the overlapped classes.</p><p>Performance Results. As <ref type="table" target="#tab_1">Table 1</ref> shows, our CGC significantly improves the performances of baseline models on both ImageNet and CIFAR-10. On Im-ageNet, our CGC improves the Top-1 accuracy of ResNet-50 under the Ad-   vanced setting by 1.41% with only 0.03M more parameters and 6M more FLOPs, which verifies our CGC's effectiveness of incorporating global context and its efficiency. We observe that our CGC outperforms DCN-v2 <ref type="bibr" target="#b59">[60]</ref>, SK-Net <ref type="bibr" target="#b32">[33]</ref> and CBAM <ref type="bibr" target="#b51">[52]</ref>, which indicates the superiority of modulating kernels. We also observe that our CGC can also improve the performance of CBAM-ResNet-50 <ref type="bibr" target="#b51">[52]</ref> consistently under both settings, which indicates that our proposed CGC is applicable with state-of-the-art global feature interaction methods. CBAM-ResNet-50 + CGC even reaches 79.74% Top-1 accuracy, which outperforms the other compared methods by a large margin. We also find that GC-ResNet-50 is hard to train from scratch unless using the fine-tuning protocol reported by <ref type="bibr" target="#b3">[4]</ref>, which indicates that modifying features may be misleading in the early training process. Although our CGC introduces a few new parameters, our model converges faster and more stably compared to vanilla ResNet-50, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. We conjecture that this is because the adaptiveness to global context improves the model's generalization ability and the gating mechanism reduces the norm of gradients back-propagated to the convolution kernels, which leads to a smaller Lipschitz constant and thus better training stability <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>To further validate the generalization ability of our CGC, we use Object-Net to evaluate models with good performances on ImageNet. ObjectNet <ref type="bibr" target="#b1">[2]</ref> is recently proposed to push image recognition models beyond their current limit of generalization. The dataset contains images "in many rotations, on differ-  ent backgrounds, from multiple viewpoints", which makes it hard for models trained on ImageNet to correctly classify these images. As <ref type="table" target="#tab_2">Table 2</ref> shows, our CGC significantly improves the generalization ability of the ResNet-50 baseline. The improvement (2.18%) is even larger than that on ImageNet validation set.</p><p>Visualization. To understand how CGC helps the model capture more informative features under the guidance of context information, we visualize the feature maps of ResNet-50 and our CGC-ResNet-50 by Grad-CAM++ <ref type="bibr" target="#b6">[7]</ref>. As <ref type="figure" target="#fig_5">Fig. 4</ref> shows, overall, the feature maps (After the CGC) produced by our CGC-ResNet-50 cover more informative regions, e.g., more instances or more parts of the ground-truth object, than vanilla ResNet-50.</p><p>Specifically, we visualize the feature maps before the last CGC in the model, the context information used by CGC, and the resulting feature maps after the CGC. As is clearly shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the proposed CGC extracts the context information from representative regions of the target object and successfully refines the feature maps with comprehensive understanding of the whole image and the target object. For example, in Gold Fish 1, the heads of the fishes are partially visible. Vanilla ResNet-50 mistakes this image as Sea Slug, because it only pays attention to the tails of the fishes, which are similar to sea slugs. However, our CGC utilizes the context of the whole image and guides the convolution with information from the entire fishes, which helps the model classify this image correctly.</p><p>Analysis of the Gate. To further validate that our CGC uses context information of the target objects to guide the convolution process, we calculate the average modulated kernel (in the last CGC of the model) for images of each class in the validation set. Then we calculate inter-class L ? 2 distance between every two average modulated kernels, i.e., class centers, and the intraclass L?2 distance (mean distance to the class center) for each class. As is shown in the supplementary material, we visualize the difference matrix between interclass distances and intra-class distances. In more than 93.99% of the cases, the inter-class distance is larger than the corresponding intra-class distance, which indicates that there are clear clusters of these modulated kernels and the clusters are aligned very well with the classes.</p><p>This observation strongly supports that our CGC successfully extracts classspecific context information and effectively modulates the convolution kernel to extract representative features. Meanwhile, the intra-class variance of the modulated kernels indicates that our CGC dynamically modulates convolution kernels according to different input contexts.</p><p>Ablation Study. In order to demonstrate the effectiveness of our module design, ablation studies are conducted on CIFAR-10, as illustrated in <ref type="table">Table 3a</ref>. Specifically, we ablate many variants of our CGC and find that our default setting is a good trade-off between parameter increment and performance gain. The experiments on the combination of G <ref type="bibr" target="#b0">(1)</ref> and G <ref type="bibr" target="#b1">(2)</ref> show that our decomposition approach in Eq. (1) is a better way to construct the gate. For channel interacting, we find that using a full linear model with g = 1 achieves better performance with more parameters, as is expected. We try removing the bottleneck structure and set d = k 1 ? k 2 , and the performance drops, which validates the necessity of the bottleneck structure.</p><p>Shared Norm indicates using the same Normalization layer for the following two branches. For Two Es, we learn another E to encode C only for the Channel Interacting Module. We also try sharing D for generating G <ref type="bibr" target="#b0">(1)</ref> and G <ref type="bibr" target="#b1">(2)</ref> , using larger resized feature maps and using max pooling instead of average pooling. All the results support our default setting. We also test different numbers of layers to replace traditional convolutions with our CGC. The result indicates that the more, the better. We select 3 variants with a similar number of parameters and performance on CIFAR-10 and further perform ablation studies for them on ImageNet. As <ref type="table">Table 3b</ref> shows, we observe the same performance rankings of these variants on ImageNet as those on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Action Recognition</head><p>Baseline Methods. For the action recognition task, we adopt three baselines to evaluate the effectiveness of our CGC: TSN <ref type="bibr" target="#b49">[50]</ref>, P3D-A <ref type="bibr" target="#b42">[43]</ref> (details are in the supplementary material), and TSM <ref type="bibr" target="#b33">[34]</ref>. Because our CGC's effectiveness of introducing 2D spatial context to CNNs has been verified on image classification, in this part, we focus on its ability of incorporating 1D temporal context and 3D spatiotemporal context. For the 1D case, we apply our CGC to temporal convolutions in every P3D-A block. For the 3D case, we apply our CGC to <ref type="table">Table 3</ref>: Ablation studies on CIFAR-10 and ImageNet. Param denotes the number of parameters in the model. ?MFLOPs is the increment of the number of multiplication-addition operations compared to ResNet-110 (256 MFLOPs). Bold indicates our default setting. Top-1 Accuracy (%) (average of 3 runs) is reported.</p><p>(a) CIFAR-10 spatial convolutions in P3D-A or 2D convolutions in TSN or TSM; the pooling layer produces c ? k ? k ? k cubes, the Context Encoding Module encodes k ? k ? k feature maps into a vector of length k 3 /2, and the Gate Decoding Module generates o ? c ? t ? k ? k gates. Note that for the first convolutional layer, we use I ? R 3?64 for the Channel Interacting Module.</p><p>Experimental Setting. The Something-Something (v1) dataset has a training split of 86,017 videos and a validation split of 11,522 videos, with 174 categories. We follow <ref type="bibr" target="#b41">[42]</ref> to train on the training set and report evaluation results on the validation set. We follow <ref type="bibr" target="#b33">[34]</ref> to process videos and augment data. Since we only use ImageNet for pretraining, we adapt the code base of TSM but the training setting from <ref type="bibr" target="#b41">[42]</ref>. We train TSN-and TSM-based models for 45 epochs (50 for P3D-A), starting from a learning rate of 0.025 (0.01 for P3D-A) and decreasing it by 0.1 at 26 and 36 epochs <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45</ref> for P3D-A). The Kinetics <ref type="bibr" target="#b4">[5]</ref> dataset has 400 action classes and 240K training samples. We follow <ref type="bibr" target="#b33">[34]</ref> to train and evaluate all the compared models.</p><p>For TSN-and TSM-based models, the batch size is 64 for 8-frame models and 32 for 16-frame models, and the dropout rate is set to 0.5. P3D-A takes 32 continuously sampled frames as input and the batch size is 64, and the dropout ratio is 0.8. We use the evaluation setting of <ref type="bibr" target="#b33">[34]</ref> for TSN-and TSM-based models and the evaluation settings of <ref type="bibr" target="#b50">[51]</ref> for P3D-A. All the models are trained with 8-GPU machines.</p><p>Performance Comparisons. As <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> show, our CGC significantly improves the performance of baseline CNN models, compared to Nonlocal <ref type="bibr" target="#b50">[51]</ref>. As aforementioned, Non-local modules modify the input feature maps of convolutional layers by reassembling local features according to the global correspondence. We apply Non-local blocks in the most effective way as is reported by <ref type="bibr" target="#b50">[51]</ref>. However, we observe that its performance gain is not consistent  when training the model from scratch. When applied to TSM on the Something-Something dataset, it even degrades the performance. Our proposed CGC consistently improves the performances of all the baseline models. We also observe that on Kinetics, our CGC and Non-local are somewhat complementary to each other since applying both of them to the baseline achieves the highest performance. This is consistent with the observation of the combination of CBAM and our CGC in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Machine Translation</head><p>Baseline Methods. The LightConv proposed by <ref type="bibr" target="#b52">[53]</ref> achieves better performance with a lightweight convolutional model, compared to Transformer <ref type="bibr" target="#b47">[48]</ref>. We take it as the baseline model and augment its Lightweight Convolution with our CGC. Note that the Lightweight Convolution is a grouped convolution L ? R H?k with weight sharing, so we remove the Channel Interacting Module since we do not need it to project latent representations. We resize the input sequence S ? R c?L to R H?3k with average pooling. For those sequences shorter than 3k, we pad them with zeros. Since the decoder decodes translated words one by one at the inference time, it is unclear how to define global context for it. Therefore, we only replace the convolutions in the encoder.</p><p>Experimental Setting. We follow <ref type="bibr" target="#b52">[53]</ref> to train all the compared models with 160K sentence pairs and 10K joint BPE vocabulary. We use the training protocol of DynamicConv <ref type="bibr" target="#b52">[53]</ref> provided in <ref type="bibr" target="#b36">[37]</ref>. The widely-used BLEU-4 <ref type="bibr" target="#b37">[38]</ref> is reported for evaluation of all the models. We find that it is necessary to set beam width to 6 to reproduce the results of DynamicConv reported in <ref type="bibr" target="#b52">[53]</ref>, and we fix it to be 6 for all the models.</p><p>Performance Comparisons. As <ref type="table" target="#tab_7">Table 6</ref> shows, replacing Lightweight Convolutions in the encoder of LightConv with our CGC significantly outperforms LightConv and LightConv + Dynamic Encoder by 0.37 and 0.18 BLEU, respectively, yielding the state-of-the-art performance. As was discussed previously, Dynamic Convolution leverages a linear layer to generate the convolution kernel according to the input segment, which lacks the awareness of global context. This flaw may lead to sub-optimal encoding of the source sentence and thus the unsatisfying decoded sentence. However, our CGC incorporates global context of the source sentence and helps significantly improve the quality of the translated sentence. Moreover, our CGC is much more efficient than Dynamic Convolution because of our module design. Our CGC only needs 0.01M extra parameters, but Dynamic Convolution needs 30? more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, motivated by the neuroscience research on neurons as "adaptive processors", we proposed a lightweight Context-Gated Convolution (CGC) to incorporate global context information into CNNs. Different from previous works which usually modify input feature maps, our proposed CGC directly modulates convolution kernels under the guidance of global context information. In specific, we proposed three modules to efficiently generate a gate to modify the kernel. As such, our CGC is able to extract representative local patterns according to global context. The extensive experimental results show consistent performance improvements on various tasks with a negligible computational complexity and parameter increment. In the future, our proposed CGC can be incorporated into the searching space of Neural Architecture Search (NAS) to further improve the performance of NAS models. For the advanced setting, we also use mixup <ref type="bibr" target="#b55">[56]</ref> for data augmentation, and we follow <ref type="bibr" target="#b22">[23]</ref> to use learning rate warm-up in the first 5 epochs of training. We train the networks with the cosine learning rate schedule <ref type="bibr" target="#b22">[23]</ref> for 120 epochs. The other hyper-parameters are set to be same with the default setting.</p><p>For CIFAR-10, we use 32 ? 32 random cropping with a padding of 4 and random horizontal flipping. We use a batch size of 128 and train on 1 GPU. We decrease the learning rate at the 81st and 122nd epochs, and halt training after 164 epochs. For the ablation study, the result is an average of 3 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details about P3D-A</head><p>Based on ResNet-50, we add a temporal convolution with k = 5, stride = 2 after the first convolutional layer. For convolutional layers in residual blocks, we follow <ref type="bibr" target="#b50">[51]</ref> to add 3 ? 1 ? 1 convolution (stride is 1) after every two 1 ? 3 ? 3 convolutions. The added temporal convolutional layers are initialized to imitate the behavior of TSM <ref type="bibr" target="#b33">[34]</ref> to ease the training process. We only inflate the max pooling layer after the first convolutional layer with a temporal kernel size of 3 and a stride of 2 without adding any other temporal pooling layers. Note that all the aforementioned convolutional layers come with a Batch Normalization layer and a ReLU activation function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was done when Xudong Lin interned at Tencent AI Lab. arXiv:1910.05577v4 [cs.CV] 17 Jul 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Traditional convolution only composes local information. (b) Global feature interaction methods modify input feature maps by incorporating global information. (c) Our proposed CGC, in a fundamentally different manner, modulates convolution kernels under the guidance of global context. denotes convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>g is the number of groups. The weight I is shared among different dimensions of d and different groups. A normalization layer and an activation function come after the linear layer. The final output of the Channel Interacting Module is O ? R o?d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>The training curves of ResNet-50 and ResNet-50 + CGC (ours) on Ima-geNet under the default training setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of the feature maps produced by ResNet-50 and CGC-ResNet-50 from the ImageNet validation set images. (Best viewed on a monitor when zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of the difference matrix between inter-class distances and intra-class distances of the last gate in the network on the ImageNet validation set. (Best viewed on a monitor when zoomed in) start from constants. All the extra layers in Context-Gated Convolution have a learning rate ten times smaller than convolutional kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the Channel Interacting Module transforms C to O with output dimension o; the Gate Decoding Module produces G</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pooling Gating ? ? ? Context</cell><cell>? ?? ? ?</cell><cell cols="2">Context Encoding</cell><cell>?</cell><cell cols="2">Channel Interacting</cell><cell>?</cell><cell></cell><cell>Gate Decoding</cell><cell></cell><cell>? ? 1 ? 2</cell><cell>?</cell></row><row><cell></cell><cell>? ?? ?</cell><cell>Linear</cell><cell>Normalization</cell><cell>Activation</cell><cell>?</cell><cell>Grouped Linear</cell><cell>Normalization</cell><cell>Activation</cell><cell>? ?</cell><cell>Linear</cell><cell>? 1 ? 2 ( ) ? 1 ? 2 ( )</cell></row><row><cell cols="12">Fig. 2: Our proposed CGC consists of three components, namely the Context En-</cell></row><row><cell cols="12">coding Module, the Channel Interacting Module, and the Gate Decoding Mod-</cell></row><row><cell cols="12">ule. The Context Encoding Module encodes global context information into a</cell></row><row><cell cols="2">latent representation C;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Image classification results on ImageNet and CIFAR-10. Param indicates the number of parameters in the model. ?MFLOPs is the increment of the number of multiplication-addition operations compared to ResNet-50 (R50, 4 GFLOPs) for ImageNet models and ResNet-110 (R110, 256 MFLOPs) for CIFAR-10 models. Bold indicates the best result.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Training Setting Model</cell><cell>Param</cell><cell cols="3">?MFLOPs Top-1(%) Top-5(%)</cell></row><row><cell></cell><cell></cell><cell>R50 + GloRe [8]</cell><cell>30.5M</cell><cell>1200</cell><cell>78.4</cell><cell>-</cell></row><row><cell></cell><cell>-</cell><cell>DCNv2-R50 [60]</cell><cell>27.4M</cell><cell>200</cell><cell>78.2</cell><cell>94.0</cell></row><row><cell></cell><cell></cell><cell>GC-R50 [4]</cell><cell cols="2">28.08M 100</cell><cell>77.70</cell><cell>93.66</cell></row><row><cell></cell><cell></cell><cell>SE-R50 [25]</cell><cell cols="2">28.09M 8</cell><cell>77.18</cell><cell>93.67</cell></row><row><cell></cell><cell></cell><cell>BAM-R50 [39]</cell><cell cols="2">25.92M 83</cell><cell>76.90</cell><cell>93.40</cell></row><row><cell></cell><cell></cell><cell>GC-R50 [4]</cell><cell cols="2">28.11M 8</cell><cell>73.90</cell><cell>91.70</cell></row><row><cell></cell><cell></cell><cell>DCNv2-R50 [60]</cell><cell>27.4M</cell><cell>200</cell><cell>77.21</cell><cell>93.69</cell></row><row><cell></cell><cell>Default</cell><cell>SK-R50 [33]</cell><cell cols="2">37.25M 1837</cell><cell>77.15</cell><cell>93.54</cell></row><row><cell>ImageNet</cell><cell></cell><cell>R50 [21] R50 + CGC(Ours)</cell><cell cols="2">25.56M -25.59M 6</cell><cell>76.16 77.48</cell><cell>92.91 93.81</cell></row><row><cell></cell><cell></cell><cell>CBAM-R50 [52]</cell><cell cols="2">28.09M 15</cell><cell>77.34</cell><cell>93.69</cell></row><row><cell></cell><cell></cell><cell cols="3">CBAM-R50 + CGC(Ours) 28.12M 21</cell><cell>77.68</cell><cell>93.68</cell></row><row><cell></cell><cell></cell><cell>DCNv2-R50 [60]</cell><cell>27.4M</cell><cell>200</cell><cell>78.89</cell><cell>94.60</cell></row><row><cell></cell><cell></cell><cell>SE-R50 [25]</cell><cell cols="2">28.09M 8</cell><cell>78.79</cell><cell>94.52</cell></row><row><cell></cell><cell>Advanced</cell><cell>R0 [21] R50 + CGC(Ours)</cell><cell cols="2">25.56M -25.59M 6</cell><cell>78.13 79.54</cell><cell>94.06 94.78</cell></row><row><cell></cell><cell></cell><cell>CBAM-R50 [52]</cell><cell cols="2">28.09M 15</cell><cell>78.86</cell><cell>94.58</cell></row><row><cell></cell><cell></cell><cell cols="3">CBAM-R50 + CGC(Ours) 28.12M 21</cell><cell>79.74</cell><cell>94.83</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell>R110 [22] R110 + CGC(Ours)</cell><cell cols="2">1.73M 1.80M 2 -</cell><cell>93.96 94.86</cell><cell>99.73 99.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Image classification results on ObjectNet. Bold indicates the best result.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1(%) Top-5(%)</cell></row><row><cell>R50 [21]</cell><cell>29.35</cell><cell>48.42</cell></row><row><cell>SE-R50 [25]</cell><cell>29.48</cell><cell>45.55</cell></row><row><cell>DCNv2-R50 [60]</cell><cell>29.74</cell><cell>48.83</cell></row><row><cell>CBAM-R50 [52]</cell><cell>29.56</cell><cell>48.68</cell></row><row><cell cols="2">R50 + CGC(Ours) 31.53</cell><cell>50.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Action recognition results on Something-Something (v1). Backbone indicates the backbone network architecture. Param indicates the number of parameters in the model. Frame indicates the number of frames used for evaluation. Bold indicates the best result.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="4">Param Frame Top-1(%) Top-5(%)</cell></row><row><cell>TRN [58]</cell><cell cols="3">BNInception 18.3M 8</cell><cell>34.4</cell><cell>-</cell></row><row><cell>TRN [34]</cell><cell>ResNet-50</cell><cell cols="2">31.8M 8</cell><cell>38.9</cell><cell>68.1</cell></row><row><cell>ECO [61]</cell><cell cols="3">BNInc+Res18 47.5M 8</cell><cell>39.6</cell><cell>-</cell></row><row><cell>ECO [61]</cell><cell cols="3">BNInc+Res18 47.5M 16</cell><cell>41.4</cell><cell>-</cell></row><row><cell>ECO En Lite [61]</cell><cell cols="2">BNInc+Res18 150M</cell><cell>92</cell><cell>46.4</cell><cell>-</cell></row><row><cell>TSN [50]</cell><cell>ResNet-50</cell><cell cols="2">23.86M 8</cell><cell>19.00</cell><cell>44.98</cell></row><row><cell>TSN + Non-local [51]</cell><cell>ResNet-50</cell><cell cols="2">31.22M 8</cell><cell>25.73</cell><cell>55.17</cell></row><row><cell>TSN + CGC (Ours)</cell><cell>ResNet-50</cell><cell cols="2">24.07M 8</cell><cell>32.58</cell><cell>60.06</cell></row><row><cell>P3D [43]</cell><cell>ResNet-50</cell><cell cols="3">25.38M 32 ? 30 45.17</cell><cell>74.61</cell></row><row><cell>lslsP3D + Non-local [51]</cell><cell>ResNet-50</cell><cell cols="3">32.73M 32 ? 30 45.88</cell><cell>74.94</cell></row><row><cell>P3D + CGC 1D (Ours)</cell><cell>ResNet-50</cell><cell cols="3">25.39M 32 ? 30 46.14</cell><cell>75.92</cell></row><row><cell>P3D + CGC 3D (Ours)</cell><cell>ResNet-50</cell><cell cols="3">25.61M 32 ? 30 46.35</cell><cell>75.97</cell></row><row><cell cols="2">P3D + CGC 1D &amp; 3D (Ours) ResNet-50</cell><cell cols="3">25.62M 32 ? 30 46.73</cell><cell>76.04</cell></row><row><cell>TSM [34]</cell><cell>ResNet-50</cell><cell cols="2">23.86M 8</cell><cell>44.65</cell><cell>73.94</cell></row><row><cell>TSM + Non-local [51]</cell><cell>ResNet-50</cell><cell cols="2">31.22M 8</cell><cell>43.91</cell><cell>72.18</cell></row><row><cell>TSM + CGC (Ours)</cell><cell>ResNet-50</cell><cell cols="2">24.07M 8</cell><cell>46.00</cell><cell>75.11</cell></row><row><cell>TSM [34]</cell><cell>ResNet-50</cell><cell cols="2">23.86M 16</cell><cell>46.61</cell><cell>76.18</cell></row><row><cell>TSM + CGC (Ours)</cell><cell>ResNet-50</cell><cell cols="2">24.09M 16</cell><cell>47.87</cell><cell>77.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Action recognition results on Kinetics. Backbone indicates the backbone network architecture. Param indicates the number of parameters in the model. Bold indicates the best result.</figDesc><table><row><cell>Model</cell><cell cols="2">Backbone Param Top-1(%) Top-5(%)</cell></row><row><cell>TSM [34]</cell><cell>ResNet-50 23.86M 74.12</cell><cell>91.21</cell></row><row><cell>TSM + Non-local [51]</cell><cell>ResNet-50 31.22M 75.60</cell><cell>92.15</cell></row><row><cell>TSM + CGC (Ours)</cell><cell>ResNet-50 24.07M 76.06</cell><cell>92.50</cell></row><row><cell cols="2">TSM + Non-local + CGC (Ours) ResNet-50 31.43M 76.40</cell><cell>92.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Machine translation results on IWSLT'14 De-En. Param indicates the number of parameters in the model. Bold indicates the best result.</figDesc><table><row><cell>Model</cell><cell cols="2">Param BLEU-4</cell></row><row><cell>Deng et al. [13]</cell><cell>-</cell><cell>33.08</cell></row><row><cell>Transformer [48]</cell><cell cols="2">39.47M 34.41</cell></row><row><cell>LightConv [53]</cell><cell cols="2">38.14M 34.84</cell></row><row><cell>LightConv + Dynamic Encoder [53]</cell><cell cols="2">38.44M 35.03</cell></row><row><cell cols="3">LightConv + CGC Encoder (Ours) 38.15M 35.21</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Analysis of the Gate</head><p>To further validate that our CGC uses context information of the target objects to guide the convolution process, we calculate the average modulated kernel (in the last CGC of the model) for images of each class in the validation set. Then we calculate inter-class L?2 distance between every two average modulated kernels, i.e., class centers, and the intra-class L ? 2 distance (mean distance to the class center) for each class. As is shown in <ref type="figure">Fig. 5</ref>, we visualize the difference matrix between inter-class distances and intra-class distances. In more than 93.99% of the cases, the inter-class distance is larger than the corresponding intra-class distance, which indicates that there are clear clusters of these modulated kernels and the clusters are aligned very well with the classes.</p><p>This observation strongly supports that our CGC successfully extracts classspecific context information and effectively modulates the convolution kernel to extract representative features. Meanwhile, the intra-class variance of the modulated kernels indicates that our CGC dynamically modulates convolution kernels according to different input contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of training settings on ImageNet and CIFAR-10</head><p>For the default setting on ImageNet, we use 224 ? 224 random resized cropping and random horizontal flipping for data augmentation. Then we standardize the data with mean and variance per channel. We use a traditional cross-entropy loss to train all the networks with a batch size of 256 on 8 GPUs by SGD, a weight decay of 0.0001, and a momentum of 0.9 for 100 epochs. We start from a learning rate of 0.1 and decrease it by a factor of 10 every 30 epochs. The last normalization layers in the module are zero-initialized to make the gates</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9448" to="9458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09925</idno>
		<title level="m">Attention augmented convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<title level="m">Gcnet: Non-local networks meet squeezeexcitation networks and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<title level="m">Report on the 11th iwslt evaluation campaign</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>iwslt</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphbased global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual skipping networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4071" to="4079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent alignment and variational attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9712" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Top-down influences on visual processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">350</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>Teh, Y.W., Titterington, M.</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. Machine Learning Research<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, Chia Laguna Resort</publisher>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual learning and top-down influences in primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pi?ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="651" to="657" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06514</idno>
		<title level="m">Bottleneck attention module</title>
		<meeting><address><addrLine>Bam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<title level="m">Weight standardization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3545" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.0797110</idno>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<title level="m">Pay less attention with lightweight and dynamic convolutions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with alternately updated clique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2413" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feedback networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
