<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristians</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Seeliger</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Mascagni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Fondazione Policlinico Universitario Agostino Gemelli IRCCS</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Mutter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Marescaux</surname></persName>
							<affiliation key="aff4">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical workflow analysis</term>
					<term>Tool-tissue interaction</term>
					<term>CholecT50</term>
					<term>Attention</term>
					<term>Transformer</term>
					<term>Laparoscopic surgery</term>
					<term>Surgical Action Recognition</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Out of all existing frameworks for surgical workflow analysis in endoscopic videos, action triplet recognition stands out as the only one aiming to provide truly fine-grained and comprehensive information on surgical activities. This information, presented as instrument, verb, target combinations, is highly challenging to be accurately identified. Triplet components can be difficult to recognize individually; in this task, it requires not only performing recognition simultaneously for all three triplet components, but also correctly establishing the data association between them. To achieve this task, we introduce our new model, the Rendezvous (RDV), which recognizes triplets directly from surgical videos by leveraging attention at two different levels. We first introduce a new form of spatial attention to capture individual action triplet components in a scene; called Class Activation Guided Attention Mechanism (CAGAM). This technique focuses on the recognition of verbs and targets using activations resulting from instruments. To solve the association problem, our RDV model adds a new form of semantic attention inspired by Transformer networks; called Multi-Head of Mixed Attention (MHMA). This technique uses several cross and self attentions to effectively capture relationships between instruments, verbs, and targets. We also introduce CholecT50 -a dataset of 50 endoscopic videos in which every frame has been annotated with labels from 100 triplet classes. Our proposed RDV model significantly improves the triplet prediction mean AP by over 9% compared to the state-of-the-art methods on this dataset. <ref type="figure">(</ref>Chinedu Innocent Nwoye), npadoy@unistra.fr (Nicolas Padoy) &gt;&gt; hook dissect gallbladder &gt;&gt; grasper retract gallbladder &gt;&gt; clipper clip cystic-duct &gt;&gt; bipolar coagulate cystic-artery &gt;&gt; grasper retract gallbladder &gt;&gt; scissors cut cystic-duct &gt;&gt; grasper retract gallbladder &gt;&gt; irrigator aspirate fluid &gt;&gt; grasper retract gallbladder &gt;&gt; grasper retract gallbladder &gt;&gt; grasper grasp gallbladder &gt;&gt; grasper grasp specimen-bag arXiv:2109.03223v2 [cs.CV]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Laparoscopic cholecystectomy, as one of the most commonly performed surgical procedures in the world <ref type="bibr" target="#b64">(Shaffer, 2006;</ref><ref type="bibr" target="#b49">Majumder et al., 2020)</ref>, has become the gold standard approach <ref type="bibr" target="#b59">(Pucher et al., 2018)</ref> over its open surgery counterpart. As a minimally invasive procedure, it significantly alleviates some of the preoperative, intraoperative, and postoperative burden: the patient generally experiences decreased odds of nosocomial infection, less pain, less bleeding, and faster recovery times <ref type="bibr" target="#b74">(Velanovich, 2000</ref>). Yet, this success comes at a price for the surgeon, who now has to deal with increased technical difficulty coming from the indirect vision and laparoscopic instruments <ref type="bibr" target="#b4">(Ballantyne, 2002)</ref>, especially during complex cases <ref type="bibr" target="#b19">(Felli et al., 2019)</ref>. The elevated complexity of laparoscopy is one of the motivations driving the development of context-aware support systems for surgery <ref type="bibr" target="#b48">(Maier-Hein et al., 2017)</ref>; i.e. systems capable of assisting surgeons, for example via automated warnings <ref type="bibr" target="#b75">(Vercauteren et al., 2019)</ref>, based on their dynamic perception and understanding of the surgical scene and workflow. Developing this understanding is the focus of surgical workflow analysis methods: given a scene from surgery, what is happening in it? The finer-grained the answer becomes, the more value it gains in terms of clinical utility: for instance according to <ref type="bibr" target="#b52">Mascagni et al. (2021)</ref>, an automated surgical safety system would benefit from the ability to identify individual actions such as a clipper applying a clip to the cysticartery or other blood vessels.</p><p>Methods from the literature have so far only given incomplete answers, with our previous work as the only exception <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>. The main task studied by the community, surgical phase recognition <ref type="bibr" target="#b0">(Ahmadi et al., 2006;</ref><ref type="bibr" target="#b45">Lo et al., 2003a)</ref>, only describes scenes at a very coarse level. As an example the clipping and cutting phase <ref type="bibr" target="#b69">(Twinanda et al., 2017)</ref> in cholecystectomy contains a multitude of important actions: graspers holding anatomical landmarks, a clipper applying several clips, laparoscopic scissors cutting the cysticduct and so on. The phase information on its own does not, by any means, provide an accurate picture of the activities taking place. Even finer-grained workflow divisions such as steps <ref type="bibr" target="#b60">(Ramesh et al., 2021)</ref> are composed of multiple individual actions. Limited attempts were made in other works <ref type="bibr" target="#b34">(Khatibi and Dezyani, 2020;</ref><ref type="bibr" target="#b61">Rupprecht et al., 2016)</ref> as well as in MIC-CAI 2019's Endoscopic Vision challenge 1 to capture those actions focusing on key verbs such as dissect, cut, or coagulate. This type of framework, however, overlooks interactions with the anatomy.</p><p>To obtain a comprehensive account of a surgical scene, simultaneous recognition of the instruments, verbs, target anatomy and the relationships between them needs to be achieved. This goes beyond the conventional action recognition used in the EndoVis 2019 sub-challenge to a deeper understanding of visual semantics that depicts the complex relationships between instruments and tissues. <ref type="bibr" target="#b32">Kati? et al. (2014)</ref> proposed the surgical action triplet as the most detailed and expressive formalism for surgical workflow analysis. However, while <ref type="bibr" target="#b32">Kati? et al. (2014</ref><ref type="bibr" target="#b31">Kati? et al. ( , 2015</ref> leveraged triplet formulation provided by manual annotation to better recognize surgical phases, no attempt outside of our previous work <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> has been made to directly recognize those triplets from surgical images or videos.</p><p>Nonetheless, the difficulty of this recognition task, for all its utility, is not to be overlooked. First, action triplets are instrument-centric: meaning that visibility alone does not determine the consideration of an anatomy as a part of a triplet, but also their involvement in an interaction carried out by an instrument. For instance, the liver, which is visible most of the time in laparoscopic cholecystectomy, is labeled a target only when being acted upon by an instrument. Similarly, a verb is defined by the instrument's action, and so, without an instrument, there cannot be a verb. Furthermore, the level of spatial reasoning involved is highly challenging: given an instrument, its role (verb) with respect to a given target can imperceptibly change: grasper, retract, gallbladder , grasper, grasp, gallbladder , grasper, dissect, gallbladder are tough 1 https://endovissub-workflowandskill.grand-challenge.org/ to distinguish even for experienced surgeons and require careful observation of the area surrounding the tooltip. The applications of the surgical instruments vary according to the surgeon's intention for use. Multiplicity and semantic reasoning are the other major challenges. Overlaps are found between different instruments used for the same action (or verb), e.g. dissection performed by bipolar, grasper, hook, irrigator, and scissors (see <ref type="table" target="#tab_1">Table 2</ref>). Similarly, when operating on an organ or structure, multiple instruments can interact with the target. Thus, a target can be simultaneously involved in multiple distinct actions, e.g. grasper, retract, cystic-duct and hook, dissect, cystic-duct happening at the same time. Since multiple triplets can occur in one frame, associating the matching components of the triplets is akin to solving a complex tripartite data association problem between the entities. The model from our previous work, as a first attempt to tackle the triplet recognition problem, addresses those challenges in a limited manner, without explicit spatial focus or an advanced enough model of the instrument -verb -target relationships.</p><p>In this paper, we extend <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref>, our conference paper published in MICCAI 2020. Our extension is both in data and in methods. On the data contribution, we introduce the CholecT50 dataset, which is a quantitative and qualitative expansion of the CholecT40 dataset <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>. The dataset consists of 50 videos of cholecystectomy annotated with 161K instances from 100 triplet classes. Examples of such action triplets include: grasper, retract, gallbladder , hook, dissect, omentum , clipper, clip, cystic-artery , scissors, cut, cystic-duct , irrigator, aspirate, fluid , etc, as also shown in <ref type="figure" target="#fig_0">Fig 1.</ref> Method-wise, we develop a new recognition model called Rendezvous (RDV), which is a transformer-inspired neural network for surgical action triplet recognition. RDV improves on the existing Tripnet model proposed in <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref> by leveraging the attention mechanism to detect the various components of the triplets and learn their association. Our first effort at exploiting attention mechanisms in this task led to the development of a Class Activation Guided Attention Mechanism (CAGAM) to better detect the verb and target components of the triplet, which are instrument-centric. The CAGAM is achieved by redesigning the saliency-guided attention mechanism in <ref type="bibr" target="#b30">(Ji et al., 2019;</ref><ref type="bibr" target="#b81">Yao and Gong, 2020)</ref> to utilize a more adequate and easier to learn class activation map (CAM). While our approach is similar in the attention guiding principle, it differs in three respects: (a) our attention network is guided by the instrument's activations which are learnable in the same network, using a global pooling layer without relying on a third-party saliency generation network, (b) our attention guide implements a combination of position and channel attention for the target and verb detection tasks respectively, (c) we employ cross-attention from the instrument domain to the other task domains (i.e.: verb and target) as opposed to self-attention in <ref type="bibr" target="#b81">Yao and Gong (2020)</ref>. Meanwhile, the CAGAM is an improvement on the class activation guide (CAG) module introduced in <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref> which is simply a concatenation of the model's intermediary features with the instrument's activation features. As an ablation ex-periment, we show the improved performance of our previous Tripnet model, only upgraded with the CAGAM. This upgraded model is called Attention Tripnet.</p><p>The proposed RDV uses the CAGAM unit as part of its encoder and a multiple heads of mixed attention (MHMA) decoder to learn the action triplets. The MHMA in RDV is inspired by the Transformer model <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> used in Natural Language Processing (NLP), particularly, its multihead attention mechanism. Unlike in the NLP Transformer, which implements a multi-head of self-attention, we design a novel multi-head attention module that is a mixture of selfand cross-attention suitable for the action triplet recognition task. As opposed to Transformers in NLP which use multihead attention temporally (i.e. focusing on a sequence of words in a sentence), and the Vision Transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref> which uses it spatially (by forming its sequence from different patches of an image), our RDV model takes a different approach by employing it semantically, with redesigned multi-head modeling attention across the discriminating features of various components that are interacting to form action triplets. With this method, we outperform the state-of-the-art models significantly on triplet recognition. We plan to release our source code along with the evaluation script on our public github 2,3,4 upon acceptance of this paper. The dataset will also become public on the CAMMA team's website 5 .</p><p>In summary, the contributions of this work are as follows:</p><p>1. We present a comprehensive study on surgical action triplet recognition directly from videos. 2. We propose a Class Activation Guided Attention Mechanism (CAGAM) for detecting the target and verb components of the triplets conditioned on the instrument's appearance cue. 3. We propose a Multi-Head of Mixed Attention (MHMA) by modeling self-and cross-attention on semantic sequences of class-wise representations to learn the interaction between the instrument, verb, and target in a surgical scene. 4. We develop Rendezvous (RDV): a transformer-inspired neural network model that utilizes CAGAM and MHMA for surgical triplet recognition in laparoscopic videos. 5. We present a large endoscopic action triplet dataset, CholecT50, for this task. 6. We analyze the surgical relevance of our methods and results, setting the stage for clinical translation and future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Surgical Workflow Analysis</head><p>The paradigm shift brought by Artificial Intelligence (AI) across several fields has seen the application of deep learn-ing techniques for the recognition of surgical workflow activities to provide assisted interventions in the operating room (OR). However, compared to other fields such as natural Vision, NLP, Commerce, etc., there has been a delay in introducing large-scale data science to interventional medicine. This is partly due to the unavailability of large annotated dataset <ref type="bibr" target="#b48">(Maier-Hein et al., 2017)</ref> and the particular need for precision in medicine. Some research focuses on detecting elements such as instruments/tools used during surgery <ref type="bibr" target="#b1">(Al Hajj et al., 2018;</ref><ref type="bibr" target="#b23">Garcia-Peraza-Herrera et al., 2017;</ref><ref type="bibr" target="#b56">Nwoye et al., 2019;</ref><ref type="bibr" target="#b67">Sznitman et al., 2014;</ref><ref type="bibr" target="#b72">Vardazaryan et al., 2018;</ref><ref type="bibr" target="#b76">Voros et al., 2007)</ref>, while others model the sequential workflow by recognizing surgical phases either from endoscopic videos <ref type="bibr" target="#b7">(Blum et al., 2010;</ref><ref type="bibr" target="#b15">Dergachyova et al., 2016;</ref><ref type="bibr" target="#b21">Funke et al., 2018;</ref><ref type="bibr" target="#b45">Lo et al., 2003a;</ref><ref type="bibr" target="#b69">Twinanda et al., 2017;</ref><ref type="bibr" target="#b82">Yu et al., 2018;</ref><ref type="bibr" target="#b84">Zisimopoulos et al., 2018)</ref> or from ceiling-mounted cameras <ref type="bibr" target="#b9">(Chakraborty et al., 2013;</ref><ref type="bibr" target="#b68">Twinanda et al., 2015)</ref>. Some works go deeper in the level of granularity, recognizing the steps within each surgical phase <ref type="bibr" target="#b12">(Charriere et al., 2014;</ref><ref type="bibr" target="#b41">Lecuyer et al., 2020;</ref><ref type="bibr" target="#b60">Ramesh et al., 2021)</ref>, while others learn phase transitions <ref type="bibr" target="#b62">(Sahu et al., 2020)</ref>. Another work <ref type="bibr" target="#b46">(Lo et al., 2003b)</ref> investigated the four major events in minimally invasive surgery (MIS) and categorized these events into their main actions; namely, idle, retraction, cauterization, and suturing. From the perspective of robotic surgery, similar research focused more on gesture recognition from kinematic data <ref type="bibr" target="#b17">(DiPietro et al., 2016</ref><ref type="bibr" target="#b16">(DiPietro et al., , 2019</ref>, and robotized surgeries <ref type="bibr" target="#b36">(Kitaguchi et al., 2019;</ref><ref type="bibr" target="#b83">Zia et al., 2018)</ref>, system events , and the recognition of other events, such as the presence of smoke or bleeding <ref type="bibr" target="#b47">(Loukas and Georgiou, 2015)</ref>. These surgical events are explored for the recognition of surgeon's deviation from standard processes in laparoscopic videos <ref type="bibr" target="#b29">(Huaulm? et al., 2020)</ref>.</p><p>Aside the coarse-grained activities, some works <ref type="bibr" target="#b34">(Khatibi and Dezyani, 2020;</ref><ref type="bibr" target="#b61">Rupprecht et al., 2016)</ref> explored finegrained action in laparoscopic videos, however, the recognition task is limited to verb classification. Within the EndoVis challenge at MICCAI 2019, <ref type="bibr" target="#b77">Wagner et al. (2021)</ref> introduced a similar action recognition task for only four prevalent verbs in surgery (cut, grasp, hold, and clip), however, this does not consider the target anatomy or the instrument performing the action. In the SARAS-ESAD challenge organized within MIDL 2020, the proposed action labels encompass 21 classes <ref type="bibr" target="#b5">(Bawa et al., 2021)</ref>. The EASD challenge dataset is an effort to capture more details in surgical action recognition. While this dataset provides spatial labels for action detection, just like some human-object interaction (HOI) datasets, it formalizes action labels as verb-anatomy relationship such as clippingTissue, pullingTissue, cuttingTissue, etc., and thus, do not take into account the instrument performing the actions. Although humans are not categorized in the general vision HOI problem, it is imperative to recognize the surgical instruments by their categories as they play semantically different roles; their categories are informative in distinguishing the surgical phases. Recognizing surgical actions as single verbs is also being explored in other closely related procedures such as gynecologic laparoscopy <ref type="bibr" target="#b34">(Khatibi and Dezyani, 2020;</ref><ref type="bibr" target="#b37">Kletz et al., 2017;</ref><ref type="bibr" target="#b58">Petscharnig et al., 2018)</ref>.</p><p>For a more detailed workflow analysis, <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref> proposed to recognize surgical actions at a fine-grained level directly from laparoscopic cholecystectomy videos, modeling them as triplets of the used instrument, its role (verb), and its underlying target anatomy. Such fine-grained activity recognition gives a detailed understanding of the image contents in laparoscopic videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Surgical Action Triplet Recognition</head><p>In the existing surgical ontology, an action is described as a triplet of the used instrument, a verb representing the action performed, and the anatomy acted upon <ref type="bibr" target="#b54">(Neumuth et al., 2006;</ref><ref type="bibr" target="#b32">Kati? et al., 2014)</ref>. Earlier works such as <ref type="bibr" target="#b32">Kati? et al. (2014</ref><ref type="bibr" target="#b31">Kati? et al. ( , 2015</ref> used triplet annotation information to improve surgical phase recognition. Recently, <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref> introduced CholecT40, an endoscopic video dataset annotated with action triplets. Tripnet <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> is the first deep learning model designed to recognize action triplets directly from surgical videos. The model relies on a class activation guide (CAG) module to detect the verb and target in triplets, leveraging instrument appearance cues. It models the final triplet association by projecting the detected components to a 3D interaction space (3Dis) to learn their association while maintaining a triplet structure. In this paper, we improve on the verb and target detections using an attention mechanism. The triplet dataset <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> is also expanded and refined.</p><p>With fine-grained action recognition now gaining momentum, a recent work in robotic surgery <ref type="bibr" target="#b80">(Xu et al., 2021)</ref> extended two robotic surgery datasets, MICCAI's robotic scene segmentation challenge <ref type="bibr" target="#b2">(Allan et al., 2020)</ref> and Transoral Robotic Surgery (TORS), with 11 and 5 semantic relationship labels respectively. They in turn proposed a cross-domain method for the two datasets generating surgical captions that are comparable to action triplets.</p><p>Detecting multi-object interaction in natural images/videos is widely explored by the research on human-object interaction (HOI) <ref type="bibr" target="#b27">(Hu et al., 2013;</ref><ref type="bibr" target="#b50">Mallya and Lazebnik, 2016)</ref> where activities are formulated as triplets of human, verb, object <ref type="bibr" target="#b11">(Chao et al., 2015)</ref>. Detecting or recognizing HOI is enabled by triplet datasets with spatial annotations (e.g. HICO-DET <ref type="bibr" target="#b10">(Chao et al., 2018)</ref>, VCOCO <ref type="bibr" target="#b42">(Lin et al., 2014)</ref>) or simply binary presence labels (e.g. HICO <ref type="bibr" target="#b11">(Chao et al., 2015)</ref>). CNN models with simple <ref type="bibr" target="#b50">(Mallya and Lazebnik, 2016)</ref> or multistream architectures <ref type="bibr" target="#b10">(Chao et al., 2018)</ref> are widely used to model human and object detections as well as resolving spatial relationships between them. Considering the often large number of possible combinations, <ref type="bibr" target="#b65">Shen et al. (2018)</ref> proposed a zero-shot method to predict unseen verb-object pairs at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attention Mechanism</head><p>Since the advent of the attention mechanism <ref type="bibr" target="#b3">(Bahdanau et al., 2014)</ref>, many deep learning models have exploited it in various forms: from self <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> to cross <ref type="bibr" target="#b53">(Mohla et al., 2020)</ref>, and from spatial <ref type="bibr" target="#b20">(Fu et al., 2019)</ref> to temporal <ref type="bibr" target="#b63">(Sankaran et al., 2016)</ref>. Methods relying on attention mechanisms <ref type="bibr" target="#b38">Kolesnikov et al., 2019)</ref> are proposed to focus the HOI detection networks only on crucial human and object context features. An action-guided attention mining loss <ref type="bibr">(Lin et al.)</ref> has also been used in HOI recognition tasks; however, all these attention models rely on expensive spatial annotations.</p><p>Recently, <ref type="bibr" target="#b30">Ji et al. (2019)</ref> proposed a form of attention that rely on saliency features without requiring additional supervision. While <ref type="bibr" target="#b30">Ji et al. (2019)</ref> used a combination of spatial and textual attention modules to capture fine-grained imagesentence correlations, another work by <ref type="bibr" target="#b81">Yao and Gong (2020)</ref> utilized image saliency to guide an attention network for weakly supervised object segmentation. In medical imaging, Attention U-Net <ref type="bibr" target="#b57">(Oktay et al., 2018</ref>) is used to focus on target structures for pancreas segmentation.</p><p>Action triplets are instrument-centric: the instrument is the verb's subject, and a visible anatomical part is only considered a target if an instrument operates on it; therefore learning the verb and target are conditioned on the instrument's presence and position. <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref> addressed this with an activation guide layer named CAG, where the verb and target features are each attuned to instrument activation maps. Even for HOI detection, which is human-centric, human appearance cues are leveraged to predict action-specific densities over target object locations, albeit fully supervised on human bounding boxes <ref type="bibr" target="#b26">(Gkioxari et al., 2018)</ref>. <ref type="bibr" target="#b70">Ulutan et al. (2020)</ref> opined that attention modeling is superior to feature concatenation in terms of spatial reasoning. We improve on the CAG principle with a class activation-guided attention mechanism (CAGAM) achieved by redesigning the saliency-guided attention mechanism in <ref type="bibr" target="#b30">(Ji et al., 2019;</ref><ref type="bibr" target="#b81">Yao and Gong, 2020)</ref> to utilize a more adequate and easier to learn class activation map (CAM). Our implementation combines both channel and position attention mechanisms for the verb and target detections respectively.</p><p>The Transformer model <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> introduced in NLP shows that attention can be expanded to capture longrange dependencies without recurrence. The Vision Transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref> explored this technique for image understanding with encouraging performance. Another Transformer with end-to-end self-attention <ref type="bibr" target="#b85">(Zou et al., 2021;</ref><ref type="bibr" target="#b35">Kim et al., 2021)</ref> modeled long-range attentions for both HOI components detection and their interaction association. In surgical data science, transformers have been explored for surgical instrument classification <ref type="bibr" target="#b39">(Kondo, 2020)</ref> and recently for phase recognition <ref type="bibr" target="#b22">(Gao et al., 2021;</ref><ref type="bibr" target="#b14">Czempiel et al., 2021)</ref>. Similarly, we propose Rendezvous (RDV), a transformer-inspired method, for online surgical action triplet recognition. The novelty of RDV is found in the powerful way it incorporates self-and cross-attentions in its multi-head layers to decode the interactions between the detected instruments and tissues in a laparoscopic procedure.</p><p>The Transformer, as used in Natural Language Processing <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref>, learns attention maps over a temporal sequence, considering a sentence to be a sequence of words. In computer vision, many works have tried to replicate this by modeling input sequence over temporal frames <ref type="bibr" target="#b25">(Girdhar et al., 2019)</ref>. A single image, however, can be as informative as a complete sentence. Even the Vision Transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref> shows that an image is equivalent to 16 ? 16 words, modeled as a sequence of patches from a single image. Many works have followed similar approaches in image understanding <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref>, object detection <ref type="bibr" target="#b8">(Carion et al., 2020)</ref>, segmentation <ref type="bibr" target="#b71">Valanarasu et al., 2021)</ref>, captioning <ref type="bibr" target="#b66">Sundaramoorthy et al., 2021)</ref>, and activity recognition <ref type="bibr" target="#b6">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b24">Gavrilyuk et al., 2020)</ref>. Alternatively, the hybrid architecture of the Vision Transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref> shows that, aside from the raw image, the input sequence can also be obtained from CNN features. It also shows that a patch can have a 1 ? 1 spatial size which is akin to using an image with no explicit sequence modeling. We propose another hybrid approach to obtain the appropriate feature, one that can preserve the spatial and class-wise relationships of the interacting triplet components in a surgical image frame, a semantic sequence in this regard. While our attention input features are extracted from a CNN as done in the hybrid Vision Transformer, our attention is design to leverage the CNN's features in a manner that helps the attention network benefit from the learned class representations. This means we preserve the spatial relationship in the grid of features without breaking it up into patches. This provides further insight into the decisionmaking of attention networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CholecT50: Cholecystectomy Action Triplet Dataset</head><p>CholecT50 is a dataset of endoscopic videos of laparoscopic cholecystectomy surgery introduced to enable research on fine-grained action recognition in laparoscopic surgery. It is annotated with triplet information in the form of instrument, verb, target . The dataset is a collection of 50 videos consisting of 45 videos from the Cholec80 dataset <ref type="bibr" target="#b69">(Twinanda et al., 2017)</ref> and 5 videos from an in-house dataset of the same surgical procedure. It is an extension of CholecT40 <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> with 10 additional videos and standardized classes.</p><p>The cholecystectomy recordings were annotated by two surgeons using the software Surgery Workflow Toolbox-Annotate from the B-com institute 6 . Annotators set the beginning and end on a timeline for each identified action, then assigned to the corresponding instrument, verb and target class labels. An action ends when the corresponding instrument exits the frame, or if the verb or target changes. Out-of-frame actions are not reported, and video frames that are recorded outside the patient's body are zeroed out.</p><p>We then define classes for the triplet. Due to the number of instruments, verbs, and targets available, the theoretical number of all possible triplet configurations (900) is prohibitively high. Even limiting those configurations to the approximately 300 observed in the dataset has little clinical relevance due to the presence of many spurious classes. To have a reasonable 6 https://b-com.com/ In addition to class grouping, surgical relevance rating and label mediation of the annotated data are carried out by three clinicians. For the rating, the clinicians assigned a score from a range of [1-5] to each triplet composition based on their possibility and usefulness in the considered procedure. Their average scores, as well as the triplet's number of occurrences, is used to order the triplet classes, after which the top relevant classes are selected. Moreso, the third clinician performed label mediation in the case of label disagreement.</p><p>The final dataset comprises 100 triplet classes that follow the format of instrument, verb, target . The triplets are composed from 6 instruments, 10 verbs and 15 target classes, presented with their instance counts in <ref type="table" target="#tab_0">Table 1</ref>. We present the CholecT50 dataset triplet labels including their number of occurrences in <ref type="table" target="#tab_1">Table 2</ref>. We also present the co-occurrence statistics for instrument, target and instrument, verb pairs within triplets in the supplementary material.</p><p>For our experiment, we down-sampled the videos to 1fps yielding 100.86K frames annotated with 161K triplet instances. The video dataset is split into training, validation, and testing sets as in <ref type="table" target="#tab_2">Table 3</ref>. The videos in the dataset splits are distributed in the same ratio to include annotations from each surgeon.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>Action triplet recognition is a complex and challenging task, since it requires: (1) simultaneously solving three multilabel classification problems, and (2) performing associations while accounting for multiple triplet instances. In this work, we propose two methods that tackle each aspect of these tasks.</p><p>We address the first point with the class activation guided attention mechanism or CAGAM, which explicitly uses tool type and location information to highlight discriminative features for verbs and targets respectively. We demonstrate its utility by replacing our previous Tripnet <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> model's class activation guide (CAG) with CAGAM, resulting in a preliminary model which we call Attention Tripnet.</p><p>The second point is addressed by the multi-head of mixed attention (MHMA), as an advanced model of semantic attention for triplet association, and a successor to the previous state-of-the-art Tripnet's more primitive 3D interaction space (3Dis). The MHMA resolves the triplet's components association using multiple heads of self and cross attention mechanisms. Our final model is called the Rendezvous (RDV): a transformer-inspired neural network for surgical action triplet recognition. The RDV combines the CAGAM in its encoder with the MHMA in its Transformer-inspired decoder for enhanced triplet component detection and association respectively. This model provides the highest performance on action triplet recognition.</p><p>The proposed RDV network is conceptually divided into four segments: feature extraction backbone, encoder, decoder, and classifier as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Extraction</head><p>We model the visual feature extraction using the ResNet-18 base model. Our choice is motivated by the excellent performance of residual networks in visual object classification tasks. To facilitate more precise localization, the strides of the last two blocks of the ResNet are lowered to one pixel providing higher output resolution. The Resnet-18 base model takes an RGB image frame from a video as input and extracts its visual features X ? R 32?56?512 . The extracted feature is triplicated into (X I , X V , X T ) for multitask learning of the instrument, verb, and target components of the triplets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Components Encoding</head><p>The encoder is responsible for detecting the various components of the triplets, while the decoder resolves the relationships between them. The encoder is composed of the weaklysupervised localization (WSL) module for instrument detection, class activation guided attention mechanism (CAGAM) module for verb and target recognition, and a bottleneck layer collecting unfiltered low-level features from Resnet-18's lower layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Weakly Supervised Localization (WSL)</head><p>While this work primarily focuses on recognizing surgical action triplets, localizing actions -similarly to HOI tasks-is an interesting addition. We therefore go beyond simply detecting the presence of surgical instruments by locating their position, which represents the region of interaction. In the absence of spatial annotations we achieve this with weak supervision. As shown in <ref type="figure">Fig. 3</ref>, the WSL module consists of a 3 ? 3 convolution layer (Conv) of 64 channels, then followed by a 1 ? 1 Conv of C I = 6 channels for instrument localization in form of class activation maps (CAM).</p><p>Specifically, the WSL module takes X I from the feature extraction layer as input and returns the instruments' CAM, marked as (H I ), from its last Conv layer. The output CAM (H I ) are trained for localization via their Global Maximum Pooled (GMP) values Y I representing instrument class-wise presence probabilities similar to <ref type="bibr" target="#b72">Vardazaryan et al. (2018)</ref>.</p><p>The discriminative CAM features (H I ) alongside these remaining extracted features (X V , X T ) are passed to the CAGAM for verb and target detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Class Activation Guided Attention Mechanism</head><p>(CAGAM) Surgical action triplets are instrument-centric. Detecting the correct verbs and target anatomies is very challenging, because the visibility as well as the subtly involvement of a tool and anatomy in an action have to be taken into consideration.</p><p>A limited effort is made in our previous method, Tripnet <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>, to handle this using a CAG module conditioning the detection of verbs and targets on the instruments activations, via concatenated features. Since attention modeling is found to be superior to feature concatenation <ref type="bibr" target="#b70">(Ulutan et al., 2020)</ref>, we explore several types of attention and propose a new form of spatial attention, named CAGAM.</p><p>According to <ref type="bibr" target="#b73">Vaswani et al. (2017)</ref>, an attention function can be described as matching a query (Q) and a set of keyvalue (K, V) pairs to form an output. The output is computed as a weighted sum of the values (wV), where the weight (w = QK T ) is computed by an affinity score function of the query with the corresponding key. CAGAM is a new form of spatial attention mechanism that propagates attention from known to unknown context features, thereby enhancing the unknown context for relevant pattern discovery. It is an adaptation of the saliency-guided attention mechanism in <ref type="bibr" target="#b30">(Ji et al., 2019;</ref><ref type="bibr" target="#b81">Yao and Gong, 2020)</ref> to utilize a more adequate and easier to learn class activation map (CAM) suitable for action triplet recognition. It is used, in this case, to discover the verbs and targets that are involved in tool-tissue interactions leveraging the instrument's contextual dependencies, by propagating attention from the discriminative H I to the non-discriminative X V and X T features. The CAM (H I ) serves as the known con-  text features in this regard, since they are already discriminated class-wise and localized for the instruments.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we model the CAGAM to enhance the verb's and target's unfiltered features by element-wise addition of an enhancement: this enhancement is a computed spatial attention A from the instrument affinity maps (P D ) as well as the component affinity maps (P ? ) themselves. The P D are termed discriminative because they originate from the instrument CAM features, whereas P ? are termed nondiscriminative because they are formed from the unfiltered component features.</p><p>We observe that verbs and targets behave differently with regards to their instrument; that is, verbs are mostly affected by the instrument's type, while targets tend to be determined by instrument's position. This distinction is a key factor in the choices of attention mechanism in the CAGAM which indeed combines channel attention for verb detection <ref type="figure" target="#fig_2">(Fig.  4: left)</ref> and position attention for target detection <ref type="figure" target="#fig_2">(Fig. 4:  right)</ref>. Both types of spatial attention mechanisms are similar, except for the dimensions used, and therefore the nature of the information attended to. The channel attention is captured in the C I ? C I channel dimensions, informed by instrument type, whereas the position attention is captured in the HW ? HW spatial dimensions, informed by instrument location. This choice is well validated in ablation studies shown further <ref type="table" target="#tab_6">(Table 4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAG channel attention for verbs</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 4 (left)</ref>, verb features are first remapped to X-Z ? R H?W?C I , which we call the context features. Following two separate 1 ? 1 Conv and reshapings, mapping it to a query Q and a key K of size HW ? C I , the non-discriminative affinity map P ? ? R C I ?C I is obtained via matrix multiplication of the transposed Q by K as illustrated in Equation 1:</p><formula xml:id="formula_0">P ? = Q T K.<label>(1)</label></formula><p>Applying a similar process to the CAM results in the discriminative affinity map: P D ? R C I ?C I . As done in <ref type="bibr" target="#b81">Yao and Gong (2020)</ref>, an element-wise product of the two affinity maps, scaled by a factor ? and passed through red softmax (?) gives the attention A:</p><formula xml:id="formula_1">A = ? P D P ? ? .<label>(2)</label></formula><p>Meanwhile, we obtain the value features V ? R HW?C I by reshaping the verb context X-Z to R HW?C I . Next, we obtain an enhancement by matrix multiplication of A by V, weighted by a learnable temperature ?. This enhancement is reshaped to R H?W?C I and added back to X-Z to produce the enhanced features, E.</p><formula xml:id="formula_2">E = ?(VA) + X-Z .<label>(3)</label></formula><p>The features E are transformed into per-verb activation maps H V ? R H?W?C V via a 1 ? 1 Conv. Finally, verb logits Y T ? R 1?C V are obtained by global average pooling of H V , where C V = 10 is the number of verb classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAG position attention for targets</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 4 (right)</ref>, obtaining the Q, K, and V terms for the CAG position attention is similar to the CAG channel attention mechanism. However to obtain an instrument location-aware attention, we multiply Q by K T (instead of Q T by K as done for verbs in Equation 1) producing affinity maps (P D , P ? ) and a subsequent attention map A of the desired size HW ? HW, informed by instrument position rather than instrument type.</p><p>Furthermore, we obtain enhanced target features (E), which we also feed to a 1 ? 1 Conv of C T = 15 channels to obtain the per-target activation maps H T ? R H?W?C T . Using a global pooling on H T , we then obtain the target logits Y T ? R 1?C T .</p><p>To ensure that the H I , H V and H T class maps properly capture their corresponding components, we train their global pooled logits (Y I , Y V , Y T ) as auxiliary classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Bottleneck Layer</head><p>In addition to these refined, component-specific features (H I , H V , H T ), a global context feature is also necessary for modeling their contextual relationship; which is why we also draw a unfiltered low-level feature X 0 from the first block of ResNet and feed it to the bottleneck layer that consists of 3 ? 3 ? 256 and 1 ? 1 ? C convolution layers, where C = 100 is the number of triplet classes. This gives the global context feature for triplets H IVT , with channels matched to the triplet classes.</p><p>The unfiltered triplets feature H IVT as well as the triplet component's class maps (H I , H V , H T ) are fed to the decoder layer for decoding the triplet association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interaction Decoding</head><p>We describe here the modeling of the triplet components' relationship in the RDV decoder. An existing approach attempts to model every triplet possibility from the outer product combination of the three components using a 3D feature space <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>. This design models more than the required triplets, including irrelevant and impossible combinations, making the module hard to train. Hence, we follow a Transformer-like architecture <ref type="bibr" target="#b73">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b18">Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2021)</ref> leveraging long-range attention to efficiently model the required relationships. To take into consideration the constituting components of the triplets <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>, we utilize the semantic features of each component, captured in their class maps (H I , H V , H T ). Unlike the Vision Transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref>, however, we do not break class maps into patches. As shown by ablation results in the supplementary material, the patch sequence degrades representations, especially information on instruments that is important for locating actions. Hence, we model the RDV attention decoder on the semantic sequence of learnt class-wise representations. From H I , H V , H T and the global triplet feature H IVT , RDV decodes all the self-and cross-interactions between the triplet's global context feature and the three features corresponding to individual components, using scaled dot-product attention <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> without using recurrence. In addition to self-attention, cross-attention adds the capability to better model the relationships with components participating in the action triplet. This is important when resolving interactions: for instance, an anatomical part can appear in the frame without being a target, often making the interaction with the instrument ambiguous.</p><p>To understand the attention decoder used in this work, we explain the decoding-by-attention concept below:</p><p>1. Firstly, attention decoding is described as a search process whereby a query (Q), that is issued by a user (sink or receiver), is used to retrieve data from a repository (source). Normally, Q is a user's abridged description of the requested data also known as search terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The source context consists of a key-value (K,V) pair</head><p>where V is a collection of several data points or records and K is the mean descriptor for each record also known as keywords. 3. To retrieve the requested data, the issued Q is matched with the available Ks to create an affinity (P), also known as the attention weight. 4. The P, when matched with V, creates an attention map (A) which helps retrieve the most appropriate data to the sink.</p><p>We implement a transformer-inspired decoder that is composed of a stack of L = 8 identical layers as shown in successively by its two internal modules: MHMA and feedforward, to produce refined triplet features, H IVT . The output of each module is followed by a residual connection and a layer normalization (AddNorm) as it is done in other multihead attention networks. The entire cycle repeats, with a more refined H IVT output, until the L th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Multi-head of Mixed Attention (MHMA)</head><p>The multi-head attention combines both self-and crossattentions, encouraging high-level learning of triplets from the interacting components as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. It starts with a projection function, pf, which generates a set of value V, key K, and/or query Q for each context feature (H I , H V , H T , H IVT ). In the implementation as shown in Equation 4, the pf function generates vectors of Q ? R 1?C and K ? R 1?C-Z that represent the abridged mean descriptors of the contexts by leveraging the global average pooling (GAP) operation. Here, C = 100 for triplet, whereas C-Z = [6, 10, 15, 100] for either instrument, verb, target, or triplet classes, respectively. We map each descriptor to a feature embedding layer where we mask (dropout ? = 0.3) parts of Q to avoid repeating the same query in the L alternating layers. Using the pf function, we also obtain the V ? R H?W?C-Z by a convolution operation on the feature context and reshape to R HW?C-Z . Hence, the extracted Q, K, and V features follow the aforementioned decoding-by-attention concept (items 1 &amp; 2). The pf function generates each K and Q using FC layers as done in <ref type="bibr" target="#b73">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b18">Dosovitskiy et al., 2020)</ref>, and generates the V using convolution layers as done in <ref type="bibr" target="#b20">(Fu et al., 2019;</ref><ref type="bibr" target="#b79">Wang et al., 2018;</ref><ref type="bibr" target="#b28">Huang et al., 2019)</ref>.</p><formula xml:id="formula_3">pf (H) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Q : FC DROPOUT GAP H , K : FC GAP H , V : CONV H .<label>(4)</label></formula><p>Next, we build 4 attention heads for the instrument, verb, target, and triplet attention features. In the existing Trans- former and Transformer-based models, each of the heads learns a self-attention. Self-attention helps a model understand the underlying meaning and patterns within its own feature representation. This is needed for initial scene understanding. However, when each feature representation (such as a class-map) has been discriminated to attend to only one component in an image scene, understanding their underlying relationship requires a cross-attention across the component features. In a cross-attention mechanism, the attention built from one context (the source) is used to highlight features in another context (the sink) as done in <ref type="bibr" target="#b53">Mohla et al. (2020)</ref>. While the self-attention mechanism computes the focal representation on the same triplet features, cross attentions learn the triplet representations by drawing attention from the individual components: namely instrument, verb, and target. This models how the features of each component affect the triplet composition, by propagating the affinities from their respective context features to the required triplet features. To utilize both self and cross attentions, we model the source context from the encoded class-map features (H I , H V , H T ) representing the triplet components and the sink context from the triplet features (H IVT ). Of course, the source context remains the same as the sink in the self-attention mechanism. This means we generate the corresponding Ks and Vs from both the source and sink contexts, but generate the Q only from the sink context using the projection function, pf, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. With Q coming from the triplet features, we actually focus the image understanding on the actions of interest by pointing the cross-attention heads at the component's discriminative features (H I , H V , H T ) in a manner that helps the attention network benefit from the learnt class representations. This also respects the aforementioned decoding-by-attention concept. We then learn a scaled dot product attention of the Q on the (K,V) pair for each attention head as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. Specifically, we derive the scaled dot product attention using the widely used attention formula <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> in Equation 5:</p><formula xml:id="formula_4">A(Q, K, V) = V.? KQ T ? d K ,<label>(5)</label></formula><p>where ? is a softmax activation function, ? d K is a scaling factor, and d K is the dimension of K after linear transformation. The cross attention is implemented on the instrument, verb, and target attention heads, whereas self-attention is implemented on the triplet attention head. While each attention head simultaneously concentrates on its own features of interest, the multi-head module combines heads A 1..N to jointly capture the triplet features as in <ref type="figure" target="#fig_4">Equation 6</ref>:</p><formula xml:id="formula_5">A 1..N = W N i=1 A i ,<label>(6)</label></formula><p>where is a concatenation operator for N = 4 attention heads. A 1 is the triplet self-attention, A 2...N are the triplet cross attentions with the interacting components. W is the matrix of convolution weights. This packed convolution scheme merges the information from all attention heads while preserving its spatial structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Feed-forward</head><p>The output of the multi-head attention is further refined by a feed-forward layer which is a stack of 2 convolutions with an AddNorm. The output is a refined H IVT with each channel attending to each triplet class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Triplet Classification</head><p>The RDV model terminates with a linear classifier for the final classification of the triplets. In this layer, we apply a global pooling operation on the H IVT from the L th layer of the RDV decoder, followed by an FC-layer (with C = 100 neurons) for the triplet classification. The output logits (Y IVT ) are trained jointly end-to-end with the auxiliary logits from the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Attention Tripnet</head><p>In our previous work <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>, Tripnet relies on two modules: (1) the class activation guide (CAG), which leverages instrument activations to detect verbs and targets via concatenated features, and (2) the 3D interaction space (3Dis), where features corresponding to the three components are projected in an attempt to resolve their interactions.</p><p>As an ablation model, we extend this to Attention Tripnet by only replacing the CAG in Tripnet <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> with CAGAM as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. This validates the contribution of attention modeling for verb and target detections using Attention Tripnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Setup</head><p>Due to variability in the video dataset, frame resolution varies from 480 ? 854 to 1080 ? 1920. We unified these spatial dimensions by resizing them to 256 ? 448. We also employed random scaling [0.5, 1.5] and brightness/contrast shift (delta = 0.2) as data augmentation for training. The models are trained on 35 videos, validated on 5 videos, and tested on 10 videos according to the data split in  function, which extracts per-component labels from the triplet labels; those are three vectors of binary presence labels with length N = [6, 10, 15] per frame, where n ? N is the class size for each triplet's component trained as auxiliary task. For a high-performance data loading pipeline, we store our training data as serialized TFRecords binaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training and Loss Functions</head><p>Since classifying each triplet component, namely instrument, verb, and target, is a multi-label classification problem, we employ weighted sigmoid cross-entropy losses: L I , L V , and L T respectively. The weighted cross-entropy with logits is as follows:</p><formula xml:id="formula_6">L = C c=1 ?1 N W c y c log ? (? c ) + 1 ? y c log 1 ? ? (? c ) ,<label>(7)</label></formula><p>where y c and? c are respectively the ground truth and predicted labels for class c, ? is the sigmoid function, and W c is a weight for class balancing. The three component detection tasks are jointly learned in a multi-task manner following the uncertainty loss procedure given in <ref type="bibr" target="#b33">Kendall et al. (2018)</ref> that uses learnable parameters w I , w V , w T to automatically balance the tasks training as follows:</p><formula xml:id="formula_7">L comp = 1 3 1 e w I L I + 1 e w V L V + 1 e w T L T + w I + w V + w T (8)</formula><p>This is only used for the auxiliary tasks captured by multi-task learning.</p><p>The triplet association loss L assoc is also modeled as a sigmoid cross-entropy. To jointly learn the complete tasks endto-end, we define the total loss (L total ) using the equation:</p><formula xml:id="formula_8">L total = L comp + ?L assoc + ?L 2 ,<label>(9)</label></formula><p>where ? is a warm-up parameter that allows the network to focus solely on learning the individual components' information within the first 18 epochs. ? = 1e ?5 is a regularization weight decay for the L 2 normalization loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hyper-parameters</head><p>The feature extraction backbone is pretrained on ImageNet. All the models are trained using Stochastic Gradient Descent with Momentum (? = 0.95) as optimizer. We maintain a stepwise learning rate (? = 0.001) policy, decayed by ? = 0.1 after every 50 epochs. The models are trained in batches of size 8 for 200 epochs. The final model weights are selected based on their validation loss saturation. All the hyper-parameters are tuned on the validation set (5 videos) with up to 74 grid search experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Hardware and Schedule</head><p>Our networks are implemented using TensorFlow and trained on GeForce GTX 1080 Ti, Tesla P40, RTX6000, and V100 GPUs. Full training takes approximately 118-180 hours on a single GTX 1080 Ti. Total storage space consumption for the model, input data, output weights, and summaries is under 10GB. Parameter counts for the MTL baseline, Tripnet, Attention Tripnet, and 8-layer RDV models reach 14.94M, 14.95M, 11.81M, 16.61M respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Inference and Evaluation Protocols</head><p>Model outputs are probability scores that can be thresholded to indicate class presence or absence. We statistically evaluate the model's performance at recognizing surgical actions as a triplet using three metrics:</p><p>1. Component average precision: This measures the average precision (AP) of detecting the correct components of the triplet, as the area under the precision-recall curve per class. Using this, we measure the AP for instrument (AP I ), verb (AP V ), and target (AP T ) detections.</p><p>To use these metrics for the naive models or for any model that predicts only the triplet labels Y IVT , we decompose their predictions into the constituting components (Y I , Y V , Y T ) following Equation 10:</p><formula xml:id="formula_9">Y I = [ MAX (Y IVT |I = i) ? i ? {0, 1, .., C 1 } ], Y V = [ MAX (Y IVT |V = v) ?v ? {0, 1, .., C 2 } ], Y T = [ MAX (Y IVT |T = t) ?t ? {0, 1, .., C 3 } ],<label>(10)</label></formula><p>where C 1 , C 2 and C 3 are the class sizes for the instrument, verb, and target components respectively. This directly translates to obtaining the probability of a given component class as the maximum probability value among all triplet labels having the same component class label in a given frame. For instance, the predicted probability of a grasper instrument in a frame is the maximum probability of all triplet labels having grasper as their instrument component label. The ground truth for these components is also derived in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Triplet average precision:</head><p>This measures the AP of recognizing the tool-tissue interactions by observing elements of the triplet in conjunction. Using the same metrics as <ref type="bibr" target="#b55">Nwoye et al. (2020)</ref>, we measure the APs for the instrument-verb (AP IV ), instrument-target (AP IT ), and instrument-verb-target (AP IVT ). During the AP computation, a prediction is registered as correct if all of the components of interest are correctly identified (e.g. instrument and verb for AP IV ). The main metric in this study is AP IVT , which evaluates the recognition of the complete triplets. 3. Top-N recognition performance: Due to high similarities between triplets, we also measure the ability of a model to predict the exact triplets within its top N confidence scores. For every given test sample x i , a model made an error if the correct label y i does not appear in its top N confident predictions? i for that sample. Using this setup, we measure the top-5, top-10, and top-20 accuracies for the triplet prediction. We also show the top 10 predicted triplet class labels and their AP scores for a more insightful analysis of the model's performance.</p><p>Video-specific AP scores are computed per category, across all frames of a given video. Averaging those APs over all videos gives us the mean AP (mAP), serving as our main metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>In this section, we rigorously validate individual components of the Attention Tripnet and Rendezvous (RDV) through careful ablation studies. We then provide a comparative analysis with baseline and state-of-the-art (SOTA) methods to show our methods' superiority.</p><p>6.1. Quantitative Results 6.1.1. Ablation study on the encoder's attention type We begin with an ablation study for the choice of the attention type in the CAGAM module. We compare the module with a baseline model (MTL) <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref>, which implements a multi-task learning of instruments, verbs, and targets in separate branches with no attention (None), and show that attention guidance helps better detect the components in general <ref type="table" target="#tab_6">(Table 4</ref>). We also justify the distinct attention types for verbs and targets. Firstly, the channel attention is used for both verb and target detections (row 3), and the position attention is used for both verb and target detections (row 4), before they are combined (Dual attention) in the last row. Channel attention is better suited for verbs than targets, with +10.6% vs +3.5% improvement respectively. Position attention behaves the opposite: +2.8% vs +6.9%. Matching verbs with channel attention and targets with position attention gives the most balanced and highest improvement: +12.4% verbs, +12.0% targets. We, therefore, retain this choice in the proposed models. One of the novel contributions of this work is its hybrid multi-head attention mechanism for resolving tool-tissue interactions, combining self-and cross-attention. This is a substantial innovation over transformers found in sequence modeling, which instead rely on multi-heads of self-attention only. Our choice of multi-head attention is justified in the following ablation study presented in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Ablation Study on Decoder's Attention Type</head><p>Our first ablation model in this regards (Single Self) uses a multi-head attention with the input feature coming from the high-level features (X) of ResNet-18 to compute a successive scale dot-product attention over 8 decoder layers as in RDV. It can be observed that using a multi-head of self-attention coming from a single source (triplet features) yields insufficient results for action triplet recognition.</p><p>The Multiple Self ablation model, as a "self-attention only" version of the RDV, uses self-attention in all four contexts: instrument, verb, target, and triplet. The RDV clearly performs the best in terms of association, justifying our use of crossattention. We carried out a scalability study to observe the performance of the RDV when increasing the number of multi-head layers while keeping track of the number of parameters and GPU requirements. These results presented in <ref type="table" target="#tab_8">Table 6</ref> show that the proposed model improves when scaled up, at the cost More ablation studies on the sequence modeling of the class-wise features, use of auxiliary classification loss, etc., are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">Scalability Study on Multi-Head Layer Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4.">Component Detection and Association mAP</head><p>For ease of reference, we summarize the overall performance of the experimented models on the considered metrics for both triplet component detection and the recognition of their interactions in <ref type="table" target="#tab_9">Table 7</ref>. As a baseline, we design a CNN model that models the triplet recognition as a simple classification of 100 distinct labels without taking any special reference to the constituting components. The performance of this baseline model shows that it is not sufficient to naively classify the triplet IDs without considering the triplet components. Even a temporal refinement of the naive CNN model outputs using a (TCN)  is still sub-optimal. Multi-task learning (MTL) of the triplet components helps the model gain some performance, but still scores low on triplet association. The MTL outperforms the TCN here likely because a temporal refinement would not matter much if a Naive CNN does not capture significant representative features for triplet recognition. The Tripnet model proposed in <ref type="bibr" target="#b55">(Nwoye et al., 2020)</ref> leverages the CAG to improve the MTL in the triplet components detection. It also improves the interaction recognition AP IVT by 2.4% using the 3Dis.</p><p>The Attention Tripnet uses the CAGAM to further improve the Tripnet's verb detection by 5.7% and target detection by 5.3%. The Attention Tripnet is on par for instrument detection AP; this is likely due to the instrument detection being already saturated. The overall performance does increase, with indeed a 3.4% improvement for triplet recognition. The RDV, on the other hand, uses a multi-head attention decoder to further improve the association performance (+9.7% on instrumentverb, +10.5% on instrument-target). It improves the overall final triplet recognition by 9.9% mAP IVT compared to the SOTA, tripling the improvement from the Attention Tripnet. A breakdown of per-class detection of the triplet components and their association performance is presented in the supplementary material. Here we focus on the top N predictions. As shown in <ref type="table" target="#tab_10">Table 8</ref>, when considering the model's top 20 predictions, the model records an AP of ? 95%. The model's confidence however decreases when considering more top predictions, suggesting how closely related most of the triplet classes could be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6.">Surgical Relevance of the Top Detected Triplets</head><p>The result of the top 10 correctly detected triplets for the experimented models, presented in <ref type="table" target="#tab_11">Table 9</ref>, reveals the individual strengths of the experimented models in recognizing the tool-tissue interaction. All triplets predicted in the top results are clinically sensible, with none of the more unexpected instrument-verb or instrument-target pairings.</p><p>Of importance, triplets with high surgical relevance in cholecystectomy procedure, i.e., clipper, clips, cystic duct or artery and scissors, cut, cystic duct or artery , which are critical for safety monitoring, are better detected by the RDV than the SOTA. The proposed models learn to detect rare but clinically important uses of surgical instruments in their top 10 correctly predicted labels. This holds true for ambiguous instruments, like the irrigator that is mostly used to aspirate or irrigate but can as well be used to dissect in rare cases <ref type="bibr">( irrigator, dissect, cystic-pedicle )</ref>. Another detected rare case include bipolar, coagulate, blood-vessel . This suggests that the models effectively learned the surgical semantics of instrument usage even with small examples of peculiar classes.</p><p>The triplet grasper, grasp, specimen-bag always appears in the top 2 even though its prevalence (6K) is not particularly high, compared to triplets such as hook, dissect, gallbladder (29K), grasper, retract, liver (13K), etc. This may be due to its consistent appearance in the workflow, usually towards the end; another factor could be the discernability of the bag. For every triplet in the top-10 predictions of both the SOTA and the proposed models, the performance is usually higher in the proposed models. Remarkably, the entire top 10 for the RDV is recognized at an AP above 50%. Compared to SOTA, the proposed models show improvements in the more complex task of detecting complete triplets and instrument-target while showing comparable performance for the visually simpler instrument-verb detection task (see supplementary material and statistical analysis in <ref type="table" target="#tab_0">Table 10</ref>). In addition to the top 10, we also present the full extent of the model's performance on all 100 classes using the AP box plots in <ref type="figure" target="#fig_6">Fig. 8</ref>, showing upper and lower performance bounds for each model as well the spread around the mean. The rectangular box represents the middle 50% of the score for each model also known as interquartile range. As can be seen from <ref type="figure" target="#fig_6">Fig. 8</ref>, the proposed models maintain higher median and upper-quartile performance than the baselines. They also maintain higher upper-whiskers showing the extent of their performance distribution above the interquartile range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.7.">Statistical Significance Analysis</head><p>We also measure the statistical significance of the proposed model performance using the SOTA model as the alternative method. Using the Wilcoxon signed-rank test, we sample </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Proposed methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Tripnet Rendezvous</head><p>Component Detection</p><formula xml:id="formula_10">AP I p ? 0.327 p ? 0.374 AP V p 0.001 p ? 0.003 AP T p 0.001 p 0.001</formula><p>Triplet Association AP IV p ? 0.018 p &lt; 0.001 AP IT p ? 0.010 p ? 0.005 AP IVT p &lt; 0.005 p 0.001 N = 30 random batches of 100 consecutive frames instead of 30 random frames to simulate the evaluation on video clips. The null hypothesis (H 0 ) states that the difference between the proposed method and the alternative method follows a symmetric distribution around zero. We perform the significance analysis for each task, and based on the obtained p-values, presented in <ref type="table" target="#tab_0">Table 10</ref>, we draw the following conclusions:</p><p>1. Both proposed models do not significantly improve the instrument detection sub-task. Their p-values fall short of the standard 0.05. This is mainly because the instrument detection performance is already saturated in the alternative method; there is no new modeling in the proposed methods targeting their improvement. Being a two-tailed test, the p-value also shows that the SOTA does not outperform the proposed models on instrument detection. 2. The guided attention mechanism is very useful in improving the verb and target detections in both the Attention Tripnet and RDV models. Their contributions are significant enough to even beat a more narrow 0.01 significant level. 3. Our contributions are also significant in improving the recognition of the tool-tissue interaction, with Attention Tripnet's improvement on AP IVT relevant at a 0.005 significance level. Our best method (RDV) is more significant, with a p-value far below 0.001. In summary, we reject the null hypothesis H 0 at a confidence level of 5%, concluding that there is a significant difference between the proposed models and the alternative method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Triplet Recognition with Weak Localization</head><p>The predicted class labels are obtained by applying a 0.5 threshold on the output probabilities of the proposed RDV model. We present those predicted labels in <ref type="figure" target="#fig_7">Fig. 9</ref>, alongside the localization of the regions of action obtained from the weakly supervised learning (WSL) module of the network. This localization, depicted by bounding boxes overlaid on the image, shows the focus of the model when it makes a prediction, thereby providing insight into its rationale. Those results are solid arguments in favor of the model's ability for spatial reasoning when recognizing surgical actions. This suggests that the model can be further exploited for action triplet detection and segmentation. We also provide a short video of this qualitative performance in the supplementary material (also accessible via: https://youtu.be/d yHdJtCa98).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Qualitative Analysis of Top 5 Predicted Triplets</head><p>We also examine the top 5 prediction confidence of the proposed models compared to baselines on random frames <ref type="figure" target="#fig_0">(Fig.  10)</ref>. Fully correct predictions are signaled by the color green, while red indicates errors on all three components. Other colors indicate partially correct predictions. RDV and Attention Tripnet outperform the baseline (MTL) and SOTA (Tripnet) each time, with the two actions correctly recognized each time within their top 5 predictions. Moreover, other actions in their top 5 have relevant components, showing these models' understanding of surgical actions by clustering triplets related to the performed actions. More qualitative results on this are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3.">Attention Map Visualization</head><p>To understand the benefit of the CAGAM's attention mechanism, we visualize its attention maps in <ref type="figure" target="#fig_0">Fig. 11</ref>. For each input image, we randomly selected a few points (marked i ? [1, 2, 3, 4]) in the images and reveal the corresponding attention maps for the tool-tissue interaction captured in the CAGAM's position attention map. We observe that the attention module could capture semantic similarity and full-image dependencies, which change based on the contribution of the selected pixel to the action understanding. This shows that the model learns attention maps that contextualize every pixel in the image feature in relation to the action performed. For instance in the top image: point 2, a pixel location on the instrumentgrasper, creates an attention map that highlights both the instrument and its targetgallbladder. Indeed, the attention guidance introduced in this model helps to highlight the triplet's interest regions while suppressing the rest. This effect is shown further in the supplementary video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented methods featuring new forms of attention mechanisms that surpass the state-of-the-art for surgical actions triplet instrument, verb, target recognition. We first proposed a novel approach for attention intended for verbs and targets, using instrument class activation maps. We have also introduced a novel hybrid attention mechanism that resolves component association in triplets by leveraging multiple heads of both self and cross attentions on the component features.</p><p>We have rigorously validated our performance claims on CholecT50, a new large-scale endoscopic video dataset also contributed in this work. We also discussed the benefits of the proposed methods in terms of clinical significance. Qualitative results suggest possible extensions to different tasks,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTL Baseline Tripnet Attention Tripnet Rendezvous</head><p>Correct complete triplet <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Point <ref type="formula" target="#formula_0">1</ref>  including automated surgical report generation and spatial action segmentation. While these initial results are encouraging, many challenges remain. One is the scalability on unseen triplets which may likely be tackled by zero-, one-or few-shot learning. Our results on rare triplets already hint at promising prospects for this approach. Inference speed is another challenge: increasing the number of layers generally drives up the performance, but is computationally very costly. Implementing a more lightweight Rendezvous would help alleviate some of these costs.</p><p>One limitation of this work is that target localization using the same weakly-supervised technique as for instruments is not yet achieved. This is likely due to the target's visibility not being the sole indicator for a positive binary label, both in the ground truth annotations and the model predictions. We also observed that it is not possible to recognize multiple instances of the same triplet, e.g., two grasper, grasp, gallblad-der . This is due to the nature of the binary presence annotation, which does not provide an instance count for each unique triplet class. Only actions performed with the grasper instrument can have multi-instance occurrence in this dataset. Nonetheless, this does not affect recognition but is considered a limitation in future work on triplet localization, where multiple instances would need to be detected differently.</p><p>With high-profile potential applications such as safety monitoring, skill evaluation, and objective reporting, our surgical action triplet method, as well as the release of our dataset for the 2021 Endoscopic Vision challenge, bring considerable value to the field of surgical activity understanding.</p><p>Future work will consider temporal modeling as some of the action verbs could be better recognized by the temporal dynamics of the tool-tissue interaction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Some examples of action triplets from the CholecT50 dataset. The localization is not part of the dataset, but a representation of the weaklysupervised output of our recognition model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture of the Rendezvous: meeting of attentions, a transformerinspired model with channel and position spatial attention for triplet components detection and a multi-head of self amd cross semantic attention for action triplet recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Class Activation Guided Attention Mechanism (CAGAM): uses the attention learned from the instrument's CAM to highlight the verb class (left) and the anatomy in contact with the instrument (right). Feature dimension (height H = 32, width W = 56, depth D = 64, instrument's class size C I = 6, verb's class size C V = 10, target's class size C T = 15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 2. Each layer receives the triplet features H IVT and the encoded class maps (H I , H V , H T ) as inputs which are processed Architecture of the multi-head of mixed attention (MHMA): showing the feature projection into Q, K and V, and subsequent multiple heads of self and cross attentions using scale-dot product attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Structure of scale dot-product attention mechanisms: in self-attention, the (K,V,Q) triple comes from one feature context, whereas in cross-attention, the (K,V) pair comes from the source feature context while Q comes from the sink feature context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Architecture of the Attention Tripnet showing the base (feature extraction backbone), neck (instrument detection branch and CAGAM module), and head (3D interaction space). Feature dimension values (H = 32, W = 56, C I = 6, C V = 10, C T = 15, C = 100)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Distribution of the model AP for the 100 triplet class predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative results showing the triplet predictions and the heatmaps for the triplet detection. Localization bounding boxes are obtained from the WSL module of the proposed RDV model. Predicted and ground truth triplets are displayed below each image: black = ground truth, green = correct prediction, red = incorrect prediction. A missed triplet is marked as false negative and a false detection is marked as false positive (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results showing the top-5 triplet predictions for the best performing baseline, SOTA, and the proposed models (Best viewed in colour).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the triplet's component labels in the dataset</figDesc><table><row><cell cols="2">instrument</cell><cell>Verb</cell><cell></cell><cell>Target</cell><cell></cell></row><row><cell>Label</cell><cell>Count</cell><cell>Label</cell><cell>Count</cell><cell>Label</cell><cell>Count</cell></row><row><cell>bipolar</cell><cell>6697</cell><cell>aspirate</cell><cell>3122</cell><cell>abd-wall/cavity</cell><cell>847</cell></row><row><cell>clipper</cell><cell>3379</cell><cell>clip</cell><cell>3070</cell><cell>adhesion</cell><cell>228</cell></row><row><cell>grasper</cell><cell>90969</cell><cell>coagulate</cell><cell>5202</cell><cell>blood-vessel</cell><cell>416</cell></row><row><cell>hook</cell><cell>52820</cell><cell>cut</cell><cell>1897</cell><cell>cystic-artery</cell><cell>5035</cell></row><row><cell>irrigator</cell><cell>5005</cell><cell>dissect</cell><cell>49247</cell><cell>cystic-duct</cell><cell>11883</cell></row><row><cell>scissors</cell><cell>2135</cell><cell>grasp</cell><cell>15931</cell><cell>cystic-pedicle</cell><cell>299</cell></row><row><cell></cell><cell></cell><cell>irrigate</cell><cell>572</cell><cell>cystic-plate</cell><cell>4920</cell></row><row><cell></cell><cell></cell><cell cols="2">null-verb 10841</cell><cell>fluid</cell><cell>3122</cell></row><row><cell></cell><cell></cell><cell>pack</cell><cell>328</cell><cell>gallbladder</cell><cell>87808</cell></row><row><cell></cell><cell></cell><cell>retract</cell><cell>70795</cell><cell>gut</cell><cell>719</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>liver</cell><cell>17521</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>null-target</cell><cell>10841</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>omentum</cell><cell>9220</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>peritoneum</cell><cell>1227</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>specimen-bag</cell><cell>6919</cell></row></table><note>number of classes with maximum clinical utility, a team of clinical experts selected the top relevant labels for the triplet dataset. This is achieved in two steps. In the first instance, class grouping (?) is carried out to super-class triplets that are semantically the same. Some examples of triplets grouped in- clude:1. irrigator, aspirate, bile ? irrigator, aspirate, fluid ? irriga- tor, aspirate, blood ?? irrigator, aspirate, fluid 2. grasper, pack, gallbladder ? grasper, store, gallbladder ?? grasper, pack, gallbladder 3. grasper, retract, gut ? grasper, retract, duodenum ? grasper, retract, colon ?? grasper, retract, gut 4. bipolar, coagulate, liver ? bipolar-grasper, coagulate, liver ? bipolar, coagulate, liver-bed ?? bipolar, coagulate, liver 5. grasper, grasp, gallbladder-fundus ? grasper, grasp, gallbladder-neck ? grasper, grasp, gallbladder ? grasper, grasp, gallbladder-body ?? grasper, grasp, gallbladder</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics showing the number of occurrences of the triplets</figDesc><table><row><cell>Name</cell><cell>Count</cell><cell>Name</cell><cell>Count</cell><cell>Name</cell><cell>Count</cell></row><row><cell>bipolar,coagulate,abdominal-wall/cavity</cell><cell>434</cell><cell>grasper,grasp,cystic-artery</cell><cell>76</cell><cell>hook,dissect,gallbladder</cell><cell>29292</cell></row><row><cell>bipolar,coagulate,blood-vessel</cell><cell>251</cell><cell>grasper,grasp,cystic-duct</cell><cell>560</cell><cell>hook,dissect,omentum</cell><cell>3649</cell></row><row><cell>bipolar,coagulate,cystic-artery</cell><cell>68</cell><cell>grasper,grasp,cystic-pedicle</cell><cell>26</cell><cell>hook,dissect,peritoneum</cell><cell>337</cell></row><row><cell>bipolar,coagulate,cystic-duct</cell><cell>56</cell><cell>grasper,grasp,cystic-plate</cell><cell>163</cell><cell>hook,null-verb,null-target</cell><cell>4397</cell></row><row><cell>bipolar,coagulate,cystic-pedicle</cell><cell>77</cell><cell>grasper,grasp,gallbladder</cell><cell>7381</cell><cell>hook,retract,gallbladder</cell><cell>479</cell></row><row><cell>bipolar,coagulate,cystic-plate</cell><cell>410</cell><cell>grasper,grasp,gut</cell><cell>33</cell><cell>hook,retract,liver</cell><cell>179</cell></row><row><cell>bipolar,coagulate,gallbladder</cell><cell>343</cell><cell>grasper,grasp,liver</cell><cell>83</cell><cell>irrigator,aspirate,fluid</cell><cell>3122</cell></row><row><cell>bipolar,coagulate,liver</cell><cell>2595</cell><cell>grasper,grasp,omentum</cell><cell>207</cell><cell>irrigator,dissect,cystic-duct</cell><cell>41</cell></row><row><cell>bipolar,coagulate,omentum</cell><cell>262</cell><cell>grasper,grasp,peritoneum</cell><cell>380</cell><cell>irrigator,dissect,cystic-pedicle</cell><cell>89</cell></row><row><cell>bipolar,coagulate,peritoneum</cell><cell>73</cell><cell>grasper,grasp,specimen-bag</cell><cell>6834</cell><cell>irrigator,dissect,cystic-plate</cell><cell>10</cell></row><row><cell>bipolar,dissect,adhesion</cell><cell>73</cell><cell>grasper,null-verb,null-target</cell><cell>4759</cell><cell>irrigator,dissect,gallbladder</cell><cell>29</cell></row><row><cell>bipolar,dissect,cystic-artery</cell><cell>187</cell><cell>grasper,pack,gallbladder</cell><cell>328</cell><cell>irrigator,dissect,omentum</cell><cell>100</cell></row><row><cell>bipolar,dissect,cystic-duct</cell><cell>183</cell><cell>grasper,retract,cystic-duct</cell><cell>469</cell><cell>irrigator,irrigate,abdominal-wall/cavity</cell><cell>413</cell></row><row><cell>bipolar,dissect,cystic-plate</cell><cell>54</cell><cell>grasper,retract,cystic-pedicle</cell><cell>41</cell><cell>irrigator,irrigate,cystic-pedicle</cell><cell>29</cell></row><row><cell>bipolar,dissect,gallbladder</cell><cell>353</cell><cell>grasper,retract,cystic-plate</cell><cell>1205</cell><cell>irrigator,irrigate,liver</cell><cell>130</cell></row><row><cell>bipolar,dissect,omentum</cell><cell>176</cell><cell>grasper,retract,gallbladder</cell><cell>48628</cell><cell>irrigator,null-verb,null-target</cell><cell>573</cell></row><row><cell>bipolar,grasp,cystic-plate</cell><cell>8</cell><cell>grasper,retract,gut</cell><cell>686</cell><cell>irrigator,retract,gallbladder</cell><cell>30</cell></row><row><cell>bipolar,grasp,liver</cell><cell>95</cell><cell>grasper,retract,liver</cell><cell>13646</cell><cell>irrigator,retract,liver</cell><cell>350</cell></row><row><cell>bipolar,grasp,specimen-bag</cell><cell>85</cell><cell>grasper,retract,omentum</cell><cell>4422</cell><cell>irrigator,retract,omentum</cell><cell>89</cell></row><row><cell>bipolar,null-verb,null-target</cell><cell>632</cell><cell>grasper,retract,peritoneum</cell><cell>289</cell><cell>scissors,coagulate,omentum</cell><cell>17</cell></row><row><cell>bipolar,retract,cystic-duct</cell><cell>8</cell><cell>hook,coagulate,blood-vessel</cell><cell>57</cell><cell>scissors,cut,adhesion</cell><cell>155</cell></row><row><cell>bipolar,retract,cystic-pedicle</cell><cell>9</cell><cell>hook,coagulate,cystic-artery</cell><cell>10</cell><cell>scissors,cut,blood-vessel</cell><cell>21</cell></row><row><cell>bipolar,retract,gallbladder</cell><cell>32</cell><cell>hook,coagulate,cystic-duct</cell><cell>41</cell><cell>scissors,cut,cystic-artery</cell><cell>613</cell></row><row><cell>bipolar,retract,liver</cell><cell>164</cell><cell>hook,coagulate,cystic-pedicle</cell><cell>15</cell><cell>scissors,cut,cystic-duct</cell><cell>808</cell></row><row><cell>bipolar,retract,omentum</cell><cell>69</cell><cell>hook,coagulate,cystic-plate</cell><cell>9</cell><cell>scissors,cut,cystic-plate</cell><cell>20</cell></row><row><cell>clipper,clip,blood-vessel</cell><cell>51</cell><cell>hook,coagulate,gallbladder</cell><cell>217</cell><cell>scissors,cut,liver</cell><cell>90</cell></row><row><cell>clipper,clip,cystic-artery</cell><cell>1097</cell><cell>hook,coagulate,liver</cell><cell>189</cell><cell>scissors,cut,omentum</cell><cell>27</cell></row><row><cell>clipper,clip,cystic-duct</cell><cell>1856</cell><cell>hook,coagulate,omentum</cell><cell>78</cell><cell>scissors,cut,peritoneum</cell><cell>56</cell></row><row><cell>clipper,clip,cystic-pedicle</cell><cell>13</cell><cell>hook,cut,blood-vessel</cell><cell>15</cell><cell>scissors,dissect,cystic-plate</cell><cell>12</cell></row><row><cell>clipper,clip,cystic-plate</cell><cell>53</cell><cell>hook,cut,peritoneum</cell><cell>92</cell><cell>scissors,dissect,gallbladder</cell><cell>52</cell></row><row><cell>clipper,null-verb,null-target</cell><cell>309</cell><cell>hook,dissect,blood-vessel</cell><cell>21</cell><cell>scissors,dissect,omentum</cell><cell>93</cell></row><row><cell>grasper,dissect,cystic-plate</cell><cell>78</cell><cell>hook,dissect,cystic-artery</cell><cell>2984</cell><cell>scissors,null-verb,null-target</cell><cell>171</cell></row><row><cell>grasper,dissect,gallbladder</cell><cell>644</cell><cell>hook,dissect,cystic-duct</cell><cell>7861</cell><cell></cell><cell></cell></row><row><cell>grasper,dissect,omentum</cell><cell>31</cell><cell>hook,dissect,cystic-plate</cell><cell>2898</cell><cell>Total</cell><cell>161005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the dataset split</figDesc><table><row><cell>Data split</cell><cell>Videos</cell><cell>Frames</cell><cell>Labels</cell></row><row><cell>Training</cell><cell>35</cell><cell>72815</cell><cell>113884</cell></row><row><cell>Validation</cell><cell>5</cell><cell>6797</cell><cell>10267</cell></row><row><cell>Testing</cell><cell>10</cell><cell>21251</cell><cell>36854</cell></row><row><cell>Total</cell><cell>50</cell><cell>100863</cell><cell>161005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Fig. 3: Weakly supervised localization (WSL) layer for instrument detection. Feature dimension (height H = 32, width W = 56, depth (class size) C I = 6).</figDesc><table><row><cell></cell><cell></cell><cell>IVT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Weakly-Supervised Localization (WSL) module</cell><cell></cell></row><row><cell>Decoder</cell><cell>L?</cell><cell>Feed Forward</cell><cell>AddNorm</cell><cell>X I H x W x 512</cell><cell>Conv 3x3 H x W x 64</cell><cell>H x W x C I Conv 1x1</cell><cell>CAM</cell><cell>H I H x W x C I C I GMP</cell><cell>Y I 1 x C I</cell></row><row><cell></cell><cell></cell><cell cols="2">Multi-Head of Mixed Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(MHMA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell>WSL</cell><cell>CAGAM</cell><cell>Bottleneck</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Low-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feature Extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Input Images from a Video</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>To obtained specific labels for the component tasks, we design a mapping</figDesc><table><row><cell></cell><cell></cell><cell>WSL</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Conv 3x3</cell><cell>Conv 1x1</cell><cell>CAM</cell><cell>GMP</cell><cell>Y I</cell></row><row><cell></cell><cell></cell><cell cols="2">H x W x 64</cell><cell>H x W x CI</cell><cell>C I</cell><cell>1 x C I</cell></row><row><cell>Feature extraction</cell><cell>X</cell><cell cols="3">instrument detection</cell></row><row><cell>ResNet-18</cell><cell>H x W x 512</cell><cell></cell><cell></cell><cell></cell><cell>H I</cell></row><row><cell>input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>triplets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CAGAM</cell><cell>Y V 1 x CV</cell></row><row><cell></cell><cell></cell><cell>X , X</cell><cell cols="3">verb-target detection</cell><cell>Y T 1 x C T</cell><cell>3Dis</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the task-attention suitability</figDesc><table><row><cell>Guided detection</cell><cell>AP V</cell><cell>AP T</cell></row><row><cell>None (as in MTL baseline)</cell><cell>48.4</cell><cell>28.2</cell></row><row><cell>CAM (as in Tripnet's CAG)</cell><cell>51.3</cell><cell>32.1</cell></row><row><cell>CAM + Channel attention</cell><cell>59.0</cell><cell>31.5</cell></row><row><cell>CAM + Position attention</cell><cell>51.2</cell><cell>35.1</cell></row><row><cell>CAM + Dual 1 attention</cell><cell>61.1</cell><cell>40.2</cell></row></table><note>1 Dual = (channel + position) attentions</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the attention type in the multi-head decoderModelLayer size AP IV AP IT AP IVT</figDesc><table><row><cell>Single Self</cell><cell>6</cell><cell>29.8</cell><cell>23.3</cell><cell>18.8</cell></row><row><cell>Multiple Self</cell><cell>6</cell><cell>35.7</cell><cell>32.8</cell><cell>26.1</cell></row><row><cell>Self + Cross (RDV)</cell><cell>6</cell><cell>39.4</cell><cell>36.9</cell><cell>29.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>A scalability study on the multi-head layer size: showing the mean average precision (mAP) for varying triplet associations, number of learning parameters (Params) in millions (M), and inference time (i-Time) in frame per seconds (FPS) on GTX 1080 Ti GPU.</figDesc><table><row><cell>Layer size</cell><cell>mAP IV (%)?</cell><cell>mAP IT (%)?</cell><cell>mAP IVT (%)?</cell><cell>Params (M)?</cell><cell>i-Time (FPS)?</cell></row><row><cell>1</cell><cell>35.8</cell><cell>30.7</cell><cell>24.6</cell><cell>12.6</cell><cell>54.2</cell></row><row><cell>2</cell><cell>36.0</cell><cell>41.1</cell><cell>27.0</cell><cell>13.1</cell><cell>47.9</cell></row><row><cell>4</cell><cell>38.5</cell><cell>32.9</cell><cell>27.3</cell><cell>14.3</cell><cell>39.2</cell></row><row><cell>8</cell><cell>39.4</cell><cell>36.9</cell><cell>29.9</cell><cell>16.6</cell><cell>28.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance summary of the proposed models compared to state-of-the-art and baseline models</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">Component detection</cell><cell></cell><cell>Triplet association</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IVT</cell></row><row><cell></cell><cell>CNN</cell><cell>57.7</cell><cell>39.2</cell><cell>28.3</cell><cell>21.7</cell><cell>18.0</cell><cell>13.6</cell></row><row><cell>Naive Baseline</cell><cell>TCN</cell><cell>48.9</cell><cell>29.4</cell><cell>21.4</cell><cell>17.7</cell><cell>15.5</cell><cell>12.4</cell></row><row><cell></cell><cell>MTL</cell><cell>84.5</cell><cell>48.4</cell><cell>28.2</cell><cell>26.6</cell><cell>21.2</cell><cell>17.6</cell></row><row><cell>SOTA</cell><cell>Tripnet (Nwoye et al., 2020)</cell><cell>92.1</cell><cell>54.5</cell><cell>33.2</cell><cell>29.7</cell><cell>26.4</cell><cell>20.0</cell></row><row><cell>Ours</cell><cell>Attention Tripnet Rendezvous</cell><cell>92.0 92.0</cell><cell>60.2 60.7</cell><cell>38.5 38.3</cell><cell>31.1 39.4</cell><cell>29.8 36.9</cell><cell>23.4 29.9</cell></row><row><cell cols="3">of increased computational requirements. To balance perfor-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mance and resource usage, we choose L = 8 as default settings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">in all our experiments. An 8-layer RDV with &gt; 25 FPS pro-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">cessing speed can be used in real-time for OR assistance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Top N Accuracy of the triplet predictions</figDesc><table><row><cell></cell><cell>Method</cell><cell>Top-5</cell><cell>Top-10</cell><cell>Top-20</cell></row><row><cell>Naive Baseline</cell><cell>CNN TCN MTL</cell><cell>67.0 54.5 70.2</cell><cell>80.0 69.4 80.2</cell><cell>90.2 84.3 89.5</cell></row><row><cell>SOTA</cell><cell>Tripnet</cell><cell>70.5</cell><cell>81.9</cell><cell>91.4</cell></row><row><cell>Ours</cell><cell>Attention Tripnet Rendezvous</cell><cell>75.3 76.3</cell><cell>86.0 88.7</cell><cell>93.8 95.9</cell></row><row><cell cols="4">6.1.5. Top-N Triplet Recognition Performance</cell><cell></cell></row><row><cell cols="5">In our multi-class problem with 100 action triplet classes,</cell></row><row><cell cols="5">getting a comprehensive view of a model's strength is difficult.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Top-10 predicted Triplets (AP IVT for Instrument-Verb-Target Interactions).</figDesc><table><row><cell>Tripnet (SOTA)</cell><cell></cell><cell>Attention Tripnet</cell><cell></cell><cell>Rendezvous</cell><cell></cell></row><row><cell>Triplet</cell><cell>AP</cell><cell>Triplet</cell><cell>AP</cell><cell>Triplet</cell><cell>AP</cell></row><row><cell>grasper,retract,gallbladder</cell><cell>77.30</cell><cell>grasper,grasp,specimen-bag</cell><cell>82.34</cell><cell>grasper,retract,gallbladder</cell><cell>85.34</cell></row><row><cell cols="2">grasper,grasp,specimen-bag 76.50</cell><cell>grasper,retract,gallbladder</cell><cell>78.41</cell><cell>grasper,grasp,specimen-bag</cell><cell>81.75</cell></row><row><cell>bipolar,coagulate,liver</cell><cell>67.39</cell><cell>bipolar,coagulate,liver</cell><cell>68.85</cell><cell>hook,dissect,gallbladder</cell><cell>75.90</cell></row><row><cell>hook,dissect,gallbladder</cell><cell>57.54</cell><cell>irrigator,dissect,cystic-pedicle</cell><cell>66.21</cell><cell>grasper,retract,liver</cell><cell>66.70</cell></row><row><cell>irrigator,aspirate,fluid</cell><cell>57.51</cell><cell>hook,dissect,gallbladder</cell><cell>63.22</cell><cell>bipolar,coagulate,liver</cell><cell>63.12</cell></row><row><cell>grasper,retract,liver</cell><cell>54.25</cell><cell>grasper,retract,liver</cell><cell>58.06</cell><cell>clipper,clip,cystic-duct</cell><cell>59.68</cell></row><row><cell>clipper,clip,cystic-artery</cell><cell>47.44</cell><cell>grasper,grasp,cystic-pedicle</cell><cell>55.35</cell><cell cols="2">bipolar,coagulate,blood-vessel 57.18</cell></row><row><cell>scissors,cut,cystic-duct</cell><cell>42.57</cell><cell>scissors,cut,cystic-artery</cell><cell>48.44</cell><cell>scissors,cut,cystic-artery</cell><cell>53.84</cell></row><row><cell>scissors,cut,cystic-artery</cell><cell>40.37</cell><cell>irrigator,aspirate,fluid</cell><cell>47.11</cell><cell>irrigator,aspirate,fluid</cell><cell>51.95</cell></row><row><cell>clipper,clip,cystic-duct</cell><cell>39.62</cell><cell cols="2">bipolar,coagulate,abdominal-wall-cavity 46.07</cell><cell>clipper,clip,cystic-artery</cell><cell>51.52</cell></row><row><cell>mean</cell><cell>56.05</cell><cell></cell><cell>61.41</cell><cell></cell><cell>64.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>The p-values obtained in Wilcoxon signed-rank test of the proposed methods using the SOTA model (Tripnet) as the alternative method. (Lower p-value is preferred)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Fig. 11: Attention maps in the CAGAM module on the CholecT50 test set. The left column is the input image, the subsequent columns are the attention maps captured by the different points as marked in the input image. The attention map shows the focus on the target (best seen in color).</figDesc><table><row><cell>Point 2</cell><cell>Point 3</cell><cell>Point 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table B .</head><label>B</label><figDesc>13: Result Breakdown for Per-Class Instrument (AP I ) Detection: The proposed models correctly detect all the various categories of the surgical instrument at a performance higher than 80.0%Table B.14: Result Breakdown for Verb (AP V ) Per-Class Recognition: The proposed models predict correctly the most dominantly used verbs such as retract, dissect, coagulate, clip, and cut over 70.0% of the timeTable B.15: Results Breakdown for Target (AP T ) Per-Class Detection. The target ids 1..14 correspond to gallbladder, cystic-plate, cystic-duct, cystic-artery, cystic-pedicle, blood-vessel, fluid, abdominal-wall-cavity, liver, omentum, peritoneum, gut, specimen-bag, null respectively 14.8 26.3 18.7 14.3 03.6 32.4 10.1 49.8 35.2 08.4 08.4 69.3 15.9 28.3 TCN 79.9 10.0 21.4 19.6 07.0 01.3 14.8 06.9 43.1 27.9 01.9 09.0 37.4 15.4 21.4 MTL 85.1 12.2 29.3 18.6 06.5 06.4 30.6 09.8 55.7 35.8 02.1 08.4 71.1 17.5 28.2 SOTA Tripnet 87.0 22.5 29.7 21.9 04.7 15.0 42.9 32.3 57.5 36.7 02.0 11.9 74.1 20.9 33.2 Ours Attention Tripnet 87.8 15.6 37.1 30.1 16.6 26.5 53.2 37.5 59.8 48.7 03.5 08.3 85.6 23.5 38.5 Rendezvous 89.1 15.3 35.2 34.5 22.7 11.4 53.7 40.6 59.3 46.6 04.3 12.5 84.0 25.0 38.3Table B.16: Top-10 Predicted Instrument-Verb (AP IV ) Association: The proposed models show a higher capability of detecting the top combinations that represent the most likely usage pattern of the individual instrument class, as well as the most clinical relevant instrument roles.Table B.17: Top-10 Predicted Instrument-Target (AP IT ) : The proposed models predict well the clinically most relevant situations, which are clipping and cutting of cystic-artery and cystic-duct among their top detected instrument-target labels.Table B.18: Ablation Study on Attention Sequence Modeling: This study shows the usefulness of our class-wise mapping over the contemporally patch-base sequence in Vision Transformer<ref type="bibr" target="#b18">(Dosovitskiy et al., 2020)</ref> Model Layer Size AP IV AP IT AP IVTTable B.19: Ablation Study on Use of Auxiliary Loss: This study shows that learning the individual components of the triplets in the same network pipeline helps the model to better understand the triplets.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="9">Grasp Retract Dissect Coagulate Clip</cell><cell>Cut</cell><cell>Aspirate Irrigate Pack Null mAP</cell></row><row><cell>Naive Baseline Triplet</cell><cell cols="2">CNN TCN Tripnet MTL</cell><cell>AP</cell><cell>48.6 24.9 47.9</cell><cell></cell><cell cols="2">82.1 80.2 85.0 Triplet 80.5 66.4 84.8</cell><cell cols="3">30.5 27.4 Attention Tripnet 55.0</cell><cell cols="3">49.5 23.8 31.9 14.7 79.1 44.1 AP</cell><cell>32.4 14.8 35.4</cell><cell>16.0 13.9 13.4 Triplet</cell><cell>09.2 2.0 Rendezvous 15.9 15.4 18.0 17.0</cell><cell>39.2 29.4 48.4 AP</cell></row><row><cell cols="4">SOTA Ours grasper,gallbladder Tripnet Attention Tripnet 82.49 Rendezvous grasper,specimen-bag 76.50 bipolar,liver 67.09</cell><cell>45.8 62.4 60.4</cell><cell></cell><cell cols="4">88.1 89.4 grasper,gallbladder 86.7 66.3 89.4 69.7 90.5 89.5 68.7 grasper,specimen-bag bipolar,liver</cell><cell></cell><cell cols="3">85.1 68.3 88.5 84.3 83.65 86.7 87.8 82.34 68.37</cell><cell>44.9 48.5 50.4</cell><cell>12.2 20.8 grasper,gallbladder 22.5 20.1 21.4 22.7 17.4 30.5 21.0 grasper,specimen-bag 81.75 54.5 89.96 60.2 60.7 hook,gallbladder 76.20</cell></row><row><cell cols="3">hook,gallbladder</cell><cell>57.96</cell><cell></cell><cell></cell><cell cols="3">hook,gallbladder</cell><cell></cell><cell></cell><cell></cell><cell cols="2">63.82</cell><cell>grasper,liver</cell><cell>66.22</cell></row><row><cell cols="2">irrigator,fluid</cell><cell></cell><cell>57.51</cell><cell></cell><cell></cell><cell cols="3">grasper,liver</cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.26</cell><cell>bipolar,liver</cell><cell>62.45</cell></row><row><cell cols="3">clipper,cystic-artery</cell><cell>47.44</cell><cell></cell><cell></cell><cell cols="4">irrigator,cystic-pedicle</cell><cell></cell><cell></cell><cell cols="2">49.38</cell><cell>clipper,cystic-duct</cell><cell>59.68</cell></row><row><cell cols="2">grasper,liver</cell><cell></cell><cell>44.52</cell><cell></cell><cell></cell><cell cols="4">scissors,cystic-artery</cell><cell></cell><cell></cell><cell cols="2">48.44</cell><cell>bipolar,blood-vessel</cell><cell>57.18</cell></row><row><cell cols="3">scissors,cystic-duct</cell><cell>42.57</cell><cell></cell><cell></cell><cell cols="3">grasper,omentum</cell><cell></cell><cell></cell><cell></cell><cell cols="2">47.20</cell><cell>grasper,omentum</cell><cell>54.98</cell></row><row><cell cols="3">scissors,cystic-artery</cell><cell>40.37</cell><cell></cell><cell></cell><cell cols="3">irrigator,fluid</cell><cell></cell><cell></cell><cell></cell><cell cols="2">47.11</cell><cell>scissors,cystic-artery</cell><cell>53.84</cell></row><row><cell cols="3">clipper,cystic-duct</cell><cell>39.62</cell><cell></cell><cell></cell><cell cols="8">bipolar,abdominal-wall-cavity 46.07</cell><cell>irrigator,fluid</cell><cell>51.95</cell></row><row><cell>mean</cell><cell></cell><cell></cell><cell>55.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">59.06</cell><cell>65.42</cell></row><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Target</cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell cols="2">7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell></row><row><cell>Naive Baseline</cell><cell cols="4">Method CNN 84.2 Tripnet</cell><cell cols="2">Grasper</cell><cell cols="8">Bipolar Attention Tripnet Hook Patch-base sequence Scissors Class-wise mapping Class-wise mapping</cell><cell>Clipper 6 2 8</cell><cell>Irrigator Rendezvous 33.4 29.3 36.0 34.1 39.4 36.9</cell><cell>mAP 24.1 27.0 29.9</cell></row><row><cell cols="3">Naive Triplet Baseline bipolar,coagulate CNN TCN MTL</cell><cell cols="2">AP 88.71</cell><cell></cell><cell>91.4 90.5 95.5</cell><cell cols="2">47.9 37.6 85.8 grasper,retract Triplet</cell><cell></cell><cell cols="2">89.1 86.2 96.6</cell><cell>AP 90.29</cell><cell>24.0 15.9 74.8</cell><cell>50.2 33.3 Triplet 85.8 grasper,retract</cell><cell>43.2 29.6 68.2</cell><cell>57.7 AP 48.9 84.5 90.51</cell></row><row><cell cols="2">SOTA grasper,retract hook,dissect</cell><cell>Tripnet</cell><cell cols="2">87.58 86.88</cell><cell></cell><cell>97.8</cell><cell cols="3">91.2 hook,dissect bipolar,coagulate</cell><cell cols="2">98.1</cell><cell>87.18 78.17</cell><cell>90.7</cell><cell>92.1 hook,dissect bipolar,coagulate 82.7</cell><cell>90.38 92.1 88.05</cell></row><row><cell cols="2">scissors,cut Ours clipper,clip</cell><cell cols="3">Attention Tripnet 68.93 Rendezvous 67.10</cell><cell></cell><cell>97.8 97.7</cell><cell cols="2">91.5 scissors,cut 89.4 clipper,clip</cell><cell></cell><cell cols="2">98.1 98.1</cell><cell>77.99 70.66</cell><cell>89.7 92.0</cell><cell>92.8 scissors,cut 92.2 clipper,clip</cell><cell>82.1 82.7</cell><cell>92.0 86.40 92.0 82.65</cell></row><row><cell cols="3">irrigator,aspirate grasper,grasp</cell><cell cols="2">57.51 23.54</cell><cell></cell><cell></cell><cell cols="4">irrigator,aspirate grasper,grasp Model</cell><cell></cell><cell>57.10 37.1</cell><cell></cell><cell>irrigator,aspirate grasper,grasp AP IV AP IT</cell><cell>51.95 48.97 AP IVT</cell></row><row><cell cols="3">irrigator,null-verb</cell><cell cols="2">16.28</cell><cell></cell><cell></cell><cell cols="2">irrigator,dissect</cell><cell cols="5">24.49 Without aux-loss</cell><cell>grasper,null-verb 33.6 27.0</cell><cell>28.91 21.2</cell></row><row><cell cols="3">clipper,null-verb</cell><cell cols="2">16.10</cell><cell></cell><cell></cell><cell cols="6">grasper,null-verb With aux-loss 20.81</cell><cell></cell><cell>scissors,null-verb 36.0 34.1</cell><cell>21.68 29.9</cell></row><row><cell cols="3">grasper,null-verb</cell><cell cols="2">12.47</cell><cell></cell><cell></cell><cell cols="2">irrigator,irrigate</cell><cell></cell><cell></cell><cell></cell><cell>16.47</cell><cell></cell><cell>irrigator,dissect</cell><cell>20.42</cell></row><row><cell>mean</cell><cell></cell><cell></cell><cell cols="2">52.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.03</cell><cell></cell><cell>60.99</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/CAMMA-public/tripnet 3 https://github.com/CAMMA-public/rendezvous 4 https://github.com/CAMMA-public/attention-tripnet 5 http://camma.u-strasbg.fr/datasets</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by French state funds managed within the Investissements d'Avenir program by BPI France (project CONDOR) and by the ANR under references ANR-11-LABX-0004 (Labex CAMI), ANR-16-CE33-0009 (Deep-Surg), ANR-10-IAHU-02 (IHU Strasbourg) and ANR-20-CHIA-0029-01 (National AI Chair AI4ORSafety). It was granted access to HPC resources of Unistra Mesocentre and GENCI-IDRIS (Grant 2021-AD011011638R1). The authors also thank the IHU and IRCAD research teams for their help with the data annotation during the CONDOR project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos</head><p>===== Supplementary Material ===== Appendix A. Additional Dataset Statistics  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Supplementary Video</head><p>Vid. D.1: A video showing some qualitative results of the RDV model for triplet prediction, action region localization, and attention maps is included as supplementary material, accessible online via https://youtu.be/d yHdJtCa98</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovery of surgical workflow without explicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sielhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monitoring tool usage in surgery videos using boosted convolutional and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="203" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">robotic scene segmentation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pitfalls of laparoscopic surgery: challenges for robotics and telerobotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Ballantyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Laparoscopy Endoscopy &amp; Percutaneous Techniques</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Bawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kapinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Skarga-Bandurova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oleari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leporini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Landolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03178</idno>
		<title level="m">The saras endoscopic surgeon action detection (esad) dataset: Challenges and methods</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of surgical workflow from laparoscopic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feu?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video based activity recognition in trauma resuscitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Burd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
	<note>ieee winter conference on applications of computer vision (wacv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated surgical step recognition in normalized cataract surgery videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Charriere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4647" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Opera: Attention-regularized transformers for surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03873</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmenting and classifying activities in robot-assisted surgery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waldram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing surgical activities with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feasibility and value of the critical view of safety in difficult cholecystectomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Felli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wakabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pessaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of surgery</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal coherence-based self-supervised learning for laparoscopic workflow analysis, in: OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Image-Based Procedures, and Skin Image Analysis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
		<respStmt>
			<orgName>Computer Assisted Robotic Endoscopy</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Trans-svnet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09712</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toolnet: holistically-nested real-time segmentation of robotic surgical tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gruijthuijsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devreker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Attilakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vander Poorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5717" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Actor-transformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognising humanobject interaction via exemplar based modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3144" to="3151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Offline identification of surgical deviations in laparoscopic rectopexy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Faucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moreau-Gaudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">101837</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lapontospm: an ontology for laparoscopic surgeries and its application to surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kati?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Julliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gibaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1427" to="1434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-driven formalization of laparoscopic surgeries for rule-based intraoperative context-aware assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kati?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Computer-Assisted Interventions</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Proposing novel methods for gynecologic surgical action recognition on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dezyani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="30111" to="30133" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hotr: End-toend human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13682</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Realtime automatic surgical phase recognition in laparoscopic sigmoidectomy using the convolutional neural network-based deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kitaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takeshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Owada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamanashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Endoscopy</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Surgical action retrieval for assisting video review of laparoscopic skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kletz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>M?nzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Primus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husslein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Workshop on Multimedia-based Educational and Knowledge Technologies for Personalized and Social Online Training</title>
		<meeting>the 2017 ACM Workshop on Multimedia-based Educational and Knowledge Technologies for Personalized and Social Online Training</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting visual relationships using box attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lapformer: surgical tool detection in laparoscopic surgical video using transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Temporal convolutional networks: A unified approach to action segmentation, in: European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assisted phase and step annotation for surgical videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ragot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Action-guided attention mining and relation reasoning network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10804</idno>
		<title level="m">Cptr: Full transformer network for image captioning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Episode classification for the analysis of tissue/instrument interaction with multiple visual cues, in: Int. conference on medical image computing and computer-assisted intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Episode classification for the analysis of tissue/instrument interaction with multiple visual cues, in: Int. conference on medical image computing and computer-assisted intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Smoke detection in endoscopic surgery videos: a first step towards retrieval of semantic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Medical Robotics and Computer Assisted Surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Surgical data science: Enabling next-generation surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How do i do it: laparoscopic cholecystectomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Altieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Brunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Laparoscopic and Endoscopic Surgery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning models for actions and personobject interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="414" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">System events: readily accessible features for surgical phase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1209" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Artificial intelligence for surgical safety: automatic assessment of the critical view of safety in laparoscopic cholecystectomy using deep learning. Annals of Surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alapatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Urade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Emre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fiorillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pessaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costamagna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fusatnet: Dual attention based spectrospatial multimodal fusion network for hyperspectral and lidar classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="92" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Acquisition of process descriptions from surgical interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strau?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meixensberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Burgert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database and Expert Systems Applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recognition of instrument-tissue interactions in endoscopic videos via action triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention u-net: Learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Early and late fusion of temporal information for classification of surgical actions in laparoscopic gynecology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petscharnig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sch?ffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaabouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Outcome trends and safety measures after 30 years of laparoscopic cholecystectomy: a systematic review and pooled data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Brunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Linsk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fingerhut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asbun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical endoscopy</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2175" to="2183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dall&amp;apos;alba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12218</idno>
		<title level="m">Multi-task temporal convolutional networks for joint recognition of surgical phases and steps in gastric bypass procedures</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sensor substitution for video-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5230" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Surgical phase recognition by learning phase transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Temporal attention model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02927</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Epidemiology of gallbladder stone disease. Best practice &amp; research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Shaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical gastroenterology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="981" to="996" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Scaling humanobject interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">End-to-end attention-based image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14721</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast part-based classification for instrument detection in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Data-driven spatio-temporal rgbd feature encoding for action recognition in operating rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Alkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="737" to="747" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Endonet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13617" to="13626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Medical transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10662</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Weaklysupervised learning for tool localization in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05573</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Laparoscopic vs open surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Velanovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical endoscopy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cai4cai: the rise of contextual artificial intelligence in computer-assisted interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Automatic detection of instruments in laparoscopic images: A first step towards high-level command of robotic endoscopic holders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cinquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1173" to="1190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Comparative validation of machine learning algorithms for surgical workflow and skill analysis with the heichole benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kisilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>M?ndermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Lubotsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davitashvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14956</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep contextual attention for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5694" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Learning domain adaptation with model calibration for surgical report generation in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17120</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Saliency guided self-attention network for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14413" to="14423" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00033</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Surgical activity recognition in robot-assisted radical prostatectomy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deepphase: surgical phase recognition in cataracts videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04503</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
