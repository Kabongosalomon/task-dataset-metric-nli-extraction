<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MENTION MEMORY: INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH EN- TITY MENTION ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">? Google</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">? Google</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">? Google</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><forename type="middle">?</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">? Google</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">? Google</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MENTION MEMORY: INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH EN- TITY MENTION ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with "mention memory", a table of dense vector representations of every entity mention in a corpus. The proposed model -TOME -is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural models have greatly advanced the state of the art in natural language processing and generation tasks. Accordingly, there has been increasing interest in applying neural language models to tasks which require extensive world knowledge to solve <ref type="bibr" target="#b18">(Petroni et al., 2021)</ref>. Much of this world knowledge can be found distributed over text corpora, which raises the question whether language models pre-trained on text corpora capture this information. Recent work suggests that while language models may successfully predict facts about the world <ref type="bibr" target="#b17">(Petroni et al., 2019)</ref> such knowledge is superficial and unreliable . Our goal is to reliably incorporate information from across a text corpus into a language model.</p><p>Recent work has represented the information present in a text corpus explicitly by constructing a virtual knowledge base (VKB) <ref type="bibr" target="#b2">(Dhingra et al., 2020;</ref>. A VKB consists of dense representations of entity mentions in the text, designed to reflect the property or relation expressed by the entity mention. We propose to incorporate a VKB into a language model by using it as an external memory, performing attention over the entire VKB within a Transformer model. In this way the model can synthesise and reason over many disparate sources of information from the text corpus. We refer to the VKB used in such a way as Mention Memory, and the model as TOME <ref type="bibr">(Transformer Over Mention Encodings)</ref>. We first pre-train a mention encoder to specifically encourage mention representations that are useful for a Transformer model, and construct a Mention Memory from 150 million entity mentions in English Wikipedia. Then we train a TOME model with attention layers over the Mention Memory, which is kept frozen (see <ref type="figure">Figure 1</ref>).</p><p>We argue that the Mention Memory approach has several appealing properties. First, TOME retrieves entity mention representations corresponding to specific entity attributes or relations described in the Figure 1: Overview of Mention Memory. A pre-trained mention encoder is used to generate dense representations for each entity mention in Wikipedia (approximately 150 million total) which are stored in a table. The TOME model takes a passage annotated with entity mention boundaries as input, and applies a Transformer block. Next, the TOME model applies one or more TOMEBlocks. Each TOMEBlock contains a memory attention layer and a Transformer block.</p><p>corpus. This retrieval is much more fine-grained than aggregate entity retrieval methods such as Entities as Experts (EaE) <ref type="bibr" target="#b4">(F?vry et al., 2020)</ref>, and we show large improvements in accuracy over EaE on tasks that require detailed entity information, such as claim verification and entity-based question answering. The fine-grained retrieval also allows potential users to see more precisely what knowledge the model's predictions is based on (see <ref type="table">Table 4</ref>). Second, TOME retrieves dense representations, which are easy to incorporate into a Transformer model without reprocessing the input, unlike raw text. Therefore, TOME is able to retrieve, assimilate and reason over information from many different sources within a single Transformer model, allowing for multi-source and multi-hop reasoning without the beam search machinery that is required for multi-hop retrieve-and-read <ref type="bibr" target="#b25">(Zhao et al., 2021)</ref>. This also makes TOME much more scalable: retrieve-and-read approaches have to read many retrieved passages which becomes expensive with larger reader models, while the cost of memory layers does not scale with reader size and is negligible for larger readers. Third, the retrieval is latent, without direct or distant supervision on the retrieved results. We show that, even without supervision, the model learns to retrieve highly specific and informative entity attributes and perform multiple reasoning steps. Finally, the memory table is semi-parametric, so knowledge can be added or updated by applying the mention encoder to new text without retraining.</p><p>In order to verify the model's capacity to capture accurate factual information in the corpus, we start by evaluating TOME on the HoVer <ref type="bibr" target="#b9">(Jiang et al., 2020)</ref>, FEVER <ref type="bibr" target="#b22">(Thorne et al., 2018) and</ref><ref type="bibr">FM2 (Eisenschlos et al., 2021)</ref> claim verification datasets, on which it strongly improves performance over entity aggregate and comparable retrieve-and-read baselines. We demonstrate that the model learns to attend to informative mentions for verifying claims using only the verification accuracy as a signal. Ablations show the memory is crucial for performance, and that the model can effectively use larger memory than it was pre-trained on. In a second set of experiments we evaluate TOME on question-answering benchmarks TriviaQA <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref>, ComplexWebQuestions <ref type="bibr" target="#b21">(Talmor &amp; Berant, 2018)</ref> and EntityQuestions <ref type="bibr" target="#b19">(Sciavolino et al., 2021)</ref>, improving performance over comparable baselines. Finally we show that the model can be adapted to generalize to new unseen entities by updating the memory, without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>Our method represents knowledge in a corpus as a collection of "mention encodings" -dense vector representations for every entity mention that appears in the corpus. Every time an entity appears in a passage -"[Barack Obama] was elected president in 2008" -some property of the entity or its relation to other entities is described. The first component of our method, the Mention Encoder model, is responsible for distilling information from entity mentions in the corpus into high-dimensional mention encodings. We use the Mention Encoder to encode each entity mention in English Wikipedia and gather encodings into a Mention Memory. The purpose of the Mention Memory is to capture all knowledge contained in the corpus in a way that can be easily integrated into a Transformer. The second component of our method, the TOME model, applies sparse attention over the Mention Memory to incorporate external information from the corpus into a Transformer model. An overview of the whole method is shown in <ref type="figure">Figure 1</ref>.</p><p>Jointly training the Mention Encoder and TOME models is computationally costly, since it would require backpropagating through the Mention Encoder for each attended mention. Consequently, we propose to train the models in two stages. First, we pre-train the Mention Encoder and generate the Mention Memory. Second, we pre-train the TOME model while keeping the Mention Memory frozen: the gradient does not propagate through it and the memories are not modified. Mention Encoder pre-training is specifically designed such that mention encodings capture relevant contextual information about each mention and are useful for TOME even without joint training. We formally define these models in sections 2.1 and 2.2, and their pre-training procedures in 2.3 and 2.4.</p><p>Notation. An input to the model is a passage x = x 1 , . . . , x T of length T . We assume that each passage has been annotated with an NER system. Following Baldini Soares et al. (2019) we use special entity markers to highlight entity mentions in the passage. We introduce tokens [E start ] and [E end ] to the vocabulary and insert them before and after each mention in the passage. For example, the original passage "What is the nationality of the hero who killed Medusa" turns into "What is the</p><formula xml:id="formula_0">[E start ] nationality [E end ] of the [E start ] hero [E end ] who killed [E start ] Medusa [E end ]".</formula><p>Each mention m in a passage is described by a tuple (s, e), where s and e are start and end positions of the mention. We consider entity markers to be part of the corresponding mention, so that x s = [E start ] and x e = [E end ]. Representations of these tokens are later used to generate mention encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CONSTRUCTING MENTION MEMORY FROM CORPUS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">MENTION ENCODER</head><p>Let H ? R T ?d be token representations where d is the hidden dimension, such that H i ? R d is the contextualized embedding for the i-th token. Following <ref type="bibr" target="#b4">F?vry et al. (2020)</ref> we compute the encoding of a span (s, e) as a learnable linear projection W of the concatenation of its start and end token representations H s and H e SpanEncodingLayer(H, (s, e)) = W [H s ; H e ] (1) The Mention Encoder is a Transformer model with two final SpanEncodingLayers that produce key and value mention encodings. Value mention encodings store context-level information about each mention and are used as inputs to the TOME model. Key mention encodings identify the type of information stored in the value encodings and serve as attention keys for the memory layer. These two SpanEncodingLayers do not share weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">MENTION MEMORY</head><p>After the Mention Encoder is pre-trained (see section 2.3), we use it to generate a Mention Memory from entity mentions in Wikipedia. While we could include encodings of any corpus mention in the Mention Memory, we focus on grounded mentions which can be linked to Wikipedia entities. We denote these as linked mentions, which we hypothesize contain information that can be retrieved and grounded. We gather mention encodings into matrices MemKey ? R N ?d K and MemValue ? R N ?d V , where N is the total number of linked entity mentions in English Wikipedia (approximately 150 million) and d K and d V are dimensions of key and value encodings. Additionally, we record entity (Wikipedia) IDs of mentions in MemEnt ? R N , which we use as labels for auxiliary losses, not as inputs to the model or supervision on retrieval. MemKey(i), MemValue(i), MemEnt(i) correspond to the key encoding, value encoding and entity ID for the i-th linked mention in Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TOME MODEL</head><p>The TOME model incorporates information from a text corpus into a Transformer by applying sparse attention over the Mention Memory. The model consists of one or more TOMEBlocks, each containing a memory attention layer followed by a post-processing Transformer block. Memory attention layers retrieve and attend to relevant "memories" for every mention in the input passage. The model then processes the retrieval-augmented representation with the Transformer block, allowing it to access and combine information from multiple sources in the corpus. Finally, multiple TOMEBlocks enable the model to refine retrievals and perform multi-hop reasoning. More formally, a TOMEBlock receives the output representation of the previous layer H and produces new representations H M = MemoryAttention(H),</p><formula xml:id="formula_1">H = TransformerBlock(M )<label>(2)</label></formula><p>The TOME model encodes input passages x with the word embedding layer and initial Transformer block and then applies one or more TOMEBlocks</p><formula xml:id="formula_3">H 0 = InitialTransformerBlock(TokenEmbedding(x)),<label>(4)</label></formula><formula xml:id="formula_4">H l = TOMEBlock l (H l?1 ), l = 1 . . . L<label>(5)</label></formula><p>In this work we consider two configurations of the TOME model: TOME-1 and TOME-2, with one and two TOMEBlocks respectively. Each TOMEBlock of TOME-2 contains half as many Transformer layers as in TOME-1 to hold the total number of Transformer layers fixed between models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">MemoryAttention</head><p>Each memory attention layer is implemented as a sparse dot-product attention layer that takes the output H of the previous Transformer block, incorporates information from the Mention Memory, and returns a representation M (omitting layer indices). Consider a mention m that starts at position s and ends at position e. We start by computing its query mention encoding Query(m) by applying a SpanEncodingLayer</p><formula xml:id="formula_5">Query(m) = SpanEncodingLayer(H, (s, e)),<label>(6)</label></formula><p>Query mention encodings are used to retrieve relevant memories from the Mention Memory table. However, applying standard attention over 150 million mention encodings is infeasible. Instead, we first perform approximate nearest neighbor search to retrieve the top-K mentions with the largest dot product between query Query(m) and key mention encoding from MemKey. We denote the set of these memories as TopMem(Query(m)). We compute attention over these memories and incorporate the result into the token contextual representation at position s</p><formula xml:id="formula_6">? i ? exp(Query(m) ? MemKey(i)), i ? TopMem(Query(m))<label>(7)</label></formula><p>Value(m) = i?TopMem(Query(m))</p><formula xml:id="formula_7">? i ? MemValue(i) (8) M s = LayerNorm(H s + W U Value(m))<label>(9)</label></formula><p>where W U is a learnable matrix of shape d ? d V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">SPARSE LARGE-SCALE RETRIEVAL</head><p>Approximate nearest neighbor search (ANNS) can be performed cheaply using one of multiple ANNS libraries, for example ScaNN <ref type="bibr" target="#b6">(Guo et al., 2020)</ref>. We implemented two on-device search methods to avoid the engineering complexity of real-time communication with an ANNS server, though we have verified this is also viable. The first naively computes a simple dot-product between passage queries and memory keys, and was used in our main experiments as it was easiest to implement. We also implemented and will be releasing a much faster version based on CPU ANNS methods. The memory is sharded over devices, so that the device-memory overhead is negligible.</p><p>Holding the number of entries in memory fixed, the compute cost of retrieval from memory does not grow with the size of the reader or the dimensionality of the memory values, so that the relative cost of the memory layer becomes smaller with reader size. In particular, the overhead from the memory used in our pre-training setting is small for BERT-Large and up. More details on ANNS implementation and overhead can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">MENTION ENCODER PRE-TRAINING</head><p>While backpropagating through a Wikipedia-scale mention memory is challenging, it is possible to train smaller-scale memory architectures end-to-end. We take an approach inspired by MARGE <ref type="bibr" target="#b13">(Lewis et al., 2020a)</ref> and READTWICE <ref type="bibr" target="#b24">(Zemlyanskiy et al., 2021)</ref> which apply crossattention over documents within a batch. In particular, we process passages in each batch twice. As a first step, the Mention Encoder model generates mention encodings from each passage and aggregates the mention encodings into a batch-wide memory table. In the second step, we apply a TOME architecture that attends to the batch memory, which we call BATCH-TOME. Note that BATCH-TOME is just used for pre-training the Mention Encoder and not evaluated on any downstream tasks.</p><p>Mention Encoder and BATCH-TOME are jointly trained end-to-end so that the Mention Encoder is encouraged to produce mention encodings that contain useful information for BATCH-TOME.</p><p>We want to make sure the batch memory contains relevant mentions, so we pre-train the models on batches of passages constructed from related Wikipedia articles with high entity overlap. Appendix A.1 provides more details on Mention Encoder data generation. We use the pre-trained Mention Encoder to construct the Mention Memory table from corpus, and use the BATCH-TOME model as the initialization point for TOME-specific pre-training (described in Section 2.4).</p><p>Masked language model. Our primary pre-training objective is the standard masked language modeling task, with the loss computed based on the output of the second read (BATCH-TOME). To encourage the model to rely on memory, we increase the task's difficulty relative to standard BERT pre-training by masking entity mention tokens more aggressively.</p><p>Coreference resolution. We wish to encourage the Mention Encoder to represent the entity attributes expressed by entity mentions, so we also employ an entity-oriented pre-training task to the output of BATCH-TOME for which such attribute information is likely to be especially helpful. Unlike Entities as Experts <ref type="bibr" target="#b4">(F?vry et al., 2020)</ref>, BATCH-TOME does not use entity embeddings, so we cannot use the entity linking task. Instead, we apply a related entity coreference resolution objective, which asks the model to predict whether two linked mentions correspond to the same entity based on the similarity of their encodings. Given that entity surface forms are frequently masked, the model needs to instead use the properties of other mentions in the batch to determine which entity it is most compatible with, incentivizing the Mention Encoder to encode such properties. We compute a coreference mention encoding for every linked mention in the batch by applying a separate SpanEncodingLayer on the output of BATCH-TOME. The loss is implemented using cross-entropy over dot-product similarity scores. See Appendix A.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">TOME PRE-TRAINING</head><p>As TOME attends to the full Mention Memory instead of in-batch memory, we do not employ the batching procedure from Mention Encoder pre-training, instead sampling Wikipedia passages randomly. For the same reason, we replace the in-batch entity coreference objective by Mention Memory entity coreference, in which the model has to predict which mentions from the Mention Memory share an entity with the input mention. The goal of this auxiliary objective is to incentivize the model to learn to retrieve informative mention encodings to solve the semantically challenging task. Mention Memory entity coreference also allows us to solve tasks like TriviaQA or ComplexWebQA without a decoder by directly predicting the answer entity.</p><p>Entity prediction. Analogous to batch coreference resolution loss we compute mention encoding z m using the output of the TOME model. As in section 2.2, TopMem(z m ) returns the top K memories with the largest dot product between the mention encodings z m and key mention encodings MemKey from the Mention Memory. The score EntProb(m, j) of entity j equals the sum of attention weights of memories corresponding to this entity.</p><formula xml:id="formula_8">EntProb(m, j) = i?TopMem(zm) exp(z m ? MemKey(i)) ? 1{MemEnt(i) = j} i?TopMem(zm) exp(z m ? MemKey(i))<label>(10)</label></formula><p>The final entity prediction is arg max j EntProb(m, j). Entity prediction loss L ep (m) for a mention m of entity Ent(m) is L ep (m) = ? log EntProb(m, Ent(m)). Total loss equals the average loss over linked input mentions for which at least one memory of the same entity is retrieved.</p><p>Disallowed same passage retrieval. For each passage in the pre-training corpus, there exist memories corresponding to mentions in the passage generated from the unmasked version of the same passage. In order to prevent the model from 'cheating' by attending to such memories, we set the attention weight for all memories from the same passage to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Our approach lies at the intersection of three lines of work: i) knowledge-augmented language models, ii) employing a text corpus as a virtual knowledge base, iii) retrieve-and-read methods.</p><p>Knowledge-augmented language models. Entities as Experts (EaE) <ref type="bibr" target="#b4">(F?vry et al., 2020)</ref> injects information into a Transformer model model with an intermediate attention layer over trainable entity embeddings, which serve as an aggregate representation of entity information in a text corpus. In contrast, TOME attends to a much larger table of mention encodings, allowing for retrieval of more fine-grained information. Attending to mentions as opposed to entity representations also enables TOME to generalize to unseen entities. FiLM  extends EaE by adding an attention layer over facts from a KB on the output of the Transformer. The fact attention layer enables more fine-grained queries but still retrieves aggregate entity embeddings as values, which are also not reasoned over by the Transformer. KnowBERT <ref type="bibr" target="#b16">(Peters et al., 2019</ref>) is similar to EaE, but with entity embeddings generated from a KB instead of trained end-to-end with a text corpus. MARGE <ref type="bibr" target="#b13">(Lewis et al., 2020a)</ref> and READTWICE <ref type="bibr" target="#b24">(Zemlyanskiy et al., 2021)</ref> incorporate dense representations from other passages within the same batch into a Transformer through sparse top-k attention. The first pre-training stage of our method for training the Mention Encoder is similar to MARGE and READ-TWICE. However, TOME performs global attention over a full corpus, rather than a single batch. Furthermore, TOME attends to a Mention Memory consisting of pre-computed dense representations. Therefore TOME is not limited to downstream task with batches of relevant documents, and does not need to apply an expensive reader model to an entire batch of documents for each input.</p><p>Text corpus as virtual knowledge base. DrKIT <ref type="bibr" target="#b2">(Dhingra et al., 2020)</ref> performs multi-hop question answering by using a text corpus as a virtual knowledge base. Similar to TOME, the authors apply a mention encoder to convert the corpus into a table of mention encodings. A Transformer model encodes the question into dense queries, which are compared with the mention encodings to traverse the VKB. Conversely, TOME retrieves mention encodings, and then jointly processes them inside the Transformer. In follow-up work to DrKIT, OPQL  uses a FiLM-like approach to access a memory of relation mentions, which are encoded with a self-supervised relation encoder. However, the relation mention encoding combines a mention-specific relation representation with EaE-like entity encodings, so they are less fine-grained than TOME's encodings. Unlike TOME, OPQL also lacks a sparse large-scale retrieval mechanism, and relies on ad hoc heuristics to limit the size of the memory. 1 MOLEMAN (FitzGerald et al., 2021) compares a passage mention encoding with mention encodings from a corpus to perform entity linking, but does not retrieve the mentions.</p><p>Retrieve-and-read methods. REALM <ref type="bibr" target="#b7">(Guu et al., 2020)</ref> learns to retrieve relevant passages from a text corpus in a self-supervised manner. Retrieved passages are concatenated to the input passage which is then re-encoded by a Transformer model to perform a task. The key difference between retrieve-and-read approaches <ref type="bibr" target="#b7">(Guu et al., 2020;</ref><ref type="bibr" target="#b12">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b14">Lewis et al., 2020b;</ref><ref type="bibr" target="#b8">Izacard &amp; Grave, 2021)</ref> and TOME is that TOME retrieves dense representations, as opposed to text. That means that TOME only applies a reader model once to a single input, while retrieve-and-read approaches have to apply an expensive BERT reader to many different passages. In addition, Transformer models can only process relatively short sequences, which imposes a binding constraint on the number of retrieved text passages that can be processed together, whereas TOME can retrieve and reason over information from many sources inside the same reader. Generative models like RAG <ref type="bibr" target="#b14">(Lewis et al., 2020b)</ref> or FiD <ref type="bibr" target="#b8">(Izacard &amp; Grave, 2021)</ref> attend to different retrieved documents in the decoder, but still have to apply a BERT read for every retrieved document, do not consider interaction between retrievals while encoding the question, and cannot perform iterative retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>The Mention Encoder is based on a BERT-base model with two final SpanEncodingLayers that produce key and value encodings. Mention Encoder and BATCH-TOME share Transformer weights during Mention Encoder pre-training. The Mention Memory consists of mention encodings for N = 150 million linked Wikipedia entity mentions. Transformer layers in TOME and BATCH-TOME models are equivalent to those in the BERT-base model. The TOME InitialTransformerBlock contains 4 Transformer layers. TOME-1 has a single TOMEBlock with 8 Transformer layers, and TOME-2 has two TOMEBlocks with 4 Transformer layers each. Therefore, the number of trainable parameters in TOME-1 and TOME-2 is approximately the same as in BERT-base. We use a smaller Mention Memory containing 38m uniformly sampled memories for TOME pre-training. During fine-tuning and evaluation we utilize the full Mention Memory. Appendix A contains more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BASELINES</head><p>We compare TOME with existing methods that utilize textual information from a corpus in a language model. These can be divided into generative LLMs (T5), entity embedding retrieval (Entities as Experts, OPQL), extractive retrieve-and-read (REALM) and generative retrieve-and-read (RAG, Fusion-in-Decoder). TOME occupies a novel position in the space of retrieval models, being more fine-grained than entity embedding retrieval methods, but performing all its reasoning with a single BERT read, unlike retrieve-and-read methods. The most closely comparable models are Entities as Experts and REALM, and we use these as our primary baselines. We report other baselines for reference, with the caveat that these results are not apples-to-apples: RAG and Fusion-in-Decoder have large decoders and retrievers and encode a large number of passages with a BERT reader for each question compared to TOME's single read. Fusion-in-Decoder and RAG 2 also use ground-truth supervision for retrieval. We mark the number of parameters and BERT applications for each baseline in the result tables. Consistent with retrieve-and-read, we count the parameters of the Mention Encoder and TOME, but not the size of the non-trainable and sparsely accessed Mention Memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CLAIM VERIFICATION</head><p>Data. Our first set of experiments evaluates TOME on the claim verification tasks FEVER <ref type="bibr" target="#b22">(Thorne et al., 2018)</ref>, HoVer <ref type="bibr" target="#b9">(Jiang et al., 2020)</ref>, and FM2 (Eisenschlos et al., 2021) in which the model is provided with a claim and has to determine whether the claim is supported by the Wikipedia corpus. FEVER is a larger dataset with 186k claims for which most of the claims can be verified with a single Wikipedia passage. In contrast, HoVer is smaller with 26k claims, but is explicitly constructed to require evidence from multiple sources and multiple reasoning steps. FM2 is also smaller and is constructed through an adversarial game that leads to more challenging retrieval. The claim verification training data contains gold evidence passages, but unlike most published results we do not use these, leaving only the accuracy of the claim verification to guide the retrieval.</p><p>Results. <ref type="table" target="#tab_0">Table 1</ref> contains our claim verification results. TOME outperforms both Entities as Experts and REALM, especially on HoVer and FM2. This is consistent with the properties of TOME: HoVer requires combining detailed information from multiple sources, which TOME is especially well equipped to do compared to aggregate entity-based or retrieve-and-read models. FM2 features generally challenging retrieval and may benefit from contextualizing retrieved evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">QUESTION ANSWERING</head><p>Data. In a second set of experiments we evaluate TOME on TriviaQA (TQA) <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref>, ComplexWebQuestions (CWQ) <ref type="bibr" target="#b21">(Talmor &amp; Berant, 2018)</ref> and EntityQuestions (EQ) <ref type="bibr" target="#b19">(Sciavolino et al., 2021)</ref>, open-domain QA tasks for which most answers are Wikipedia entities. We approach these datasets as entity-linking tasks, as in <ref type="bibr" target="#b4">F?vry et al. (2020)</ref>. We append a mask token to each question, which is marked as a question mention. The probability for each candidate entity is predicted as the aggregate attention weight on mentions of the entity (Section 2.4). Questions with answers that do not correspond to entities in our entity vocabulary are marked as answered incorrectly. TQA  Results. <ref type="table" target="#tab_1">Table 2</ref> contains the results for TQA, CWQ and EQ experiments. Like TOME, Entities as Experts and OPQL treat the above datasets as entity-linking tasks. REALM performs extractive QA, while T5, RAG and Fusion-in-Decoder generate the answer. We note a similar pattern of results as for claim verification. TOME strongly outperforms Entities as Experts on all tasks. TOME performs slightly better than REALM on a simple task like TriviaQA (entity subset) and strongly outperforms REALM on more challenging tasks that require multiple (CWQ) or challenging (EQ) retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">QUALITATIVE PROPERTIES OF TOME</head><p>What memories does TOME retrieve? Given that TOME retrieval is unsupervised, it is natural to ask what memories it learns to retrieve. First, we observe that BATCH-TOME and TOME trained on just the MLM objective learn to attend to memories of the same entity as the passage linked mention (55% and 41% average attention score). This is promising as entity mentions from the same entity often contain mutually relevant information. Quantitative evaluation of downstream retrieval is challenging as TOME often retrieves mentions that are not part of, but equally informative as gold passages. Instead, we provide TOME retrievals on the first three samples of the HoVer dev set to demonstrate its retrieval behavior without cherry-picking. <ref type="table" target="#tab_2">Table 3 demonstrates a successful simple  retrieval, while Table 4</ref> displays interesting multi-hop retrieval. The last is found in Appendix D.</p><p>Importance of memory size. <ref type="figure" target="#fig_0">Figure 2</ref> shows claim verification performance as a function of memory-size during fine-tuning (pre-training memory size is held constant). For smaller memory sizes, entries in memory are uniformly sampled from the full Mention Memory. Performance increases smoothly with memory size. Larger memory size yields diminishing returns, perhaps reflecting that entity mentions may contain overlapping information. <ref type="table">Table 4</ref>: TOME-2 retrievals for the first HoVer dev sample. We show top-1 retrieval results for the first (?? 1 ) and the second (?? 2 ) memory attention layers for passage mentions "Life Goes On" and "Hungry" 3 . Memory mentions are in brackets. The first retrieval for the "Life Goes On" is a different song with the same name and the first retrieval for "Hungry" is related but not useful. However, the second retrieval for "Life Goes On" identifies the correct song and describes its position on the album while the second retrieval for "Hungry" captures its position relative to "Life Goes On".</p><p>Claim: The song recorded by Fergie that was produced by Polow Da Don and was followed by  <ref type="table">Table 5</ref>: Accuracy on held-out subset of Trivi-aQA and ComplexWebQuestions (CWQ) questions. TOME-1-unseen was pre-trained and finetuned with memory without entities from heldout set and evaluated with full memory. Note that performance is considerably lower than on the full dev set as answers in the held-out set (which are in dev but not train) are more likely to be rare entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>TriviaQA dev CWQ dev TOME-1 17.4 16.4 TOME-1-unseen 17.6 16.7</p><p>Zero-shot transfer to unseen entities. An important advantage of memory architectures is that the behavior of the model can be steered by deciding what to include in the memory. Here we show that the TOME model can use information that was not present in memory during training. We sample questions in the TQA and CQA dev sets, and generate a subset of the memory without any mentions corresponding to the answer entities for those questions. Then we pre-train and fine-tune a model on this smaller memory, which we call TOME-unseen. We evaluate TOME-unseen on the sampled questions using the full memory for evaluation only, and compare to standard TOME. <ref type="table">Table 5</ref> shows that using full memory only during evaluation does not lower performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced TOME, a Transformer model that performs attention over a semi-parametric representation of the entire Wikipedia text corpus. This representation, or Mention Memory, consists of a dense encoding for each entity mention in Wikipedia. TOME can retrieve information from multiple sources without supervision, aggregate information within the Transformer, and reason over the retrieved information. TOME leads to strong improvements on multiple open-domain claim verification and entity-based question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PRE-TRAINING</head><p>We train on English Wikipedia, processed with the entity linking and named entity recognition tools from the Google Cloud NLP API 4 . We use existing hyperlinks in Wikipedia as additional entity annotations. All models are pre-trained on 128 TPUs using AdamW optimizer <ref type="bibr" target="#b15">(Loshchilov &amp; Hutter, 2019)</ref> with learning rate 1e-4 and batch size of 4096. Each passage in the batch has length T = 128, excluding entity tokens. The Mention Encoder and BATCH-TOME are pre-trained for 1 million steps with 50k warmup steps, and TOME is trained for 500k additional steps with 25k warmup steps after initialization from BATCH-TOME. Both models are trained with linear learning rate decay. Mention Encoder and BATCH-TOME share Transformer weights during Mention Encoder pre-training. We apply gradient clipping with a norm of 1.0 and weight decay of 0.01. Weight decay is applied to all weights except layer norm and bias weights.</p><p>BATCH-TOME and TOME are trained with weight 0.85 on the MLM objective and 0.15 on the entity coreference resolution objective. We mask 20% of whole entity mentions and 10% of other tokens. We limit the coreference resolution objective to mentions of the 1 million most frequent Wikipedia entities. We use 24 mentions per sample, with a batch size of 32 samples per TPU. We subsample mentions uniformly if the average number of annotated mentions on a TPU exceeds 24. Key mention encoding have dimension d K = 128 and value and coreference mention encodings have dimension d V = d C = 512.</p><p>Disallowed same passage retrieval for Mention Encoder. We want the model to use memory as a source of additional information for processing a passage. Therefore, we explicitly set attention weights to 0 for memories generated from the same passage as the current one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MENTION ENCODER DATA GENERATION</head><p>We pre-train Mention Encoder to produce mention encodings that are useful for BATCH-TOME. In order to provide BATCH-TOME with an incentive to use the memory, we need to ensure that mentions from different samples within a batch are relevant to each other. We achieve this by batching passages from the same or related Wikipedia articles.</p><p>We generate clusters of 256 passages from Wikipedia articles using a greedy method. First, we create a cluster from the longest unused Wikipedia article and add related articles until the cluster consists of 256 passages. In particular, at each step we add the article with the largest Jaccard similarity between its entity set and the entity set of articles in the current cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 COREFERENCE RESOLUTION LOSS</head><p>For every linked mention m in the batch we compute a mention encoding z m by applying a separate SpanEncodingLayer on the output of the BATCH-TOME. First, we compute the loss for every linked mention m in the batch. To this end, we denote linked mentions in every other passage in the batch as positive, P + (m), if they have the same entity ID as m and negative, P ? (m), otherwise. The loss per mention is an average of cross-entropy losses per every positive mention m + ? P + (m)</p><formula xml:id="formula_9">L coref (m) = ? 1 |P + (m)| log m + ?P + (m) exp(z T m z m + ) exp(z T m z m + ) + m ? ?P ? (m) exp(z T m z m ? )</formula><p>The total loss is average of losses per linked mentions which have at least one positive mention (set P + (m) is not empty).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTS</head><p>B.1 FINE-TUNING SETUP TOME is fine-tuned on 32 TPUs using the Adam optimizer with a learning rate of 1e-5 and total batch size 32. In contrast to pre-training we set max mentions to 32 per sample for fine-tuning. We use 1000 warmup steps and linear learning rate decay. Gradient clipping and weight decay are the same as during pre-training. We take the highest scoring checkpoint on dev sets and evaluate it on the test set. We use the spaCy noun chunker to detect noun phrases and treat these as claim/question entity mentions.</p><p>The model can be fine-tuned with full memory on a server of 8 A100 GPUs or 16 v3/v4 TPUs. A model with half memory (75M mentions) can be fine-tuned on 8 V100s/P100s or 8 TPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 BASELINES</head><p>Following <ref type="bibr" target="#b7">Guu et al. (2020)</ref> we used REALM to perform extractive question answering on TriviaQA and ComplexWebQuestions datasets. We also adapted the model to the classification setting in order to apply it to claim verification tasks. Given an input claim X we compute probability of a prediction Y (whether the claim holds true or not) as a marginalization over retrieval Z.</p><formula xml:id="formula_10">Pr(Y |X, Z) = z?Z Pr(Y |X, Z = z) ? Pr(Z = z|X)</formula><p>where Pr(Y |X, Z = z) is the output probability produced by the reader model and Pr(Z = z|X) is produced by the retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 CLAIM VERIFICATION</head><p>See <ref type="table" target="#tab_3">Table 6</ref> for the results on development and test splits of claim verification datasets. Additionally, <ref type="table">Table 7</ref> compares our FM2 results to the original dataset baselines. We report additional results on the EntityQuestions dataset from <ref type="bibr" target="#b19">Sciavolino et al. (2021)</ref>. The dataset consists of questions involving rare entities, making it especially challenging for modern retrievals methods such as DPR. Evaluation results for TOME models and baselines are shown in <ref type="table" target="#tab_4">Table 8</ref> and <ref type="table" target="#tab_5">Table 9</ref>. Following <ref type="bibr" target="#b19">(Sciavolino et al., 2021)</ref> we report recall at 20 as an evaluation metric. Since TOME retrieves mentions rather than passages, a direct comparison is difficult. We evaluate TOME conservatively, treating recall at 20 as successful if one of the 20 highest scoring mentions belongs to the correct entity (in contrast to DPR, for which the correct answer only has to be somewhere in the retrieved 100 word document). TOME sets the state of the art on this dataset and outperforms DPR by a very large margin. REALM cannot be fairly compared to DPR due to longer retrieved passages (100 vs 288 tokens). Therefore, we perform a separate experiment using accuracy with REALM as a baseline, showing large performance gains over REALM as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 IMPORTANCE OF PRE-TRAINING OBJECTIVES</head><p>We perform several ablation experiments for the pre-training procedure (see <ref type="table" target="#tab_0">Table 10</ref>). First, results show that the entity prediction objective (c.f. Section 2.4) is not essential for TOME pre-training. Performance on claim verification datasets (FEVER and HoVer) is not affected by whether we use entity prediction for pre-training. More surprisingly, removing this objective only slightly decreases the performance on entity question answering datasets (TriviaQA and ComplexWebQuestions). We predict entities for question-answering in the same way as we do for the entity prediction objective during pre-training (c.f. Equation 10), so we expected the entity prediction auxiliary loss to be important.   On the other hand, a related entity coreference objective (c.f. Section A.1 and Appendix A.2) is crucial for Batch-TOME and Mention Encoder pre-training. That is consistent with our intuition that semantically challenging tasks incentivize the model to store useful information in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 TOME INITIALIZATION</head><p>We initialize TOME model with a pre-trained BATCH-TOME model which we find to be especially important for warming up retrieval. If TOME is initialized from scratch (or even from BERT weights), TOME does not learn to use the memory. In fact, TOME has to be initialized from the same BATCH-TOME used to generate the memory. This implies that multi-stage training is a vital ingredient for TOME to succeed. Our explanation for why TOME is sensitive to initialization is that TOME needs to learn two skills: first, to effectively use retrieved mentions for its predictions, and second, to retrieve relevant mentions. Learning both capabilities end to end gives rise to a mutual dependence: to get a signal for learning how to use retrieved mentions, the retrieved mentions have to be useful, and to learn to retrieve useful mentions, the model needs to utilize retrieved mentions. If initialized from scratch, the model is not able to learn both skills simultaneously. The pre-training stage with the smaller in-batch memory functions as a curriculum to address that problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C NEAREST NEIGHBOR SEARCH</head><p>Nearest neighbor search is an extremely common problem, and there exist numerous approaches and packages for fast approximate nearest neighbor search (ANNS). <ref type="bibr" target="#b6">(Guo et al., 2020;</ref><ref type="bibr" target="#b10">Johnson et al., 2017)</ref>. Most approaches employ two methods for fast search: 1) compress the search table through projecting to a lower dimension and quantization and perform comparisons in this compressed space and 2) divide the search table in buckets of similar items, and search only a subset of the buckets. Retrieve-and-read use ANNS packages to search for related passages <ref type="bibr" target="#b7">(Guu et al., 2020;</ref><ref type="bibr" target="#b14">Lewis et al., 2020b)</ref>.</p><p>Applying such packages for TOME is slightly trickier, as TOME needs to perform ANNS inside the model. One viable route is to compute queries on-device, transmit them to a separate ANNS server and them transmit them back. We would recommend this approach for GPU accelerators, with faster host-device communication and slower device-device communication. As we are using TPU accelerators, we decided to use on-device ANNS, which does not require coordinating additional servers and will potentially allow for backpropagating through memory in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ON-DEVICE NEAREST NEIGHBOR SEARCH</head><p>We shard the Mention Memory over all TPU devices. We perform search by distributing each query to all devices and retrieving top-K search results from each local memory shard. Then, the results are distributed back to the original devices and the local search results are aggregated through another, global top-K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 DOT-PRODUCT</head><p>The first method we describe is naive dot-product search, taking advantage of matrix multiplication capacity of TPU accelerators. In this method we perform search over local shards by taking the dot product between the query and the local memory shard and performing an approximate top-k operation over the results. Dot-product search is easy to implement and fast for smaller memory sizes (up to 10 million entries). We implemented this method first due to its simplicity and our primary experimental results employ this search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 ANNS</head><p>To speed up search we implemented method 2) from standard CPU-based ANNS, bucketing the search table and searching only a subset of buckets. In particular we perform k-means clustering to divide the Mention Memory into clusters, and perform dot-product search over the top n s clusters on each device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.3 OVERHEAD</head><p>While the Mention Memory is stored on-device, memory overhead is negligible as the memory table is sharded. For pre-training the Mention Memory took up 2.2% of available device memory. <ref type="table" target="#tab_0">Table  11</ref> shows percentage of time spent on ANNS in TOME-1 pretraining for different reader architectures. The relative overhead of search becomes smaller with reader size, and ANNS overhead in particular becomes negligible for BERT-Large and up. We did not measure standard CPU ANNS overhead, but it should be comparable to or faster than our ANNS numbers. For ANNS in TOMEBlocks we take top-2 search results from each local memory shard, and apply top-128 over the retrieved results. For ANNS in the entity prediction layer we take top-32 search results from each local shard, and aggregate across shards without applying an additional top-K operation. <ref type="table" target="#tab_0">Table 12</ref>: TOME-2 retrievals for the second HoVer dev sample. We show top-1 retrieval results for the first (?? 1 ) memory attention layer for passage mentions "the novel", "the movie" and "the album". Memory mentions are in brackets. We can see that the model can retrieve relevant mentions for non-named passage mentions, and generally understands it is looking for mentions related to music. However, while the best retrieval for "album" is from a passage that mentions sampling the Shining, it is quite far removed and it is likely the retrieval is not sufficiently accurate here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RETRIEVAL EXAMPLES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Life Goes On was Hungry. Label: TRUE Life Goes On ?? 1 . . . and Johnny J produced the chart topping hits "All Bout U", "How Do U Want It" and ["Life Goes On"]. . . . Life Goes On ?? 2 . . . On November 11, 2016, Fergie released the third single from the album, ["Life Goes On"]. . .Hungry ?? 1 . . . Polow da Don, is an American record producer, songwriter and rapper. His cousin is [Atlanta] singer Monica. Jones has produced a variety of singles for a multitude of artists including "Anaconda" by Nicki Minaj(2014), "Love In This Club" by Usher(2008), "Buttons" by the Pussycat Dolls(2006), "Hungry" by Fergie . . . Hungry ?? 2 . . . "Life Goes On" is a song recorded by American singer Fergie for her second studio album, Double Dutchess (2017). . . . The song serves as the third single from [Fergie's] second studio album, following "Hungry". Claim verification accuracy as a function of fine-tuning memory size (in millions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on claim verification datasets. #Encoded refers to the number of passages encoded by a BERT reader to answer a single question.</figDesc><table><row><cell>Model</cell><cell cols="5">#Params #Encoded HoVer test FEVER test FM2 dev</cell></row><row><cell>RAG</cell><cell>620M</cell><cell>100</cell><cell>-</cell><cell>72.5</cell><cell>-</cell></row><row><cell>REALM</cell><cell>330M</cell><cell>5</cell><cell>66.1</cell><cell>67.1</cell><cell>65.8</cell></row><row><cell>Entities as Experts</cell><cell>360M</cell><cell>1</cell><cell>66.6</cell><cell>63.6</cell><cell>63.5</cell></row><row><cell>TOME-1</cell><cell>220M</cell><cell>1</cell><cell>72.8</cell><cell>67.8</cell><cell>67.7</cell></row><row><cell>TOME-2</cell><cell>220M</cell><cell>1</cell><cell>73.1</cell><cell>68.1</cell><cell>68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on open-domain QA datasets TriviaQA (TQA), ComplexWebQuestions (CWQ) and EntityQuestion (EQ). #Encoded refers to the number of passages encoded by a BERT reader to answer a question. TQA e-dev corresponds to TQA with train and dev samples limited to those with Wikipedia entity as an answer. See Appendix B.3 for full results.</figDesc><table><row><cell>Model</cell><cell cols="7">#Params #Encoded TQA dev TQA test TQA e-dev CWQ dev EQ dev</cell></row><row><cell>RAG</cell><cell>620M</cell><cell>100</cell><cell>56.8</cell><cell>68.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fusion-in-Decoder</cell><cell>440M</cell><cell>100</cell><cell>65.0</cell><cell>77.1</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>REALM</cell><cell>330M</cell><cell>5</cell><cell>55.8</cell><cell>67.1</cell><cell>63.4</cell><cell>46.7</cell><cell>59.0</cell></row><row><cell>T5-3B</cell><cell>3B</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.7</cell><cell>-</cell></row><row><cell>T5-11B</cell><cell>11B</cell><cell>1</cell><cell>42.3</cell><cell>50.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Entities as Experts</cell><cell>360M</cell><cell>1</cell><cell>43.2</cell><cell>53.4</cell><cell>51.3</cell><cell>42.7</cell><cell>32.5</cell></row><row><cell>OPQL</cell><cell>220M</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.1</cell><cell>-</cell></row><row><cell>TOME-1</cell><cell>220M</cell><cell>1</cell><cell>50.8</cell><cell>61.1</cell><cell>60.3</cell><cell>44.9</cell><cell>62.1</cell></row><row><cell>TOME-2</cell><cell>220M</cell><cell>1</cell><cell>54.6</cell><cell>65.8</cell><cell>64.8</cell><cell>47.7</cell><cell>66.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The harrier is a medium-sized dog breed of the [hound] class, used for hunting. . . consists of 96k trivia questions, for which 84% of answers correspond to a Wikipedia entity. We use the open-domain setting without gold evidence passages. In order to compare head-to-head performance, we also report results on a subset of TQA with only questions with Wikipedia entities as an answer. CWQ consists of 35k complex questions (compositions, conjunctions, etc.) for which 94% of answers correspond to a Wikipedia entity. EQ contains challenging questions involving rare entities, with Wikipedia entities as answers.</figDesc><table /><note>TOME-2 retrievals for the second HoVer dev sample. We show top-1 retrieval results for the first (?? 1 ) memory attention layer for two passage mentions. Memory mentions are in brackets.Claim: Greater Swiss Mountain Dog and Harrier are both dog breeds. Label: TRUE Greater Swiss Mountain Dog ?? 1 Breed History the origin of the [Greater Swiss Mountain Dog] is not definitively known. . . . Harrier ?? 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on claim verification datasets. #Encoded refers to the number of passages encoded by a BERT reader to answer a single question. EaE stands for Entities as Experts model. Model #Params #Encoded HoVer dev HoVer test FEVER dev FEVER test FM2 dev</figDesc><table><row><cell>RAG</cell><cell>620M</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>74.5</cell><cell>72.5</cell><cell>-</cell></row><row><cell>REALM</cell><cell>330M</cell><cell>5</cell><cell>67.3</cell><cell>66.1</cell><cell>70.4</cell><cell>67.1</cell><cell>65.8</cell></row><row><cell>EaE</cell><cell>360M</cell><cell>1</cell><cell>66.2</cell><cell>66.6</cell><cell>66.1</cell><cell>63.6</cell><cell>63.5</cell></row><row><cell>TOME-1</cell><cell>220M</cell><cell>1</cell><cell>73.6</cell><cell>72.8</cell><cell>70.5</cell><cell>67.8</cell><cell>67.7</cell></row><row><cell>TOME-2</cell><cell>220M</cell><cell>1</cell><cell>74.1</cell><cell>73.1</cell><cell>71.1</cell><cell>68.1</cell><cell>68.4</cell></row><row><cell cols="8">Table 7: Accuracy on FM2 compared with original dataset baselines. Oracle refers to oracle retrieval</cell></row><row><cell cols="3">followed by a BERT-Base reader.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Oracle (Eisenschlos et al., 2021)</cell><cell>69.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">DPR (Eisenschlos et al., 2021)</cell><cell>64.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>EaE</cell><cell></cell><cell></cell><cell>63.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>REALM</cell><cell></cell><cell></cell><cell>65.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TOME-1</cell><cell></cell><cell></cell><cell>67.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TOME-2</cell><cell></cell><cell></cell><cell>68.4</cell><cell></cell><cell></cell></row><row><cell cols="3">B.4 QUESTION ANSWERING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">: EntityQuestions recall@20</cell></row><row><cell>Model</cell><cell>Recall@20</cell></row><row><cell>DPR (Sciavolino et al., 2021)</cell><cell>65.4</cell></row><row><cell>BM25 (Sciavolino et al., 2021)</cell><cell>71.2</cell></row><row><cell>TOME-1</cell><cell>83.3</cell></row><row><cell>TOME-2</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="2">: EntityQuestions top-1 accuracy</cell></row><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Entities as Experts</cell><cell>32.5</cell></row><row><cell>REALM</cell><cell>59.0</cell></row><row><cell>TOME-1</cell><cell>62.1</cell></row><row><cell>TOME-2</cell><cell>66.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 :</head><label>10</label><figDesc>Performance ablations for pre-training objectives experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="4">HoVer dev FEVER dev TriviaQA dev CWQ dev</cell></row><row><cell>TOME-1</cell><cell>73.6</cell><cell>70.5</cell><cell>50.8</cell><cell>44.9</cell></row><row><cell>w/o entity coreference loss</cell><cell>69.8</cell><cell>68.4</cell><cell>42.5</cell><cell>40.5</cell></row><row><cell>w/o entity prediction loss</cell><cell>73.7</cell><cell>70.7</cell><cell>49.4</cell><cell>43.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 :</head><label>11</label><figDesc>Proportion of time spent on ANNS for TOME-1 pre-training setting.</figDesc><table><row><cell>Model</cell><cell cols="2">Dot-product ANNS</cell></row><row><cell>BERT-Base</cell><cell>0.79</cell><cell>0.22</cell></row><row><cell>BERT-Large</cell><cell>0.48</cell><cell>0.07</cell></row><row><cell>T5-11B Encoder</cell><cell>0.17</cell><cell>0.02</cell></row><row><cell>C.1.4 HYPERPARAMETERS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Claim: Stephen King wrote the novel that the movie directed by Stanley Kubrick that was sampled in the album "Where Blood and Fire Bring Rest" was based on. Label: TRUE the novel ?? 1 Music Video. The video is a homage to Stanley Kubrick's 1980 film The Shining based on the [Stephen King] novel . . . the movie ?? 1 Music Video. The video is a homage to Stanley Kubrick's 1980 film The Shining based on the [Stephen King] novel . . . the album ?? 1 Where Blood and Fire Bring Rest is the third full-length album released by [metalcore] band ZAO. It was the first album to feature vocalist Dan Weyandt after the departure of Shawn Jonas along with new bassists/guitarists, Russ Cogdell and Brett Detar. The album contains a sample from the film The Shining . . .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It should be noted that absent heuristics, the number of potential relation mentions (i.e., entity mention pairs) is much larger than the number of entity mentions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">RAG is initialized from DPR which is trained with gold retrieval passages for TriviaQA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We replaced the original song title with the song "Hungry" as the original may be inappropriate. 9</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://cloud.google.com/natural-language/docs/basics#entity_analysis</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEGEMENTS</head><p>We thank Livio Baldini Soares, Kenton Lee, Tom Kwiatkowski, Ilya Eckstein and others at Google Research for insightful discussions. This work is partially supported by NSF Awards IIS-1513966/ 1632803/1833137, CCF-1139148, DARPA Awards#: FA8750-18-2-0117, FA8750-19-1-0504, DARPA-D3M -Award UCB-00009528, Google Research Awards, gifts from Facebook and Netflix, and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledgeable or educated guess? revisiting language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Differentiable reasoning over a virtual knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>ICLR 2020</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>B?rschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04725</idno>
		<title level="m">Fool me twice: Entailment from wikipedia gamification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entities as experts: Sparse memory access with entity supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MOLEMAN: mention-only linking of entities with a mention annotation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hover: A dataset for many-hop fact extraction and claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vassilis Plachouras, Tim Rockt?schel, and Sebastian Riedel</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>NAACL-HLT 2021</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple entity-centric questions challenge dense retrievers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Sciavolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<editor>Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning over virtual knowledge bases with open predicate relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FEVER: a largescale dataset for fact extraction and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptable and interpretable neural memory over symbolic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Readtwice: Reading very large documents with memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Michiel De Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-step reasoning over unstructured text with beam dense retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
