<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
							<email>yuxiang.wu@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
							<email>mattgardner@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Semantic Machines ? Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
							<email>p.stenetorp@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
							<email>pradeepd@allenai.org</email>
						</author>
						<title level="a" type="main">Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural language processing models often exploit spurious correlations between taskindependent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, 1 and we evaluate on a large suite of debiased, outof-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-ofthe-art debiasing strategies, and when combined with an orthogonal technique, productof-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Processing (NLP) datasets inevitably contain biases that are unrelated to the tasks they are supposed to represent. These biases are usually artifacts of the annotation processes, task framing, or design decisions <ref type="bibr" target="#b34">(Schwartz et al., 2017;</ref><ref type="bibr" target="#b11">Geva et al., 2019;</ref>. Such biases often manifest as spurious correlations between simple features of the data points and their * Work done while at the Allen Institute for AI. <ref type="bibr">1</ref> All our code and the generated datasets are available at https://github.com/jimmycode/ gen-debiased-nli. Figure 1: Overview of our dataset bias mitigation approach. We minimise spurious correlations between labels (represented by the shapes of data points) and taskindependent features (represented by their colours) with our proposed data generation pipeline. labels . Trained models can exploit these spurious correlations to correctly predict the labels of the data points within the same distributions as those they are trained on, but fail to generalise to other distributions within the same tasks. Consequently, the models risk modelling the datasets, but not the tasks <ref type="bibr" target="#b14">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b29">Poliak et al., 2018;</ref><ref type="bibr" target="#b25">McCoy et al., 2019;</ref><ref type="bibr" target="#b33">Schuster et al., 2019)</ref>.</p><p>We address this issue by adjusting existing dataset distributions to mitigate the correlations between task-independent features and labels. First, we train data generators that generate high quality data samples in the distribution of existing datasets (Section 2). Then, we identify a set of simple features that are known to be task-independent, and use the theoretical framework (i.e., z-statistics) proposed by  to measure correlations between those features and the labels (Section 3.1). Finally, we adjust the distribution of the generated samples by post-hoc filtering (Section 3.2) to remove the data points that contribute to high z-statistics with task-independent features, or finetuning the data generator (Section 4.1) to make such data points less likely. Unlike prior model-arXiv:2203.12942v1 [cs.CL] 24 Mar 2022 centric approaches to mitigate spurious correlations <ref type="bibr">(Belinkov et al., 2019a,b;</ref><ref type="bibr" target="#b8">Clark et al., 2019;</ref><ref type="bibr" target="#b15">He et al., 2019;</ref><ref type="bibr" target="#b17">Karimi Mahabadi et al., 2020)</ref> that define new training objectives or model architectures, our approach has the advantage of keeping the objective and the model fixed, as we only alter the training data.</p><p>To evaluate our approach, we use the task of Natural Language Inference (NLI), which offers a wide range of datasets (including challenge datasets) for various domains. We generate debiased SNLI <ref type="bibr" target="#b5">(Bowman et al., 2015)</ref> and MNLI <ref type="bibr" target="#b41">(Williams et al., 2018)</ref> distributions and evaluate the generalisability of models trained on them to out-of-distribution hard evaluation sets <ref type="bibr" target="#b14">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b25">McCoy et al., 2019)</ref>, and the adversarial attack suite for NLI proposed by <ref type="bibr" target="#b23">Liu et al. (2020b)</ref>. Furthermore, we compare our method to strong debiasing strategies from the literature <ref type="bibr" target="#b2">(Belinkov et al., 2019b;</ref><ref type="bibr" target="#b36">Stacey et al., 2020;</ref><ref type="bibr" target="#b8">Clark et al., 2019;</ref><ref type="bibr" target="#b17">Karimi Mahabadi et al., 2020;</ref><ref type="bibr" target="#b38">Utama et al., 2020;</ref><ref type="bibr" target="#b31">Sanh et al., 2021;</ref><ref type="bibr" target="#b12">Ghaddar et al., 2021)</ref>.</p><p>Our results show that models trained on our debiased datasets generalise better than those trained on the original datasets to evaluation sets targeting hypothesis-only biases (by up to 2.8 percentage points) and syntactic biases (by up to 13.3pp), and to a suite of adversarial tests sets (by up to 4.2pp on average). Since our contributions are orthogonal to model-centric approaches, we show that when combined with product-of-experts <ref type="bibr" target="#b17">(Karimi Mahabadi et al., 2020)</ref>, our method yields further improvements and outperforms previous state-of-the-art results of SNLI-hard and MNLI-hard. Finally, we train stronger and larger pretrained language models with our debiased datasets, and demonstrate that the performance gain by our method generalises to these larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generating High-Quality Data Samples</head><p>First, we need to train a data generator G to generate data samples automatically. Our goal for the data generator is to model the true distribution as well as possible so that we can generate valid and high-quality data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finetuning Pretrained Language Model to Generate NLI Samples</head><p>We finetune a pretrained language model on the NLI datasets to serve as our data generator. We choose GPT-2 because it is a powerful and widelyused autoregressive language model, and it can be easily adapted to generated the premise, label, and hypothesis of an instance sequentially. Given an NLI dataset D 0 , the training objective is to minimise the following negative log-likelihood loss of generating the premise-label-hypothesis sequence, in that order:</p><formula xml:id="formula_0">L M LE = ? |D 0 | i=1 log p(P (i) , l (i) , H (i) ) = ? |D 0 | i=1 log p(P (i) )p(l (i) |P (i) )p(H (i) |l (i) , P (i) ),<label>(1)</label></formula><p>where P (i) , l (i) and H (i) are the premise, label and hypothesis respectively. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improving Data Generation Quality</head><p>We find that samples generated by a generator trained with only L M LE often contain ungrammatical text or incorrect label. In this section, we introduce two techniques to improve data quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Unlikelihood Training to Improve Label Consistency</head><p>We observe poor label consistency in samples generated by a generator trained with vanilla L M LE objective -given a generated sample (P ,H,l), the labell often does not correctly describe the relationship betweenP andH. To alleviate this issue, we apply unlikelihood training <ref type="bibr" target="#b40">(Welleck et al., 2020)</ref> to make generating such label inconsistent instances less likely. First we perturb the label to construct negative samples (P, H, l ) where l = l for each sample in the dataset. Then we apply a token-level unlikelihood objective on the hypothesis tokens:</p><formula xml:id="formula_1">L consistency = ? |D 0 | i=1 |H| (i) t=1 log(1 ? p(H (i) t |l (i) , P (i) , H (i) &lt;t )).</formula><p>This objective decreases the probability of generating H when given an incorrect label l , hence improves the label consistency at generation time.</p><p>We combine L M LE and L consistency to finetune our generator G with</p><formula xml:id="formula_2">L G = L M LE + ?L consistency ,</formula><p>where ? is a hyperparameter that balances the two objectives. We can randomly sample from the trained generator to obtain a large amount of the synthetic data D G ? G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Filtering Based on Model Confidence</head><p>We add a consistency filtering step <ref type="bibr" target="#b20">(Lewis et al., 2021;</ref><ref type="bibr" target="#b0">Bartolo et al., 2021)</ref> to further improve the quality of the generated dataset. We train an NLI model M with the original dataset D 0 to filter out samples in which M has low confidence:</p><formula xml:id="formula_3">D G = {(P, H, l) ? D G | p M (l|P, H) &gt; ? },</formula><p>where ? is a confidence threshold. We found that the filtered out data samples generally had ungrammatical text or incorrect labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mitigating Spurious Correlations using z-filtering</head><p>We now define a method to reject samples that contribute to the high spurious correlations between task-independent features of the samples and their labels. Our approach is based on the theoretical framework proposed by  to measure these correlations, known as zstatistics. Our filtering method, called z-filtering (Section 3.2), will serve as the basis to construct debiased datasets in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identifying and Measuring Spurious Correlations</head><p>As a first step towards addressing spurious correlations, we need to be able to quantify them. We start by selecting a set of task-independent features -features that give away the labels and allow models to exploit them without actually solving the task. For NLI, we choose the following features: 1) unigrams and bigrams; 2) hypothesis length and hypothesis-premise length ratio; 3) lexical overlap between hypothesis and premise; 4) the predictions of a BERT-base <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>  2018; <ref type="bibr" target="#b29">Poliak et al., 2018)</ref>. Note that our method does not rely on the specific choice of features, and one can easily add alternative features that should not be correlated with the labels.</p><p>Following , we assume there should be no correlation between each of these features and the class labels. More formally, for any feature x from our feature set X , p(l|x) should be uniform over the class labels l. We definep(l|x) = 1 n n j=1 l j to be the empirical expectation of p(l|x) over n samples containing x. Then we compute the standardised version of zstatistics to quantify its deviation from the uniform distribution for each feature x and label l:</p><formula xml:id="formula_4">z * (x, l) =p (l|x) ? p 0 p 0 (1 ? p 0 )/n ,<label>(2)</label></formula><p>where p 0 is the probability of uniform distribution (p 0 = 1/3 in NLI tasks with three labels).</p><p>These z-statistics scores can be used to identify the most biased features for each label l -we select k features with the highest z-statistic to define the biased features set B D (l). <ref type="table" target="#tab_2">Table 12</ref> shows examples of these biased features on SNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">z-filtering</head><p>To mitigate the biases in the dataset, we propose z-filtering, an algorithm that iteratively selects and filters instances from a dataset D to build a debiased dataset Z. At each step, we find the set of biased features B Z (l) on the partially constructed Z. We then select a new batch of samples from D and filter out the samples that contain these biased features. This process is applied iteratively until it has exhausted all samples from D . It removes the samples that contribute to the spurious correlations in D , thus it finds a debiased subset Z(D ) ? D . We denote the removed samples as Z ? (D ). The full z-filtering algorithm is illustrated in Algorithm 1.</p><p>Optionally, one can initialise Z with a seed dataset D seed . In this case, the samples from D are only added to Z when they do not contain the biased features of D seed . Thus it can be seen as a data-augmentation technique targeted to debias a given dataset. We refer to it as conditional zfiltering and denote the produced debiased dataset as Z(D |D seed ).</p><p>Algorithm 1: z-filtering algorithm.</p><p>Data: input dataset D [with optional seed dataset D seed ] Result: debiased dataset Z and the rejected samples</p><formula xml:id="formula_5">Z ? Z ? ? (or Z ? D seed ); Z ? ? ?; for sample batch D t ? D do compute or update z-statistics z * (x, l|Z), ?x ? X of Z; find the biased features B Z (l), ?l ? {entailment, neutral, contradiction}; foreach instance I = (P, H, l) ? D t do get the features f of the instance I; if f ? B Z (l) = ? then Z ? Z ? {I}; else Z ? ? Z ? ? {I}; end end end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Constructing Debiased NLI Datasets via Data Generation</head><p>We use z-filtering in two ways: 1) to further finetune G (the one trained in Section 2.2.1 with consistency unlikelihood) with an objective that downweighs samples that should be rejected (Section 4.1); 2) to post-hoc filter the generated samples to obtain debiased datasets (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning to Generate Unbiased Samples</head><p>The generator G can learn to exploit taskindependent features during its finetuning stage (Section 2), causing the synthetic dataD G to contain many spurious correlations. While it is tempting to apply z-filtering to remove these spurious correlations fromD G , we find that this will lead to the removal of majority of the generated data. For example, when the generator is finetuned on SNLI, z-filtering removes around 85% ofD G SN LI . 4 This leads to a very inefficient data generation process to mitigate the spurious correlations.</p><p>To alleviate this issue, we can incorporate the debiasing objectives into the training of the generator, so that the samples produced by the generator are more likely to be accepted by the z-filtering process. More specifically, we can encourage the model to generate Z(D 0 ), while discouraging it from generating Z ? (D 0 ). For the latter part, we again apply an unlikelihood training objective L U L to unlearn Z ? (D 0 ). Hence, the overall debiasing training objective is:</p><formula xml:id="formula_6">L debias = L M LE (Z(D 0 )) + ?L U L (Z ? (D 0 ))</formula><p>where ? is a hyperparameter.</p><p>A naive use of an unlikelihood objective on all tokens gives the model mixed signals for good tokens and leads to ungrammatical, degenerate outputs. To avoid this degeneracy, we apply the unlikelihood loss only to tokens that contribute to biased features. Concretely, for each token</p><formula xml:id="formula_7">I ? t of instance I ? ? Z ? (D 0 ), we define a mask m t as m t = 0, if I ? t contributes to B Z (l I ? ) 1, otherwise. where B Z (l I ? ) represent the biased features corre- sponding the label of I ? .</formula><p>For biases towards unigram and bigram features (as defined in Section 3.1), we consider only the corresponding tokens to be relevant (i.e., m t = 0 if I ? t is part of the unigram or the bigram). For biases towards other features (e.g. length of the hypothesis), we consider all the tokens on the hypothesis to be relevant. The unlikelihood training objective is defined as follows:</p><formula xml:id="formula_8">L U L (Z ? (D 0 )) = I ?Z ? (D 0 ) L U L (I ), L U L (I ) = ? |I | t=1 log(m t p(I t |I &lt;t ) +(1 ? m t )(1 ? p(I t |I &lt;t ))).</formula><p>We further finetune G with L debias to obtain a new generator G * , that is trained to generate more unbiased data samples. We then randomly sample from G * and conduct data filtering (Section 2.2.2) to obtain a large set of high-quality debiased data samplesD G * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combining with z-filtering to Construct the Debiased NLI Datasets</head><p>Given the original dataset D 0 and the synthetic datasetD G * , our goal is produce a large-scale unbiased dataset D * . There are various ways to do this given that we can either apply conditional zfiltering, or simply z-filter both D 0 andD G * and merge them. We explore the following options:</p><formula xml:id="formula_9">1. Z-Augmentation (Z-Aug) Z(D G * |D 0 ):</formula><p>we keep the original dataset as is, and augment it by conducting conditional z-filtering onD G * using D 0 as seed dataset.</p><formula xml:id="formula_10">2. Parallel z-filter (Par-Z) Z(D 0 ) ? Z(D G * ):</formula><p>we conduct z-filtering on D 0 andD G * separately, and then merge them.</p><formula xml:id="formula_11">3. Sequential z-filter (Seq-Z) Z(D G * |Z(D 0 )):</formula><p>we first conduct z-filtering on D 0 , then conduct conditional z-filtering onD G * with Z(D 0 ) as seed dataset. Generating Debiased Datasets We conduct debiased data generation for SNLI and MNLI separately. For SNLI, we use the proposed method described in Section 4.1 to train a generator G * SNLI . Then we randomly sample a large number of instances from the generator to construct D G * SNLI . The samples are filtered with a strong NLI model M trained on SNLI to obtainD G * SNLI . Finally, different options (Section 4.2) can be adopted to merge the synthetic data with the original data D SNLI to construct debiased versions of SNLI. The same</p><formula xml:id="formula_12">Options D 0 = D SNLI D 0 = D MNLI Original D 0 549,367 382,702 Z-Aug Z(D G * |D 0 ) 1,142,475 744,326 Par-Z Z(D 0 ) ? Z(D G * ) 933,085 740,811 Seq-Z Z(D G * |Z(D 0 ))</formula><p>927,906 744,200 procedure is used to produce debiased datasets for MNLI, by simply replacing the original dataset with MNLI. We choose GPT-2 large and Robertalarge as the pretrained language models for G * and M respectively. <ref type="bibr">5</ref> The size of the constructed debiased datasets are listed in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLI Model Training</head><p>Since our method directly debiases the training data itself, we keep the model and training objective fixed and only replace the training data with our generated debiased datasets. For comparability with previous work (Karimi <ref type="bibr" target="#b17">Mahabadi et al., 2020;</ref><ref type="bibr" target="#b38">Utama et al., 2020;</ref><ref type="bibr" target="#b31">Sanh et al., 2021)</ref>, we train BERT-base <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> on our debiased datasets. The NLI models are trained with ordinary cross-entropy classification loss, and the training hyperparameters are listed in Appendix A. We run our experiments five times and report the average and standard deviation of the scores. <ref type="bibr">6</ref> We also conduct statistical significance testing using a 2-tailed t-test at 95% confidence level.</p><p>State-of-the-art Debiasing Models We compare our method with the following three stateof-the-art debiasing models on each of our evaluation datasets. </p><formula xml:id="formula_13">BERT-base w/ Z-Aug Z(D G * |D SNLI ) 90.67 81.78 ?0.53 BERT-base w/ Par-Z Z(D SNLI ) ? Z(D G * ) 88.11 82.81 ?0.37 BERT-base w/ Seq-Z Z(D G * |Z(D SNLI ))</formula><p>88.08 82.82 <ref type="bibr">?0.15</ref> Combining PoE with our debiased datasets BERT-base + PoE w/ D SNLI 90.25 82.92 BERT-base + PoE w/ Seq-Z Z(D G * |Z(D SNLI )) 87.65 84.48 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining PoE with Our Debiased Datasets</head><p>Our approach changes the training data distribution instead of the model's training objective, and hence is orthogonal to prior work method-wise. We also report the results of combining PoE with our proposed method, simply by training a PoE model on our debiased datasets. We adapt the PoE implementation by Karimi Mahabadi et al. <ref type="formula" target="#formula_4">(2020)</ref>, and we follow their approach to conduct hyperparameter tuning for PoE. <ref type="bibr">7</ref> The hyperparameters of the PoE models are reported in <ref type="table" target="#tab_2">Table 10</ref> of Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hypothesis-only Bias in NLI</head><p>Gururangan et al. <ref type="bibr">(2018)</ref> found that, on SNLI and MNLI, a model that only has access to the hypothesis can perform surprisingly well, which indicates that the datasets contain hypothesis-only bias. To alleviate this problem, SNLI-hard and MNLIhard (Gururangan et al., 2018) subsets were constructed by filtering the test set with a hypothesisonly model and only accepting those that the hypothesis-only model predicts incorrectly. We examine whether our method successfully mitigates the hypothesis-only bias in NLI, by evaluating the models trained with our debiased datasets on SNLIhard and MNLI-hard. <ref type="table" target="#tab_4">Table 2</ref> shows the results of our method on SNLI and SNLI-hard. The results show that, compared to training on SNLI, training with our debiased datasets significantly improves the performance on SNLI-hard. The 7 https://github.com/rabeehk/robust-nli debiased dataset produced by Seq-Z achieves a 2.48% gain in accuracy on SNLI-hard compared to the SNLI baseline, whereas Z-Aug improves both SNLI and SNLI-hard accuracy. <ref type="table" target="#tab_6">Table 3</ref> shows the results of our method on MNLI-matched (MNLI-m) and MNLI-mismatched (MNLI-mm), and their corresponding hard sets. We use the development sets of MNLI-hard reconstructed by <ref type="bibr" target="#b17">(Karimi Mahabadi et al., 2020)</ref> to develop our methods. To comply with the submission limit of MNLI leaderboard system, we select the best checkpoint among the five runs using the development set, and report its test set performance in <ref type="table" target="#tab_6">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on SNLI-hard</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MNLI-hard</head><p>The results show that BERT-base models trained on our debiased MNLI datasets outperform the models trained on the original MNLI by a large margin on the MNLI-hard sets. In particular, the Z-Aug version of the debiased datasets gives a 2.72% and 2.76% gain in accuracy on MNLI-m hard and MNLI-mm hard respectively, and outperforms the previous state-of-the-art on MNLI-m, MNLI-mm, and MNLI-mm hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining PoE with Our Debiased Datasets</head><p>We investigate the combination of our method and PoE, to see if the two orthogonal techniques can work together to achieve better performance. Since hyperparameter tuning of PoE is costly, we choose the best version of the debiased dataset (Seq-Z for SNLI and Z-Aug for MNLI) using the development set accuracy, and train PoE with it. The results are listed in the last rows of <ref type="table" target="#tab_4">Table 2 and Table 3</ref>. We can find that, on both SNLI and MNLI, combining PoE with our debiased dataset yields further improvements on SNLI-hard, MNLI-m hard, and MNLI-mm hard, outperforming previous state-ofthe-art results on all three datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Syntactic Bias in NLI</head><formula xml:id="formula_14">BERT-base w/ Z-Aug Z(D G * |D MNLI ) 62.57 ?5.91 BERT-base w/ Par-Z Z(D MNLI ) ? Z(D G * ) 65.11 ?5.62 BERT-base w/ Seq-Z Z(D G * |Z(D MNLI )) 67.69 ?3.53 BERT-base + PoE w/ D MNLI (baseline) 63.40 BERT-base + PoE w/ Z-Aug Z(D G * |D MNLI ) 68.75 Roberta-large w/ D MNLI 75.74 ?2.82 Roberta-large w/ Z-Aug Z(D G * |D MNLI )</formula><p>78.65 ?2.26 of the scores on HANS, we run five times for each experiment (except PoE), and report the average and standard deviation of the scores.   <ref type="bibr" target="#b23">(Liu et al., 2020b)</ref>. We compare with the data augmentation techniques investigated by <ref type="bibr" target="#b23">Liu et al. (2020b)</ref>. * are reported results and underscore indicates statistical significance against the baseline. Training on our debiased MNLI datasets significantly improves the performance on majority of the categories (PI-CD, PI-SP, IS-SD, IS-CS, LI-LI) and on average. ence ability (LI), and stress test (ST). 8 Several data augmentation strategies were investigated by <ref type="bibr" target="#b23">Liu et al. (2020b)</ref>: 1) text swap: swapping the premise and hypothesis in the original data; 2) word substitution: replacing words in the hypothesis with synonyms or generations from a masked language model; 3) paraphrase: using back translation to paraphrase the hypothesis. We compare our approach with their dataaugmentation heuristics, and the results are shown in <ref type="table" target="#tab_9">Table 5</ref>. Comparing with the MNLI baseline, our debiased MNLI datasets lead to better performance across all categories, which indicates that our method successfully mitigates various distinct biases simultaneously. All three variants of our debiased datasets outperform the data augmentation heuristics by , which demonstrates the efficacy of our method when compared against manually designed heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on HANS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Generalisation to Larger Pretrained Language Models</head><p>Since our method mitigates the spurious correlations in the dataset, not the model, our approach is model-agnostic and has the potential to benefit larger future models. To test this hypothesis, we train stronger and more modern models than BERT with our debiased datasets, and see if it can still improve the performance. More specifically, we choose Roberta-base, Roberta-large <ref type="bibr" target="#b24">(Liu et al., 2019)</ref>, and Albert-xxlarge <ref type="bibr" target="#b18">(Lan et al., 2020)</ref>, train them with Seq-Z SNLI and Z-Aug MNLI.</p><p>The results in <ref type="table" target="#tab_11">Table 6</ref> show that: 1) these larger models achieve better generalisation performance than BERT-base, which agrees with <ref type="bibr">Bhargava et al. 8</ref> Details of the subcategories are described in Appendix C.</p><p>(2021); Bowman (2021); 2) training on our debiased datasets can still improve the performance of these models, yielding an average 2.30%, 1.23%, 1.13% gain for Roberta-base, Roberta-large and Albert-xxlarge respectively. This indicates that our method generalises to larger pretrained language models and could potentially enhance future models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spurious Correlations in Datasets</head><p>The issue of spurious correlations in datasets between labels and simple input features has recently received significant attention <ref type="bibr" target="#b14">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b29">Poliak et al., 2018;</ref><ref type="bibr" target="#b1">Belinkov et al., 2019a;</ref><ref type="bibr" target="#b17">Karimi Mahabadi et al., 2020)</ref>. It has been shown that this issue is often inherent in the data annotation process, caused by biases in the framing of the task <ref type="bibr" target="#b34">(Schwartz et al., 2017)</ref>, noisy annotations <ref type="bibr" target="#b7">(Chen et al., 2016)</ref>, or personal <ref type="bibr" target="#b11">(Geva et al., 2019)</ref> or group-level  annotator biases.  provide a theoretical framework for analyzing spurious correlations, which we use to define our filtering mechanism in Section 3.2.</p><p>Debiasing NLI Models Much prior work follows a model-centric approach towards mitigating biases in NLI models -they propose novel model architectures or training objectives to ensure that the models do not exploit the shortcuts presented by the dataset biases. At the representation level, <ref type="bibr">Belinkov et al. (2019a,b)</ref>  Debiasing NLI Datasets  introduce TAILOR, a semantically-controlled perturbation method for data augmentation based on a small number of manually defined perturbation strategies.  propose AFLite, a dataset filtering method that learns feature representations with a model and conduct adversarial filtering based on model predictions. Unlike these approaches, our method requires no manually-written perturbation heuristics and is model-agnostic, hence it is more generally applicable.</p><p>Generative Data Augmentation Several works investigate generative data augmentation techniques to improve model robustness in other areas. <ref type="bibr" target="#b42">Yang et al. (2020)</ref> conduct generative data augmentation for commonsense reasoning and show that it can improve out-of-domain generalisation. <ref type="bibr" target="#b19">Lee et al. (2021)</ref> trains a generator to generate new claims and evidence for debiasing fact verification datasets like FEVER <ref type="bibr" target="#b37">(Thorne et al., 2018)</ref>. <ref type="bibr" target="#b32">Schick and Sch?tze (2021)</ref> exploit large pretrained language models to generate semantic textual similarity datasets. <ref type="bibr" target="#b0">Bartolo et al. (2021)</ref> improve robustness of question answering models by generating adversarial dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>To address the issue of spurious correlations between task-independent features and labels in NLI datasets, we propose methods to generate labelconsistent data and then filter out instances from existing datasets that contribute to those spurious correlations; thereby generating debiased datasets. Models trained on our debiased versions of the SNLI and MNLI datasets generalise better than the equivalent model trained on the original datasets to a large suite of test sets focusing on various kinds of known biases. Future work in this direction includes investigating whether our techniques are applicable to tasks beyond NLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>A.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Tuning of PoE</head><p>The learning objective of PoE is defined as follows:</p><formula xml:id="formula_15">L PoE = |D| i=1 CE(l i , p i ) + ?CE(l i , b i ),</formula><p>where CE stands for cross-entropy loss, l i is the label, and ? is a hyperparameter. p i = sof tmax(log p i + ? log b i ) is the ensemble of the main model's prediction p i , and the bias-only model's prediction b i weighted by a hyperparameter ?.</p><p>We find that the result of PoE is very sensitive to the hyperparameters ? and ?. Following Karimi Mahabadi et al. (2020), we conduct grid search for the two hyperparameters, with ? ? {0.05, 0.1, 0.2, 0.4, 0.8, 1.0, 2.0} and ? ? {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}. The best hyperparameters found for each evaluation dataset is listed in <ref type="table" target="#tab_2">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train data</head><p>Eval. data  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Task-independent Features</head><p>We list the chosen set of task-independent features that we aim to mitigate in this work in <ref type="table" target="#tab_2">Table 11</ref>. Note that our method does not depend on the choice of task-independent features. One can easily add their own features in the future to mitigate newlyidentified spurious correlations. <ref type="table" target="#tab_2">Table 12</ref> shows the most salient taskindependent features (ranked by z-statistics) in SNLI and our debiased SNLI dataset. It shows that the correlation between task-independent features and labels is massively reduced, dropping from over 400 to roughly 17. These results verify that our method successfully mitigates the spurious correlations in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unigrams &amp; Bigrams</head><p>All unigrams and bigrams. The n-grams from premise and hypothesis are treated separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis length</head><p>Number of tokens in the hypothesis.</p><p>Hypothesis-premise length ratio Number of tokens in hypothesis divided by number of tokens in the premise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical overlap</head><p>Ratio of tokens in the hypothesis that overlap with the premise.</p><p>Hypothesis-only model's prediction We train a hypothesis-only model on the original dataset and use its prediction as a feature.</p><p>Null feature A dummy feature added for all instances to avoid skewed label distribution.  The adversarial test benchmark <ref type="bibr" target="#b23">(Liu et al., 2020b)</ref> includes the following subcategories from various sources:</p><p>? PI-CD: classifier detected partial-input <ref type="bibr" target="#b14">(Gururangan et al., 2018)</ref>.</p><p>? PI-SP: HypoNLI <ref type="bibr" target="#b22">(Liu et al., 2020a)</ref> dataset that tackles surface patterns heuristics.</p><p>? IS-SD: syntactic diagnostic dataset HANS <ref type="bibr" target="#b25">(McCoy et al., 2019)</ref>.</p><p>? IS-CS: lexically misleading instances constructed by <ref type="bibr" target="#b28">Nie et al. (2019)</ref>.</p><p>? LI-LI: lexical inference test by <ref type="bibr" target="#b27">(Naik et al., 2018;</ref><ref type="bibr" target="#b13">Glockner et al., 2018)</ref>.</p><p>? LI-TS: text-fragment swap test by swapping the premise and hypothesis <ref type="bibr" target="#b26">Minervini and Riedel, 2018)</ref>.</p><p>? ST: an aggregation of word-overlap (ST-WO), negation (ST-NE), length mismatch (ST-LM), and spelling errors (ST-SE) tests in <ref type="bibr" target="#b27">(Naik et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualisation of z-statistics</head><p>Following , we visualise the statistics of the features on both SNLI and our debiased SNLI (Seq-Z) dataset in <ref type="figure" target="#fig_3">Fig. 2. 9</ref> Comparing the two plots, it confirms that our method successfully suppresses the spurious correlations in the dataset.   F Generated Samples of Debiased Dataset <ref type="table" target="#tab_2">Table 14</ref> and <ref type="table" target="#tab_2">Table 15</ref> show generated samples in the debiased SNLI and MNLI datasets respectively. The samples are quite diverse and the quality is reasonably good, which demonstrates the effectiveness of our quality ensuring techniques presented in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Premise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Label</head><p>Thanksgiving dinner is a fun time for everyone. The dinner is a fun event. entailment A father is letting his toddler drink from his glass.</p><p>A toddler is having a drink entailment Hair stylist performing a haircut.</p><p>A hailer is performing surgery contradiction Then there are two men in white shirts, one of which is holding a cigarette and the other an open book.</p><p>Two men sit at a conference table with a book and a cigarette.</p><p>neutral Three men playing basketball on a court with an audience in the background. Three people playing basketball entailment Six children, boys and girls, jumping into a swimming pool.</p><p>Six children are jumping into a pool entailment Three girls jump for joy in front of a building.</p><p>The kids are sitting on their front steps. contradiction The child in the green one piece suit is running in the playground.</p><p>The child is playing outside entailment View of an intersection with city buses and a police car.</p><p>The intersection is surrounded by vehicles. entailment The man on the yellow basketball team tries to score while the men on the opposing team try to block his shot.</p><p>Two men on different teams are competing in a game of a male is trying to score while other men on the opposing one defend his basket in basketball entailment Young child wearing orange shirt eating a ice cream cone.</p><p>A child eats ice cream at the ice cream stand. neutral Five people standing in front of a shopping center.</p><p>Five people outside the building entailment He's taking a break after a long workout.</p><p>He is taking a break from his workout entailment Two men are sitting on a couch, playing music together.</p><p>The two people play guitars. neutral Everyone is out enjoying the winter weather and having fun with their children. Everyone is out enjoying the summer contradiction Many people walking through a city street.</p><p>There are a group of people in Times Square. neutral A woman in a black dress walks down the street. a person in dresses walks entailment Four children, riding unicycles, are on a sidewalk in front of a brick building. Four children ride unicycles on the sidewalk entailment Four kids playing soccer in a field.</p><p>The children played with bubbles. contradiction MADISON, Wis. (AP) -The man in the white jersey and orange visor threw the ball for the two boys in uniforms with blue jerseys.</p><p>A man in white is throwing a ball to two boys in blue uniforms. entailment Mikhail Kasyapkin, who plays Bart on The Simpsons, is talking to a woman. The woman tells him to stop making couples sit neutral Shutterstock photo of a woman with a heart tattoo on her calf.</p><p>A woman with a pumpkin tattoo on her back contradiction Three women and a man sing their hearts out in the microphone.</p><p>A group singing entailment With so many people on the beach, the woman in yellow has to make a quick decision.</p><p>Many people are at a beach, one has to make a decision entailment Celebrants are walking with American flags.</p><p>People are walking. entailment Customer examines flowers at a market.</p><p>A customer examines flowers. entailment He is in the air on his skateboard.</p><p>A guy is in a tree. contradiction thousands of people enjoying a fireworks show.</p><p>There is an audience for a show. entailment Bicyclists in a race, with a blue bike leaving the ground in the lead.</p><p>Bikers resting after a long ride. contradiction He has a pet bird in a cage, and it is sleeping.</p><p>He is walking the dogs. contradiction There aren, always hasn't usually been oancy situation with violence among theatrical people because they don't have to work because it isn't employment.</p><p>contradiction Nowhere in the book does Hatfield warn the reader that he has altered details or created composite characters to protect his sources.</p><p>Hatfield didn't inform the readers in any part in the book that the details of the altered information was to protect his sources entailment The young inhabitants are brought up knowing nothing else. The young inhabitants have been brought up knowing of nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>entailment</head><p>The 5th floor of the Royal Palace is open to the public, with restricted access for foreign guests.</p><p>Foreign guest have restricted access in the royal palace for visitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>entailment</head><p>Pulitzer Prizes are given to books, magazines, paintings, and sculpture. You won a prize when you eat blueberries at dinner. contradiction I admit I didn't have much reason to think that.</p><p>After all, most of the people don;t think that way. neutral In the past, Medicare's fiscal health has generally been gauged by the solvency of the HI trust fund projected over a 75-year period.</p><p>Medicare's soliesic fitness is displayed in the form of the surplus projected over a 50 year term. contradiction A case study where the only people interviewed were senior officials would be seen as a not-good case study, in contrast to one where the views of individuals at all levels affected was obtained.</p><p>If senior editors were interviewed they would not be considered the best examples for case studies. entailment If you've ever spent an evening plunging your wrists into ice water, you are an easy mark for devices that promise to relieve carpal tunnel syndrome.</p><p>People are easy marks for devices that may cure cat paral tunnel syndrome entailment It's a sign of a permanently altered world that natural blondness should have such sacred power no longer.</p><p>The people still believe blondness has a special significance. contradiction</p><p>The Three-Arched Bridge, by Ismail Kadare, translated by John Hodgson (Arcade).</p><p>Ismail Marare translated The Three-Aral. contradiction</p><p>Many of these organizations found themselves in an environment similar to the one confronting federal managers today-one in which they were called upon to improve performance while simultaneously reducing costs.</p><p>This was the only option for all their group. neutral</p><p>The long-sought, the elusive, the elusive Jane Finn! She is easily obtainable. contradiction And now, to-day, he puts forward a suggestion that he himself must have known was ridiculous.</p><p>He is making the ridiculous suggestion that himself must have been aware of. entailment Jupiter's moon, Callisto, has a thick atmosphere and is a good destination for a quiet tour.</p><p>Callisto's atmosphere makes for a pleasant journey to explore. entailment Founded in 1995, the Agora formed to address the enormous security challenges brought about by new computer, network, and Internet technologies.</p><p>The Agora was formed to address the challenge of nuclear proliferation.</p><p>contradiction Just last week in The New Yorker, Malcolm Gladwell argued that Gen. Just last week in Newsweek, Johnny Chung argued that Gen.</p><p>contradiction Muller and most of the boys can be counted on not to cause any more than the normal pay-night disturbances.</p><p>Muller will not start a fist fight. neutral Don't call me Shirley. My last name is Shirley and that is how I want to be referred to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>contradiction</head><p>The vast majority of the approximately 1,700 lawyers at LSC-funded programs around the country volunteer for only a single case, whether it is a class action suit, a simple civil rights case or a case involving a dangerous person.</p><p>There's no reason to get one or do the work otherwise. neutral</p><p>The Promise Keepers talk far less about abortion and homosexuality than their critics and the media do.</p><p>They're surrounded far less with the issues that the media and other critics deal with. entailment It was Susan in his head. Susan was telling him exactly to his surprise. neutral In 1782, after only a few years, the city decided to impose planning guidelines.</p><p>It took a few decades for 17 year-olds. contradiction <ref type="table" target="#tab_2">Table 15</ref>: Generated samples in the debiased MNLI datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>introduce an adversarial architecture to debias hypothesis representations to tackle hypothesis-only bias (Gururangan et al., 2018), and Stacey et al. (2020) strengthen the debiasing by using multiple adversarial classifiers. Zhou and Bansal (2020) use HEX projection to project the representation to the space orthogonal to the biased features to debias the model. At the model level, Clark et al. (2019); He et al. (2019); Karimi Mahabadi et al. (2020) propose methods based on Product-of-Expert (PoE)<ref type="bibr" target="#b16">(Hinton, 2002)</ref> for mitigating biases by ensembling a biased-only model with a main model.<ref type="bibr" target="#b38">Utama et al. (2020)</ref> propose the use of confidence regularization to improve out-of-distribution performance while retaining in-distribution accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Statistics of the features on SNLI and our debiased SNLI (Seq-Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data size of the constructed debiased datasets for SNLI and MNLI.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Product-of-Experts<ref type="bibr" target="#b15">(He et al., 2019;</ref><ref type="bibr" target="#b17">Karimi Mahabadi et al., 2020)</ref> ensembles a biasonly model's prediction b i with the main model's p i using p i = sof tmax(log p i + log b i ). This ensembling enforces that the main model focuses on the samples that the bias-only model does not predict well. Learned-Mixin (Clark et al., 2019) is a variant of PoE that introduces a learnable weight for the bias-only model's prediction. Regularizedconf (Utama et al., 2020) uses confidence regularisation to retain the in-distribution performance while conducting model debiasing. SNLI SNLI-hard Prior debiasing strategies trained on SNLI AdvCls (Belinkov et al., 2019a) * 83.56 66.27 Ens. AdvCls (Stacey et al., 2020) * 84.09 67.42 DFL (Karimi Mahabadi et al., 2020) * 89.57 83.01 PoE (Karimi Mahabadi et al., 2020) * 90.11 82.15 BERT-base w/ D SNLI baseline 90.45 80.34 ?0.46 Models trained on our debiased datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on SNLI and SNLI-hard. * are reported results and underscore indicates statistical significance against the baseline. Training on our debiased SNLI datasets significantly boosts the performance on SNLI-hard compared to the baseline, and it improves further when combined with PoE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>McCoy et al. (2019) show that NLI models trained on MNLI can exploit syntactic heuristics present in the data, such as lexical overlap, subsequence, and constituent features. They introduce HANS, an evaluation dataset that contains examples where the syntactic heuristics fail. To test whether our method mitigates the syntactic biases in NLI, we evaluate models trained on our debiased datasets on HANS. If our debiased dataset contains less syntactic bias than the original dataset, the model would not exploit the syntactic heuristics and thus perform better on HANS. Due to the high variance MNLI baseline 83.87 84.11 84.22 83.51 76.39 ?0.64 75.88 77.75 ?0.45 75.75 Models trained on our debiased datasets BERT-base w/ Z-Aug Z(D G * |D MNLI ) 84.72 85.12 85.14 84.09 78.95 ?0.76 78.60 80.29 ?0.54 78.51 BERT-base w/ Par-Z Z(D MNLI ) ? Z(D G * ) 82.48 83.27 82.95 82.95 78.88 ?0.80 79.19 80.02 ?0.62 78.49 BERT-base w/ Seq-Z Z(D G * |Z(D MNLI )) 82.55 83.41 82.70 83.17 78.88 ?0.83 79.19 79.65 ?0.44 78.44</figDesc><table><row><cell>Method (model w/ data)</cell><cell cols="2">MNLI-m</cell><cell cols="2">MNLI-mm</cell><cell cols="2">MNLI-m hard</cell><cell cols="2">MNLI-mm hard</cell></row><row><cell></cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell></row><row><cell>Prior debiasing strategies trained on MNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PoE (Karimi Mahabadi et al., 2020) *</cell><cell cols="5">84.58 84.11 84.85 83.47 78.02</cell><cell cols="2">76.81 79.23</cell><cell>76.83</cell></row><row><cell>Learned-Mixin (Clark et al., 2019) *</cell><cell>80.5</cell><cell>79.5</cell><cell>81.2</cell><cell>80.4</cell><cell>-</cell><cell>79.2</cell><cell>-</cell><cell>78.2</cell></row><row><cell>Regularized-conf (Utama et al., 2020) *</cell><cell>84.6</cell><cell>84.1</cell><cell>85.0</cell><cell>84.2</cell><cell>-</cell><cell>78.3</cell><cell>-</cell><cell>77.3</cell></row><row><cell cols="3">BERT-base Main PoE+CE (Sanh et al., 2021) *  83.32 -</cell><cell cols="2">83.54 -</cell><cell>-</cell><cell cols="2">77.63 -</cell><cell>76.39</cell></row><row><cell>BERT-base w/ D Combining PoE with our debiased dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base + PoE w/ D MNLI</cell><cell cols="5">84.39 84.69 84.25 83.75 78.37</cell><cell cols="2">77.54 79.45</cell><cell>78.33</cell></row><row><cell>BERT-base + PoE w/ Z-Aug Z(D G  *  |D MNLI )</cell><cell cols="5">85.22 85.38 85.72 84.53 80.49</cell><cell cols="2">80.03 81.52</cell><cell>79.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>HANS</cell></row><row><cell>Methods trained on SNLI</cell><cell></cell></row><row><cell>BERT-base Attention (Stacey et al., 2021) *</cell><cell>58.42</cell></row><row><cell>Roberta-large w/ AFLite (Bras et al., 2020) *</cell><cell>59.6</cell></row><row><cell cols="2">Roberta-base w/ TAILOR (Ross et al., 2021) *  70.5</cell></row><row><cell>Methods trained on MNLI</cell><cell></cell></row><row><cell>Learned-Mixin (Clark et al., 2019) *</cell><cell>64.00</cell></row><row><cell>Learned-Mixin+H (Clark et al., 2019) *</cell><cell>66.15</cell></row><row><cell>PoE (Karimi Mahabadi et al., 2020) *</cell><cell>66.31 ?0.6</cell></row><row><cell>DFL (Karimi Mahabadi et al., 2020) *</cell><cell>69.26 ?0.2</cell></row><row><cell>PoE+CE (Sanh et al., 2021) *</cell><cell>67.9</cell></row><row><cell>Regularized-conf (Utama et al., 2020) *</cell><cell>69.1 ?1.2</cell></row><row><cell>E2E Self-debias (Ghaddar et al., 2021) *</cell><cell>71.2 ?0.2</cell></row><row><cell>Models trained on our debiased datasets</cell><cell></cell></row><row><cell>Roberta-base w/ D SNLI</cell><cell>65.32 ?2.22</cell></row><row><cell>Roberta-base w/ Seq-Z Z(D G</cell><cell></cell></row></table><note>Accuracy on MNLI-matched (MNLI-m), MNLI-mismatched (MNLI-mm), MNLI-matched hard, and MNLI-mismatched hard. * are reported results and underscore indicates statistical significance against the baseline. Training on our debiased MNLI datasets significantly boosts the performance on MNLI-matched hard and MNLI- mismatched hard. When combined with PoE, our method improves further and outperforms previous methods.* |Z(D SNLI )) 66.87 ?1.47 BERT-base w/ D MNLI baseline 54.36 ?2.56</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on HANS (McCoy et al., 2019). * are reported results and underscore indicates statistical significance against the baseline. BERT-base trained on our debiased MNLI datasets performs significantly bet-</figDesc><table /><note>ter than the one trained on the original MNLI, and it im- proves further when combined with PoE. Roberta-large also benefits from training on our debiased dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>?0.5 73.7 ?1.4 53.5 ?2.3 64.8 ?1.4 85.5 ?0.9 81.6 ?1.4 69.2 ?0.8 71.2 ?0.8 Models trained on our debiased datasetsBERT-base w/ Z-Aug Z(D G * |D MNLI )73.1 ?0.9 76.1 ?1.2 61.8 ?6.1 69.1 ?1.3 86.9 ?0.6 83.1 ?0.9 70.1 ?0.5 74.3 ?1.3 BERT-base w/ Par-Z Z(D MNLI ) ? Z(D G * ) 72.0 ?0.9 78.7 ?1.2 64.5 ?5.8 70.7 ?1.7 88.5 ?0.7 82.6 ?0.3 69.6 ?1.0 75.2 ?1.4 BERT-base w/ Seq-Z Z(D G * |Z(D MNLI )) 71.7 ?0.9 77.8 ?1.2 66.9 ?3.7 71.1 ?0.7 89.1 ?1.0 82.3 ?0.9 69.3 ?0.8 75.4 ?0.8</figDesc><table><row><cell>shows the results on</cell></row></table><note>ten tie to one particular known bias and it is non- trivial to mitigate multiple NLI biases at the same time. They introduce a suite of test datasets for NLI models that targets various aspects of robustness, including partial input heuristics (PI), logical infer-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on the NLI adversarial test benchmark</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>?0.44 83.14 ?0.25 1.40 MNLI-mm hard 81.93 ?0.30 83.12 ?0.24 1.19 HANS 71.17 ?2.95 76.15 ?1.52 4.98 Adv.Test avg 77.63 ?0.49 79.89 ?0.38 2.26 ?0.62 85.69 ?0.24 0.25 MNLI-mm hard 85.37 ?0.63 85.94 ?0.21 0.57 HANS 75.74 ?2.82 78.65 ?2.26 2.91 Adv.Test avg 80.92 ?0.46 81.86 ?0.31 0.94</figDesc><table><row><cell>Test data</cell><cell>Original</cell><cell>Debiased</cell><cell>?</cell></row><row><cell cols="4">SNLI-hard MNLI-m hard SNLI-hard 81.74 Roberta-large 82.02 ?0.24 83.71 ?0.31 1.69 Roberta-base 83.61 ?0.31 85.09 ?0.32 1.48 MNLI-m hard SNLI-hard 83.59 84.82 1.23 MNLI-m hard 86.42 86.40 -0.02 MNLI-mm hard 86.38 86.82 0.44 HANS 76.32 79.05 2.73 85.44 Albert-xxlarge Adv.Test avg 81.91 83.18 1.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performance gain when training larger models with our debiased datasets. Underscore indicates statistical significance against the baseline that is trained on the original datasets. For evaluation on SNLI-hard, the models are trained with SNLI or our debiased Seq-Z SNLI; for other evaluation datasets, the models are trained with MNLI or our debiased Z-Aug MNLI.</figDesc><table /><note>Albert-xxlarge is experimented with one run due to its higher training cost.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for training the generator G</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>number of samples from G  *  SNLI number of samples from G  *  MNLI</cell><cell>5,000,000 4,000,000</cell></row><row><cell>data filtering threshold ?</cell><cell>0.95</cell></row><row><cell>data filtering model</cell><cell>Roberta-large</cell></row><row><cell>z-filtering number of biased features</cell><cell>20</cell></row></table><note>* .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters of the data generation pipeline.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>learning rate</cell><cell>1e-5</cell></row><row><cell>batch size</cell><cell>32</cell></row><row><cell>epoch</cell><cell>5</cell></row><row><cell>optimiser</cell><cell>Adam</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row><row><cell>Adam (? 1 , ? 2 )</cell><cell>(0.9, 0.999)</cell></row><row><cell cols="2">learning rate scheduler constant with warmup</cell></row><row><cell>warm up steps</cell><cell>2000</cell></row><row><cell>max sequence length</cell><cell>128</cell></row><row><cell>pretrained model</cell><cell>BERT-base</cell></row><row><cell>device</cell><cell>Nvidia A100</cell></row><row><cell>early stop patience</cell><cell>3 epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters for training the NLI models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Best hyperparameters found for PoE models with different training and evaluation datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Descriptions of the features used to debias the datasets in Section 3.</figDesc><table><row><cell>SNLI</cell><cell></cell><cell cols="2">Debiased SNLI (Seq-Z)</cell></row><row><cell>Biased feature</cell><cell cols="2">z-statistics Biased feature</cell><cell>z-statistics</cell></row><row><cell>Entailment</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hypo-only-pred=0</cell><cell>422.1</cell><cell>theres@hypothesis</cell><cell>17.5</cell></row><row><cell>lex-overlap&gt; 0.8</cell><cell>123.3</cell><cell>hypo-len&lt; 5</cell><cell>17.4</cell></row><row><cell>full-lex-overlap</cell><cell>117.3</cell><cell>full-lex-overlap</cell><cell>17.4</cell></row><row><cell>outside@hypothesis</cell><cell>102.2</cell><cell>politician@hypothesis</cell><cell>17.4</cell></row><row><cell>lex-overlap&gt; 0.9</cell><cell>90.4</cell><cell>speaking@hypothesis</cell><cell>17.4</cell></row><row><cell>Neutral</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hypo-only-pred=1</cell><cell>436.1</cell><cell>championship@hypothesis</cell><cell>15.3</cell></row><row><cell>for a@hypothesis</cell><cell>63.6</cell><cell>living room@hypothesis</cell><cell>15.2</cell></row><row><cell>his@hypothesis</cell><cell>56.8</cell><cell>many men@hypothesis</cell><cell>15.2</cell></row><row><cell>friends@hypothesis</cell><cell>55.6</cell><cell>green suit@hypothesis</cell><cell>15.2</cell></row><row><cell>tall@hypothesis</cell><cell>52.7</cell><cell>are wearing@hypothesis</cell><cell>15.2</cell></row><row><cell>Contradiction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hypo-only-pred=2</cell><cell>433.9</cell><cell>nothing@hypothesis</cell><cell>17.0</cell></row><row><cell>sleeping@hypothesis</cell><cell>92.9</cell><cell>hypo-only-pred=2</cell><cell>16.9</cell></row><row><cell>is sleeping@hypothesis</cell><cell>68.7</cell><cell>at home@hypothesis</cell><cell>16.9</cell></row><row><cell>nobody@hypothesis</cell><cell>68.4</cell><cell>is no@hypothesis</cell><cell>16.9</cell></row><row><cell>no@hypothesis</cell><cell>62.7</cell><cell>york yankees@hypothesis</cell><cell>16.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: Top-5 biased features with the highest z-</cell></row><row><cell>statistics on SNLI (left) and debiased Seq-Z SNLI</cell></row><row><cell>(right) for each label class.</cell></row><row><cell>C Description of Adversarial Test (Liu</cell></row><row><cell>et al., 2020b) Subcategories</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Ablation study conducted on SNLI and SNLIhard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Generated samples in the debiased SNLI datasets.</figDesc><table><row><cell>Premise</cell><cell>Hypothesis</cell><cell>Label</cell></row><row><cell>As I noted earlier, the board and the auditors should have a strategic</cell><cell>The board should align to increase efficiency.</cell><cell>neutral</cell></row><row><cell>alignment of interests.</cell><cell></cell><cell></cell></row><row><cell>This story was originally published in Slate. For more on the U.S. role in</cell><cell>The U.S. played very little part in the war.</cell><cell>neutral</cell></row><row><cell>that war, subscribe to Slate's Subscribe now!</cell><cell></cell><cell></cell></row><row><cell>Via Newsday's a poll finds that 84 percent of Americans think Monica</cell><cell>A majority of the public thinks Lewinsky should come</cell><cell>entailment</cell></row><row><cell>Lewinsky should tell the truth about her encounter with Clinton.</cell><cell>forward.</cell><cell></cell></row><row><cell>Violence among theatrical people, on the other hand, can be entertainingly</cell><cell></cell><cell></cell></row><row><cell>savage, cf, All About Eve (1884) and The Mousetrap (1928).</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In our preliminary study, we found the factorization order premise-label-hypothesis in Eq. (1) performs better than hypothesis-label-premise and premise-hypothesis-label.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is also strong confirmation that these biases are problematic, as the generative model easily finds them and relies on them during data generation. Conducting naive data augmentation withDG SN LI will strengthen the spurious correlations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">On one A100 GPU, training the generator takes around 24 hours and generating the samples takes roughly 35 hours for each dataset.6  With the exception of our PoE experiments which single run, as hyperparameter tuning for PoE is costlier.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We sample 10% of the points under the z = 10.0 curve to compress the figure, but it may still be slow to render the figures because the number of points is still large.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Max Bartolo, Alexis Ross, Doug Downey, Jesse Dodge, Pasquale Minervini, and Sebastian Riedel for their helpful discussion and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving question answering model robustness with synthetic adversarial data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.696</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8830" to="8848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t take the premise for granted: Mitigating artifacts in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="877" to="891" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On adversarial removal of hypothesis-only bias in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</title>
		<meeting>the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="256" to="262" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalization in NLI: Ways (not) to go beyond simple heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.insights-1.18</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Insights from Negative Results in NLP</title>
		<meeting>the Second Workshop on Insights from Negative Results in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">When combating hype, proceed with caution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<idno>abs/2110.08300</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial filters of dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1078" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4069" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Competency problems: On finding and removing artifacts in language data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1801" to="1813" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1166" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end self-debiasing framework for robust NLU training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillippe</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Rashid</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.168</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1923" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Breaking NLI systems with sentences that require simple lexical inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="650" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unlearn dataset bias in natural language inference by fitting the residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="132" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end bias mitigation by modelling biases in corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Rabeeh Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.769</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8706" to="8716" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crossaug: A contrastive data augmentation method for debiasing fact verification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungpil</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheoneum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3181" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PAQ: 65 million probably-asked questions and what you can do with them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00415</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1098" to="1115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Toward annotator group bias in crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Thekinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinem</forename><surname>Mollaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2110.08038</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HypoNLI: Exploring the artificial patterns of hypothesis-only bias in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6852" to="6860" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical study on model-agnostic debiasing strategies for robust natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.conll-1.48</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
		<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarially regularising neural NLI models to integrate logical background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stress test evaluation for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2340" to="2353" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing compositionality-sensitivity of NLI models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016867</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6867" to="6874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-2023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tailor: Generating and perturbing text with semantic controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno>abs/2107.07150</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from others&apos; mistakes: Avoiding dataset biases without modeling them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating datasets with pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6943" to="6951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards debiasing fact verification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun Jie Serene</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Roberto Filizzola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1341</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3419" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Supervising model attention with human explanations for robust natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno>abs/2104.08142</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.665</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8281" to="8291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafise Sadat</forename><surname>Prasetya Ajie Utama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.770</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8717" to="8729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What if we simply swap the two text fragments? A straightforward yet effective way to test the robustness of methods to confounding signals in nature language inference tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017136</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7136" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">G-daug: Generative data augmentation for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiben</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Ping</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings), volume EMNLP 2020 of Findings of ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1008" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards robustifying NLI models against lexical dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.773</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8759" to="8771" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
