<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Referring Transformer: A One-step Approach to Multi-task Visual Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
							<email>muchenli@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CIFAR AI Chair</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">NSERC CRC Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Referring Transformer: A One-step Approach to Multi-task Visual Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an important step towards visual reasoning, visual grounding (e.g., phrase localization, referring expression comprehension / segmentation) has been widely explored. Previous approaches to referring expression comprehension (REC) or segmentation (RES) either suffer from limited performance, due to a two-stage setup, or require the designing of complex task-specific one-stage architectures. In this paper, we propose a simple one-stage multi-task framework for visual grounding tasks. Specifically, we leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. With this simple but highly contextualized model, we outperform state-of-the-art methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model benefits greatly from contextualized information and multi-task training.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal grounding 1 tasks (e.g., phrase localization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref>, referring expression comprehension <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> and segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>) aim to generalize traditional object detection and segmentation to localization of regions (rectangular or at a pixel level) in images that correspond to free-form linguistic expressions. These tasks have emerged as core problems in vision and ML due to the breadth of applications that can make use of such techniques, spanning image captioning, visual question answering, visual reasoning and others.</p><p>The majority of multi-modal grounding architectures, to date, take the form of two-stage approaches, inspired by Faster RCNN <ref type="bibr" target="#b43">[44]</ref> and others, which first generate a set of image region proposals and then associate/ground one, or more, of these regions to a phrase by considering how well the content matches the query phrase. Context among the regions and multiple query phrases, which often come parsed from a single sentence, has also been considered in various ways (e.g., using LSTM stacks <ref type="bibr" target="#b8">[9]</ref>, graph neural networks <ref type="bibr" target="#b0">[1]</ref> and others). More recent variants leverage pre-trained multi-modal Transformers (e.g., ViLBERT <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>) to fine-tune to the grounding tasks. Such models have an added benefit of being able to learn sophisticated cross-modal feature representations from external <ref type="bibr" target="#b0">1</ref> Grounding and referring expression comprehension have been used interchangeably in the literature. While the two terms are indeed trying to characterize the same task of associating a lingual phrase with an image region, there is a subtle difference in that referring expressions tend to be unique and hence need to be grounded to a single region, e.g., "person in a red coat next to a bus". The grounding task, as originally defined in <ref type="bibr" target="#b40">[41]</ref>, is more general where a lingual phrase may be ambiguous and therefore grounded to multiple regions, e.g., "a person". large-scale data, which further improve the performance. However, a significant limitation of all such two-stage methods is their inability to condition the proposal mechanism on the query phrase itself, which inherently limits the upper bound of performance (see <ref type="table" target="#tab_2">Table 3</ref> in <ref type="bibr" target="#b49">[50]</ref>).</p><p>To address these limitations, more recently, a number of one-stage approaches have been introduced <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. Most of these take inspiration from Yolo <ref type="bibr" target="#b42">[43]</ref> and the variants, and rely on more integrated visual-linguistic fusion and a dense anchoring mechanism to directly predict the grounding regions. While this alleviates the need for a proposal stage, it instead requires somewhat ad hoc anchor definitions, often obtained by clustering of labeled regions, and also limits ability to contextualize grounding decisions as each query phrase is effectively processed independently. Finally, little attention in the literature has been given to leveraging relationship among the REC and RES tasks.</p><p>In this work we propose an end-to-end one-stage architecture, inspired by the recent DETR <ref type="bibr" target="#b1">[2]</ref> detection framework, which is capable of simultaneous language grounding at both a boundingbox and segmentation level, without requiring dense anchor definitions. This model also enables contextualized reasoning by taking into account the entire image, all referring query phrases of interest and (optionally) lingual context (e.g., a sentence from which referring phrases are parsed). Specifically, we leverage a transfomer architecture, with a visual-lingual encoder, to encode image and lingual context, and a two-headed (detection and segmentation) custom contextualized tranformer decoder. The contextualized decoder takes as input learned contextualized phrase queries and decodes them directly to bounding boxes and segmentation masks. Implicit 1-to-1 correspondence between input referring phrases and resulting outputs also enables a more direct formulation of the loss without requiring Hungarian matching. With this simple model we outperform state-of-the-art methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model benefit greatly from the contextualized information and the multi-task training.</p><p>Contributions. Our contributions are: (1) We propose a simple and general one-stage transformerbased architecture for referring expression comprehension and segmentation. The core of this model is the novel transformer decoder that leverages contextualized phrase queries and is able to directly decode those, subject to contextualized image embeddings, into corresponding image regions and segments; (2) Our approach is unique in enabling simultaneous REC and RES using a single trained model (the only other method capable of this is <ref type="bibr" target="#b35">[36]</ref>); showing that such multi-task learning leads to improvements on both tasks; (3) As with other transformer-based architectures, we show that pre-training can further improve the performance and both vanila and pre-trained models outperform state-of-the-art on both tasks by significant margins (up to 8.5% on refcoco dataset for REC and 19.4% for RES). We also thoroughly validate our design in detailed ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Referring Expression Comprehension (REC). REC focuses on producing an image bounding box tightly encompassing a language query. Previous two-staged works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">55]</ref> reformulate this as a ranking task with a set of candidate regions predicted from a pre-trained proposal mechanism. Despite achieving great success, the performance of two-staged methods is capped by the speed and accuracy of region proposals in the first stage. More recently, one-stage approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> have been used to alleviate the aforementioned limitations. Yang et al. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> proposed to fuse query information with visual features and pick the bounding box with maximum activation scores from YOLOv3 <ref type="bibr" target="#b42">[43]</ref>. Liao et al. <ref type="bibr" target="#b25">[26]</ref> utilizes CenterNet <ref type="bibr" target="#b10">[11]</ref> to perform correlation filtering for region center localization. However, such methods either require manually tuned anchor boxes or suffer from semantic loss due to modality misalignment. In contrast, our model learns to better align modalities using a cross-modal transformer and directly decode bounding boxes for each query.</p><p>Referring Expression Segmentation (RES). Similar to REC, RES, proposed in <ref type="bibr" target="#b17">[18]</ref>, aims to predict segmentation masks to better describe the shape of the referred region. A typical solution for referring expression segmentation is to fuse multi-modal information with a segmentation network (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>) and train it to output the segmented masks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>. More recent approaches focus on designing module to enable better multi-modal interactions, e.g., progressive multi-scale fusion used in <ref type="bibr" target="#b20">[21]</ref> and cross-modal attention block used in <ref type="bibr" target="#b19">[20]</ref>. Since localization information matters in predicting instance segmentations (as noted in Mask RCNN <ref type="bibr" target="#b15">[16]</ref>), very recent work <ref type="bibr" target="#b21">[22]</ref> aims to explicitly localize object before doing segmentation. Despite the relatively high performance being achieved in RES, existing approaches still struggle to determine the correct referent region and tend to output noisy segmentation results with an irregular shape, while our model is able to produce segmentations with fine-grained shapes even on challenging scenarios with occlusions or shadows.</p><p>Multi-task Learning for REC and RES. Multi-task learning is widely applied in object detection and segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, often, by leveraging shared backbone and task-specific heads. Building on this idea, Luo et al. <ref type="bibr" target="#b35">[36]</ref> proposed a multi-task collaborative network (MCN) to jointly address REC and RES. They introduce consistency energy maximization loss that constrains the feature activation map in REC and RES to be similar. While our model is also set up to learn REC and RES tasks jointly, we argue that an explicit constraint tends to downplay the quality of the final predicted mask since the feature map from the REC branch can blur out fine-grained region shape information needed by the RES branch (see <ref type="figure" target="#fig_1">Figure 2</ref>). Hence, we use an implicit constraint where tasks head of REC and RES are trained to output corresponding bounding box and mask from the same joint multi-modal representation. We illustrate that our model can benefit from multi-task supervision, leading to more accurate results as compared to single-task variants.</p><p>Pretrained Multi-modal Transformers. Transformer-based pretrained models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref> have recently showed strong potential in multi-modal understanding. LXMERT <ref type="bibr" target="#b46">[47]</ref> and ViLBERT <ref type="bibr" target="#b32">[33]</ref> use two stream transformers with cross-attention transformer layers on top for multimodal fusion. More recent works, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> advocate a single-stream design to fuse two modalities earlier. The success of the aforementioned models can largely be attributed to the cross-modal representations obtained by multi-task pretraining on a large amount of aligned image-text pairs. Despite state-of-the-art performance of such models on the downstream REC task, these models, fundamentally, are still a form of a two-stage pipeline where image features are extracted using pretrained detectors or proposal mechanisms. We focus on a one-stage architecture variant that allows visual and lingual features to be aligned at the early stages. Although the focus of our work is not to design a better pretraining scheme, we show that our model can outperform the existing state-of-the-art with proper pretraining.</p><p>Transformer-based Detectors. More recently, DETR <ref type="bibr" target="#b1">[2]</ref> and its variants <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b56">57]</ref>, were proposed to enable end-to-end object detection. DETR reformulates detection as a set prediction tasks and uses transformers to decode learnable queries to bounding boxes. Despite state-of-art performance, DETR is disadvantaged by its optimization difficulty and, usually, extra-long training time. While adopting a similar pipeline, our model focus on aligning different modalities to generate contextualized expression-specific referring queries. We also design our model to get rid of Hungarian matching loss by leveraging one-to-one correspondences between predicted bounding boxes and referring expressions, which leads to faster convergence for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contemporaneous</head><p>Works. Concurrent and independent to us, very recently, there are some works that use transformers for visual referring tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. All three of these are nonrefereed/unpublished at the time of this manuscript upload to ArXiv. Importantly, unlike <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref> our approach formulated in multi-task setting and solves both REC and RES tasks simultaneously; while the aforementioned works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> focus on REC specifically. In addition, our model is faster and is capable of grounding multiple contextualized phrases, while <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> follow previous one-stage approaches and are only able to infer a single expression at a time; leading, in our case, to more accurate results. Finally, <ref type="bibr" target="#b22">[23]</ref> builds on DETR <ref type="bibr" target="#b1">[2]</ref> while our approach deviates from DETR in a number of ways, including in terms of the loss, and focuses more on the end-to-end joint multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Given an image I and a set of query phrases / referring expressions Q p = {p i } i=1,...,M , that we assume to come from an (optional) contextual text source 2 Q, our goal is to predict a set of bounding boxes B = {b i } i=1,...,M and corresponding segmentation masks S = {s i } i=1,...,M , one for each query phrase i that localizes that phrase in the image. Note, M is the number of phrases / referring expressions for a given image I and is typically between 1 and 16 for the Flickr30k <ref type="bibr" target="#b40">[41]</ref> dataset.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our referring transformer is composed of four stages. Given an image-(con)text pair, &lt; I, Q &gt;, a cross-modal encoder generates joint image-text embeddings for each visual and textual token -feature columns and word embeddings respectively. Phrases / referring expressions Q p and the corresponding pooled joint embeddings (from cross-modal encoder) are then fed into a query generator which produces phrase-specific queries. The decoder jointly reasons across all these queries and decodes joint image-(con)text embeddings. The decoded feature is then sent to the detection and segmentation head to produce a set of detection boxes B and segmentation masks S.</p><p>The result is a one-staged end-to-end model that solves the REC and RES tasks at the same time. We will now introduce constituent architectural components for the four stages briefly described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction</head><p>Visual &amp; Text Backbone. Starting from an initial image I ? R 3?H0?W0 , we adopt the widely used ResNet <ref type="bibr" target="#b14">[15]</ref> to generate its low-resolution feature map f I ? R Ci?HW . For the corresponding expression or sentence, we use the uncased base of BERT <ref type="bibr" target="#b7">[8]</ref> to obtain the representation f Q ? R Ct?N , while N is the length of the input context sentence. Visual-Lingual Encoder. The visual-lingual encoder is designed to fuse information from multimodal sources. For cross-modality encoding, we use a transformer encoder based model, which is composed of 6 transformer encoder layers. Specifically, given both image and text features, multilayer perceptions are applied first to project different modalities to a joint embedding space with a hidden dimension of C. Since transformer based encoders are permutation invariant and do not preserve positional information, we follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> to add cosine positional embedding P img for image features and learnable positional embedding P text for text features. We then concatenate the projected features into a single sequence f ? R C?(HW +N ) . To distinguish between modalities, we also deign a learnable modal label embedding E label : {E img , E text } which is added to the original sequences. The visual-lingual encoder then takes a sequence as input, and {P img , P text , E label } are fed into each encoder layer. The encoder output is a multi-modal feature sequence f vl ? R C?(HW +N ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Referring Decoder</head><p>The referring decoder aims to decode the phrase query, conditioned on visual-lingual features from the encoder, into an output bounding box and segmentation. In this stage, we first generate query embeddings corresponding to each referring phrase. Then these query embeddings are fed into the decoder together with the joint embedding from the encoder to generate outputs.</p><p>Encoding Referring Queries. To enable the decoder to generate the desired output (bounding box and/or segmentation) the queries must encode several bits of crucial information. Mainly, (1) encoding of the referring phrase, (2) image encoding and (3) phrase-specific optional (con)text information.</p><p>For phrase encoding in <ref type="bibr" target="#b0">(1)</ref> we use a BERT model with pooling heads which share weights with (con)text encoder; this results in the phrase feature vector f pi ? R C for the i-th referring phrase. We note that because we use visual-lingual encoder, <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> are jointly encoded in multi-modal features f vl described in Section 3.1 above. However, f vl is phrase-agnostic encoding of the image and (con)text. To generate phrase-specific context, given a phrase p i , average pooling is used to extract the phrase-specific context information f c (p i ) from the visual-lingual feature sequence as follows:</p><formula xml:id="formula_0">f c (p i ) = f vl [l pi : r pi ] r pi ? l pi<label>(1)</label></formula><p>where l pi and r pi denotes the left and right bounds of phrase p i in the original (con)text sentence. Finally, given phrase encoding f pi and phrase-specific context f c (p i ) we construct our phrase queries using a multi-later perceptron:</p><formula xml:id="formula_1">Q pi = MLP ([f c (p i ); f pi ]) + E p ,<label>(2)</label></formula><p>where E p ? R C is a learnable embedding which serves as a bias to the formed query.</p><p>Decoding. In the decoder, an attention graph convolution layer is used to enable information flow in a dense connected graph of phrase queries. This allows phrase queries to contextualize and refine each other; the inspiration for this step is taken from <ref type="bibr" target="#b0">[1]</ref>. After that, a cross attention layer decodes visual-lingual information given the updated phrase query and feature sequence from the encoder. The design of our decoder is similar to the transformer decoder, except attention is non-causal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task training</head><p>In this section, we demonstrate how the decoded phrase-specific query features can be naturally used to train multiple heads for different referring tasks (regression for REC and segmentation for RES).</p><p>Referring Comprehension/Detection (REC). For referring detection tasks, the final output is computed by a simple two-layer perceptron over the decoded phrase-specific query features. We let the detection head directly output center coordinatesb = (x, y, h, w) for the referred image. To supervise the training, we use a weighted sum of an L1 loss and a Generalized IOU loss <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_2">L det = ? iou L iou (b,b) + ? L1 ||b ?b|| 1 .<label>(3)</label></formula><p>The ? iou and ? L1 control the relative weighting of the two losses in the REC objective.</p><p>Referring Segmentation (RES). Following previous work <ref type="bibr" target="#b1">[2]</ref>, we design an FPN-like architecture to predict a referring segmentation mask for each phrase expression. Attention masks from the decoder and image features from the visual-lingual encoder are concatenated as the FPN input, while features from different stages of image backbones are used as skip connections to refine the final output. The last linear layer project the upsampled feature to a single channel heatmap and a sigmoid function is used to map the feature to mask scoress ? R H0/4?W0/4 . The loss for training RES task is:</p><formula xml:id="formula_3">L seg = ? f ocal L f ocal (s,s) + ? dice L dice (s,s).<label>(4)</label></formula><p>Here L f ocal is the focal loss for classifying pixels used in <ref type="bibr" target="#b27">[28]</ref>, L dice is the DICE/F-1 loss proposed in <ref type="bibr" target="#b38">[39]</ref>; ? f ocal and ? dice are hyper-parameters controlling the relative importance of the two losses.</p><p>Joint Training. While it is possible to train referring segmentation and referring detection tasks separately, we find that joint training is highly beneficial. Therefore the combined training loss which we optimize is L = L seg + L det .</p><p>Pretraining the Transformer. Transformers are generally data hungry and requires a lot of data to train <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>. Although in this paper we do not use a large pretraining model and a lot of data. We found that simple preatraining strategy on the region description splits of Visual-Genome dataset <ref type="bibr" target="#b24">[25]</ref> makes our model achieve comparable and even better performance against some of state-of-the-art pretrained models. Interestingly, we found that although there is no ground truth segmentation provided in Visual-Genome, the RES task can still benefit greatly from pretrained models, likely due to the fine-tuned multi-task representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>RefCOCO/RefCOCO+/RefCOCOg (REC&amp;RES). RefCOCO, RefCOCO+ <ref type="bibr" target="#b53">[54]</ref> and RefCOCOg <ref type="bibr" target="#b39">[40]</ref> are collections of images and referred objects from MSCOCO <ref type="bibr" target="#b26">[27]</ref>. On RefCOCO and Ref-COCO+ we follow the split used in <ref type="bibr" target="#b53">[54]</ref> and report scores on the validation, testA and testB splits. On RefCOCOg, we use the RefCOCO-umd splits proposed in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Flickr30k Entities (REC). Flickr30k Entities <ref type="bibr" target="#b40">[41]</ref> contains 31,783 images and 158k caption sentences with 427k annotated phrase. We use splits from <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Bounding boxes and phrase annotations are consistent with the previous one-stage approaches <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> for fair comparisons.</p><p>ReferIt (REC). The ReferItGame dataset <ref type="bibr" target="#b23">[24]</ref> contains 20,000 images. We follow setup in <ref type="bibr" target="#b2">[3]</ref> for splitting train, validation and test set; resulting in 54k, 6k and 6k referring expressions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementing Details</head><p>We train our model with AdamW <ref type="bibr" target="#b31">[32]</ref>. The initial learning rate is set to 1e-4 while the learning rate of image backbone and context encoder is set to 1e-5. We initialized weights in the transformer encoder and decoder with Xavier initialization <ref type="bibr" target="#b13">[14]</ref>. For image backbone, we experiment with the popular ResNet-50 and ResNet-101 networks <ref type="bibr" target="#b14">[15]</ref> where weights are initialized from corresponding ImageNet-pretrained models. For the context encoder and phrase encoder, we use an uncased version of BERT model <ref type="bibr" target="#b7">[8]</ref> with weights initialized from pretrained checkpoints provided by HuggingFace <ref type="bibr" target="#b48">[49]</ref>. For data augmentation, we scale images such that the longest side is 640 pixels and follow <ref type="bibr" target="#b49">[50]</ref> to do random intensity saturation and affine transforms. We remove the random horizontal flip augmentation used in previous work <ref type="bibr" target="#b49">[50]</ref> since we notice it causes semantic ambiguity on RefCOCO, likely due to relative location (e.g., left of/right of) specific queries in the dataset.</p><p>On Flickr30k dataset, we set the maxium length of context sentence to 90 and maximum number of referring phrases to 16. On the ReferIt and the RefCOCO dataset, only phrase expressions are provided and the task aims to predict a single bounding box for each of the expressions. In those cases, the context sentence is taken as the referring phrase expression itself. We set the maximum length of context sentence on these two datasets to 40. To fairly compare with pretrained methods, we use region description split in the VisualGenome <ref type="bibr" target="#b24">[25]</ref> to pretrain our model. The dataset contains approximately 100k images and we remove the images that appears in Flickr30k Entities and RefCOCO/RefCOCOg/RefCOCO+'s validation and test set to avoid potential test data leak. For all the pretrained methods, we only train the model on pretraining dataset for 6 epoches. We find that longer pretraining schedule gives better performance, but since the focus of this paper is not on pretraining methods, we stick to shorter pretraining schedules to save computational resources. All experiments are conducted using 4 Nvidia 2080TI GPU with batch size as 8. For all the results given, we run experiments several times with random seeds and the error bars are within ?0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Analysis</head><p>Evaluation Metrics. For referring expression comprehension (REC), consistent with prior works, we use precision as the evaluation metric. We mark a referring detection as correct when the intersection-over-union (IoU) between the predicted bounding box and ground truth is larger than 0.5. For referring expression segmentation (RES), we reported the Mean IoU (MIoU) between the predicted segmentation mask and ground truth mask.</p><p>REC and RES on Refcoco/Refcoco+/Refcocog. Our model addresses REC and RES tasks jointly. We compare their respective performances with the state-of-the-art in <ref type="table" target="#tab_1">Table 6 and Table 2</ref>. In <ref type="table">Table 6</ref>, the model is first compared with previous one-stage and two-stage approaches for REC. Without bells and whistles, we observe a consistent performance boost of +2.7%/ + 4%/ + 2.1% on RefCOCO, +6.6%/ + 4.3%/ + 8.5% on RefCOCO+ and +4.4%/ + 5.1% on RefCOCOg. To compare with pretrained BERT methods, we use the pretraining strategy discussed in Section 3.3. As results show, our model achieves comprehensive advantage and shows distinct improvement on some splits, even compared to advance BERT models that use 40? more data in pretraining. <ref type="table" target="#tab_1">Table 2</ref> illustrates results on RES task in terms of MIoU score. It can be seen that our model achieves the best performance; substantially better than the state-of-art. We further observe that pretraining on the REC task gives a   <ref type="bibr">39 41</ref> huge performance boost to the RES task, even when no segmentation mask is used in pre-training. Multi-task training enables the model to leverage performance boost in one task to improve the other.</p><p>We show the inference time for our model in <ref type="table" target="#tab_1">Table 2</ref>. Our model can directly decode all referring queries in an image in parallel, allowing it to reach real-time performance. Importanly, note the corresponding scores for our model in <ref type="table">Table 6</ref> and <ref type="table" target="#tab_1">Table 2</ref> are based on the output of a single multitask model that predicts referring detection box and segmentation mask simultaneously. The only related work that shares this property is the MCN <ref type="bibr" target="#b35">[36]</ref>, which has substantially inferior performance.</p><p>REC on Flickr30k-Entities. For Flickr30k dataset, previous one-stage works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> extract short phrases from sentences and treated them as separate referring expression comprehension tasks. We argue that in this setting, queries are mostly short phrases and therefore cannot well reflect the model's ability to comprehend them in context. In contrast, our model, given an image and a caption (con)text sentence, aims to predict bounding boxes for all referred entities in the sentence. Doing so gives several advantages: 1. We are able to contextualize referring expressions given all other referring expressions and (con)text provided by the sentence. 2. Locations for all phrases can be inferred in one forward pass of the network, which saves a lot of computation as compared to previous one-stage approaches <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> that process one phrase at a time. Note that our task formulation is consistent with some two-stage models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>, but is unique for a one stage approach.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> we compare our results with state-of-the-art methods. Without pretraining, we obtain a huge performance boost compared to both previous one-stage (+13.54%) and two-stage (+7.31%) We add mosaic to all human face to protect personal information. state-of-the art methods. By using pretrained models, we observe that our model tends to generalize better on the test set and gives even better performance. We also provide comparison of inference time both per image and (per-expression), since our model can amortize inference across expressions. Per-expression, our inference time is substantially lower than all prior methods.</p><p>REC on ReferIt. Since ReferIt is a relative small dataset, we use a slightly smaller model which contains 3 cross attention layers in the query decoder. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Our model is able to perform better, by a large margin, than even the latest one-stage methods. We also observe a consistent boost brought by pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we show our qualitative comparison with previous state-of-the-art multi-task model -MCN <ref type="bibr" target="#b35">[36]</ref>. The first two rows of <ref type="figure" target="#fig_1">Figure 2(a)</ref> show failure cases of MCN that can be better handled by our model. We observe that MCN appears to fail because it neglects some attributes in referring expression (e.g., "yellow drink" and "blue striped shirt"), while our model is able to better model the query and pay attention to object attributes. In the last row, we shows several failure cases of our model. For the first case, the query requires the model to have the ability to recognize number "44". For the second and third case, there is visual ambiguity to identify the nearest glass to the bowl or to determine which bear(brown or white) has the longest leg.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>(b), we show qualitative comparison in terms of referred segmentation mask. Compared to MCN, our model is able to output more detailed object shape and finer outlines. Moreover, our model shows the ability to handle shadows (e.g., the right bottom of the donut) and occlusions (e.g., the man occluded by another man's arm) and predict smoother segmentation mask. We also give a result on a challenging case in the last row, where the texture boundary of the two giraffe is hard to distinguish.</p><p>Despite imperfections, our model is still able to focus on the giraffe's head in the foreground and performs much better than MCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We first consider the importance of the multi-task setup in <ref type="table" target="#tab_3">Table 4</ref> (left). Results indicate that multitask training boosts both REC and RES performance by a considerable margin. More specifically, we observe that REC loss helps the transformer to better locate the referred object and converge faster in early stages of training. At the same time, RES loss aids the model with more fine-grained information on the shape of the referred region, which helps to further enhance the accuracy. IE here is Inconsistency Error metric originally used in <ref type="bibr" target="#b35">[36]</ref> to measure the prediction conflict between the REC and RES task. We can see that joint training of RES and REC greatly reduce the inconsistency between the two tasks. Note that our model also has a much lower multi-task inconsistency compared to MCN <ref type="bibr" target="#b35">[36]</ref>, with a corresponding IE score of 7.54%(-40%). This shows that our model can do better collaborative learning.</p><p>Next, we validate the design of our network architecture. We report our scores on the Flickr30k test sets. In <ref type="table" target="#tab_3">Table 4</ref> (right), we ablate the model's major components and features used to form the query. Without (w/o) context encoder indicates that we directly use learnable embedding to encode text; w/o Query Decoder means that we directly use the average pooled feature from the encoder to predict a single referred output. We can see that the context encoder plays an important role in providing good textual representation for further multi-modal fusion. Query encoders are also quite important without which we also observe a big performance drop. For the ablation on query features, we observe that both context feature and phrase features are crucial without which the performance will decrease considerably. Learnable embeddings also help to enhance the performance by adding bias to fused query semantics. The table also showed that the network will not work without guidance of both context and phrase features since we cannot establish a correspondence between multiple queries and outputs in such a case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we present Referring Transformer, a one-step approach to referring expression comprehension (REC) and segmentation (RES). We jointly train our model for RES and REC tasks while enabling contextualized multi-expression references. Our models outperform state-of-the-art by a large margin on five / three datasets for REC / RES respectively, while achieving real-time runtime.</p><p>One limitation for our model is that we follow the setup in previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and assume that each expression refers to only one region. In the future, we plan to explore learning to predict multiple regions for each referring entity if necessary. Large-scale multi-task pretraining has been demonstrated to be very effective for ViLBEERT and other similar architectures; this is complementary to our focus in this paper, and we expect such strategies to further improve the performance. A More Details for Referring Expression Segmentation (RES) <ref type="figure">Figure 3</ref>: RES Task Head. A detailed illustration of our model for RES task.</p><p>We provide a more detailed illustration of our model for the RES task in <ref type="figure">Figure 3</ref>. With decoded query embedding from the query decoder and visual feature coming from visual-lingual decoder, a query attention score S att ? R M ?(HW ) is computed using a dot product. Here M denotes the number of attention heads, which is 8 in our implementation. The attention score is then concatenated with visual feature and sent into several up-sampling blocks (convolution layer with stride of 2). We also add residual connections from different stages of the ResNet backbone to help refine the up-sampled features. All convolution layers here use a kernel size of 3. The design is motivated by MaskRCNN <ref type="bibr" target="#b15">[16]</ref> and DETR <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Implementation Details</head><p>Pretraining. We use the description split of Visual Genome <ref type="bibr" target="#b24">[25]</ref> for pretraining, it contains 100k images with an average of 40 region descriptions per-image. We pretrain our model with REC task on the Visual Genome dataset for 6 epochs. We set the learning rate at 1e-4 and decay it by 10x after 4 epochs. The trained model is then used to initialize the model for dataset-specific fine-tuning.</p><p>RefCOCO Training. For experiments on RefCOCO(+/g) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b53">54]</ref>, since auxiliary loss is expensive for RES task, we first train our model with auxiliary loss on REC task for 60 epochs using a learning rate of 1e-4. Then, we disable auxiliary loss and train the model jointly on RES and REC task for 30 epochs with learning rate of 1e-4 that decays on the 10th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReferItGame / Flickr30k</head><p>Training. On ReferItGame <ref type="bibr" target="#b23">[24]</ref> and Flickr30k Entities <ref type="bibr" target="#b40">[41]</ref>, our model is trained for 90 and 60 epochs respectively, with learning rate decays on the 60th and 40th epoch.</p><p>Source Code. We include core codes for our model in the Supplemental for the reference. We will release complete code with checkpoints to reproduce reported scores upon acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>More Qualitative Comparison. Additional qualitative results on REC and RES tasks, compared to the previous multi-task framework of <ref type="bibr" target="#b35">[36]</ref>, are shown in <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure">Figure 5</ref> respectively. Note that the score of RES can benefit greatly from better localization of corresponding REC task. In <ref type="figure">Figure 5</ref>, to better compare the quality of generated referred masks, we compare the mask quality in the case where both MCN <ref type="bibr" target="#b35">[36]</ref> and our method assume correct REC localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results with Different Input Resolution.</head><p>In RES and REC tasks, the size of input image is a matter of trade off between performance and speed, which is largely effected by the network architecture. Despite that our model is designed to be able to process 640 ? 640 images at real time speed, we provide our model with different input resolution for reference, as showed in <ref type="table" target="#tab_5">Table 5</ref>. Note that we replace convolution in the final stage with dilated convolution to make sure the number of visual features sent into visual-lingual encoder are roughly the same.  <ref type="bibr" target="#b35">[36]</ref>, our model and the ground truth. The first row, second row and third row comes from RefCOCO+ testA, testB and RefCOCOg test set respectively. Comparison with Contemporaneous Work As discussed in the main paper, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref> are nonrefereed/unpublished works that appear on ArXiv recently. We provide quantitative comparison on REC task with these approaches, based on their reported numbers in <ref type="table">Table 6</ref>. We stress that none of these works address or show performance on multi-task performance of REC &amp; RES task, which is one of distinctive qualities of our model in comparison.</p><p>Compared to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, our model performs substantially better in all, with an exception of RefCOCO testB (where <ref type="bibr" target="#b6">[7]</ref> is marginally better), datasets and splits. The biggest improvements can be seen on RefCOCO+, where our model is 10.4% better (or 6.76 points better), than the closest concurrent work of <ref type="bibr" target="#b6">[7]</ref>, on the Val split; similar sizable improvements are illustrated on other splits, e.g., 9.2% on RefCOCO+ testB. In addition, our approach is considerably faster in runtime, since our model is able to handle multiple queries simultaneously (unlike <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>). Unfortunately, we are not able to report specific numbers for this at this time, as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> do not report them in their ArXiv papers.</p><p>Compared to <ref type="bibr" target="#b22">[23]</ref>, in a pretrained model setting, we see that our model performs similarly to <ref type="bibr" target="#b22">[23]</ref> on RefCOCO and marginally worse on RefCOCO+ and RefCOCOg. We believe this difference can be attributed to two factors: (1) pretraining on a larger dataset (200k images vs. 100k for us, plus we use a shorter 6 epochs pretraining schedule) (2) using more sophisticated language model (RoBERTa for <ref type="bibr" target="#b22">[23]</ref> vs. vanilla BERT for us). We were unable to explore effect those choices would have on our model for the moment, but expect them to further boost the performance. In addition, we setup our method in multi-task setting to solve RES and REC task at the same time, so our formulation while perhaps marginally inferior on REC is more general overall. Finally, our formulation, which does away with Hungarian matching loss, is likely to also be significantly faster to train. Exploring this would require re-running <ref type="bibr" target="#b22">[23]</ref>, which we hope to do for the camera ready.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Referring Transformer. An overview of the proposed architecture is shown in (a). For an image and (con)text input, a visual-lingual encoder is used to refine image features, extracted from a convolutional backbone, and lingual features, extracted by a BERT. A query encoder and decoder produce features for REC and RES heads, given multi-modal features and referring expressions. The detailed structure of the query encoder and decoder is shown in (b). Colored squares denote embeddings for corresponding referring queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative Evaluation. In (a) comparison to MCN [36] on REC is shown; orange, blue and red bounding boxes correspond to outputs from MCN, our model and the ground truth. In (b) similar comparison on RES is made. The attention map is drawn from the last layer of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Additional Qualitative Results on REC Task. Orange, blue and red bounding boxes correspond to outputs from MCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison on REC task. Performance on RefCOCO/RefCOCO+/RefCOCOg datasets<ref type="bibr" target="#b53">[54]</ref> is reported. Ours * denotes that pretraining is used. RN50 and RN101 refer to ResNet50 and ResNet101<ref type="bibr" target="#b14">[15]</ref> respectively; DN53 refers to DarkNet53<ref type="bibr" target="#b42">[43]</ref> backbone.</figDesc><table><row><cell>Models</cell><cell cols="3">Visual Features Images Pretrain Multi-task</cell><cell>val</cell><cell cols="2">RefCOCO testA testB</cell><cell>val</cell><cell>RefCOCO+ testA testB</cell><cell cols="2">RefCOCOg val-u test-u</cell></row><row><cell>Two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMN [19]</cell><cell>VGG16</cell><cell>None</cell><cell>?</cell><cell>-</cell><cell cols="2">71.03 65.77</cell><cell>-</cell><cell>54.32 47.76</cell><cell>-</cell><cell>-</cell></row><row><cell>MAttNet [55]</cell><cell>RN101</cell><cell>None</cell><cell>?</cell><cell cols="7">76.65 81.14 69.99 65.33 71.62 56.02 66.58 67.27</cell></row><row><cell>RvG-Tree [17]</cell><cell>RN101</cell><cell>None</cell><cell>?</cell><cell cols="7">75.06 78.61 69.85 63.51 67.45 56.66 66.95 66.51</cell></row><row><cell>NMTree [29]</cell><cell>RN101</cell><cell>None</cell><cell>?</cell><cell cols="7">76.41 81.21 70.09 66.46 72.02 57.52 65.87 66.44</cell></row><row><cell>CM-Att-Erase [30]</cell><cell>RN101</cell><cell>None</cell><cell>?</cell><cell cols="7">78.35 83.14 71.32 68.09 73.65 58.03 67.99 68.67</cell></row><row><cell>One-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCCF [26]</cell><cell>DLA34</cell><cell>None</cell><cell>?</cell><cell>-</cell><cell cols="2">81.06 71.85</cell><cell>-</cell><cell>70.35 56.32</cell><cell>-</cell><cell>65.73</cell></row><row><cell>SSG [4]</cell><cell>DN53</cell><cell>None</cell><cell>?</cell><cell>-</cell><cell cols="2">76.51 67.50</cell><cell>-</cell><cell cols="2">62.14 49.27 58.80</cell><cell>-</cell></row><row><cell>FAOA [50]</cell><cell>DN53</cell><cell>None</cell><cell>?</cell><cell cols="7">72.54 74.35 68.50 56.81 60.23 49.60 61.33 60.36</cell></row><row><cell>ReSC-Large [51]</cell><cell>DN53</cell><cell>None</cell><cell>?</cell><cell cols="7">77.63 80.45 72.30 63.59 68.36 56.81 67.30 67.20</cell></row><row><cell>MCN [36]</cell><cell>DN53</cell><cell>None</cell><cell></cell><cell cols="7">80.08 82.29 74.98 67.16 72.86 57.31 66.46 66.01</cell></row><row><cell>Ours</cell><cell>RN50</cell><cell>None</cell><cell></cell><cell cols="7">81.82 85.33 76.31 71.13 75.58 61.91 69.32 69.10</cell></row><row><cell>Ours</cell><cell>RN101</cell><cell>None</cell><cell></cell><cell cols="7">82.23 85.59 76.57 71.58 75.96 62.16 69.41 69.40</cell></row><row><cell>Pretrained:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VilBERT[33]</cell><cell>RN101</cell><cell>3.3M</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">72.34 78.52 62.61</cell><cell>-</cell><cell>-</cell></row><row><cell>ERNIE-ViL_L[53]</cell><cell>RN101</cell><cell>4.3M</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">75.89 82.37 66.91</cell><cell>-</cell><cell>-</cell></row><row><cell>UNTIER_L[5]</cell><cell>RN101</cell><cell>4.6M</cell><cell>?</cell><cell cols="7">81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77</cell></row><row><cell>VILLA_L[12]</cell><cell>RN101</cell><cell>4.6M</cell><cell>?</cell><cell cols="7">82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71</cell></row><row><cell>Ours*</cell><cell>RN50</cell><cell>100k</cell><cell></cell><cell cols="7">85.43 87.48 79.86 76.40 81.35 66.59 78.43 77.86</cell></row><row><cell>Ours*</cell><cell>RN101</cell><cell>100k</cell><cell></cell><cell cols="7">85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>RN101 74.34 76.77 70.87 66.75 70.58 59.40 66.63 67.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>val</cell><cell>RefCOCO testA testB</cell><cell>val</cell><cell cols="2">RefCOCO+ testA testB</cell><cell cols="2">RefCOCOg val test</cell><cell>Inference time(ms)</cell></row><row><cell>DMN [38]</cell><cell>RN101</cell><cell cols="5">49.78 54.83 45.13 38.88 44.22 32.29</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAttNet [55]</cell><cell>RN101</cell><cell cols="7">56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61</cell><cell>378</cell></row><row><cell>NMTree [29]</cell><cell>RN101</cell><cell cols="7">56.59 63.02 52.06 47.40 53.01 41.56 46.59 47.88</cell><cell>-</cell></row><row><cell>Lang2seg [6]</cell><cell>RN101</cell><cell cols="2">58.90 61.77 53.81</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">46.37 46.95</cell><cell>-</cell></row><row><cell>BCAM [20]</cell><cell>RN101</cell><cell cols="5">61.35 63.37 59.57 48.57 52.87 42.13</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMPC [21]</cell><cell>RN101</cell><cell cols="5">61.36 64.53 59.64 49.56 53.44 43.23</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CGAN [35]</cell><cell>DN53</cell><cell cols="7">64.86 68.04 62.07 51.03 55.51 44.06 51.01 51.69</cell><cell>-</cell></row><row><cell>LTS [22]</cell><cell>DN53</cell><cell cols="7">65.43 67.76 63.08 54.21 58.32 48.02 54.40 54.25</cell><cell>-</cell></row><row><cell>MCN+ASNLS [36]</cell><cell>DN53</cell><cell cols="7">62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40</cell><cell>56</cell></row><row><cell>Ours</cell><cell>RN50</cell><cell cols="2">69.94 72.80 66.13</cell><cell>60.9</cell><cell cols="4">65.20 53.45 57.69 58.37</cell><cell>38</cell></row><row><cell>Ours</cell><cell>RN101</cell><cell cols="7">70.56 73.49 66.57 61.08 64.69 52.73 58.73 58.51</cell><cell>41</cell></row><row><cell>Ours  *</cell><cell>RN50</cell><cell cols="7">73.61 75.22 69.80 65.30 69.69 56.98 65.70 65.41</cell><cell>38</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison on RES tasks. Performance on RefCOCO/RefCOCO+/RefCOCOg datasets [54] is reported. Ours* denotes that pretraining is used. RN50 abd RN101 refer to ResNet50 and ResNet101 [15] respectively; DN53 refers to DarkNet53 [43] backbone.*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with State-of-The-Art Methods. Table illustrates performance on the test set of ReferItGame<ref type="bibr" target="#b23">[24]</ref> and Flickr30K Entities<ref type="bibr" target="#b40">[41]</ref> datasets in terms of top-1 accuracy (%).</figDesc><table><row><cell>Models</cell><cell>Backbone</cell><cell cols="2">ReferItGame Flickr30K test test</cell><cell>Inference time on Flickr30k(ms)</cell></row><row><cell></cell><cell></cell><cell>Two-stage</cell><cell></cell><cell></cell></row><row><cell>MAttNet [55]</cell><cell>RN101</cell><cell>29.04</cell><cell>-</cell><cell>320</cell></row><row><cell>Similarity Net [48]</cell><cell>RN101</cell><cell>34.54</cell><cell>60.89</cell><cell>184</cell></row><row><cell>CITE [42]</cell><cell>RN101</cell><cell>35.07</cell><cell>61.33</cell><cell>196</cell></row><row><cell>DDPN [56]</cell><cell>RN101</cell><cell>63.00</cell><cell>73.30</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>One-stage</cell><cell></cell><cell></cell></row><row><cell>SSG [4]</cell><cell>DN53</cell><cell>54.24</cell><cell>-</cell><cell>25</cell></row><row><cell>ZSGNet [46]</cell><cell>RN50</cell><cell>58.63</cell><cell>58.63</cell><cell>-</cell></row><row><cell>FAOA [50]</cell><cell>DN53</cell><cell>60.67</cell><cell>68.71</cell><cell>23</cell></row><row><cell>RCCF [26]</cell><cell>DLA34</cell><cell>63.79</cell><cell>-</cell><cell>25</cell></row><row><cell>ReSC-Large [51]</cell><cell>DN53</cell><cell>64.60</cell><cell>69.28</cell><cell>36</cell></row><row><cell>Ours</cell><cell>RN50</cell><cell>70.81</cell><cell>78.13</cell><cell>37(14)</cell></row><row><cell>Ours</cell><cell>RN101</cell><cell>71.42</cell><cell>78.66</cell><cell>40(15)</cell></row><row><cell>Ours*</cell><cell>RN50</cell><cell>75.49</cell><cell>79.46</cell><cell>37(14)</cell></row><row><cell>Ours*</cell><cell>RN101</cell><cell>76.18</cell><cell>81.18</cell><cell>40(15)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies. Table on the left ablates our multi-task and pretraining schehme on RefCOCO+ validation set. Table on the right ablates on core components of our model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Flickr30k</cell></row><row><cell>REC RES Pretrain Loss Loss</cell><cell>RefCOCO+ REC? RES? -58.39 23.52% IE? 70.02 -71.13 61.08 4.73% 75.07 -76.40 65.30 4.48%</cell><cell>Model component -w/o Query Decoder -w/o Context Encoder Query Encoder -w/o Context &amp; Phrase Feature -w/o Context Feature -w/o Phrase Feature -w/o Learnable Embedding</cell><cell>49.38 73.68 42.05 76.64 77.02 77.64</cell></row><row><cell></cell><cell></cell><cell>Full model</cell><cell>78.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>6 Acknowledgments and Disclosure of FundingThis work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC CRC and an NSERC DG and Discovery Accelerator Grants. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute www.vectorinstitute.ai/#partners. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on RefCOCO+ Dataset with Different Input Resolutions. Our methods correspond to the model without pretraining. .16 72.86 57.31 50.62 54.99 44.69 Ours 256? 256 70.05 73.29 61.48 58.26 61.09 52.20 Ours 320? 320 70.03 73.23 61.52 58.42 61.48 52.34 Ours 416? 416 71.50 75.87 61.71 61.00 64.48 52.44 Ours 640? 640 71.58 75.96 62.16 61.08 64.69 52.73</figDesc><table><row><cell>Models</cell><cell>Resolution</cell><cell cols="2">REC(prec@0.5)</cell><cell></cell><cell></cell><cell>RES(MIoU)</cell></row><row><cell></cell><cell cols="2">val</cell><cell cols="2">testA testB</cell><cell>val</cell><cell>testA testB</cell></row><row><cell>FAOA [50]</cell><cell cols="4">256? 256 56.81 60.23 49.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">ReSC-Large [51] 256? 256 63.59 68.36 56.81</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMPC [21]</cell><cell>320? 320</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">49.56 53.44 43.23</cell></row><row><cell>LTS [22]</cell><cell>416? 416</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">54.21 58.32 48.02</cell></row><row><cell>MCN [36]</cell><cell>416? 416 67</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, a sentence from which noun phrases/referring expressions pi were parsed. Where contextual text source is unavailable and only one phrase exists, we simply let Q = Qp.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 5</ref><p>: Additional Results on RES Task. Images come from RefCOCO+ testA and testB splits. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">G3raphground: Graph-based language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4281" to="4290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Real-time referring expression comprehension by single-stage grounding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Referring expression object segmentation with caption-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08541</idno>
		<title level="m">TransVG: End-to-end visual grounding with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural sequential phrase grounding (seqground)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4175" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04281</idno>
		<title level="m">Visual grounding with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for vision-andlanguage representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to compose and reason with language tree structures for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Locate then segment: A strong pipeline for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16284</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MDETR-modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12763</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to assemble neural module tree networks for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4673" to="4682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with crossmodal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1950" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascade grouped attention network for referring expression segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-task collaborative network for joint referring expression comprehension and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="792" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conditional image-text embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="249" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot grounding of objects from natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="394" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A fast and accurate one-stage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4683" to="4693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving one-stage visual grounding by recursive sub-query construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
