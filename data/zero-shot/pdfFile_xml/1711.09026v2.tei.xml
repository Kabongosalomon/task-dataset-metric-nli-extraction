<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<email>mfritz@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Last Observation: t Prediction: t + 5 Prediction: t + 10 Prediction: t + 15 Probability Figure 1: Our predictive distribution upto t + 15 frames. The heat map encodes the probability of a certain pixel belonging to the person. The variance of the distribution encodes the uncertainty. Row 1: Low uncertainty. Row 2: High uncertainty.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While methods for automatic scene understanding have progressed rapidly over the past years, it is just one key ingredient for assisted and autonomous driving. Human capabilities go beyond inference of scene structure and en-compass a broader type of scene understanding that also lends itself to anticipating the future.</p><p>Anticipation is key in preventing collisions by predicting future movements of dynamic agents e.g. people and cars in inner cities. It is also the key to operating at practical safety distances. Without anticipation, domain knowledge and experience, drivers would have to maintain an equally large safety distance to all objects, which is clearly impractical in dense and cluttered inner city traffic. Additionally, anticipation enables decision making, e.g. passing cars and pedestrians while respecting the safety of all participants. Even at conservative and careful driving speeds of 25miles/hour (? 40km/hour) in residential areas, the distance traveled in 1 second corresponds roughly to the breaking distance. Anticipation of traffic scenes on a time horizons of at least 1 second would therefore enable safe driving at such speeds.</p><p>We propose the first approach to predict people (pedestrians including cyclists) trajectories from on-board cameras over such long-time horizons with uncertainty estimates. Due to the particular importance for safety, we are focusing on the people class. While pedestrian trajectory prediction has been approached in prior work, we propose the first approach for on-board prediction. As predictions are made with respect to the moving vehicle, we formulate a novel two stream model for long-term person bounding box prediction and vehicle ego motion (odometry). In contrast to prior work, we model both aleatoric (observation) uncertainty and epistemic (model) uncertainty <ref type="bibr" target="#b3">[4]</ref> in order to arrive at an estimate of the overall uncertainty.</p><p>Our contributions in detail are: 1. First approach to long-term prediction of pedestrian bounding box sequences from a mobile platform; 2. Novel sequence to sequence model which provides a theoretically grounded approach to quantify uncertainty associated with each prediction; 3. Detailed experimental evaluation of alternative architectures illustrating the importance and effectiveness of using a two-stream architecture; 4. Analysis of dependencies between uncertainty estimates and actual prediction error leading to an empirical error bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Human Trajectory Prediction. Recent works such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> focus on the task of pedestrian trajectory prediction in 3D space. However, 3D world coordinates are difficult to obtain in unconstrained scenarios. It requires expensive stereo camera and/or LIDAR setups and obtained depth maps are typically noisy especially in unknown environments. Our method does not depend upon unreliable 3D coordinates and it is widely applicable as it requires only one camera. Many vehicles worldwide already have installed dash-cams. Another class of models such as <ref type="bibr">[9, 28, 23, 1, ?]</ref> consider the problem of (2D) pedestrian trajectory prediction in a social context by modelling human-human interactions. The state of the art model <ref type="bibr" target="#b0">[1]</ref> proposes to estimate the trajectories of each person in the scene by an instance of a "Social" LSTM. The instances of the Social LSTM can communicate with a special pooling layer. This enables the modelling of interactions and joint estimation of trajectories of all pedestrians in the scene. In <ref type="bibr" target="#b25">[26]</ref> the joint estimation of robot and human trajectories are considered in a social context. However, in case of on-board prediction vehicle ego-motion dominates social aspects. Moreover, most methods are trained/tested on static camera datasets which are hand annotated with minimum observation noise. Apart from these, the class of models such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref> aim at discovering motion patterns of humans and vehicles. Such methods cannot be used for trajectory prediction and do not consider vehicle ego-motion. Modeling Uncertainty in Deep Learning. Popular deep learning architectures do not model uncertainty. They assume uniform constant observation noise (aleatoric uncertainty). Heteroscedastic regression methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref> estimate aleatoric uncertanity by predicting the parameters of a assumed observation noise distribution (also in <ref type="bibr" target="#b0">[1]</ref>). Bayesian neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> offer a probabilistic view of deep learning and provide model (epistemic) uncertainty estimates. However, inference of model posterior in such networks is difficult. Variational Inference is a popular method. Gal et. al. in <ref type="bibr" target="#b5">[6]</ref> showed that dropout training in deep neural networks approximates Bayesian inference in deep Gaussian processes. Extending these results it was shown in <ref type="bibr" target="#b4">[5]</ref> that dropout training can be cast as approximate Bernoulli variational inference in Bayesian neural networks. These results were extended to RNNs in <ref type="bibr" target="#b6">[7]</ref>. The developed Bayesian RNNs showed superior performance to standard RNNs with dropout in various tasks. More recently, <ref type="bibr" target="#b11">[12]</ref> presents a Bayesian deep learning framework jointly estimating aleatoric uncertainty together with epistemic uncertainty. The resulting framework gives new state-of-the-art results on segmentation and depth regression benchmarks. Assisted and Autonomous driving. One of the earliest works on vehicle ego-motion (odometry) prediction or popularly, autonomous driving, was ALVINN by <ref type="bibr" target="#b20">[21]</ref>. This work showed the possibility of directly predicting steering angles from visual input. This system used a simple fullyconnected network. More recently, <ref type="bibr" target="#b1">[2]</ref> uses a convolutional neural network for this task and achieves a autonomy of 90% using a relatively small training set. However, the focus is on highway driving. <ref type="bibr" target="#b26">[27]</ref> proposes a FCN-LSTM that predicts the next vehicle odometry based on the visual input captured by an on-board camera and previous odometry of the vehicle. Here, a diverse crowed sourced dataset is used. However, these methods predict vehicle odometry (e.g. steering angle) only for the next time-step. In contrast, we focus on inner-city driving and predict multiple time-steps into the future. <ref type="bibr" target="#b23">[24]</ref> proposes a driving simulator that predicts the future in form of frames but suffers from blurriness problems in the long-term important details get lost. In <ref type="bibr" target="#b15">[16]</ref> future segmentation masks are predicted, but only mid-term (upto 0.5sec) future is predicted and there is no pedestrian specific evaluation. We predict the future in terms of bounding box coordinates which remain well defined by design in the long-term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">On-board Pedestrian Prediction under Uncertainty</head><p>In order to anticipate motion of people in real-world traffic scenes from on-board cameras, we propose a novel approach that conditions the prediction of motion (subsection 3.1) of people on predicted odometry (subsection 3.4). Moreover, our approach models both aleatoric and epistemic uncertainty. Our model (see <ref type="figure" target="#fig_0">Figure 2</ref>) consists of two specialized streams for prediction of pedestrian motion and odometry. The odometry specialist stream predicts the most likely future vehicle odometry sequence. The bounding box specialist stream consists of a novel Bayesian RNN encoderdecoder architecture to predict odomerty conditioned distributions over pedestrian trajectories and to capture epistemic and aleatoric uncertainty. Bayesian probability theory provides us with a theoretically grounded approach to dealing with both types of uncertainties (subsection 3.2).</p><p>We start by describing the bounding box prediction stream of our model and introduce our novel Bayesian RNN encoder-decoder which provides theoretically grounded uncertainty estimates.  p</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prediction of Pedestrian Trajectories</head><formula xml:id="formula_0">(B f = [b t+1 i , ..., b t+n i ]|B p , O p , O f ) B p = [b t?m i , ..., b t i ], O p = [o t?m , ..., o t ], O f = [o t+1 , ..., o t+n ]</formula><p>The variance of the predictive distribution p(B f |B p , O p , O f ) provides a measure of the associated uncertainty.</p><p>We will describe a basic sequence to sequence RNN first and then extend it to predict distributions and provide uncertainty estimates. Our sequence to sequence RNN ( <ref type="figure" target="#fig_0">Figure 2</ref>) consists of two embedding layers, an encoder RNN and a decoder RNN. The input sequence consists of the concatenated past bounding box and odometry sequences B p , O p . The input embedding layer embeds the inputs sequence x t into the representationx t . This embedded sequence is read by the encoder RNN (RNN enc ) which produces a summary vector v bbox . This summary vector is concatenated with predicted odometry O f and this summary sequence is embedded using the second embedding layer. This embedded summary sequencev (containing information about past pedestrian motion, past and future vehicle odometry) is used by the decoder RNN (RNN dec ) for prediction.</p><p>In the following, we extend this model to predict distributions and estimate uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bayesian Modelling of Uncertainty</head><p>We phrase our novel RNN encoder-decoder model in a Bayesian framework <ref type="bibr" target="#b11">[12]</ref>. We capture epistemic (model) uncertainty by learning a distribution of models p(f |X, Y ) likely to have generated our data {X, Y }. Here, models f are RNN encoder-decoders with varying parameters. We infer the posterior distribution of RNN encoder-decoders p(f |X, Y ) , given the prior belief of the distribution of RNN encoder-decoders p(f ). The predictive probability over the future sequence B f given the past sequence B p is obtained by marginalizing over the posterior distribution of RNN encoder-decoders,</p><formula xml:id="formula_1">p(B f |B p , O p , O f ,X, Y ) = p(B f |B f , , O p , O f , f )p(f |X, Y )df.<label>(1)</label></formula><p>However, the integral in (1) is intractable. But, we can approximate it in two steps <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. First, we assume that our RNN encoder-decoder models can be described by a finite set of variables ?. Thus, we constrain the set of possible RNN encoder-decoders to ones that can be described with ?. Now, (1) can be equivalently written as,</p><formula xml:id="formula_2">p(B f |B p , O p , O f ,X, Y ) = p(B f |B p , O p , O f , ?)p(?|X, Y )d?<label>(2)</label></formula><p>Second, we assume an approximating variational distri-bution q(?) which allows efficient sampling,</p><formula xml:id="formula_3">p(B f |B p , O p , O f ) = p(B f |B p , O p , O f , ?)q(?)d? (3)</formula><p>We choose the set of weight matrices {W 1 , .., W L } ? W of our RNN enocder-decoder as the set of variables ?. Then we define an approximating Bernoulli variational distribution q(?) over the columns w c k of the weight matrices W k ? W,</p><formula xml:id="formula_4">q(W k ) = M k ? diag([z i,j ] C k j=1 ) z i,j = Bernoulli(p i ), i = 1, ..., L, j = 1, ..., K i?1 .<label>(4)</label></formula><p>where, M k are the variational parameters. This distribution allows for efficient sampling during training and testing which we discuss in the following subsection.</p><p>For an accurate approximation, we minimize the KL divergence between q(?) and the true posterior p(?|X, Y ) as the training step. It can be shown that,</p><formula xml:id="formula_5">KL(q(?) || p(?|X, Y )) ? KL(q(?) || p(?)) ? t q(?) log p(b t+n t |b t+n?1 t , B p , O p , O f , ?)d?.<label>(5)</label></formula><p>The first part corresponds to the distance to the prior model distribution and the second to the data fit. During training and prediction, we use Monte-Carlo integration to approximate the integrals <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_5">(5)</ref> (more details about <ref type="bibr" target="#b4">(5)</ref> in the Supplementary and the exact objective in subsection 3.5). Aleatoric uncertainty can be captured along with epistemic uncertainty, by assuming a distribution of observation noise and estimating the sufficient statistics of the distribution. Here, we assume it to be a 4-d Gaussian at each time-step,</p><formula xml:id="formula_6">N (b t+n i , ? t+n i ), where, ? t+n i = diag (? t+n x ) i , (? t+n y ) i , (? t+n x ) i , (? t+n y ) i in x and y direc- tions in pixel space at time-step t + n. The predictive distri- bution of models parametrized by ?, p(B f |B p , , O p , O f , ?) is Gaussian at every time-step.</formula><p>Uncertainty is the variance of our predictive distribution (3) and can be obtained through moment matching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. If we have T samples of future pedestrian bounding box sequencesB f , the total uncertainty at time-step t is,</p><formula xml:id="formula_7">1 T T i=1 (b t i ) b t i ? 1 T T i=1 (b t i ) T i=1b t i + 1 T T i=1 (? t i ) x + T i=1 (? t i ) y .<label>(6)</label></formula><p>The first part of the sum correspond to the epistemic uncertainty u e i and the second part corresponds to the aleatoric uncertainty u a i . We average the uncertainty across time-steps to arrive at the complete uncertainty estimate. Next, we describe how we sample from the Bernoulli distribution of RNN encoder-decoder weight matrices and the final sampling from the predictive distribution</p><formula xml:id="formula_8">p(B f |B p , O p , O f ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bayesian RNN Encoder-Decoder</head><p>The RNN encoder-decoder model of subsection 3.1 contains four weight matrices. In detail, the two embedding layers contains two weight matrices W emi , W ems . The other two weight matrices belong to the encoder and decoder RNNs. We use an LSTM formulation as RNNs. Following <ref type="bibr" target="#b7">[8]</ref> the weight matrices of an LSTM can be concatenated into a matrix W and the LSTM can be formulated as in, ?</p><formula xml:id="formula_9">? ? ? i f ? c ? ? ? ? = ? ? ? ? sigm sigm sigm tanh ? ? ? ? x t h t?1 ? W c t = f c t?1 + i ? , h t = o tanh(c t )<label>(7)</label></formula><p>where i is the input gate, f is the forget gate, o is the output gate, c t is the cell state,? is the candidate cell state and h t is the hidden state.</p><p>We define the Bernoulli variational distribution q(?) (as in <ref type="formula" target="#formula_4">(4)</ref>) over the union of all the weight matrices of our model,</p><formula xml:id="formula_10">? = {W emi , W ems , W enc , W dec } .<label>(8)</label></formula><p>where, W enc , W dec are the weight matrices of our RNN encoder and decoder. Sampling from q(W emi ), q(W ems ) can be done efficiently by sampling random Bernoulli masks z emi , z ems and applying these masks after the linear transformations. In case of the input embedding,</p><formula xml:id="formula_11">x t = (x t ? W emi ) z emi<label>(9)</label></formula><p>Similarly, it was shown in <ref type="bibr" target="#b6">[7]</ref> sampling weight matrices of a LSTM (here, q(W enc ), q(W dec )) can be efficiently performed by sampling random Bernoulli masks z x , z h and applying them at each time-step, while the LSTM encoder and decoder are unrolled, ?</p><formula xml:id="formula_12">? ? ? i f ? c ? ? ? ? = ? ? ? ? sigm sigm sigm tanh ? ? ? ? x t z x h t?1 z h ? W<label>(10)</label></formula><p>Sampling</p><formula xml:id="formula_13">from our predictive distribution p(B f |B p , O f , O p )</formula><p>is done by first sampling weights matrices of our Bayesian RNN encoder-decoder. Then the parameters of the Gaussian observation noise distribution at each time-step is predicted. For this, we use the hidden state sequence h t dec of the RNN dec and an additional linear transformation,</p><formula xml:id="formula_14">h t+n dec = RNN dec (h t+n?1 dec , v bbox ; z x , z h ) b t+n i , (? i t+n ) x , (? t+n i ) y = W bbox * h t+n dec + bias bbox .</formula><p>We then draw a sample from the predicted Gaussian distribution.</p><p>Next, we describe the second stream of our two-stream model -our model for long-term odometry prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prediction of Odometry</head><p>The odomtery prediction stream predicts a mean estimate of the future vehicle ego-motion. We use a similar RNN encoder-decoder architecture used for bounding box prediction, but without the embedding layers. We condition the predicted sequence O f on the past odometry sequence O p and last visual observation on-board the vehicle. The past odometry O p is input to an encoder RNN which produces a summary vector v odo . The past odometry of the vehicle O p gives a strong cue about the future velocity especially in the short term (?100ms). We use the same LSTM formulation described previously as the RNN encoder; with the final hidden state h t as the summary. The last visual observation can help in the longer term prediction of odometry; e.g. visual cues about bends in the road, obstacles etc. Similar to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref> we employ a convolutional neural network (CNN-encoder) to embed the visual information provided by the currently observed frame; a visual summary vector v vis . Next we describe our CNN-encoder architecture. CNN-encoder. Our CNN-encoder should extract visual features to improve longer-term (multi-step versus single-step in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>) prediction. Therefore, we use a more complex CNN compared to <ref type="bibr" target="#b1">[2]</ref> and during training we learn the parameters from scratch, unlike <ref type="bibr" target="#b26">[27]</ref> which uses a pre-trained VGG network. Our CNN-encoder has 10 convolutional layers with ReLU non-linearities. We use a fixed, small filter size of 3x3 pixels. We use max-pooling after every two layers. After max-pooling we double the number of convolutional filters; we use {32,64,128,256,512} convolutional filters. The convolutional layers are followed by three fully connected layers with 1024, 256 and 128 neurons and ReLU non-linearities. The output of the last fully connected layer is the visual summary v vis .</p><p>The odometry and visual summary vectors are concatenated v = {v odo , v vis } and read by the RNN decoder (RNN dec ). We use the same LSTM formulation described previously as the RNN-decoder. As before, the hidden state of the LSTM decoder is used for predicting the future odometry sequence through a linear transformation.</p><formula xml:id="formula_15">h t+n dec = RNN dec (h t+n?1 dec , {v odo , v vis }) o t+n i = W odo * h t+n dec + bias odo .</formula><p>We next describe our training and inference processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Inference</head><p>Training. The two streams are trained separately. As the odometry prediction stream predicts point estimates, it is trained first by minimizing the MSE over the training set. The Bayesian bounding-box prediction stream is trained by estimating (Monte-Carlo) and minimizing the KL divergence of its approximate weight distribution q(?) (5). More specifically, 1. We sample a mini-batch of size T of exam-  <ref type="formula" target="#formula_11">(9)</ref> and <ref type="bibr" target="#b9">(10)</ref>. 3. For each example, the predicted meansB f and variances? of the heteroscedastic models parameterized by ? are inferred. 4. The KL divergence (5) can be equivalently minimized by (similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>) the following loss,</p><formula xml:id="formula_16">1 4N N i=1 n j=1 b t+j i ? b t+j i 2 2 (? t+j i ) ?2 + ? W W k 2 + log? 2 i</formula><p>where, |B f | = n and N pedestrians. The left part is the equivalent of the negative log likelihood term in <ref type="bibr" target="#b4">(5)</ref>. The middle part is weight regularization parameterized by ?, equivalent to the KL term in <ref type="bibr" target="#b4">(5)</ref>. The right part is additional regularization as in <ref type="bibr" target="#b11">[12]</ref>, to ensure finite predicted variance. The ADAM optimizer <ref type="bibr" target="#b13">[14]</ref> is used during training. For training sequences longer than |B p | + |B f | (|O p + O f | respectively) we use a sliding window to convert to multiple sequences. Moreover, as the sequences in the training set are of varying lengths, we use a curriculum learning (CL) approach. We fix the length of the conditioning sequence |B p |, |O p | and train for increasing longer time horizons |B f |, |O f | (initializing the model parameters with those for shorter horizons). This allows us to train on a larger part of the Cityscapes training set (also on sequences shorter than |B p | + |B f | of the final model) and leads to faster convergence. Inference. Given B p and O p (and the visual observation), the odometry prediction stream is first used to predict O f . We sample from the predictive distribution (3) by, 1. Sampling T samples of the weight matrices {W emi , W ems , W enc , W dec } of the Bayesian bounding box prediction stream from the (learned) approximate distribution q(?), by sampling Bernoulli masks as in <ref type="formula" target="#formula_11">(9)</ref> and (10), 2. The RNN dec is unrolled to obtain a sample B f ,? x ,? y from each of the T predicted Gaussian distributions. The associated uncertainty is obtained using the T samples (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on real-world on-board street scene data and show predictions over a 1 second time horizon along with the associated uncertainty. Dataset and Evaluation Metric. We evaluate on the Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref> which contains 2975 training, 500 validation and 1525 test video sequences of length 1.8 seconds (30 frames). The video resolution is 2048?1024 pixels. The sequences were recorded on-board a vehicle in inner cities. Each sequence has associated odometry information. Pedestrian tracks were automatically extracted using the tracking by detection method of <ref type="bibr" target="#b24">[25]</ref>. Detections were obtained using the Faster R-CNN based method of <ref type="bibr" target="#b29">[30]</ref>   driving systems where detections/tracks are obtained with a state-of-the-art detector/tracker and we have to deal with noise introduced by the detector and on rare occasions detector false positives and tracker failures. We use as evaluation metric MSE in pixels (of the mean of the predictive distribution) and the negative log-likelihood L. The L metric measures the probability assigned to the true sequence by our predictive distribution. We report these metrics averaged across all time-steps and plots per time-step. We use a dropout rate of 0.35, ? = 10 ?4 (tuned on validation set) and use 50 Monte-Carlo samples across all Bayesian models.</p><p>Evaluation of Bounding Box Prediction. We independently evaluate the first Bayesian LSTM stream of our two stream model, without conditioning it on predicted odometry. We predict 15 time-steps into the future and report the results in <ref type="table" target="#tab_1">Table 1</ref>. We compare its performance with,  tional introduced regularization. Furthermore, we see that increasing the length of the conditioning sequence improves model performance. However, the performance gain saturates at |B p | = 8. Henceforth, we will report results using |B p | = {4, 8} in the following. Finally, the odometry oracle case outperforms our Bayesian LSTM by a large margin. This shows that knowledge of vehicle odometry is crucial for good performance.</p><p>Comparison with Social LSTM <ref type="bibr" target="#b0">[1]</ref>. We compare our Bayesian LSTM model with the vanilla LSTM 1 model of <ref type="bibr" target="#b0">[1]</ref> (with 128 neurons) that predicts trajectories independently in <ref type="table" target="#tab_2">Table 2</ref>. Both models are trained to predict sequences of bounding box centers (length 15, given 8). Our Bayesian LSTM model performs better as it is more robust to mistakes during recursive prediction. The model of <ref type="bibr" target="#b0">[1]</ref> observes true past pedestrian coordinates during training. However, during prediction it observes its own predictions causing errors to be propagated though multiple steps of prediction. Furthermore, we compare both methods to the centers obtained from the predictions of our Bayesian LSTM (second row of <ref type="table" target="#tab_1">Table 1</ref>).</p><p>The results show that we can improve upon bounding box center prediction by predicting bounding boxes. Evaluation of Odometry Prediction. We train our odometry prediction LSTM encoder-decoder on the visual and odometry data of the Cityscapes training set. As many sequences have close to zero steering angle, we augment the training set to improve prediction performance. We reflect the steering angle and flip last observed image left to right of sequences with non-zero average steering angle. This <ref type="bibr" target="#b0">1</ref> The version with social pooling did not converge on our dataset.  <ref type="table">Table 4</ref>: Evaluation of our Bayesian two stream model <ref type="figure" target="#fig_0">(Figure 2</ref>).  increases the training data with non-zero steering angles by a factor of two. We use MSE between the predicted future vehicle velocity and steering angles as evaluation metric. The velocity is in meters per second and angle in degrees. We include as baselines: 1. A constant steering predictor that predicts the last observed odometry. 2. A linear Kalman filter. 3. Our LSTM encoder-decoder without visual observation (v = {v odo }). The third baseline is an ablation study. We observe no significant performance difference between |O p | = {4} and |O p | = {8}. We evaluate 15 timesteps into the future and report the results in <ref type="table" target="#tab_4">Table 3</ref>. We observe that the constant angle predictor performs significantly worse compared to the other baselines. This shows that the Cityscapes test set includes a significant number of non-trivial sequences with complex vehicle trajectories. We observe that the Kalman filter is able to quite accurately predict the vehicle speed. This is because in most vehicles are travelling with constant speed or accelerating/decelerating smoothly. However, the performance of the linear Kalman filter is worse compared to the LSTM models with respect to steering angle. This means that many sequences have non-linear vehicle trajectories. The superior performance of our model compared to the RNN baseline without visual observations, especially in the long-term shows that our CNN encoder extracts information useful for long-term prediction. We also show visual examples in the Supplementary.</p><p>Evaluation of our Two-Stream model. We perform an ablation study of our two-stream model <ref type="figure" target="#fig_0">(Figure 2</ref>) and compare with a single-stream Bayesian LSTM encoder-decoder model where the encoder observes the concatenated past bounding box and velocity sequence {B p , O p } and the decoder predicts the future bounding box sequence B f . This model does not see predicted future odometry. We evaluate the models and report the results in <ref type="table" target="#tab_6">Table 4 and plot the  MSE per time-step Table 5</ref>. The results show that jointly predicting odometry with pedestrian bounding boxes (3rd row) significantly improves performance (2nd row). The predicted odometry helps our two-stream model recover a significant fraction of the performance of the Oracle case in <ref type="table" target="#tab_1">Table 1</ref> row 5. The limiting factor here is that the odometry is sometimes highly uncertain e.g. at T-intersections, which leads to higher mean error. Apart from cases with uncertain odometry, the residual error of our two-stream (and the Oracle case) on a large part is due to the noise of the pedestrian detector and tracker failures. We show qualitative examples in  sequences have low error (note, log(530) ? 6.22 the MSE of our two stream model, <ref type="table">Table 4</ref>). We see that the epistemic and aleatoric uncertainties are correlates well with the squared error. This means that for sequences where the mean of our predictive distribution is far from the true future sequence, our predictive distribution has a high variance (and vice versa). Therefore, for sequences with multiple likely futures, where the mean estimate would have high error, our model learns to predict diverse futures. In the third plot of <ref type="figure" target="#fig_2">Figure 3</ref>, we plot the maximum log squared error (of the mean of the predictive distribution) observed at a certain predicted uncertainty level (sum of aleatoric and epistemic) in the test test. In the fourth plot, we plot the uncertainty with the maximum observed squared error at time-steps t + {5, 10, 15}. In both cases, uncertainty and observed maximum error is well correlated. This shows that, the predicted uncertainty upper bounds the error of the mean of the predictive distribution. Therefore, the predicted uncertainty helps us express trust in predictions and has the potential to serve as a basis for better decision making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We highlight the importance of anticipation for practical and safe driving in inner cities. We contribute to this important research direction the first model for long term prediction of pedestrians from on-board observations. We show predictions over a time horizon of 1 second. Predictions of our model are enriched by theoretically grounded uncertainty estimates. Key to our success is a Bayesian approach and long term prediction of odometry. We evaluate and compare several different architecture choices and arrive at a novel two-stream Bayesian LSTM encoder-decoder.</p><p>As derived in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>, in Bayesian Regression, the KL divergence between a approximate variational posterior q(?) and the true posterior p(?|X, Y ) distribution of models likely to have generated our data is given by,</p><formula xml:id="formula_17">KL(q(?) || p(?|X, Y )) ? KL(q(?) || p(?)) ? q(?) log p(Y |X, ?)d?.<label>(A1)</label></formula><p>In our case, as we train our model to predict future bounding box sequences given the past bounding box sequence, past and future vehiche odometry, we have</p><formula xml:id="formula_18">X = {B p , O f , O p } and Y = {B f }.</formula><p>Therefore, the KL divergence is given by,</p><formula xml:id="formula_19">KL(q(?) || p(?|X, Y )) ? KL(q(?) || p(?)) ? q(?) log p(B f |B p , O f , O p , ?)d?.<label>(A2)</label></formula><p>As the bounding box at time t + n in B f is predicted conditioned on the bounding box at time t + n ? 1 and the past bounding box sequence, past and future vehiche odometry, by our Bayesian RNN Encoder-Decoder, the KL divergence is given by,</p><formula xml:id="formula_20">KL(q(?) || p(?|X, Y )) ? KL(q(?) || p(?)) ? t q(?) log p(b t+n t |b t+n?1 t , B p , O p , O f , ?)d?.<label>(A3)</label></formula><p>During training (as mentioned in subsection 3.5 of the main paper), we use Monte-Carlo integration to estimate the integral in (A3) (using N samples),</p><formula xml:id="formula_21">KL(q(?) || p(?|X, Y )) ? KL(q(?) || p(?)) ? 1 N t N i=0 log p(b t+n t |b t+n?1 t , B p , O p , O f ,? i ), ? i ? q(?).<label>(A4)</label></formula><p>The probability term p(b t+n</p><formula xml:id="formula_22">t |b t+n?1 t , B p , O p , O f ,? i ) takes the form e ? b t+j i ?b t+j i 2 2 (? t+j i ) ?2</formula><p>. Therefore, replacing the log probability term with the exponential squared error term and introducing additional regularization as mentioned in subsection 3.5 of the main paper leads to the training objective used,</p><formula xml:id="formula_23">1 4N N i=1 n j=1 b t+j i ? b t+j i 2 2 (? t+j i ) ?2 + ? W W k 2 + log? 2 i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details of Two Stream Model</head><p>Here, we include details of each layer of our Two Stream Model. We refer to fully connected layers as Dense and Size refers to the number of neurons in the layer. Bayesian Bounding Box Prediction Stream. We provide the details of the Bayesian Bounding Box prediction stream in <ref type="table" target="#tab_9">Table 6</ref>.  Odometry Prediction Stream. We provide the details of the odometry prediction stream in <ref type="table" target="#tab_11">Table 7</ref>. We then provide details of the CNN encoder.   <ref type="table" target="#tab_12">Table 8</ref> C. Database Statistics In <ref type="figure" target="#fig_4">Figure 5</ref> we plot the number of pedestrian tracks of Layer Type Filters Size Activation Input Output  <ref type="table">Table 9</ref>: Evaluation with varying size of LSTM (|B p | = 8).</p><formula xml:id="formula_24">In 4 Input C 1 C 1 Conv 32 3?3 ReLU In 2 C 2 C 2 Conv 32 3?3 ReLU C 1 P 1 P 1 MaxPool 2?2 C 2 C 3 C 3 Conv 64 3?3 ReLU P 1 C 4 C 4 Conv 64 3?3 ReLU C 4 P 2 P 2 MaxPool 2?2 C 4 C 5 C 5 Conv 128 3?3 ReLU P 2 C 6 C 6 Conv 128 3?3 ReLU C 5 P 3 P 3 MaxPool 2?2 C 6 C 7 C 7 Conv 256 3?3 ReLU P 3 C 8 C 8 Conv 256 3?3 ReLU C 7 C 8 P 4 MaxPool 2?2 C 8 C 9 C 9 Conv 512 3?3 ReLU P 4 C 10 C 10 Conv 512 3?3 ReLU C 9 P 5 P 5 MaxPool 2?2 C 10 FC 1 FC 1 Dense 1024 ReLU P 5 FC 2 FC 2 Dense 256 ReLU FC 1 FC 3 FC 3 Dense 128 tanh FC 2 LSTM dec2</formula><p>In the main paper, we evaluate all models constant LSTM vector size of 128. Here, we report results for the (unconditioned) one stream homoscedastic LSTM encoder-decoder model and the one stream Bayesian LSTM encoder-decoder model using a vector size of 512 In <ref type="table">Table 9</ref>. We see that the homoscedastic version with 512 neurons performs worse than the version with 128 neurons. This is because the larger LSTM over-fits to the bounding box estimation noise in dataset. However, the Bayesian versions have comparable performance, due to dropout which prevents overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of Odometry Prediction</head><p>Visual examples of odometry prediction in <ref type="figure" target="#fig_5">Figure 6</ref>.  Here, we compare our Bayesian Two-stream model <ref type="figure" target="#fig_0">(Figure 2</ref>, of main paper) to, 1. A homoscedastic Two-stream LSTM encoder-decoder model (LSTM). 2. A heteroscedastic Two-stream LSTM encoder-decoder (LSTM-Aleatoric). Note that, both models have the same odometry prediction stream as our Bayesian Two-stream LSTM model (LSTM-Bayesian). The results mirror the evaluation of only the bounding box prediction stream. We see that the heteroscedastic LSTM (LSTM-Aleatoric, 2nd row) outperforms the homoscedastic LSTM (2nd row) with respect to the L metric. This means that the heteroscedastic Two-stream LSTM learns to capture uncertainty and assigns higher probability to the true bounding box sequence. However, when epistemic uncertainty is not modelled, aleatoric uncertainty tried to compensate and this leads to poorer MSE. Finally, our Bayesian Two-stream LSTM (3rd row) outperforms all other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Evaluation of our Two-stream Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Analysis of the Quality of our Uncertainty Metric</head><p>We compare the quality of the uncertainty metric obtained with our Two-stream LSTM-Bayesian model <ref type="figure" target="#fig_2">(Figure 3</ref>, of main paper) to that of the Two-stream LSTM-Aleatoric (the heteroscedastic Two-stream LSTM encoder-decoder in the previous section, which models only aleatoric uncertainty). In plot 1 of <ref type="figure" target="#fig_6">Figure 7</ref> the aleatoric uncertainty to the log squared error of the mean of the predictive distribution of the Two-stream LSTM-Aleatoric model is shown. We see that the distribution is more spread-out with more outliers compared to our Two-stream LSTM-Bayesian model (plot 1, <ref type="figure" target="#fig_2">Figure 3</ref>, of main paper). In plot 2 of <ref type="figure" target="#fig_6">Figure 7</ref> the maximum log squared error (of the mean of the predictive distribution) observed at a certain predicted uncertainty in the test test is shown for both our Two-stream Bayesian model and Twostream LSTM-Aleatoric. We see that the correlation is poor compared to our Two-stream LSTM-Bayesian model (also in plot 3, <ref type="figure" target="#fig_2">Figure 3</ref>, of main paper). In particular, the maximum observed log squared error rises very sharply. Therefore, for  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Additional Video Results</head><p>We include video results of prediction in video.mp4. We include examples of both point estimates and predictive distributions. We include point estimates for comparison against the Kalman Filter and One-stream baselines. The examples show accurate prediction by our Two-stream model over 15 time-steps into the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Two stream architecture for prediction of future pedestrian bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>bounding box corresponding to the i th pedestrian observed on-board a vehicle at time step t can be described by the top-left and bottom-right pixel coordinates: b t i = {(x tl , y tl ), (x br , y br )}. We want to predict the distribution of future bounding box sequences B f (where |B p | = m) of the pedestrian. We condition our predictions on the past bounding box sequence B p , the past odometry sequence O p and the corresponding future odometry sequence O f of the vehicle. The future odometry sequence O f is predicted conditioned on the past odometry sequence O p and on-board visual observation. Odometry sequences consists of the speed s t and steering angle d t of the vehicle, that is, o t = (s t , d t ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Quality of our uncertainty metric: plots 1 and 2 -uncertainty versus squared error, plots 3 and 4 -uncertainty versus maximum observed squared error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 15 Figure 4 :</head><label>4154</label><figDesc>Row 1 shows point estimates under linear vehicle ego-motion and Rows 2, 3 non-linear vehicle ego-motion. Our two-stream model (mean of predictive distribution) outperforms other methods in the second case. Rows 4-5 shows the predictive distributions of the two-stream model under linear vehicle and pedestrian motion. The distribution is symmetric and has high aleatoric uncertainty which captures detection noise and possible pedestrian motion. Row 6 shows a case of a skewed distribution with high epistemic uncertainty which captures uncertainty in vehicle motion.Quality of our Uncertainty Metric. We evaluate our uncertainty metric inFigure 3. The first two plots show the aleatoric and epistemic uncertainty to the squared error of the mean of the predictive distribution of our two-stream model. We use log-log plots for better visualization as most Last Observation: t Prediction: t + 5Prediction: t + 10 Prediction: t + Rows 1-3: Point estimates. Blue: Ground-truth, Red: Kalman Filter (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Length of recovered pedestrian tracks in Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Odometry prediction: We show predicted odometry for 15 time-steps as points (bottom to top) over-layed on the last visual observation. The distance and angle between subsequent points is the predicted (proportional) speed and steering angle. Color codes: Blue: Ground-truth, Red: Kalman Filter, Yellow: Our LSTM without visual input, Green: Our LSTM with visual input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Plot 1 -uncertainty versus squared error, plot 3uncertainty versus maximum observed squared error. a robust error bound it is essential to model both epistemic and aleatoric uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ples from the training set. 2. For each example, weights {W emi , W ems , W enc , W dec } are sampled from q(?) (8), by sampling Bernoulli masks as in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Bounding box prediction error with varying |B p |.</figDesc><table><row><cell>(statistics in the Sup-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Bounding box center prediction error.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Odometry prediction error (MSE), |O p | = {8}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>MSE per time-step of models inTable 1row 1, 4, 5 andTable 4 row 3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>row 1), Yellow: One-stream model (Table 1 row 4), Green: Two-stream model (mean of predictive distribution, Table 4 row 3). Rows 4-6: Predictive distributions of our two-stream model as heat maps. (Link to video results in the Appendix).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Details of the Bounding Box Prediction Stream. Note that, the weights of all the layers are sampled from the approximate posterior q(?).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Details of the Odometry Prediction Stream. Details of the CNN encoder (with output FC 3 ) follows in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Details of the CNN encoder used to condition the output of the Odometry prediction stream. Conv stands for 2D convolution, MaxPool stands for 2D max pooling and UpSample stands for 2D upsampling operations. lengths from 6 to 30. The track length distribution is consistent across training and test sets. We observe that there are many long tracks which stretch over the entire length (30) of the sequence.</figDesc><table><row><cell cols="4">D. Evaluation with Varying Size of LSTM</cell></row><row><cell>Method</cell><cell cols="3">LSTM size Odometry MSE</cell><cell>L</cell></row><row><cell>LSTM</cell><cell>128</cell><cell>None</cell><cell cols="2">650 7.77</cell></row><row><cell>LSTM</cell><cell>512</cell><cell>None</cell><cell cols="2">705 8.15</cell></row><row><cell>LSTM-Bayesian</cell><cell>128</cell><cell>None</cell><cell cols="2">618 4.13</cell></row><row><cell>LSTM-Bayesian</cell><cell>512</cell><cell>None</cell><cell cols="2">619 4.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of Two-stream models (|B p |, |O p | = 8).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Aleatory or epistemic? does it matter? Structural Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian convolutional neural networks with Bernoulli approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop track</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semanticbased surveillance video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1168" to="1181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Will the pedestrian cross? probabilistic path prediction based on learned motion features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hermes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="386" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gaussian process regression flow for analysis of motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heteroscedastic gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2287" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating the mean and variance of the target probability distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Alvinn, an autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Goal-directed pedestrian prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kloeden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01230</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiperson tracking by multicut and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robot navigation in dense human crowds: the case for cooperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Who are you with and where are you going? In CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding highlevel semantics by modeling traffic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random field topic model for semantic region analysis in crowded scenes from tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
