<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Limitations of Behavior Cloning for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
							<email>fcodevilla@cvc.uab.es</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Eder</forename><surname>Santana</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
							<email>alopez@cvc.uab.es</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<email>adrien.gaidon@tri.global</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center (CVC) Campus UAB</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Toyota Research Institute (TRI)</orgName>
								<address>
									<settlement>Los Altos</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Vision Center (CVC) Campus UAB</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Toyota Research Institute (TRI)</orgName>
								<address>
									<settlement>Los Altos</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Limitations of Behavior Cloning for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Work made during an internship at Toyota Research Institute (TRI)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Driving scenarios from our new benchmark where the agent needs to react to dynamic changes in the environment, handle clutter (only part of the environment is causally relevant), and predict complex sensorimotor controls (lateral and longitudinal). We show that Behavior Cloning yields state-of-the-art policies in these complex scenarios and investigate its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-of-the-art results, including in unseen environments, executing complex lateral and longitudinal maneuvers without these reactions being explicitly programmed. However, we confirm well-known limitations (due to dataset bias and overfitting), new generalization issues (due to dynamic objects and the lack of a causal model), and training instability requiring further research before behavior cloning can graduate to real-world driving. We will release our benchmark and code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end behavior cloning for autonomous driving has recently attracted renewed interest <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> as a simple alternative to traditional modular approaches used in industry <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>. In this paradigm, perception and control are learned simultaneously using a deep network. Explicit sub-tasks are not defined, but may be implicitly learned from data. These sensorimotor controllers are typically obtained by imitation learning from human demonstrations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref>. The deep network learns, without being explicitly programmed, to recognize patterns associating sensory input (e.g., a single RGB image) with a desired reaction in terms of vehicle control parameters producing a target maneuver. Behavior cloning can directly learn from large fleets of human-driven vehicles without requiring a fixed ontology and extensive amounts of labeling. Finally, end-to-end imitative systems can be learned off-line in a safe way, in contrast to reinforcement learning approaches that typically require millions of trial and error runs in the target environment <ref type="bibr" target="#b22">[23]</ref> or a faithful simulation.</p><p>End-to-end imitative systems can suffer a domain shift between the off-line training experience and the on-line behavior <ref type="bibr" target="#b33">[34]</ref>. This problem, however, can be addressed in practice by data augmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Nonetheless, in spite of the early and recent successes of behavior cloning for end-to-end driving <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>, it has not yet proved to scale to the full spectrum of driving behaviors, such as reacting to multiple dynamic objects.</p><p>In this paper, we propose a new benchmark, called NoCrash, and perform a large scale analysis of end-to-end behavioral cloning systems in complex driving conditions not studied in this context before. We use a high fidelity simulated environment based on the open source CARLA simulator <ref type="bibr" target="#b11">[12]</ref> to enable reproducible large scale off-line training and on-line evaluation in over 80 hours of driving under several different conditions. We describe a strong Conditional Imitation Learning baseline, derived from <ref type="bibr" target="#b9">[10]</ref>, that significantly improves upon state of the art modular <ref type="bibr" target="#b23">[24]</ref>, affordance based <ref type="bibr" target="#b35">[36]</ref>, and reinforcement learning <ref type="bibr" target="#b25">[26]</ref> approaches, both in terms of generalization performance in training environments and unseen ones.</p><p>Despite its positive performance, we identify limitations that prevent behavior cloning from successfully graduating to real-world applications. First, although generalization performance should scale with training data, generalizing to complex conditions is still an open problem with a lot of room for improvement. In particular, we show that no approach reliably handles dense traffic scenes with many dynamic agents. Second, we report generalization issues due to dataset biases and the lack of a causal model. We indeed observe diminishing returns after a certain amount of demonstrations, and even characterize a degradation of performance on unseen environments. Third, we observe a significant variability in generalization performance when varying the initialization or the training sample order, similar to on-policy RL issues <ref type="bibr" target="#b16">[17]</ref>. We conduct experiments estimating the impact of ImageNet pre-training and show that it is not able to fully reduce the variance. This suggests the order of training samples matters for off-policy Imitation Learning, similar to the on-policy case <ref type="bibr" target="#b45">[45]</ref>.</p><p>Our paper is organized as follows. Section 2 describes related work, Section 3 our strong behavior cloning baseline, Section 4 our evalution protocol, including our new NoCrash benchmark, Section 5 our experimental results, and Section 6 our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Behavior cloning for driving dates back to the work of Pomerleau <ref type="bibr" target="#b30">[31]</ref> on lane following, later followed by other approaches <ref type="bibr" target="#b20">[21]</ref>, including going beyond driving <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. The distributional shift between the training and testing distributions is the main known limitation of this approach, which might require on-policy data collection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, obtained by the learning agent. Nonetheless, recent works have proposed effective off-policy solutions, for instance by expanding the space of image/action pairs either using noise <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>, extra sensors <ref type="bibr" target="#b5">[6]</ref>, or modularization <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref>. We show, however, that there are other limitations impor-tant to consider in complex driving scenarios, in particular dataset bias and high variance, which both harm scaling generalization performance with training data.</p><p>Dataset bias is a core problem of real-world machine learning applications <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4]</ref> that can have dramatic effects in a safety-critical application like autonomous driving. Imitation learning approaches are particularly sensitive to this issue, as the learning objective might be dominated by the main modes in the training data. Going beyond the original CARLA benchmark <ref type="bibr" target="#b11">[12]</ref>, we use our new NoCrash benchmark to quantitatively assess the magnitude of this problem on generalization performance for more realistic and challenging driving behaviors.</p><p>High variance is a key problem in powerful deep neural networks, and we show that high performance behavior cloning models are particularly suffering from this. This problem is related to sensitivity to both initialization and sampling order <ref type="bibr" target="#b29">[30]</ref>, reproducibility issues in Reinforcement Learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>, and the need to move beyond the i.i.d. data assumption towards curriculum learning <ref type="bibr" target="#b4">[5]</ref> for sensorimotor control <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Driving benchmarks fall in two main categories: offline datasets, e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16]</ref>, or on-line environments. We focus here on on-line benchmarks, as visuomotor models performing well in dataset-based evaluations do not necessarily translate to good driving policies <ref type="bibr" target="#b8">[9]</ref>. Driving is obviously a safety-critical robotic application. Consequently, for safety and to enable reproducibility, researchers focus on using photo-realistic simulation environments. In particular, the CARLA open-source driving simulator <ref type="bibr" target="#b11">[12]</ref> is emerging as a standard platform for driving research, used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>. Note, however, that transferring policies from simulation to the real-world is an open problem <ref type="bibr" target="#b26">[27]</ref> out of the scope of this paper, although recent works have shown encouraging results <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Strong Baseline for Behavior Cloning</head><p>In this section, we first describe the behavior cloning framework we use, its limitations, and a robustified baseline that tries to tackle these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Imitation Learning</head><p>Behavior cloning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23</ref>] is a form of supervised learning that can learn sensorimotor policies from off-line collected data. The only requirements are pairs of input sensory observations associated with expert actions. We use an expanded formulation for self-driving cars called Conditional Imitation Learning, CIL <ref type="bibr" target="#b9">[10]</ref>. It uses a high-level navigational command c that disambiguates imitation around multiple types of intersections. Given an expert policy ? * (x) with access to the environment state x, we can execute this policy to produce a dataset,</p><formula xml:id="formula_0">D = { o i , c i , a i } N i=1</formula><p>, where o i are sensor data observations, c i are high-level commands (e.g., take the next right, left, or stay in lane) and a i = ? * (x i ) are the resulting vehicle actions (low-level controls). Observations o i = {i, v m } contain a single image i and the ego car speed v m [10] added for the system to properly react to dynamic objects on the road. Without the speed context, the model cannot learn if and when it should accelerate or brake to reach a desired speed or stop.</p><p>We want to learn a policy ? parametrized by ? to produce similar actions to ? * based only on observations o and highlevel commands c. The best parameters ? * are obtained by minimizing an imitation cost :</p><formula xml:id="formula_1">? * = arg min ? i ?(o i , c i ; ?), a i .<label>(1)</label></formula><p>In order to evaluate the performance of the learned policy ?(o i , c i ; ?) on-line at test time, we assume access to a score function giving a numeric value expressing the performance of the policy ? on a given benchmark (cf. section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Limitations</head><p>In addition to the distributional shift problem <ref type="bibr" target="#b33">[34]</ref>, behavior cloning presents some key limitations.</p><p>Bias in Naturalistic Driving Datasets. The appeal of behavior cloning lies in its simplicity and theoretical scalability, as it can indeed learn by imitation from large offline collected demonstrations (e.g., using driving logs from manually driven production vehicles). It is, however, susceptible to dataset biases like all learning methods. This is exacerbated in the case of imitation learning of driving policies, as most of real-world driving consists in either a few simple behaviors or a heavy tail of complex reactions to rare events. Consequently, this can result in performance degrading as more data is collected, because the diversity of the dataset does not grow fast enough compared to the main mode of demonstrations. This phenomenon was not clearly measured before. Using our new NoCrash benchmark (section 4), we confirm it may happen in practice.</p><p>Causal Confusion. Related to dataset bias, end-to-end behavior cloning can suffer from causal confusion <ref type="bibr" target="#b13">[14]</ref>: spurious correlations cannot be distinguished from true causes in observed training demonstration patterns unless an explicit causal model or on-policy demonstrations are used. Our new NoCrash benchmark confirms the theoretical observation and toy experiments of <ref type="bibr" target="#b13">[14]</ref> in realistic driving conditions. In particular, we identify a typical failure mode due to a subtle dataset bias: the inertia problem. When the ego vehicle is stopped (e.g., at a red traffic light), the probability it stays static is indeed overwhelming in the training data. This creates a spurious correlation between low speed and no acceleration, inducing excessive stopping and difficult restarting in the imitative policy. Although mediated <ref type="figure">Figure 2</ref>. Our proposed network architecture, called CILRS, for end-to-end urban driving based on CIL <ref type="bibr" target="#b9">[10]</ref>. A ResNet perception module processes an input image to a latent space followed by two prediction heads: one for controls and one for speed.</p><p>perception approaches that explicitly model causal signals like traffic lights do not suffer from this theoretical limitation, they still under-perform end-to-end learning in unconstrained environments, because not all causes might be modeled (e.g., some potential obstacles) and errors at the perception layer (e.g., missed detections) are irrecoverable.</p><p>High variance. With a fixed off-policy training dataset, one would expect CIL to always learn the same policy in different runs of the training phase. However, the cost function is optimized via Stochastic Gradient Descent (SGD), which assumes the data is independent and identically distributed <ref type="bibr" target="#b6">[7]</ref>. When training a reactive policy on snapshots of longer human demonstrations included in the training data, the i.i.d. assumption does not hold. Consequently, we might observe a high sensitivity to the initialization and the order in which the samples are seen during training. We confirm this in our experiments, finding an overall high variance due to both initialization and sampling order, following the decomposition in <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_2">Var(?) = E D V ar I (?|D) + V ar D E I [?|D] ,<label>(2)</label></formula><p>where I denotes the randomness in initialization. Because the policy ? is evaluated on-line in simulated environments, we evaluate in practice the variance of the score on the test benchmark, and report results when freezing the initialization and/or varying the sampling order for different training datasets D (including of varying sizes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model</head><p>In order to explore the aforementioned limitations of behavior cloning, we propose a robustified CIL model designed to improve on <ref type="bibr" target="#b9">[10]</ref> while remaining strictly offpolicy. Our network architecture, called CILRS, is shown in <ref type="figure">Figure 2</ref>. We describe our enhancements below.</p><p>Deeper Residual Architecture. We use a ResNet34 architecture <ref type="bibr" target="#b14">[15]</ref> for the perception backbone P(i). In the presence of large amounts of data, using deeper architectures can be an effective strategy to improve performance <ref type="bibr" target="#b14">[15]</ref>. In particular, it can reduce both bias and variance, maintaining in particular a constant variance due to training set sampling with both network width and depth <ref type="bibr" target="#b29">[30]</ref>. For end-to-end driving, the choice of architecture has been mostly limited to small networks so far <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> to avoid overfitting on limited datasets. In contrast, we notice that bigger models have better generalization performance on learning reactions to dynamic objects and traffic lights in complex urban environments.</p><p>Speed Prediction Regularization. To cope with the inertia problem without an explicit mapping of potential causes or on-policy interventions, we jointly train a sensorimotor controller with a network that predicts the ego vehicle's speed. Both neural networks share the same representation via our ResNet perception backbone. Intuitively, what happens is that this joint optimization enforces the perception module to have speed related features into the learned representation. This reduces the dependency on input speed as the only way to get dynamics of the scene, leveraging instead visual cues that are predictive of the car's velocity (e.g., free space, curves, traffic light states, etc).</p><p>Other changes. We use L1 as loss function instead of the mean squared error (MSE), as it is more correlated to driving performance <ref type="bibr" target="#b8">[9]</ref>. As our NoCrash benchmark consists of complex realistic driving conditions in the presence of dynamic agents, we collect demonstrations from an expert game AI using privileged information to drive correctly (i.e. always respecting rules of the road and not crashing into any obstacle). Robustness to heavy noise in the demonstrations is beyond the scope of our work, as we aim to explore limitations of behavior cloning methods in spite of good demonstrations. Finally, we pre-trained our perception backbone on ImageNet to reduce initialization variance and benefit from generic transfer learning, a standard practice in deep learning seldom explored for behavior cloning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section we discuss the simulated environment we use, CARLA, and review the original CARLA benchmark. Due to its limitations, we propose a new benchmark, called NoCrash, that tries to better evaluate driving controllers reaction to dynamic objects. This new benchmark, thanks to its complexity, allows further analysis on limitations of behavior cloning and other policy learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulated Environment</head><p>We use the CARLA simulator <ref type="bibr" target="#b11">[12]</ref> version 0.8.4. The CARLA environment is divided in two different towns. Town 01 contains 2.9 km of drivable roads in a suburban environment. Town 02 is approximately 1.4 km of drivable roads, also in a suburban environment.</p><p>The CARLA environment may contain dynamic obstacles that interact with the ego car. Pedestrians, for instance, might cross the road on random occasions without any apparent previous notice. This action forces the ego car to promptly react. The CARLA environment also contains a diversity of car brands that cruise at different speeds. Overall it provides a diverse, photo-realistic, and dynamic environment with challenging driving conditions (cf. <ref type="figure">Figure 1)</ref>.</p><p>The original CARLA benchmark <ref type="bibr" target="#b11">[12]</ref> evaluates driving controllers on several goal directed tasks of increasing difficulty. Three of the tasks consist of navigation in an empty town and one of them in a town with a small number of dynamic objects. Each task is tested in four different conditions of increasingly different from the training environment. The conditions are: same as training, new weather conditions that are derivatives from those seen during training, and a new town that has different buildings and different shadow patterns. Note that the biggest generalization test is the combination of new weather and new town.</p><p>The goal directed tasks are evaluated based on success rate. If the agent reaches the goal regardless of what happened during the episode, this episode is considered a success. The collisions and other infractions are considered and the average number of kilometers between infractions is measured. This evaluation induces the benchmark to be mainly focused on problems of a static nature. These problems consider the environmental conditions and the static objects of the world like buildings and trees. Thus, the original CARLA benchmark mostly evaluates skills such as lane keeping and performing 90 degrees turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NoCrash Benchmark</head><p>We propose a new larger scale CARLA driving benchmark, called NoCrash, designed to test the ability of ego vehicles to handle complex events caused by changing traffic conditions (e.g., traffic lights) and dynamic agents in the scene. For this benchmark, we propose different tasks and metrics than the original CARLA benchmark <ref type="bibr" target="#b11">[12]</ref> to precisely measure specific reaction patterns that we know good drivers must master in urban conditions.</p><p>We propose three different tasks, each one corresponding to 25 goal directed episodes. In each episode, the agent starts at a random position and is directed by a high-level planner into reaching some goal position. The three tasks have the same set of start and end positions, as well as an increasing level of difficulty as follows:</p><p>1. Empty Town: no dynamic objects.</p><p>2. Regular Traffic: moderate number of cars and pedestrians.</p><p>3. Dense Traffic: large number of pedestrians and heavy traffic (dense urban scenario).</p><p>Similar to the CARLA Benchmark, NoCrash has six different weather conditions, where four were seen in training and two reserved for testing. It also has two different towns, one that is seen during training, and the other reserved for testing. For more details about the benchmark configuration, please refer to the supplementary material. As mentioned above, the measure of success of an episode should be more representative of the agent capabilities to react to dynamic objects. The original CARLA benchmark <ref type="bibr" target="#b11">[12]</ref> has a goal conditioned success rate metric that is computed separately from a kilometers between infractions metric. The latter metric was proposed to be analogous to the one commonly used by real-world driving evaluations where the number of human interventions per kilometer is counted <ref type="bibr" target="#b17">[18]</ref>. These interventions usually happen when the safety driver notices some inconsistent behavior that would lead the vehicle to a possibly dangerous state. On a potentially inconsistent behavior, the human intervention will put the vehicle back to a safe state. However, in the CARLA benchmark analysis, when an infraction is made, the episode continues after the infraction, leading to some inaccuracy in infraction counting. An example of inaccuracy includes whether a crash after leaving the road be counted as one or two infractions.</p><p>In NoCrash, instead of counting the number of infractions per kilometer, we end the episode as failing when any collision bigger than a fixed magnitude happens. With this limitation, we are setting a lower bound and have a guarantee of acceptable behaviors based on the measured percentage of success. Furthermore, this makes the evaluation even more similar to the km/interventions evaluation used in real world, since a new episode always sends the agent back to a safe starting state. In summary, we consider an episode to be successful if the agent reaches a certain goal under a time limit without colliding with any object. We also care about the ability of the agent to obey traffic rules. In particular, we measure and report the percentage of traffic light violations in Supplementary material. Note that an episode is not terminated when a traffic light violation occurs unless they are followed by a collision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we detail our protocol for model training and briefly show that it is competitive with the state of the art. We also explore several corner cases to explore the limitations of the behavior cloning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Details</head><p>First, we collected more than 400 hours of realistic simulated driving data from a single town of the CARLA environment using more than 200 GPU-days. We used an expert driving AI agent that leverages privileged information about the scene to drive naturally and well in complex conditions. After automatically filtering the data for simulation failures, duplicates, and edge cases using simple rules, we built a dataset of 100 hours of driving, called CARLA100. To enable running a wide range of experiments, we train all methods using a subset of 10 hours of expert demonstrations by default. We also report larger scale training experiments and scalability analyses in Section 5.3 and in supplementary material. We will release the code for our demonstrator and our CARLA100 training dataset for reproducibility. More details about them are given in the supplementary material.</p><p>Training controllers on this dataset, we found that augmentation was not as crucial as reported by previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. The only regularization we found important for performance was using a 50% dropout rate <ref type="bibr" target="#b39">[40]</ref> after the last convolutional layer. Any larger dropout led us to underfitting models. All models were trained using Adam <ref type="bibr" target="#b18">[19]</ref> with minibatches of 120 samples and an initial learning rate of 0.0002. At each iteration, a minibatch is sampled randomly from the entire dataset and presented to the network for training. If we detect that the training error has not decreased for over 1, 000 iterations we divide the learning rate by 10. We used a 2 hours validation dataset to discover when to stop the training process. We validate every 20k iterations and if the validation error increases for three iterations we stop the training process and use this checkpoint to test on the benchmarks, both CARLA and NoCrash. We build a validation dataset as described in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the state of the art</head><p>We compare our results using both the original CARLA benchmark from <ref type="bibr" target="#b11">[12]</ref> and our proposed NoCrash benchmark. We compare two versions of our method: "CILRS" (our CIL extension with a ResNet architecture and speed prediction, as described in section 3), and a version without the speed prediction branch noted "CILR". We compare our method with the original CIL from <ref type="bibr" target="#b9">[10]</ref> and three state-ofthe-art approaches: CAL <ref type="bibr" target="#b35">[36]</ref>, MT <ref type="bibr" target="#b24">[25]</ref>, and CIRL <ref type="bibr" target="#b25">[26]</ref>. In contrast to end-to-end behavior cloning, these methods enforce some modularization that require extra information at training time, such as affordances (CAL), semantic segmentation (MT), or extra on-policy interaction with the environment (CIRL). Our approach only requires a fixed off-policy dataset of demonstrations.</p><p>We show results on the original CARLA benchmark <ref type="bibr" target="#b11">[12]</ref> in <ref type="table" target="#tab_0">Table 1</ref> and results on our proposed NoCrash benchmark in <ref type="table">Table 2</ref>. While most methods perform well in most conditions on the original CARLA benchmark, they all perform significantly worse on NoCrash, especially when trying to generalize to new conditions. This confirms the usefulness of NoCrash in terms of exploring the limitations of driving policy learning due to its more challenging nature. In addition, our proposed CILRS model significantly improves over the state of the art, e.g., +9% and +26% on CARLA "Nav. Dynamic" in training and new conditions respectively, +10% and +24% on NoCrash Regular traffic in training and new conditions respectively. The significant improvements in generalization conditions, both w.r.t. CIL and mediated approaches, confirm that our improved endto-end behavior cloning architecture can effectively learn complex general policies from demonstrations alone. Furthermore, our ablative analysis shows that speed prediction is helpful: CILR can indeed be up to ?14% worse than CILRS on NoCrash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of Limitations</head><p>Although clearly above the state of the art, our improved CILRS architecture nonetheless sees a strong degradation of performance similar to all other methods in the presence of challenging driving conditions. We investigate how this degradation relates to the limitations of behavior cloning mentioned in Section 3.2 by using the NoCrash benchmark, in particular to better evaluate the interaction of the agents with dynamic objects.</p><p>Generalization in the presence of dynamic objects. Limited generalization was previously reported for end-toend driving approaches <ref type="bibr" target="#b11">[12]</ref>. In our experiments, we observed additional, and more prominent, generalization issues when the control policies have to deal with dynamic objects. <ref type="table">Table 2</ref> indeed shows a large drop in performance as we change to tasks with more traffic, e.g., ?55% and ?66% from Empty to Dense traffic in NoCrash training / new conditions respectively. In contrast, results in Empty town only degrade by ?7% when changing to a new environment and weather. Therefore, the learned policies have a much harder time dealing robustly with a large number of vehicles and pedestrians. Furthermore, this impacts all policy learning methods, including those using additional supervision or on-policy demonstrations, often even more than our proposed CILRS method.</p><p>Driving Dataset Biases. <ref type="figure" target="#fig_0">Figure 3</ref> evaluates the effect of the amount of training demonstrations on the learned policy. Here we compare models trained with 2, 10, 50 and 100 hours of demonstrations. The plots show the mean success rate and standard deviation over four different training cycles with different random seeds. Our best results on most of the scenarios were obtained by using only 10 hours of training data, in particular on the "Dense Traffic" tasks and novel conditions such as New Weather and New Town.</p><p>These results quantify a limitation described in Section 3.2: the risk of overfitting to data that lacks diversity. This is here exacerbated by the limited spatial extent and visual variety of our environment, including in terms of dynamic objects. We indeed observed that some types of vehicles tend to elicit better reactions from the policy than others. The more common the vehicle model and color, the better the trained agent reacts to it. This raises ethical challenges in automated driving, requiring further research in fair machine learning for decision-making systems <ref type="bibr" target="#b3">[4]</ref>.</p><p>Causal confusion and the inertia problem. The main problem we observe caused by bias is the inertia problem stemming from causal confusion, as detailed in Section 3.2.  . The percentage of episodes that failed due to the inertia problem. We can see that by increasing the amount of data, this bias may further degrade the generalization capabilities of the models. <ref type="figure">Figure 5</ref>. Comparison between the results with and without the speed prediction and different amounts of training demonstrations. We report the results only for the case were highest generalization is needed (New Weather and Town). <ref type="figure" target="#fig_1">Figure 4</ref> shows the percentage of episodes that failed due to the agent staying still, without any intention to use the throttle, for at least 8 seconds before the timeout. Our results show the percentage of episodes failed due to that inertia problem increases with the amount of data used for training. We proposed to use a speed prediction branch as part of our CILRS model (cf. <ref type="figure">Figure 2</ref>) to mitigate this problem. <ref type="figure">Figure 5</ref> shows the percentage of successes for the New Weather &amp; Town conditions on different tasks with and without speed prediction. We observe that the speed prediction branch can substantially improve the success rate thanks to its regularization effect. It is, however, not a final solution to this problem, as we still observe instances of the inertia problem after using this approach.</p><p>High Variance. Repeatability of the training process is crucial for enhancing trust in end-to-end models. Unfortunately, we can still see drastic changes in the learned policy performance due to the variance caused by initialization and data sampling (cf. Section 3.2). <ref type="figure">Figure 6</ref> compares the cause of episode termination for two models where the only difference is the random seed during training. The Model S1 has a much higher chance of ending episodes due to vehicle collisions. Qualitatively, it seemed to have learned a less general braking policy and was more prone to rear-end collisions with other vehicles. On the other hand, Model S2 is able to complete more episodes and is less likely to fail due to vehicle crashes. However, we can see that it times out more, showing a tendency to stop a lot, even in non threatening situations. This can be seen by analyzing the histograms of the throttle applied by both models during the benchmark, as shown in <ref type="figure">Figure 7</ref>. We can see a tendency <ref type="bibr">Figure 6</ref>. Cause of episode termination on NoCrash for two CILRS models (trained on 10 hours with ImageNet initialization) with identical parameters but different random seeds. The episodes were ran under "New Weather &amp; Town" conditions of the "Dense Traffic" task.</p><p>for throttles of higher magnitude on Model S1. <ref type="figure">Figure 7</ref>. Probability distribution of having certain throttle values comparing models with two different random seeds but trained with the same hyper-parameters and data. We can see that S1 (red) is much more likely to have a higher throttle value.</p><p>As off-policy imitation learning uses a static dataset for training, this randomness comes from the order in which training data is sampled and the initialization of the random weights. This can possibly define which minima the models converges to. <ref type="table">Table 3</ref> quantifies the effect of initialization on the success rate of driving tasks by computing the variance expressed in Equation 2. The expected policy score was computed by averaging twelve different training runs. We also consider the variance with and without ImageNet initialization. We can see that the success rate can change by up to 42% for tasks with dynamic objects. ImageNet initialization tends to reduce the training variability, mainly due to smaller randomness on initialization but also due to a more stable learned policy.  <ref type="table">Table 3</ref>. Estimated variance of the success rate of CILRS on NoCrash computed by training 12 times the same model with different random seeds. The variance is reduced by fixing part of the initial weights with ImageNet pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Our new driving dataset (CARLA100), benchmark (NoCrash), and end-to-end sensorimotor architecture (CILRS) indicate that behavior cloning on large scale offpolicy demonstration datasets can vastly improve over the state of the art in terms of generalization performance, including mediated perception approaches with additional supervision. This is thanks to using a deeper residual architecture with an additional speed prediction target and good regularization.</p><p>Nonetheless, our extensive experimental analysis has shown that some big challenges remain open. First of all, the amount of dynamic objects in the scene directly hurts all policy learning methods, as multi-agent dynamics are not directly captured. Second, the self-supervised nature of behavior cloning enables it to scale to large datasets of demonstrations, but with diminishing returns (or worse) due to driving-specific dataset biases that require explicit treatment, in particular biases that create causal confusion (e.g., the inertia problem). Third, the large variance resulting from initialization and sampling order indicates that running multiple runs on the same off-policy data is key to identify the best possible policies. This is part of the broader deep learning challenges regarding non-convexity and initialization, curriculum learning and training stability.</p><p>We will release CARLA100, the code of our demonstrator AI and CILRS model, as well as our NoCrash benchmark to stimulate future research on these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>Felipe Codevilla was supported in part by FI grant 2017FI-B1-00162. Antonio M. L?pez and Felipe Codevilla acknowledges the financial support by the Spanish TIN2017-88709-R (MINECO/AEI/FEDER, UE). Antonio M. Lpez also acknowledges the financial support by ICREA under the ICREA Academia Program. As CVC/UAB researchers, Antonio and Felipe also acknowledge the Generalitat de Catalunya CERCA Program and its ACCIO agency. We also thank the generous TRI support in AWS instances to run the additional experiments after Felipe's internship. Special thanks to Yi Xiao for all the help on making the video.</p><p>Here we describe the content of the CARLA100 dataset. Note that for training our model we only used RGB sensor data, the ego-vehicle forward speed, the high level turn intentions for the conditional imitation learning and the ego vehicle controls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Expert Demonstrator</head><p>We collect the dataset, here referred as CARLA100, by executing an automated navigation expert in the simulated environment. The expert has access to privileged information about the simulation state, including the exact map of the environment and the exact positions of the ego-car, all other vehicles, and pedestrians.</p><p>The path driven by the expert is calculated using a standard planner. This planner uses an A* algorithm to determine the path to reach a certain goal. This path is then converted into waypoints used by a PID controller to generate the throttle, brake, and steering for the expert demonstrator. The expert drives steadily on the center of the lane, keeping a constant speed of 35 km/h when driving straight and reducing the speed when making turns to about 15 Km/h.</p><p>In addition, the expert is programmed to react to visible pedestrians when required to prevent collisions. The expert reduces its speed proportionally to the collision distance when the pedestrian is over 5 meters away and less than 15 meters away, or breaking to full stop when the pedestrian is less than 5 meters away.</p><p>The proposed demonstrator also reduces its speed to follow lead cars. The expert stops when the leading vehicle is closer than 5 meters. For our data collection process the expert never performs lane changes or overtakes.</p><p>To improve diversity, realism, and increase the number of visited state-action pairs, we add noise to the ego car controls. This reduces the difference between offline training and online testing scenarios <ref type="bibr" target="#b19">[20]</ref>. We input noise to the expert demonstrator in a similar way as proposed by <ref type="bibr" target="#b9">[10]</ref>. The noise simulates a gradual drift away from the desired trajectory of the experts. However, for training, the drift is not used, but only the reactions performed by the expert to correct the path. The added noise signal is detailed on Section A.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Content</head><p>The dataset collection is divided into goal directed episodes where the expert goes from a start position into a goal position while stopping to avoid collisions with dynamic obstacles. In total, we collected 2373 episodes with different characteristics. The entire dataset was collected on Town01. Each episode has the following features:</p><p>? Number of Pedestrians: the total number of spawned pedestrians around the town. This number is randomly sampled from the interval [50, 100].</p><p>? Number of Vehicles: the total number of spawned vehicle around the town. This number is randomly sampled from the interval <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">70]</ref>.</p><p>? Spawned seed for pedestrians and vehicles: the random seed used for the CARLA object spawning process.</p><p>? Weather: the weather used for the episode is sampled from the set: Clear Noon, Clear Noon After Rain, Heavy Rain Noon, Clear Sunset.</p><p>Each episode last from 1 to 5 minutes partitioned in simulation steps of 100 ms. For each step, we store data divided into two different categories: sensor data is stored as PNG images, and measurement data is stored as json files.</p><p>For the sensor data we have the different camera sensors used: RGB camera, and depth camera, and semantic segmentation pseudo sensor. For each sensor we record data in three positions: aligned with the car center, rotated 30 degrees to the left and rotated 30 degrees to the right.</p><p>As measurements, we have data measured from the egovehicle, the world status, and from all the other non player agents. The following data was collected from the egovehicle and the world status:</p><p>? Step Number: the simulation step that starts at zero and is incremented by one for every 100ms in game time.</p><p>? Game Time-stamp: the time that has passed since the simulation has started.</p><p>? Position: the world position of the ego-vehicle. It is expressed as a three dimensional vector [x, y, z] in meters.</p><p>? Orientation: the orientation of the vehicle with respect to the world expressed as Euler Angles (row, pitch and yaw).</p><p>? Acceleration: the acceleration vector of the egovehicle with respect to the world.</p><p>? Forward Speed: the scalar speed of the ego vehicle in the forward direction of movement.</p><p>? Intentions: a signal that is proportional to the effect that the dynamic objects in the scene are having in the ego car actions. We use three different intention signals: stopping for pedestrians, stopping for cars and stopping for traffic lights. For example, an intention of 1 for stopping for pedestrian means that the ego car   totally stopped for a pedestrian that is less than 5 meters away. An intention of the same class of 0.5 means that the expert noticed a pedestrians and has reduced its speed to a certain extent. An intention of 0 means there are no pedestrians nearby in the field of view of the expert.</p><p>? High Level Commands: the high level indication stating what the ego-vehicle should do in the next intersection: go straight, turn left, turn right, or do not care. Each of these commands are encoded as a integer number. 2 is do not care, 3 for turn left, 4 for turn right, 5 for go straight.</p><p>? Waypoints: a set containing the next 10 future positions the vehicle should assume. This is calculated with the path planning algorithm.</p><p>? Steering Angle: the current steering angle of the vehicle's steering wheel.</p><p>? Throttle: the current pressure on the throttle pedal.</p><p>? Brake: the current pressure on the brake pedal.</p><p>? Hand Brake: if the hand brake is activated not.</p><p>? Steer Noise: the current steering angle of the vehicle considering the noise function.</p><p>? Throttle Noise: the current pressure in the throttle pedal considering the noise function.</p><p>? Brake Noise: the current pressure in the brake pedal considering the noise function. The noise function is described in Section A.3</p><p>For each of the non-player agents (pedestrians, vehicles, traffic light), the following information is provided:</p><p>? Unique ID: an unique identifier of this agent.</p><p>? Type: if it is a pedestrian, a vehicle or a traffic light.</p><p>? Position: the world position of the agent. It is expressed as a three dimensional vector [x, y, z] in meters.</p><p>? Orientation: the orientation of the agent with respect to the world. Expressed as Euler angles (row, pitch and yaw).</p><p>? Forward Speed: the scalar speed of the agent in the forward direction of movement.</p><p>? State: only for traffic lights. Contains the state of the traffic light: either red, yellow or green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Noise Distribution</head><p>During training data collection, 20% of the time we injected noise into expert's steering signal. Namely, at random point in time we added a perturbation to the steering angle provided by the driver. The perturbation is a triangular impulse: it increases linearly, reaches the maximum value and then linearly declines. This simulates smooth drift from the desired trajectory, similar to what might happen with a poorly trained controller. The triangular impulse is parametrized by its starting time t 0 , duration ? ? + , sign ? ? {?1, +1} and intensity ? ? + :</p><formula xml:id="formula_3">s perturb (t) = ?? max 0, 1 ? 2(t ? t 0 ) ? ? 1 .<label>(3)</label></formula><p>Every second of driving we started a perturbation with probability p perturb . We used p perturb = 0.1 in our experiments. The sign of each perturbation was sampled at random, the duration was sampled uniformly from 0.5 to 2 seconds, the intensity was fixed to 0.15. Changing Architecture In <ref type="figure">Figure 12</ref>, we compare the results of the 8 layer convolutional model used by Codevilla et. al. <ref type="bibr" target="#b9">[10]</ref> and several new ResNet based configurations using our new dataset. First, we noticed that the 8 convolutions model obtained worse results than the ones reported as CIL at <ref type="table" target="#tab_0">Table 1</ref>. This happened since we trained the 8 convolutions architecture with the more complex CARLA100 dataset. The model did not have enough capacity to capture the more complex actions and only fitted the several segments where the demonstrator stands still in front of traffic lights. This shows that higher capacity models are able to better learn different sub-tasks. However, the results get worse for the deeper ResNet50 based models, showing that there is still possibility for different types of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Initialization</head><p>We also observed a considerable change in the success rate results when not using ImageNet initialization, as show in <ref type="figure">Figure 11</ref>. Without Imagnet initialization, the highest success rates obtained by models trained on 100 hours of demonstrations. However, these later results are still below what can be achieved with less data and ImageNet pre-training, specially on the dense traffic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Reacting to Traffic Lights</head><p>We show that interesting policies around traffic lights emerged for some of the models we trained. In <ref type="table" target="#tab_7">Table 7</ref> we show the percentage of traffic lights that were crossed on green light for different models. This number is computed for the "Empty Town" task from the dynamic urban scenarios benchmark. The original CIL model trained with older data <ref type="bibr" target="#b9">[10]</ref> represents an effective policy that was trained without demonstrations of stopping for red traffic light, so its number can be seen as a lower bound. The 8 convolutions is a model with the same architecture as the CIL model but trained to react to traffic lights with the proposed new dataset. We can see that this model was more reactive to traffic lights, but very poorly. On the other hand, our best   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Main Causes of Failure</head><p>On <ref type="table">Table 8</ref> we show some of our models compared with some of the literature with regard to their cause of failure. We specify the percentage of episodes that ended due to different causes of crash, due to timeout of the task or if the main cause is that the controller stopped and never resumed moving again (i.e the inertia problem). <ref type="figure">Figure 10</ref>. Percentage of episodes ended by the "inertia problem" under different conditions. We report the mean and the standard deviation over four different training runs. We compare models with different amounts of training data and without image-net pre-training. We can see that the inertia problem becomes more prominent with more data. <ref type="figure">Figure 11</ref>. The importance of data and initialization without ImageNet pre-training. We can see that the overall results improve with more data but not significantly. We can also see a case of worse performance when changing the ammount of training data from 50 to 100 hours under the New Weather &amp; Town conditions with Dense Traffic. <ref type="figure">Figure 12</ref>. Ablative analysis between different architectures. The eight convolutions architecture,"8conv", proposed by Codevilla <ref type="bibr" target="#b9">[10]</ref> obtained poor results on the more complex CARLA100 benchmark. ResNet based deeper architectures, "res18" and "res34", were able to improve the results. However, when testing ResNet 50 we notice a significant drop in the quality of the results.  <ref type="table">Table 8</ref>. Analysis of the causes of episode termination for different methods. We show the results for all tasks and weather conditions. The columns for a single method/task/condition should add up to 1. For each cause of episode termination we highlight the method with higher probability (i.e. worse performance). For the success row, we highlight the method with the best performance. The reported results are the average over three different runs of the benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Due to biases in the data, the results may get either saturated or worse with increasing amounts of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4. The percentage of episodes that failed due to the inertia problem. We can see that by increasing the amount of data, this bias may further degrade the generalization capabilities of the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 . 2 Figure 9 .</head><label>829</label><figDesc>The start and goal positions for the CARLA100 Benchmark. The start positions are in red and the goal positions are in green. Same number correspond to matching start-goal positions. Activation maps showing the increased selectivity for traffic lights in the ResNet34 case (bottom) compared to the standard 8 convolution architecture (top</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CIRL<ref type="bibr" target="#b25">[26]</ref> CAL<ref type="bibr" target="#b35">[36]</ref> MT<ref type="bibr" target="#b24">[25]</ref> CILR CILRS CIL<ref type="bibr" target="#b9">[10]</ref> CIRL<ref type="bibr" target="#b25">[26]</ref> CAL<ref type="bibr" target="#b35">[36]</ref> MT<ref type="bibr" target="#b24">[25]</ref> CILR CILRS Comparison with the state of the art on the original CARLA benchmark. The "CILRS" version corresponds to our CIL-based ResNet using the speed prediction branch, whereas "CILR" is without this speed prediction. These two models and CIL are the only ones that do not use any extra supervision or online interaction with the environment during training. The table reports the percentage of successfully completed episodes in each condition, selecting the best seed out of five runs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Training conditions</cell><cell></cell><cell></cell><cell></cell><cell cols="3">New town &amp; weather</cell><cell></cell><cell></cell></row><row><cell cols="2">Task CIL[10] Straight 98</cell><cell>98</cell><cell>100</cell><cell>96</cell><cell>94</cell><cell>96</cell><cell>80</cell><cell>98</cell><cell>94</cell><cell>96</cell><cell>92</cell><cell>96</cell></row><row><cell>One Turn</cell><cell>89</cell><cell>97</cell><cell>97</cell><cell>87</cell><cell>92</cell><cell>92</cell><cell>48</cell><cell>80</cell><cell>72</cell><cell>82</cell><cell>92</cell><cell>92</cell></row><row><cell>Navigation</cell><cell>86</cell><cell>93</cell><cell>92</cell><cell>81</cell><cell>88</cell><cell>95</cell><cell>44</cell><cell>68</cell><cell>68</cell><cell>78</cell><cell>88</cell><cell>92</cell></row><row><cell>Nav. Dynamic</cell><cell>83</cell><cell>82</cell><cell>83</cell><cell>81</cell><cell>85</cell><cell>92</cell><cell>42</cell><cell>62</cell><cell>64</cell><cell>62</cell><cell>82</cell><cell>90</cell></row><row><cell></cell><cell></cell><cell cols="3">Training conditions</cell><cell></cell><cell></cell><cell></cell><cell cols="3">New Town &amp; Weather</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="3">CIL[10] CAL[36] MT[25]</cell><cell>CILR</cell><cell cols="2">CILRS</cell><cell cols="3">CIL[10] CAL[36] MT[25]</cell><cell>CILR</cell><cell></cell><cell>CILRS</cell></row><row><cell>Empty</cell><cell>79 ? 1</cell><cell>81 ? 1</cell><cell>84 ? 1</cell><cell cols="3">92 ? 1 97 ? 2</cell><cell>24 ? 1</cell><cell>25 ? 3</cell><cell>57 ? 0</cell><cell cols="3">66 ? 2 90 ? 2</cell></row><row><cell>Regular</cell><cell>60 ? 1</cell><cell>73 ? 2</cell><cell>54 ? 2</cell><cell cols="3">72 ? 5 83 ? 0</cell><cell>13 ? 2</cell><cell>14 ? 2</cell><cell>32 ? 2</cell><cell cols="3">54 ? 2 56 ? 2</cell></row><row><cell>Dense</cell><cell>21 ? 2</cell><cell>42 ? 3</cell><cell>13 ? 4</cell><cell cols="3">28 ? 1 42 ? 2</cell><cell>2 ? 0</cell><cell>10 ? 0</cell><cell>14 ? 2</cell><cell cols="3">13 ? 4 24 ? 8</cell></row></table><note>Table 2. Results on our NoCrash benchmark. Mean and standard deviation on three runs, as CARLA 0.8.4 has significant non-determinism.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the state of the art on the original CARLA benchmark for the conditions "New Town" and "New Weather". The table reports the percentage of successfully completed episodes in each condition.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">New Weather</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>New Town</cell></row><row><cell>Task</cell><cell cols="3">CIL[10] CAL[36] MT[25]</cell><cell>CILR</cell><cell>CILRS</cell><cell cols="3">CIL[10] CAL[36] MT[25]</cell><cell>CILR</cell><cell>CILRS</cell></row><row><cell>Empty</cell><cell>83 ? 2</cell><cell>85 ? 2</cell><cell>58 ? 2</cell><cell cols="2">98 ? 1 96 ? 1</cell><cell>48 ? 3</cell><cell>36 ? 6</cell><cell>41 ? 3</cell><cell>60 ? 2 66 ? 2</cell></row><row><cell>Normal</cell><cell>55 ? 5</cell><cell>68 ? 5</cell><cell>40 ? 6</cell><cell cols="2">69 ? 4 77 ? 1</cell><cell>27 ? 1</cell><cell>26 ? 2</cell><cell>22 ? 0</cell><cell>42 ? 2 49 ? 5</cell></row><row><cell>Cluttered</cell><cell>13 ? 4</cell><cell>33 ? 2</cell><cell>7 ? 2</cell><cell cols="2">27 ? 3 47 ? 5</cell><cell>10 ? 2</cell><cell>9 ? 1</cell><cell>7 ? 1</cell><cell>12 ? 2 23 ? 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the state-of-the-art on the NoCrash Benchmark. Here we compare on two extra conditions. New Weather refers to the same town as during training but with new weather conditions. New Town refers to a town not seen during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Exact configurations of the architecture used on the experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). For the ResNet34, layer 1, refers to the attention maps obtained after a full ResNet block.</figDesc><table><row><cell>Condition</cell><cell>Models</cell><cell>Traffic Light Violations</cell></row><row><cell></cell><cell>CIL</cell><cell>83</cell></row><row><cell>Training Conditions</cell><cell>8 Conv CILRS 10</cell><cell>71 47</cell></row><row><cell></cell><cell>CILRS 100</cell><cell>27</cell></row><row><cell></cell><cell>CIL</cell><cell>82</cell></row><row><cell>New town &amp; weather</cell><cell>8 Conv CILRS 10 hours</cell><cell>81 64</cell></row><row><cell></cell><cell>CILRS 100 hours</cell><cell>78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Percentage of times the agents crossed a traffic light on red (lower is better) in the "Empty" conditions of the NoCrash benchmark.model, having only 47% of traffic light violations, is clearly stopping for a significant amount of red traffic lights. This result is even more expressive considering the version using 100 hours of training data which did only 27% of traffic light violations. However, when we analyze generalization conditions, Tab. 7 bottom, we see there is an ample room for improvement. Regardless, such improvement in this longitudinal controls task is promising for modeling lateral and longitudinal controls jointly end-to-end.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. CARLA100 <ref type="bibr">A.4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. NoCrash Benchmark</head><p>The benchmark consist of three different tasks: "Empty", "Regular" and "Cluttered". The tasks are better explained on section 3 of the main text. Each task consists of 25 goal directed episodes. In each episode, the agent is guided with a global planner to reach a certain goal position. We consider an episode as a success if the agent reaches a certain goal under a time limit without colliding with any object, static or dynamic. The tuples of start/goal positions are based on the ones used in CARLA CoRL2017 benchmark for the tasks "Navigation" and "Nav. Dynamic". However, we removed some start-goal positions that were too close to each other. <ref type="figure">Figure 8</ref> shows the start-goad positions for both Towns.</p><p>The benchmark is executed under four different conditions:</p><p>? Training: The same one as collected on the training data. As mentioned above, we collected training data only in Town01 and using the weather conditions: "Clear Noon", "Clear Noon After Rain", "Heavy Rain Noon", "Clear Sunset'</p><p>? New weather: The city as in the training data but with two different new weathers, "After Rain Sunset" and "Soft Rain Sunset".</p><p>? New Town: Same weathers as in "Training" but the tests take place in Town02.</p><p>? New Town &amp; Weather: Same weathers as in "New Weather" but played in Town 02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>B.1. Architecture <ref type="table">Table 6</ref> details the standard architecture used in the experiments. For the perception module, we also experimented with ResNet 18, ResNet 50 and with the architecture proposed in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Image Input</head><p>Starting from a raw 800 ? 600 pixels image, we cropped 125 pixels from the top and 90 at the bottom of the image and resized the resulting image to 200 ? 88 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results</head><p>Comparison Further comparisons with the state-of-theart in the CARLA CoRL 2017 benchmark can be seen in <ref type="table">Table 4</ref>. We can see that the "New Town" condition is harder for some models than the "New Weather &amp; Town" conditions. Yet the proposed methods can still outperform previously proposed methods. One would expect "New Weather &amp; Town" to be the hardest task, but this discrepancy had been previously observed in the literature <ref type="bibr" target="#b11">[12]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An application of reinforcement learning to aerobatic helicopter flight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06</title>
		<meeting>the 19th International Conference on Neural Information Processing Systems, NIPS&apos;06<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hindsight experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5048" to="5058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fairness in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On offline evaluation of vision-based driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end driving via conditional imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The development of machine vision for road vehicles in the last decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Dickmanns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicle Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="268" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Causal confusion in imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Imitation Learning and its Challenges in Robotics Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models with surround-view cameras and route planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability? Transportation Research Part A: Policy and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Paddock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="182" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dart: Optimizing noise injection in imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A perception-driven autonomous urban vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="727" to="774" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense 3D semantic slam of traffic environment based on stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ozgner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking self-driving: Multi-task knowledge for better generalization and accident explanation ability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Motoyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sugano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11100</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cirl: Controllable imitative reinforcement learning for visionbased self-driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Training my car to see using virtual worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vzquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mozafari</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09364</idno>
		<title level="m">Driving policy transfer via modularity and abstraction</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tantia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scicluna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08591</idno>
		<title level="m">A modern take on the bias-variance tradeoff in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ALVINN: An autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imitation learning for locomotion and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="661" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to noregret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01230</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Conditional affordance learning for driving in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06498</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computational approaches to motor learning by imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Autonomous indoor helicopter flight using a single onboard camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Soundararaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sujeeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="5307" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end driving simulation via angle branched network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07545</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-tovirtual domain unification for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Query-efficient imitation learning for end-to-end simulated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
