<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
						</author>
						<title level="a" type="main">Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Anticipation</term>
					<term>Egocentric Vision</term>
					<term>Recurrent Neural Networks</term>
					<term>LSTM !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE ability to anticipate what is going to happen in the near future is fundamental for human beings in order to interact with the environment and make decisions. Anticipation abilities are also fundamental to deploy intelligent systems which need to interact with a complex environment or other humans to automate challenging tasks and provide assistance. Examples of such applications include autonomous vehicles <ref type="bibr" target="#b0">[1]</ref>, human-robotic symbiotic systems <ref type="bibr" target="#b1">[2]</ref>, and wearable assistants <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, designing computational approaches to address tasks such as early action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and action anticipation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> is challenging as it often requires to model the relationship between past and future events, in the presence of incomplete observations. First-Person (Egocentric) Vision <ref type="bibr" target="#b3">[4]</ref> offers an interesting scenario to study tasks related to anticipation. On one hand, wearable cameras provide a means to collect naturally long videos containing multiple subsequent interactions with objects, which makes anticipation tasks unconstrained. On the other hand, the ability to predict in advance what actions the camera wearer is going to perform and what objects they are going to interact with is useful to build intelligent wearable systems capable of anticipating the user's goals to provide assistance <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we tackle the problem of egocentric action anticipation. The task consists in recognizing a future action from an observation of the past. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the problem as defined in <ref type="bibr" target="#b9">[10]</ref>: given an action starting at time ? s , the system should predict the related action class by observing a video segment of temporal bounds the observed video, and ? a denotes the "anticipation time", i.e., how much in advance the action has to be anticipated. Since the future is naturally uncertain, action anticipation models usually predict more than one possible action and the evaluation is performed using Top-k measures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>. While the task of action anticipation has been investigated in the domain of third person vision <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, less attention has been devoted to the challenging scenario of egocentric videos <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Our work stems from the observation that egocentric action anticipation methods need to address two sub-tasks, which we refer to as "encoding" and "inference". In the encoding stage, the model has to summarize what has been observed in the past (e.g., "a container has been washed" in the observed segment in <ref type="figure" target="#fig_0">Fig. 1</ref>). In the infer-ence stage, the algorithm makes hypotheses about what may happen in the future, given the summary of the past and the current observation (e.g., "put-down container", "close tap", "take spoon" in <ref type="figure" target="#fig_0">Fig. 1</ref>). Previous approaches have generally addressed the two sub-tasks jointly <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Our method is designed to disentangle them by using two separate LSTMs. "A Rolling" LSTM (R-LSTM) continuously encodes streaming observations and keeps an updated summary of what has been observed so far. When an anticipation is required, the "Unrolling" LSTM (U-LSTM) is initialized with the current hidden and cell states of the R-LSTM (which encode the summary of the past) and makes predictions about the future. While previous approaches considered fixed anticipation times <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, our architecture is designed to anticipate an action at multiple anticipation times. For instance, our model can anticipate actions from 2s, to 0.25s before they occur, with the prediction refined as we get closer to the beginning of the action. To encourage the disentanglement of encoding and inference, we propose to pre-train our model with a novel "Sequence Completion Pre-training" (SCP) technique. Our method processes video in a multi-modal fashion, analyzing spatial observations (RGB frames), motion (optical flow) and object-based features obtained through an object detector. We find that classic multimodal fusion techniques such as late and early fusion are limited in the context of action anticipation. Therefore, we propose a novel "Modality ATTention" (MATT) mechanism to adaptively estimate optimal fusion weights for each modality by considering the outputs of the modality-specific R-LSTM components. We perform experiments on two large-scale datasets of egocentric videos, EPIC-KTICHENS <ref type="bibr" target="#b9">[10]</ref> and EGTEA Gaze+ <ref type="bibr" target="#b15">[16]</ref>, and a standard benchmark of third person videos, Activitynet <ref type="bibr" target="#b16">[17]</ref>. the experiments show that the proposed method outperforms several state-of-the-art approaches and baselines in the task of egocentric action anticipation and generalizes to the scenario of third person vision, as well as to the tasks of early action recognition and action recognition. The proposed approach also achieved top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge <ref type="bibr" target="#b0">1</ref> The contributions of our work are the following: 1) we systematically investigate the problem of egocentric action anticipation within the framework provided by the EPIC-Kitchens dataset and its related challenges; 2) we benchmark popular ideas and approaches to action anticipation and define the proposed "Rolling-Unrolling LSTM" (RU) architecture, which is able to anticipate egocentric actions at multiple temporal scales; 3) we introduce two novel techniques specific to the investigated problem, i.e., i) "Sequence Completion Pre-training" and ii) adaptive fusion of multimodal predictions with Modality ATTention (MATT); 4) we performed extensive evaluations to highlight the limits of previous methods and report improvements of the proposed approach over the state-of-the-art. To support future research in this field, we publicly release the code of our approach.</p><p>The reminder of this paper is organized as follows. <ref type="bibr" target="#b0">1</ref>. See https://epic-kitchens.github.io/Reports/EPIC-Kitchens-Challenges-2019-Report.pdf for more details. Section 2 revises the related works. Section 3 details the proposed approach. Section 4 reports the experimental settings, whereas Section 5 discusses the results. Finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to past research on action recognition, early action recognition, and anticipation tasks in both third and first-person vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Action Recognition</head><p>Classic approaches to action recognition from video have generally relied on the extraction and processing of handdesigned features. Among the most notable approaches, Laptev <ref type="bibr" target="#b17">[18]</ref> proposed space-time interest points to classify events. Laptev et al. <ref type="bibr" target="#b18">[19]</ref> further investigated the use of space-time features, space-time pyramids and SVMs for human action classification. Later, Wang et al. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> introduced dense trajectories to encode local motion and object appearance. More recent approaches investigated the use of deep learning to learn representations suitable to recognize actions directly from video. Karpathy et al. <ref type="bibr" target="#b21">[22]</ref> considered the use of Convolutional Neural Networks (CNNs) and investigated different strategies to fuse per-frame predictions. Simonyan et al. <ref type="bibr" target="#b22">[23]</ref> proposed Two-Stream CNN (2SCNN), a multi-branch architecture which recognizes actions by processing both appearance (RGB) and motion (optical flow) data. Feichtenhofer et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> studied approaches to fuse predictions performed by the motion and appearance streams of a 2SCNN to improve recognition. Wang et al. <ref type="bibr" target="#b26">[27]</ref> designed Temporal Segment Network (TSN), a general framework to train two-stream CNNs for action recognition. Zhou et al. <ref type="bibr" target="#b27">[28]</ref> introduced Temporal Relation Network (TRN), a module capable of encoding temporal dependencies between video frames at multiple time scales. Lin et al. <ref type="bibr" target="#b28">[29]</ref> proposed the Temporal Shift Module (TSM), a component which facilitates information exchange among neighboring frames without the introduction of extra parameters in the network. Other authors investigated the use of 3D CNNs as a natural extension of 2D convolutional networks for video processing. Tran et al. <ref type="bibr" target="#b29">[30]</ref> demonstrated the use of 3D CNNs to learn spatio-temporal features for video classification. Carreira and Zisserman <ref type="bibr" target="#b30">[31]</ref> proposed Inflated 3D (I3D) CNNs and showed how the weights of this architecture can be bootstrapped from a 2D CNN pre-trained on Imagenet. Hara et al. <ref type="bibr" target="#b31">[32]</ref> studied whether 3D CNNs based on standard 2D ResNet <ref type="bibr" target="#b32">[33]</ref> architectures could be exploited for action recognition from video. Tran et al. <ref type="bibr" target="#b33">[34]</ref> proposed R(2+1)D CNNs which factorize 3D convolutions as sequences of spatial and temporal convolutions.</p><p>Egocentric action recognition has also been studied in past works. Spriggs et al. <ref type="bibr" target="#b34">[35]</ref> investigated the problem of supervised and unsupervised action segmentation using Inertial Measurement Units (IMU) and egocentric video. Fathi et al. <ref type="bibr" target="#b35">[36]</ref> proposed to recognize actions by modeling activities, hands and objects. Fathi et al. <ref type="bibr" target="#b36">[37]</ref> employed eye gaze measurements to recognize egocentric actions. Pirsiavash and Ramanan <ref type="bibr" target="#b37">[38]</ref> proposed to recognize egocentric activities using object-centric representations. Li et al. <ref type="bibr" target="#b38">[39]</ref> studied how different egocentric cues, (including gaze, the presence of hands and objects, as well as head motion), can be used to perform the task. Ryoo et al. <ref type="bibr" target="#b39">[40]</ref> proposed an approach to temporally pool features for egocentric action recognition. Ma et al. <ref type="bibr" target="#b40">[41]</ref> designed a deep learning architecture which allows to integrate different egocentric-based features to recognize actions. Singh et al. <ref type="bibr" target="#b41">[42]</ref> adapted improved dense trajectories to the problem of egocentric action recognition. Singh et al. <ref type="bibr" target="#b42">[43]</ref> proposed a multi-stream CNN to recognize egocentric actions using spatial features, temporal features and egocentric cues. Li et al. <ref type="bibr" target="#b15">[16]</ref> introduced a deep learning model for joint gaze estimation and action recognition in egocentric video. Sudhakaran et al. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> proposed to use a convolutional LSTM to recognize actions from egocentric video with an attention mechanism which learns to focus on image regions containing objects.</p><p>Our work builds on previous ideas investigated in the context of action recognition such as the use of multiple modalities for video analysis <ref type="bibr" target="#b22">[23]</ref>, the use of Temporal Segment Networks <ref type="bibr" target="#b26">[27]</ref> as a principled way to train CNNs for action recognition, as well as the explicit encoding of objectbased features <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref> to analyze egocentric video. However, in contrast with the aforementioned works, we address the problem of egocentric action anticipation and show that approaches designed for action recognition, such as TSN <ref type="bibr" target="#b26">[27]</ref> and late fusion to merge spatial and temporal predictions <ref type="bibr" target="#b22">[23]</ref> are not directly applicable to the problem of egocentric action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Early Action Recognition in Third Person Vision</head><p>Early action recognition refers to the task of recognizing an ongoing action as early as possible from partial observations <ref type="bibr" target="#b5">[6]</ref>. The problem of early action recognition has been widely investigated in the domain of third person vision. Ryoo <ref type="bibr" target="#b45">[46]</ref> introduced the problem of recognizing ongoing actions from streaming video and addressed it proposing an integral histogram of spatio-temporal features. Cao et al. <ref type="bibr" target="#b46">[47]</ref> used sparse coding to recognize actions from partially observed videos. Haoi and De la Torre <ref type="bibr" target="#b47">[48]</ref> proposed to use Structured Output SVM to detect partially observed events. Huang et al. <ref type="bibr" target="#b48">[49]</ref> introduced Sequential Max-Margin Event Detectors, a method which performs early action detection by sequentially discarding classes until one class is identified as the detected one. De Geest et al. <ref type="bibr" target="#b5">[6]</ref> released a new dataset for online action detection and benchmarked several baseline methods to address the task. Ma et al. <ref type="bibr" target="#b6">[7]</ref> used LSTMs to address the problem of early action detection from video. Aliakbarian et al. <ref type="bibr" target="#b4">[5]</ref> proposed a two stage LSTM architecture which models context and action to perform early action recognition. Beccattini et al. <ref type="bibr" target="#b49">[50]</ref> designed ProgressNet, an approach capable of estimating the progress of actions and localizing them in space and time. De Geest and Tuytelaars <ref type="bibr" target="#b50">[51]</ref> addressed early action recognition proposing a "feedback network" which uses two LSTM streams to interpret feature representations and model the temporal structure of subsequent observations. Differently from these works, we address the task of anticipating actions from egocentric video, i.e., predicting an action before it starts, and hence before it can be even partially observed. However, given the similarity between early action recognition and action anticipation, we consider and evaluate some ideas investigated in the context of early action recognition, such as the use of LSTMs to process streaming observations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b50">[51]</ref> and the use of dedicated loss functions <ref type="bibr" target="#b6">[7]</ref>. Moreover, we show that the proposed architecture also generalizes to the problem of early egocentric action recognition, achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Action Anticipation in Third Person Vision</head><p>Action anticipation refers to the task of predicting an action before it actually begins <ref type="bibr" target="#b7">[8]</ref>. Previous works investigated different forms of action and activity anticipation from third person video. Kitani et al. <ref type="bibr" target="#b51">[52]</ref> considered the task of inferring future paths followed by people observed from a static camera. Huang and Kitani <ref type="bibr" target="#b12">[13]</ref> explored the task of action anticipation in the context of dual-agent interactions, where the actions of an agent are used to predict the response of the other agent. Lan et al. <ref type="bibr" target="#b52">[53]</ref> proposed a hierarchical representation to anticipate future human actions from a still image or a short video clip. Jain et al. <ref type="bibr" target="#b53">[54]</ref> designed an Autoregressive Input-Output HMM to anticipate driving maneuvers a few seconds before they occur using video, vehicle dynamics, GPS, and street maps. Jain et al. <ref type="bibr" target="#b13">[14]</ref> proposed a learning architecture based on LSTMs for driver activity anticipation. Koppula and Saxena <ref type="bibr" target="#b1">[2]</ref> used object affordances to anticipate the possible future actions performed by a user from a robotic point of view. Vondrick et al. <ref type="bibr" target="#b8">[9]</ref> addressed action anticipation by training a CNN to regress the representations of future frames from past ones in an unsupervised way. Gao et al. <ref type="bibr" target="#b7">[8]</ref> proposed an Encoder-Decoder LSTM architecture which predicts future actions by encoding the representations of past frames and regressing the representations of future frames. Similarly to <ref type="bibr" target="#b8">[9]</ref>, the model can be pre-trained from unlabeled videos in an unsupervised way. Felsen et al. <ref type="bibr" target="#b54">[55]</ref> developed a framework to forecast future events in team sports video from visual input. Mahmud et al. <ref type="bibr" target="#b55">[56]</ref> designed a system able to infer the labels and starting frames of future actions. Zeng et al. <ref type="bibr" target="#b56">[57]</ref> introduced a general framework which uses inverse reinforcement learning to perform visual forecasting at different levels of abstraction, including story-line forecasting, action anticipation and future frames generation. Abu et al. <ref type="bibr" target="#b11">[12]</ref> explored the use of CNNs and RNNs to predict the occurrence of future actions based on past observations. In this work, we consider the problem of action anticipation from egocentric visual data. Nevertheless, our work builds on some of the ideas explored in past works such as the use of LSTMs <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> to anticipate actions, the use of the encoder-decoder framework to encode past observations and produce hypotheses of future actions <ref type="bibr" target="#b7">[8]</ref>, and the use of object specific features <ref type="bibr" target="#b55">[56]</ref> to determine which objects are present in the scene, we show that other approaches, such as the direct regression of future representations <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, do not achieve satisfactory performance in the egocentric scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Anticipation in First-Person Vision</head><p>Past works have investigated different problems related to anticipation from first-person vision. Zhou and Berg <ref type="bibr" target="#b57">[58]</ref> studied methods to infer the ordering of egocentric video Input video of length  <ref type="figure">Fig. 2</ref>. Video processing scheme adopted by the proposed method. In the example above, we set Senc = 6 and Sant = 8.</p><p>segments. Ryoo et al. <ref type="bibr" target="#b14">[15]</ref> proposed to analyze onset actions to anticipate potentially dangerous actions performed by humans against a robot. Soran et al. <ref type="bibr" target="#b2">[3]</ref> developed a system capable of inferring the next action performed in a known workflow to notify the user if a missing action is detected. Park et al. <ref type="bibr" target="#b58">[59]</ref> proposed a method to predict the future trajectories of the camera wearer from egocentric video. Zhang et al. <ref type="bibr" target="#b59">[60]</ref> developed a method to predict eye gaze fixations in future video frames. Furnari et al. <ref type="bibr" target="#b60">[61]</ref> proposed to anticipate human-object interactions by analyzing the motion of objects in egocentric video. Chenyou et al. <ref type="bibr" target="#b61">[62]</ref> designed a method capable of forecasting the position of hands and objects in future frames. Rhinehart and Kitani <ref type="bibr" target="#b62">[63]</ref> used inverse reinforcement learning to anticipate future locations, objects and activities from egocentric video. Previous works on anticipation from egocentric video have investigated different tasks and evaluated methods on different datasets and under different evaluation frameworks. Differently from these works, we consider the egocentric action anticipation challenge recently proposed by Damen et al. <ref type="bibr" target="#b9">[10]</ref>. It should be noted that few works <ref type="bibr" target="#b10">[11]</ref> have tackled the problem so far. While directly comparing our approach with respect to most of the aforementioned approach is unfeasible due to the lack of a common framework, our method incorporates some ideas from past works, such as the analysis of past actions <ref type="bibr" target="#b14">[15]</ref> and the detection of the objects present in the scene to infer future actions <ref type="bibr" target="#b60">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this Section, we discuss the proposed approach. Specifically, Section 3.1 introduces the strategy used to process video, Section 3.2 presents the proposed Rolling-Unrolling LSTMs module, Section 3.3 discusses the Sequence Completion Pre-Training (SCP) approach used to encourage the rolling and unrolling LSTMs to focus on different sub-taks, Section 3.4 introduces the modality attention mechanism used to fuse multi-modal predictions, Section 3.5 details the definition of the representation functions used in the different branches of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Processing Scheme</head><p>Past approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> performed action anticipation considering a fixed anticipation time ? a , usually set to 1 second. This has been usually achieved by training a classifier to predict the action happening ? a seconds after the end of an observed video segment. Similarly to <ref type="bibr" target="#b7">[8]</ref>, we propose to anticipate actions at different temporal scales by using recurrent models. The authors of <ref type="bibr" target="#b7">[8]</ref> obtain this multi-scale anticipation by training the model with variable anticipation times and performing inference using a fixed anticipation time chosen from the ones used during training. Also, the approach proposed in <ref type="bibr" target="#b7">[8]</ref> requires the model to consume all the observed video before anticipating actions, which results in the separation between the observation and anticipation stages. We argue that it would be natural to allow the model to make predictions while observing the video and possibly refine them as more frames are processed. We hence propose the video processing strategy illustrated in <ref type="figure">Fig. 2</ref>. According to the proposed scheme, the video is processed sequentially, with a video snippet V t consumed every ? seconds, where t indexes the current time-step. At each time-step t, the model processes an input video snippet V t and optionally outputs a set of scores s t for the anticipated actions. Since the video is processed sequentially, the prediction made at time-step t depends only on observations processed at previous time-steps. Specifically, the video is processed in two stages: an "encoding" stage, carried out for S enc time-steps and an "anticipation" stage, carried out for S ant time-steps. During the "encoding" stage, the model only observes incoming video snippets V i and does not anticipate actions. During the "anticipation" stage, the model both observes the input video snippets V i and outputs action scores s i for the anticipated actions. This scheme effectively allows to anticipate actions at different anticipation times. In particular, in our experiments we set ? = 0.25s, S enc = 6 and S ant = 8. In these settings, the model will process videos of length l = ?(S enc + S ant ) = 3.5s and output 8 predictions at the following anticipation times: ? a ? {2s, 1.75s, 1.5s, 1.25s, 1s, 0.75s, 0.5s, 0.25s}. At time step t, the effective observation time will be given by ? ? t. Therefore, the 8 predictions will be performed at the following observation times: ? o ? {1.75s, 2s, 2.25s, 2.5s, 2.75s, 3s, 3.25s, 3.5s}. It should be noted that our formulation generalizes the one proposed in <ref type="bibr" target="#b9">[10]</ref>. For instance, at time-step t = 11, our model will anticipate actions with an effective observation time equal to ? o = ? ? t = 2.75s and an anticipation time equal to ? a = ?(S ant + S enc + 1 ? t) = 1s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Rolling-Unrolling LSTMs</head><p>Our model is inspired by encoder-decoder sequence to sequence models for text processing <ref type="bibr" target="#b63">[64]</ref>. Such models include an encoder which processes the words of the input sentence and a decoder which generates the words of the output sentence. Both the encoder and decoder are often implemented using LSTMs. Rather than analyzing words, our model processes high level representations of frames obtained through a representation function ?. The decoder is initialized with the internal representation of the encoder and iterates over the last representation to anticipate future actions. To allow for continuous anticipation of actions, the decoder is attached to each encoding time-step. This allows to anticipate actions and refine predictions in a continuous fashion. We term the encoder "Rolling LSTM" (R-LSTM) and the decoder "Unrolling LSTM" (U-LSTM). Rolling-Unrolling (RU) LSTMs, which is described in details below. Following previous literature <ref type="bibr" target="#b22">[23]</ref>, we include multiple branches which analyze the video pre-processed according to different modalities. Specifically, at each time-step t, the input video snippet V t is represented using different modality-specific representation functions ? 1 , . . . , ? M , where M is the number of considered modalities. The representation functions can be learned and depend on the parameters ? ?1 , . . . , ? ? M . This process allows to obtain modality-specific representations for the input video snippets</p><formula xml:id="formula_0">f 1,t = ? 1 (V t ), . . . , f M,t = ? M (V t ),</formula><p>where f m,t is the feature vector computed at time-step t for modality m. The feature vector f m,t is hence fed to the m th branch of the architecture. In this work, we consider M = 3 modalities, i.e., RGB frames (spatial branch), optical flow (motion branch) and object-based features (object branch). <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates in details the processing happening in a single branch m. For illustration purposes, the figure shows an example for S enc = 1 and S ant = 3. At a given time step t, the feature vector f m,t is fed to the R-LSTM, which is responsible for recursively encoding the semantic content of the incoming representations. This is performed according to the following equation:</p><formula xml:id="formula_1">(h R m,t , c R m,t ) = LST M ? R m (f m,t , h R m,t?1 , c R m,t?1 ).<label>(1)</label></formula><p>In the equation above, LST M ? R m denotes the R-LSTM related to branch m, which depends on the learnable parameters ? R m , whereas h R m,t and c R m,t are the hidden and cell states computed at time step t in the branch related to modality m. The initial hidden and cell states of the R-LSTM are initialized with zeros:</p><formula xml:id="formula_2">h R m,0 = 0, c R m,0 = 0.<label>(2)</label></formula><p>Note that the LST M function follows the standard implementation of LSTMs <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. During the anticipation stage, at each time step t, the U-LSTM is used to predict future actions. The U-LSTM is  initialized with the hidden and cell states of the R-LSTM at the current time-step:</p><formula xml:id="formula_3">h U m,0 = h R m,t , c U m,0 = c R m,t<label>(3)</label></formula><p>and iterates over the representation of the current video snippet f m,t for a number of times n t equal to the number of time-steps needed to reach the beginning of the action:</p><formula xml:id="formula_4">n t = S ant + S enc + 1 ? t.</formula><p>Note that this number is proportional to the current anticipation time, which can be computed as ? ? n t . Similarly to the R-LSTM, the hidden and cell states of the U-LSTM are computed as follows at the generic iteration j:</p><formula xml:id="formula_5">(h U m,j , c U m,j ) = LST M ? U m (f m,t , h U m,j?1 , c U m,j?1 ).<label>(4)</label></formula><p>In Equation <ref type="formula" target="#formula_5">(4)</ref>, LST M ? U m represents the U-LSTM network related to branch m, which depends on the learnable parameters ? U m . The vectors h U m,t and c U m,t are the hidden and cell states computed at iteration j for modality m. It is worth noting that the input f m,t of the U-LSTM does not depend on j (see Eq. (4)), because it is fixed during the "unrolling" procedure. The main rationale of "unrolling" the U-LSTM for a different number of times at each timestep is to encourage the architecture to produce different predictions at different anticipation times.</p><p>Modality-specific anticipation scores s m,t for the anticipated actions are finally computed at time-step t by feeding the last hidden vector h U m,nt of the U-LSTM to a linear transformation with learnable parameters ? W m and ? b m :</p><formula xml:id="formula_6">s m,t = ? W m h U m,nt + ? b m .<label>(5)</label></formula><p>Anticipated action probabilities for modality m at time-step t are computed normalizing the scores s m,t with the Softmax function:</p><formula xml:id="formula_7">p m,t,i = exp(s m,t,i ) k exp(s m,t,k )<label>(6)</label></formula><p>where s m,t,i denotes the i th component of the score vector s m,t . A modality-specific RU branch is hence trained with the cross-entropy loss:</p><formula xml:id="formula_8">L(p m , y) = ? 1 S ant t log p m,t,y<label>(7)</label></formula><p>where p m is the set of probability distributions over actions computed by branch m in all time-steps, y is the ground truth class of the current sample and L is minimized with respect to the parameters ? R m , ? U m , ? W m and ? b m .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence Completion Pre-Training (SCP)</head><p>The two LSTMs included in the proposed Rolling-Unrolling architecture are introduced to address two specific subtasks: the R-LSTM should encode past observations and summarize what has happened up to a given time-step, whereas the U-LSTM should focus on anticipating future actions conditioned on the hidden and cell vectors of the R-LSTM. However, in practice, this might not happen. For instance, the R-LSTM could try to both summarize the past and anticipate future actions, which would make the task of the R-LSTM harder. To encourage the two LSTMs to focus on the two different sub-tasks, we introduce a novel Sequence Completion Pre-training (SCP) procedure. During SCP, the connections of the network are modified to allow the U-LSTM to process future representations rather than iterating on the most recent one. In practice, the U-LSTM hidden and cell states are computed as follows during SCP:</p><formula xml:id="formula_9">(h U m,j , c U m,j ) = LST M ? U m (f m,t+j?1 , h U m,j?1 , c U m,j?1 ) (8)</formula><p>where the input representations f m,t+j?1 are sampled from future time-steps t + j ? 1. <ref type="figure" target="#fig_6">Fig. 5</ref> illustrates an example of the connection scheme used during SCP for time-step t = 2. Note that this is different from Eq. (4), in which only the most recent representation is processed. After SCP, the network is fine-tuned to the action anticipation task following Eq. (4). The main goal of pre-training the model with SCP is to allow the R-LSTM to focus on summarizing past representations without trying to anticipate the future. Indeed, since the U-LSTM can "cheat" by looking into the future, the R-LSTM does not need to try to anticipate future actions to minimize the loss and is hence encouraged to focus on encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modality ATTention (MATT)</head><p>Equation <ref type="formula" target="#formula_6">(5)</ref> allows to obtain modality-specific action scores s m,t from the hidden representations of the U-LSTMs contained in each branch. One way to fuse these scores is to compute a linear combination with a set of fixed weights w 1 , . . . , w M , which is generally referred to as late fusion:</p><formula xml:id="formula_10">s t = w 1 ? s 1,t + . . . + w M ? s M,t .<label>(9)</label></formula><p>The fusion weights w m are fixed and generally found using cross validation. We observe that, in the case of action anticipation, the relative importance of each modality might depend on the observed video. For instance, in some cases <ref type="figure">6</ref>. Example of the complete architecture with two branches and the Modality ATTention mechanism (MATT).</p><formula xml:id="formula_11">R-LSTM U-LSTM U-LSTM U-LSTM 1,1 ( = 0.75 ) 1 R-LSTM U-LSTM U-LSTM U-LSTM 2,1 ( = 0.75 ) ? ? + 1 ( = 0.75 ) 1,1 2,1 S Modality Attention Network (MATT) 1, 2, S SoftMax S = 1 L L Message passing Linear transformation L Input video snippets R-LSTM U-LSTM U-LSTM 1,2 ( = 0.5 ) 2 R-LSTM U-LSTM U-LSTM 2,2 ( = 0.5 ) ? ? + 2 ( = 0.5 ) 1,2 2,2 S = 2 L L R-LSTM U-LSTM 1,3 ( = 0.25 ) 3 R-LSTM U-LSTM 2,3 ( = 0.25 ) ? ? + 3 ( = 0.25 ) 1,3 2,3 S = 3 L L Fig.</formula><p>the object detector computing object-based features might fail and hence become unreliable, or in other cases there could be little motion in the scenes, which would make the optical flow modality less useful. Inspired by previous work on attention <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> and multi-modal fusion <ref type="bibr" target="#b68">[69]</ref>, we introduce a Modality ATTention (MATT) module which computes a set of attention scores indicating the relative importance of each modality for the final prediction. At a given time-step t, the attention scores are computed by feeding the concatenation of the hidden and cell vectors of the modality specific R-LSTM networks to a feed-forward neural network D which depends on the learnable parameters ? M AT T . This computation is defined as follows:</p><formula xml:id="formula_12">? t = D ? M AT T (? M m=1 (h R m,t ? c R m,t ))<label>(10)</label></formula><p>where ? denotes the concatenation operator and</p><formula xml:id="formula_13">? M m=1 (h R m,t ? c U m,t )</formula><p>is the concatenation of the hidden and cell vectors produced by the R-LSTM at time-step t across all modalities. Late fusion weights can be obtained by normalizing the score vector ? t using the softmax function, which makes sure that the computed fusion weights sum to one:</p><formula xml:id="formula_14">w m,t = e ?t,m k e ? t,k<label>(11)</label></formula><p>where ? t,m is the m th component of the score vector ? t . The final anticipated action scores are obtained at time-step t by fusing the modality-specific predictions produced by the different branches with a linear combination as follows: <ref type="figure">Fig. 6</ref> illustrates an example of a complete RU with two modalities and the MATT fusion mechanism. For illustration purposes, the figure shows only three anticipation steps. Note that, since the result of the linear combination defined in Eq. <ref type="formula" target="#formula_1">(12)</ref> is differentiable with respect to both the weights and the scores, the whole architecture is trainable end-to-end using the loss reported in Equation <ref type="formula" target="#formula_8">(7)</ref>.</p><formula xml:id="formula_15">s t = m w m,t ? s m,t .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Branches and Representation Functions</head><p>We instantiate the proposed architecture with 3 branches: a spatial branch which processes RGB frames, a motion branch which processes optical flow fields, and an object branch which processes object-based features. The input to the representation functions are video snippets of 6 frames V t = {I t,1 , I t,2 , . . . , I t,6 }, where I t,i is the i th frame of the video snippet V t . The representation function ? 1 of the spatial branch is implemented as a Batch Normalized Inception CNN <ref type="bibr" target="#b69">[70]</ref> CN N RGB which is trained for action anticipation on the target dataset within the TSN framework <ref type="bibr" target="#b26">[27]</ref>. The CNN takes as input the last frame of each video snippet and extracts features from the last layer of 1024 units preceding the final classifier. Hence:</p><formula xml:id="formula_16">? 1 (V t ) = CN N RGB (I t,6 )<label>(13)</label></formula><p>Similarly, the representation function ? 2 of the motion branch is implemented as a Batch Normalized Inception CNN <ref type="bibr" target="#b69">[70]</ref> CN N F low pre-trained for action recognition on the target dataset following TSN <ref type="bibr" target="#b26">[27]</ref>. The network analyzes a stack of optical flow fields computed from the 6 frames (5 frame pairs) of the current video snippet as proposed in <ref type="bibr" target="#b26">[27]</ref>. Similarly to CN N RGB , CN N F low extracts 1024dimensional features from the last layer preceding the final classifier. This representation function is hence defined as:</p><formula xml:id="formula_17">? 2 (V t ) = CN N F low (f low(I t,1 , . . . , I t,6 ))<label>(14)</label></formula><p>where f low computes the optical flow fields of the input frames and returns a tensor of 10 channels obtained by stacking x and y optical flow images computed from frame pairs. It is worth noting that, since the CNNs have been trained for the action recognition task, ? 1 and ? 2 allow to obtain "action-centric" representations of the input frames, which can be used by the R-LSTM to summarize what has happened in the past. The representation function ? 3 related to the object branch includes an object detector OD which detects objects in the last frame I t,6 of the input video snippet V t . When object-level annotations are available, the detector OD is trained on the target dataset. A fixed-length "bag of objects" representation is hence obtained by accumulating the confidence scores of all bounding boxes predicted for each object category. Let <ref type="bibr" target="#b5">6</ref> ) be the set of bounding boxes b t,i predicted by the object detector along with the corresponding classes b c t,i and confidence scores b s t,i . The j th component of the fixed-length representation vector is obtained by summing the confidence scores of all objects detected for class j, i.e.,</p><formula xml:id="formula_18">{(b t,i , b c t,i , b s t,i )} = OD(I t,</formula><formula xml:id="formula_19">boo(OD(I t,6 )) j = i [b c t,i = j]b s t,i<label>(15)</label></formula><p>where boo is the "bag of objects" function and [?] denotes the Iverson bracket. The representation function can hence be defined as follows:</p><formula xml:id="formula_20">? 3 (V t ) = boo(OD(I t,6 ))<label>(16)</label></formula><p>This representation encodes only the presence of an object in the scene, discarding its position in the frame, similarly to the representation proposed in <ref type="bibr" target="#b37">[38]</ref> for egocentric activity recognition. We found this representation to be sufficient in the case of egocentric action anticipation. Differently from ? 1 and ? 2 , ? 3 produces object-centric features which indicate what objects are likely to be present in the scene. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Early Action Recognition and Action Recognition</head><p>We note that the proposed model can also be used for early action anticipation and action recognition. Specifically, this can be done by sampling a given number of frames N from the video segment containing the action to be recognized and feeding the frames to the RU-LSTM model. To perform early action recognition, i.e., classifying the video before the action is completed, we set S enc = 0 and S ant = N . The output of the model at the last time-step t = N can be used to perform action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS</head><p>This section discusses the experimental settings, including the datasets considered for the evaluation, the evaluation measures and the compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We performed experiments on two large-scale datasets of egocentric videos and a large-scale dataset of third person videos: EPIC-Kitchens <ref type="bibr" target="#b9">[10]</ref>, EGTEA Gaze+ <ref type="bibr" target="#b15">[16]</ref> and Activ-ityNet <ref type="bibr" target="#b16">[17]</ref>. While the main focus of this work is on egocentric vision, we report experiments on this challenging dataset of third person videos to investigate the differences between the two scenarios and to what extent out approach generalizes to this domain. We have extracted frames from the EPIC-Kitchens and EGTEA Gaze+ datasets using a constant frame-rate of 30f ps, whereas ActivityNet videos have been sub-sampled to 12f ps. All frames have been resized to 456 ? 256 pixels. Optical flow fields have been computed on all datasets using the TVL1 algorithm <ref type="bibr" target="#b70">[71]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Measures</head><p>We evaluate all methods using Top-k evaluation measures, i.e., we consider a prediction to be correct if the ground truth action label is included in the top-k predictions. As observed in previous works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, this evaluation scheme is appropriate given the uncertainty of future predictions (i.e., many plausible actions can be performed after an observation). Specifically, we use the Top-5 accuracy as a class-agnostic measure and the Mean Top-5 Recall as a class aware metric. To assess the timeliness of anticipations, we propose a novel evaluation measure inspired by the AMOC curve <ref type="bibr" target="#b47">[48]</ref>. Let s t be the action scores predicted at time-step t for an action of ground truth class c. Let ? t be the anticipation time at time-step t, and tk(s t ) be the set of top-k actions as ranked by the action scores s t . We define as "time to action" at rank k the largest anticipation time (i.e., the time of earliest anticipation) in which a correct prediction has been made according to the Top-k criterion:</p><formula xml:id="formula_21">T tA(k) = max{? t |c ? tk(s t ), ?t}<label>(17)</label></formula><p>If an action is not correctly anticipated in any of the timesteps, we set T tA(k) = 0. The mean time to action over the whole test set mT tA(k) indicates how early, in average, a method can anticipate actions. The time to action measure can be extended also to the case of early action recognition. If the current video is composed by N frames, we define the observation ratio at timestep t as OR(t) = t N . This number can also be interpreted as a percentage, which defines how much of the action has been observed so far. We hence define as the "Minimum Observation Ratio" (MOR) the smallest observation ratio in which a correct prediction has been made according to the Top-1 criterion:</p><formula xml:id="formula_22">M OR = min{OR(t)|c = argmax j {s t,j }, ?t}.<label>(18)</label></formula><p>We evaluated performances for verb, noun and action predictions on the EPIC-Kitchens and EGTEA Gaze+ datasets. We obtained verb and noun scores by marginalization over the action scores for all methods except the one proposed in <ref type="bibr" target="#b9">[10]</ref>, which predicts verb and noun scores directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>We compare the proposed method with respect to several state-of-the approaches and baselines. Specifically, we consider the Deep Multimodal Regressor (DMR) proposed in <ref type="bibr" target="#b8">[9]</ref>, the Anticipation Temporal Segment Network (ATSN) of <ref type="bibr" target="#b9">[10]</ref>, the anticipation Temporal Segment Network trained with verb-noun Marginal Cross Entropy Loss (MCE) described in <ref type="bibr" target="#b10">[11]</ref>, and the Encoder-Decoder LSTM (ED) introduced in <ref type="bibr" target="#b7">[8]</ref>. We also consider two standard sequenceto-sequence models: a single LSTM architecture <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> (LSTM), and Temporal Convolutional Networks <ref type="bibr" target="#b71">[72]</ref> (TCN). We further compare our approach with respect to the following methods originally proposed for the task of early action recognition: a single LSTM architecture (we use the same parameters as our R-LSTM) trained using the Ranking Loss on Detection Score proposed in <ref type="bibr" target="#b6">[7]</ref> (RL), an LSTM trained using the Exponential Anticipation Loss proposed in <ref type="bibr" target="#b13">[14]</ref> (EL), and the Feedback Network LSTM (FN) proposed in <ref type="bibr" target="#b50">[51]</ref>. Note that, being essentially sequence-to-sequence models, these approaches can be easily adapted to the considered action anticipation scenario. All these baselines adopt the video processing scheme illustrated in <ref type="figure">Fig. 2</ref>. Among them, LSTM, RL, FN and EL are implemented as two stream networks with a spatial and a temporal branch whose predictions are fused by late fusion. In our experiments, TCN obtained very low performance when processing optical flows on the EPIC-Kitchens and EGTEA Gaze+ datasets. In these cases, fusing the RGB and Flow branches actually resulted in lower performances than the RGB branch alone. On the contrary, on the ActivityNet dataset, fusing the RGB and Flow branches led to better performance. Hence, we implemented TCN as a single RGB branch on EPIC-Kitchens and EGTEA Gaze+ and as a two-branch network with late fusion on ActivityNet. <ref type="bibr" target="#b2">3</ref> Additionally, we compare our approach with Two-Stream CNNs (2SCNN) <ref type="bibr" target="#b22">[23]</ref> and the method proposed by Miech et al <ref type="bibr" target="#b72">[73]</ref> on the official test sets of EPIC-Kitchens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>This section compares the performance of the proposed Rolling-Unrolling LSTMs with other state-of-the-art approaches. Specifically, Sections 5.1-5.3 discuss the action anticipation results on the three considered datasets, Section 5.4 reports the ablation study on the EPIC-Kitchens dataset, whereas Section 5.5 reports some qualitative examples of the proposed method. <ref type="table" target="#tab_2">TABLE 1</ref> compares RU with respect to the other state-of-theart approaches on our validation set of the EPIC-Kitchens dataset. The left part of the table reports Top-5 action anticipation accuracy for the 8 considered anticipation times. Note that some methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> have been designed to anticipate actions only at a fixed anticipation time. The right part of the table reports the Top-5 accuracy and Mean Top-5 Recall for verbs, nouns and actions, for the fixed anticipation time of ? a = 1s, as well as the mean T tA(5) scores obtained across the validation set. Best results are highlighted in bold, whereas second-best results are underlined. The last row reports the improvements obtained by RU with respect to second-best results. ED* denotes the Encoder-Decoder approach proposed in <ref type="bibr" target="#b7">[8]</ref> without the unsupervised pretraining procedure proposed by the authors. These results are reported for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Egocentric Action Anticipation on EPIC-Kitchens</head><p>The proposed approach outperforms all competitors by consistent margins according to all evaluation measures, obtaining an average improvement over prior art of about 5% with respect to Top-5 action anticipation accuracy on all anticipation times. The methods based on TSN (ATSN and MCE) tend to achieve low performance, which suggests the limits of simply adapting action recognition methods to the problem of anticipation. Interestingly, DMR and ED, which are explicitly trained to anticipate future representations, achieve sub-optimal Top-5 action anticipation accuracy as compared to methods trained to predict future actions directly from input images (e.g., compare DMR with MCE, and ED with FN/RL/EL/LSTM/TCN/RU). Comparing ED* to ED reveals that the unsupervised pre-training based on the regression of future representations is not beneficial in the considered problem of egocentric action anticipation. Indeed, in most cases the results achieved by the two methods are comparable. This might be due to the fact that anticipating future representations is very challenging in the case of egocentric video, in which the visual content tend change continuously because of the mobility of the camera. The LSTM baseline consistently achieves second best results with respect to all anticipation times, except for ? a = 0.25, where it is outperformed by EL. This suggests that the loss functions employed in the RL and EL baselines, originally proposed for early action recognition in third person videos, are not effective in the case of egocentric action anticipation. TCN achieves very low performance as compared to most of the considered approach. This suggests that the nonrecurrent nature of this approach is not very well suited to the considered anticipation problem, in which it is in general beneficial to refine predictions as more observations are processed. The proposed RU model is particularly strong on nouns, achieving a Top-5 noun accuracy of 51.79% and a mean Top-5 noun recall of 49.90%, which improves over prior art by +7.26% and +7.31% respectively. The small drop in performance between class-agnostic and class-aware measures (i.e., 51.79% vs 49.90%) suggests that our method does not over-fit to the distribution of nouns seen during training set. It is worth noting that mean Top-5 Recall values are averaged over fairly large sets of 26 many-shot verbs, 71 many-shot nouns, and 819 many-shot actions, as specified in <ref type="bibr" target="#b9">[10]</ref>. Differently, all compared methods obtain large drops in verb and action performance when comparing classagnostic measures to class-aware measures. Our insight into this different pattern is that anticipating the next active object (i.e., anticipating nouns) is much less ambiguous than anticipating the way in which the object will be used (i.e., anticipating verbs and actions). It is worth noting that second best Top-5 verb and noun accuracy scores are obtained by different methods (i.e., ATSN in the case of verbs and RL in the case of nouns), while both are outperformed by the proposed RU. Despite its low performance when evaluated with class-agnostic measures, ED systematically achieves second best results with respect to mean Top-5 recall and mean T tA <ref type="bibr" target="#b4">(5)</ref>. This highlights that there is no clear second-best performing method. Finally, the mean T tA(k) highlights that the proposed method can anticipate verbs, nouns and actions 1.62, 1.11 and 0.76 seconds in advance respectively. TABLE 2 compares the performance of the proposed method with baselines and other approaches on the official test sets of the EPIC-Kitchens dataset. RU-LSTM outperforms all competitors by consistent margins on both the "seen" test, which includes scenes appearing in the training set (S1) and on the "unseen" test set, with scenes not appearing in the training set (S2). Also in this case, RU is strong on nouns, obtaining +6.31% and +8.23% in S1, as well as +2.76% and +2.18 in S2. Improvements in terms of actions are also significant: +3.63% and +5.52% in S1, as well as +0.92% and +1.81% on S2. <ref type="table" target="#tab_4">TABLE 3</ref> reports egocentric action anticipation results on EGTEA Gaze+. The proposed RU approach outperforms the competitors for all anticipation times except for ? a = 2s, in which case its performance is on par with the LSTM baseline. Note that the margins of improvement obtained by the proposed method are smaller on EGTEA Gaze+, probably due to its smaller scale (106 actions in vs 2, 513 actions in EPIC-KITCHENS). Second-best results are achieved by EL for most of the anticipation times, except for ? a ? {1.25s, 1.0s}, where LSTM achieves comparable results. The table shows similar trends to the ones observed in the case of EPIC-Kitchens. The methods based on the direct regression of future representations such as DMR and ED still achieve sub-optimal results, especially as compared to other sequence-to-sequence models. Interestingly, ED* achieves better results than ED, which seem to confirm the limited ability of approaches based on direct regression of future representations in the egocentric domain. Also in this case, the performance of TCN tend to be somewhat limited as compared to recurrent approaches. Since no object annotations are available for EGTEA Gaze+, our RU   model uses the object detector trained on EPIC-Kitchens for the object branch. Despite the use of object classes not perfectly aligned with the ones contained in the dataset, our approach is strong on nouns even in this case, obtaining an improvement of +1.80% and +3.37% with respect to Top-5 accuracy and mean Top-5 recall. <ref type="table" target="#tab_5">Table 4</ref> reports the results on the third person ActivityNet dataset. Interestingly, in this context, ED significantly outperforms ED* and achieves top performances for most of the anticipation times. This is in line with the findings of the authors of the approach <ref type="bibr" target="#b7">[8]</ref>, and highlights the different nature of the egocentric scenario as compared to the rather static third person scenario. While the proposed RU model is outperformed by ED on most anticipation times, it systematically achieves second-best performance and has a significant advantage over ED*, which, similarly to the proposed method, does not make use of the unsupervised pre-training. Differently from previous results, in this case, both RL and EL outperform the LSTM baseline, which suggests that the anticipation losses used in these baseline are more beneficial in the case of third person videos than in the case of first-person videos. This highlights again the differences between the two scenarios. Also in these experiments, TCN achieve worse performance as compared to recurrent models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Egocentric Action Anticipation on EGTEA Gaze+</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Action Anticipation on ActivityNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study on EPIC-Kitchens</head><p>We performed an ablation study on the EPIC-Kitchens dataset to assess the role of the different components involved in our architecture. Specifically, to assess the role of the proposed rolling-unrolling mechanism, we considered a strong baseline composed of a single LSTM (same configuration as R-LSTM) and three branches (RGB, Flow, OBJ) with late fusion (BL). Note that, differently from the LSTM baseline compared in the previous sections, this baseline also includes an object branch. To study the role of rollingunrolling in isolation, we compare this baseline with respect to a variant of the proposed RU architecture in which MATT has been replaced with late fusion in TABLE 5(a). As can be observed, the rolling-unrolling mechanism brings systematic improvements over the strong baseline for all anticipation times. In TABLE 5(b), we study the influence of MATT by comparing it with respect to two standard fusion approaches: early fusion (i.e., feeding the model with the concatenation of the modality-specific representations) and late fusion (i.e., averaging predictions). MATT always outperforms late fusion, which consistently achieves second best results, while early fusion always leads to sub-optimal results. All fusion   schemes always improve over the single branches. <ref type="figure" target="#fig_8">Fig. 7</ref> shows regression plots of the modality attention weights computed by the proposed method on all the samples of the validation set. The RGB and OBJ weights are characterized by a strong and steep correlation. A similar pattern is observed between Flow and OBJ weights, whereas Flow and RGB weights are characterized by a small positive correlation. This suggests that MATT gives more credit to OBJ when RGB and Flow are less informative, whereas it relies on RGB and Flow when the detected objects are not informative. <ref type="figure" target="#fig_10">Figure 8</ref> shows the top-10 and bottom-10 action categories which benefited from MATT as compared to late fusion, in terms of mean Top-5 Recall. For the analysis, we have considered only classes containing at least 5 instances in the validation set. We can observe significant improvements for some actions such as "take milk soy" and "put down kettle", while there are relatively small negative performance differences with respect to late fusion for some actions such as "take rag" and "put down tomato". TABLE 5(c) compares the performances of different versions of the proposed architecture in which MATT is used to fuse different subsets of the considered modalities. Fusing RGB with optical flow (RGB+Flow) or objects (RGB+OBJ) allows to improve over the respective single-branches. Fusing optical flow and objects (Flow+OBJ) improves over the Flow branch, but does not improve over the OBJ branch, while adding RGB (RGB+Flow+OBJ) does allow to improve over the single branches. This suggests that the model is not able to take advantage of representations based on optical flow when the RGB signal is not available. Interestingly, fusing RGB and objects (RGB+OBJ) allows to obtain better results than fusing RGB and optical flow (RGB+Flow), as it is generally considered in many standard pipelines. This further highlights the importance of objects for egocentric action anticipation. Fusing all modalities leads to the best performance.</p><p>In <ref type="figure" target="#fig_6">TABLE 5(d)</ref>, we assess the role of Sequence Completion Pre-Training (SCP). The proposed pre-training procedure brings small but consistent improvements for most anticipation times.  tive effect of all the proposed procedures/component with respect to a strong baseline which uses three modalities. It is worth noting that the proposed architecture brings improvements for all anticipation times, ranging from +1.48% to +4.06%. <ref type="figure" target="#fig_11">Figure 9</ref> finally investigates the effect of choosing different values of S enc , while the number of anticipation steps is fixed to S ant = 8. As can be noted, our approach achieves best results across most of the anticipation times for S enc = 6, while smaller and larger number of encoding steps lead to lower performance. This suggests that, while a sufficiently long temporal context is required to correctly anticipate actions, leveraging very long temporal contexts can be challenging. Under each example, are reported the top-4 predictions, whereas modality weights computed by MATT are reported in percentage on the right. We show green bounding boxes around the detected objects and orange arrows to illustrate optical flow. In the first example (top), the model can predict "close door" based on the context and the history of past actions (e.g., taking objects out of the cupboard), hence it assigns large weights to the RGB and Flow modalities and low weights to the OBJ modality. In the second example (bottom), the model initially predicts "squeeze lime" at ? a = 2s. Later, as the lemon is predicted, the prediction is corrected to "squeeze lemon". Note that in this case the network assigns larger weights to OBJ as compared to the previous example. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ADDITIONAL RESULTS ON EARLY ACTION RECOGNITION AND ACTION RECOGNITION</head><p>We note that, being a sequence-to-sequence method, our approach can also be employed to perform early action recognition. This is done by processing the video of an action sequentially and outputting a prediction at every time-step. The prediction performed at the last time-step can be also be used action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Early Action Recognition</head><p>We adapt all sequence-to-sequence models to perform early action recognition by sampling 8 video snippets form each action segment uniformly and set S enc = 0, S ant = 8. The models produce predictions at each time-step, corresponding to the observation rates: 12.5%, 25%, 37.5%, 50%, 62.5%, 75%, 87.5%, 100%. Modality-specific branches are fused by late fusion. We compared our approach with respect to the following baselines: FN, RL, EL, LSTM, TCN. TABLE 6 reports the Top-1 accuracy results obtained by the compared methods with respect to different observation rates on our validation set of EPIC-Kitchens. M OR scores are reported for verbs, nouns and actions. Best results are highlighted in bold numbers. Note that, differently from Top-1 accuracy, lower M OR scores are better than higher M OR scores, meaning that the method can, in average, recognize an action by observing less frames. The proposed method consistently outperforms the competitors at all observation rates by about +2% in average. Interestingly, RU achieves an early action recognition accuracy of 33.09% when observing only 75% of the action, which is already comparable to the accuracy of 34.07% achieved when the full action is observed. Also, the proposed method achieves M OR values lower than the competitors, meaning that, in average, it can predict action correctly by observing less frames. This suggests that RU can timely recognize actions before they are completed. Second-best results are obtained by the LSTM baseline in most cases, indicating that the losses employed by RL and EL are not effective on this dataset for early action recognition. Similarly to what observed previously, TCN achieves sub-optimal results as compared to the recurrent approaches. TABLE 7 reports the Top-1 accuracy results on EGTEA Gaze+. The proposed RU is outperformed by the LSTM baseline for observation rates up to 50%, while it performs comparably to the competitors for the other observation rates. Also, the M OR values suggest that the proposed approach predicts actions by observing marginally less video content. Second-best results are obtained by different methods, and there is not a clear second-best performer in this case. Interestingly, TCN achieves performances comparable with the recurrent methods on this dataset. TABLE 8 reports the early action recognition results obtained by the different methods on ActivityNet. Interestingly, the RL baseline achieves best results or secondbest results for most of the observation rates. This suggests that the loss employed by this baseline is effective for early action recognition in the domain of third person videos, which is the scenario for which RL has been originally designed. As we found the RL loss beneficial, we trained the compared RU method with this loss on this dataset. The proposed approach achieves performances comparable in average with the other methods, but obtains a M OR smaller by 5.8%, which highlights that it can recognize actions by observing 5.8% less video content, in average. Also in this case, the performance of TCN are lower as compared to the recurrent approaches.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Action Recognition</head><p>We finally compare the performance of our method with respect to other state-of-the-art approaches on the task of action recognition on EPIC-Kitchens and EGTEA Gaze+. We do not assess the performance of our approach on Activi-tyNet as this dataset is generally used by the community for action localization rather than recognition. Although our method does not generally outperform the competitors, it achieves competitive results in some cases. TABLE 9 compares the performance of the proposed method with the state-of-the-art approaches to egocentric action recognition on the two test sets of EPIC-Kitchens. Being designed for early egocentric action anticipation, the proposed RU approach does not outperform the competitors, but achieves competitive results with the state-of-the-art, obtaining ?4.44% and ?3% on Top-1 and Top-5 action accuracy. Also, it outperforms several action recognition baselines such as TSN, 2SCNN, TRN and TSM on Top-1 and Top-5 action accuracy. <ref type="table" target="#tab_2">Table 10</ref> reports the action recognition results on EGTEA Gaze+. Despite being designed for action anticipation, RU outperforms recent approaches, such as Li et al. <ref type="bibr" target="#b15">[16]</ref> (+6.9% wrt 53.3%) and Zhang et al. <ref type="bibr" target="#b77">[78]</ref> (+3.19% wrt 57.01%reported from <ref type="bibr" target="#b43">[44]</ref>), and obtains performances comparable to state-of-the-art approaches such as Sudhakaran and Lanz <ref type="bibr" target="#b44">[45]</ref> (?0.56% wrt 60.76) and Sudhakaran et al. <ref type="bibr" target="#b43">[44]</ref> (?1.66% wrt 61.86%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented Rolling-Unrolling LSTMs, an architecture for egocentric action anticipation. The proposed architecture includes two separate LSTMs, designed to explicitly disentangle two sub-tasks: summarizing the past (encoding) and predicting the future (inference). To encourage such disentanglement, the architecture is trained with a novel sequence-completion pre-training. A modality attention network is introduced to fuse multi-modal predictions obtained by three branches processing RGB frames, optical flow fields and object-based features. Experiments on three benchmark datasets highlight that the proposed approach achieves state-of-the-art results on the task of action anticipation on both first-person and third-person scenarios, and generalizes to the tasks of early action recognition and action recognition. To encourage research on the topic, we publicly released the source code of the proposed approach, together with pre-trained models and extracted features at our project web page: http://iplab.dmi.unict.it/rulstm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A IMPLEMENTATION AND TRAINING DETAILS OF THE PROPOSED METHOD</head><p>This section reports implementation and training details of the different components involved in the proposed method. The reader is also referred to the code available online for the implementation of the proposed approach: https://iplab.dmi.unict.it/rulstm/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Architectural Details</head><p>We use a Batch Normalized Inception architecture <ref type="bibr" target="#b69">[70]</ref> in the representation functions ? 1 and ? 2 of the spatial and motion branches. For the object branch, we use a Faster R-CNN object detector <ref type="bibr" target="#b78">[79]</ref> with a ResNet-101 backbone <ref type="bibr" target="#b32">[33]</ref>, as implemented in <ref type="bibr" target="#b79">[80]</ref>. Both the Rolling LSTM (R-LSTM) and the Unrolling LSTM (U-LSTM) contain a single hidden layer with 1024 units. Dropout with p = 0.8 is applied to the input of each LSTM and to the input of the final fully connected layer used to obtain class scores. The Modality ATTention network (MATT) is a feed-forward network with three fully connected layers containing respectively h/4, h/8 and 3 hidden units, where h is the dimension of the input to the attention network (i.e., the concatenation of the hidden and cell states of 1024 units of all modality-specific R-LSTMs). When three modalities are considered, we obtain an input of dimension h = 3 ? 2 ? 1024 = 6144. Dropout with p = 0.8 is applied to the input of the second and third layers of the attention network to avoid over-fitting. ReLU activations are used within the attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Procedure</head><p>We train the spatial and motion CNNs for the task of egocentric action recognition with TSN <ref type="bibr" target="#b26">[27]</ref>. We set the number of segments of TSN to 3 and train the model with Stochastic Gradient Descent (SGD) and standard cross entropy for 160 epochs with an initial learning rate equal to 0.001, which is decreased by a factor of 10 after 80 epochs. We use a minibatch size of 64 samples and train the models on a single Titan X GPU. For all other parameters, we use the values recommended in <ref type="bibr" target="#b26">[27]</ref>. We train the object detector to recognize the 352 object classes of the EPIC-Kitchens dataset. We use the same object detector trained on EPIC-Kitchens when performing experiments on EGTEA Gaze+, as the latter dataset does not contain object bounding box annotations. We do not use an object branch in the case of ActivityNet. This training procedure allows to learn the parameters ? 1 , ? 2 and ? 3 of the representation functions related to the three modalities (i.e., RGB, Flow, OBJ). After this procedure, these parameters are fixed and they are no further optimized. For efficiency, we pre-compute representations over the whole dataset.</p><p>Each branch of the RU-LSTM is trained with SGD and the cross entropy loss with a fixed learning rate equal to 0.01 and a momentum equal to 0.9. The loss is averaged both across the samples of the mini-batches and across the predictions obtained at different time-stamps. Each branch is first pre-trained with Sequence Completion Pre-training (SCP). Specifically, appearance and motion branches are trained for 100 epochs, whereas the object branch is trained for 200 epochs. The branches are then fine-tuned for the action anticipation task. Once each branch has been trained, the complete architecture with three branches is assembled using MATT to form a three-branch network and the model is further fine-tuned for 100 epochs using cross entropy and the same learning parameters. In the case of early action recognition, each branch is trained for 200 epochs (both SCP and main task) with a fixed learning rate equal to 0.01 and momentum equal to 0.9.</p><p>We apply early stopping at each training stage. This is done by choosing the iterations of the intermediate and final models which obtain the best Top-5 action anticipation accuracy for the anticipation time ? a = 1s on the validation set. In the case of early action recognition, we choose the epoch in which we obtain the best average Top-1 action accuracy across observation rates. The same early stopping strategy is applied to all the methods for fair comparison. The proposed architecture has been implemented using the PyTorch library <ref type="bibr" target="#b80">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION AND TRAINING DETAILS OF THE COMPARED METHODS</head><p>Since no official public implementations are available for the compared methods, we performed experiments using our own implementations. In this section, we report the implementation details of each of the compared method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Deep Multimodal Regressor (DMR)</head><p>We implemented the Deep Multimodal Regressor proposed in <ref type="bibr" target="#b8">[9]</ref> setting the number of multi-modal branches with interleaved units to k = 3. For fair comparisons, we substituted the AlexNet backbone originally considered in <ref type="bibr" target="#b8">[9]</ref> with a BNInception CNN pre-trained on ImageNet. The CNN has been trained to anticipate future representations extracted using BNInception pre-trained on ImageNet using the procedure proposed by the authors. Specifically, we performed mode update every epoch. Since training an SVM with large number of classes is challenging (we have 2, 513 different action classes in the case of EPIC-Kitchens), we substituted the SVM with a Multi Layer Perceptron (MLP) with 1024 hidden units and dropout with p = 0.8 applied to the input of the first and second layer. To comply with the pipeline proposed in <ref type="bibr" target="#b8">[9]</ref>, we pre-trained the model in an unsupervised fashion and then trained the MLP separately on representations pre-extracted from the training set using the optimal modes found at training time. As a result, during the training of the MLP, the weights of the CNN are not optimized. The DMR architecture has been trained with Stochastic Gradient Descent using a fixed learning rate equal to 0.1 and a momentum equal to 0.9. The network has been trained for several epochs until the validation loss saturated. Note that training the CNN on the EPIC-Kitchens dataset takes several days on a single Titan X GPU using our implementation. After training the DMR, we applied early stopping by selecting the iteration with the lowest validation loss. The MLP has then been trained with Stochastic Gradient Descent with fixed learning rate equal to 0.01 and momentum equal to 0.9. Early stopping has been applied also in this case considering the iteration of the MLP achieving the highest Top-5 action accuracy on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Anticipation TSN (ATSN)</head><p>We implemented this model following <ref type="bibr" target="#b9">[10]</ref>. Specifically, the model has been trained using TSN with 3 segments. We modified the network to output verb and noun scores and trained it summing the cross entropy losses applied independently to verbs and nouns. At test time, we obtained action probabilities by assuming conditional independence of verbs and nouns given the sample as follows: p(a = (v, n)|x) = p(v|x) ? p(n|x), where a = (v, n) is an action involving verb v and noun n, x is the input sample, whereas p(v|x) and p(n|x) are the verb and noun probabilities computed directly by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ATSN + VNMCE Loss (MCE)</head><p>This method has been implemented training the TSN architecture used in the case of ATSN with the Verb-Noun Marginal Cross Entropy Loss proposed in <ref type="bibr" target="#b10">[11]</ref>. We used the official code available at https://github.com/fpv-iplab/actionanticipation-losses/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Encoder-Decoder LSTM (ED)</head><p>We implemented this model following the details specified in <ref type="bibr" target="#b7">[8]</ref>. For fair comparison with respect to the proposed method, the model takes RGB and Flow features obtained using the representation functions considered for the RGB and Flow branches of our RU architecture. Differently from <ref type="bibr" target="#b7">[8]</ref>, we do not include a reinforcement learning term in the loss as our aim is not to distinguish the action from the background as early as possible as proposed in <ref type="bibr" target="#b7">[8]</ref>. The hidden state of the LSTMs is set to 2048 units. The model encodes representations for 16 steps, while decoding is carried out for 8 steps at a step-size of 0.25s. The architecture is trained in two stages. In the first stage, the model is trained to predict future representations. This stage is carried out for 100 epochs. The training data for a given epoch is obtained by sampling 100 random sequences for each video in the case of EPIC-Kitchens and EGTEA Gaze+ and 10 random sequences for each video in the case of ActivityNet. The difference in the number of sampled sequences reflects the different natures of the datasets: EPIC-Kitchens and EGTEA Gaze+ tend to contain few long videos, while ActivityNet tends to contain many shorter videos. The sequences are re-sampled at each epoch, which reduces the risk of overfitting. In all cases, the models converged within 100 epochs. Similarly to the other approaches, we apply early stopping by selecting the model with the lowest validation loss. In the second stage, the architecture is fine-tuned to predict future representations and anticipate future actions for 100 epochs. In both stages we use the Adam optimizer and a learning rate of 0.001 as suggested by the authors of <ref type="bibr" target="#b7">[8]</ref>. We also compare this method with a version of the approach in which the unsupervised pre-training stage is skipped (termed ED*). In this case, only the second stage of training is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Feedback-Network LSTM (FN)</head><p>The method proposed in <ref type="bibr" target="#b50">[51]</ref> has been implemented considering the best performing architecture among the ones investigated by the authors. This architecture comprises the "optional" LSTM layer and performs fusion by concatenation. The network uses our proposed video processing strategy. For fair comparison, we implemented the network as a two-stream architecture with two branches processing independently RGB and Flow features. The final predictions have been obtained with late fusion (equal weights for the two modalities). For fair comparisons, we used the representation functions of our architecture to obtain RGB and Flow features. The model has hidden layers of 1024 units, which in our experiments leaded to improved results with respect to the 128 features proposed by the authors <ref type="bibr" target="#b50">[51]</ref>. The model has been trained using the same parameters used in the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Temporal Convolutional Network (TCN)</head><p>This baseline has been implemented using the code provided by the authors <ref type="bibr" target="#b71">[72]</ref>. Similarly to FN, this method uses the our proposed video processing strategy. The network has 5 layers with kernels of size 7 in each layer. A dropout with p = 0.8 is applied to the input of each layer of the network. The model has been trained using cross entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 LSTM, RL &amp; EL</head><p>These three methods have been implemented considering a single LSTM with the same parameters of our Rolling LSTM. Similarly to FN, the models have been trained as two-stream models with late fusion used to obtain final predictions (equal weights). The input RGB and Flow features have been computed using the representation functions considered in our architecture. The models have been trained with the same parameters used in the proposed architecture. LSTM has been trained using cross entropy, RL has been trained using the ranking loss on the detection score proposed in <ref type="bibr" target="#b6">[7]</ref>, whereas EL ihas been trained using the exponential anticipation loss proposed in <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_0">Fig. 11</ref> reports qualitative results of three additional success action anticipation examples. For improved clarity, we report frames with and without optical flows for each example. In the top example, MATT assigns a small weight to the object branch as the contextual appearance features (i.e., RGB) are already enough to reliably anticipate the next actions. In the middle example object detection is fundamental to correctly anticipate "put down spoon", as soon as the object is detected. The bottom example shows a complex scene with many objects. The ability to correctly recognize objects is fundamental to anticipate certain actions (i.e., "wash spoon"). The algorithm can anticipate "wash" well in advance. As soon as the spoon is detected (? a = 2s), "wash spoon" is correctly anticipated. Note that, even if the spoon is not correctly detected at time ? a = 0.5s, "wash spoon" is still correctly anticipated. <ref type="figure" target="#fig_0">Fig. 12</ref> reports three failure examples. In the top example, the model fails to predict "adjust chair", probably due to  the inability of the object detector to identify the chair. Note that, when the object "pan" on the table is detected, "take curry" is wrongly anticipated. In the middle example, the algorithm successfully detects the fridge and tries to anticipate "close fridge" and some actions involving the "take" action, with wrong objects. This is probably due to the inability of the detector to detect "mozzarella", which is not yet appearing in the scene. In the bottom example, the method tries to anticipate actions involving "jar", as soon as "jar" is detected. This misleads the algorithm as the correct action is "pour coffee".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL QUALITATIVE EXAMPLES</head><p>The reader is referred to the videos available at https://iplab.dmi.unict.it/rulstm/ for additional success and failure qualitative examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>[? s ? (? o + ? a ), ? s ? ? a ],where ? o denotes the "observation time", i.e. the length of Egocentric Action Anticipation. See text for notation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Figure 3shows a diagram of the overall architecture of the proposed Overall architecture of the proposed Rolling-Unrolling LSTM architecture based on encoder-decoder models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Example of modality-specific branch with Senc = 1 and Sant = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Example of connection scheme used during SCP for time-step t = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The Top-5 recall for a given class c is defined as the fraction of samples of ground truth class c for which the class c is in the list of the top-5 anticipated actions [11]. The mean Top-5 Recall is obtained by averaging the Top-5 recall values over classes. When evaluating on EPIC-Kitchens, Top-5 Recalls are averaged over the provided list of many-shot verbs, nouns and actions. Results on the EPIC-Kitchens official test set are reported using the suggested evaluation measures, i.e., Top-1 accuracy, Top-5 accuracy, Precision and Recall. Early action recognition and action recognition models are evaluated using Top-1 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Correlations between modality attention weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Top-10 and bottom-10 actions which benefited from modality attention in terms of Top-5 Recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Impact of the choice of Senc on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10</head><label>10</label><figDesc>reports two qualitative examples of predictions made by the proposed approach at four anticipation times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative examples (best seen on screen). Legend for attention weights: blue -RGB, orange -Flow, green -objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Success action anticipation example qualitative results (best seen on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>Failure action anticipation example qualitative results (best seen on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>EPIC-Kitchens contains 39, 596 action annotations, 125 verbs, and 352 nouns. We split the public training set of EPIC-Kitchens (28, 472 action segments) into training (23, 493 segments) and validation (4, 979 segments) sets by randomly choosing 232 videos for training and 40 videos for validation. We considered all unique (verb, noun) pairs in the public training set, thus obtaining 2, 513 unique actions.</figDesc><table /><note>EGTEA Gaze+ contains 10, 325 action annotations, 19 verbs, 51 nouns and 106 unique actions. Methods are evaluated on EGTEA Gaze+ reporting the average performance across the three splits provided by the authors of the dataset [16]. We considered the 1.3 release of ActivityNet, which contains 10024 training videos, 4926 validation videos, and 5044 test videos. Each video is labeled with one or more action segments belonging to one of 200 action classes. Videos are provided as YouTube links. Hence, depending on the coun- try and time of download, some videos are not available. We have been able to download 7911 training videos and 3832 validation videos. Test videos are not used in our experi- ments as their labels are not publicly available. The total number of training annotations amounts to 11890, while the total amount of validation annotations is equal to 5786.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Egocentric action anticipation results on the EPIC-KITCHENS dataset. Improvement +2.99 +3.62 +4.02 +4.17 +5.43 +5.31 +5.49 +5.43</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-5 ACTION Accuracy% @ different ?a(s)</cell><cell></cell><cell cols="3">Top-5 Acc.% @1s</cell><cell cols="3">M. Top-5 Rec.% @1s</cell><cell></cell><cell>Mean T tA(5)</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell></row><row><cell>DMR [9]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>16.86</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>73.66</cell><cell>29.99</cell><cell>16.86</cell><cell>24.50</cell><cell>20.89</cell><cell>03.23</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ATSN [10]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>16.29</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>77.30</cell><cell>39.93</cell><cell>16.29</cell><cell>33.08</cell><cell>32.77</cell><cell>07.60</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MCE [11]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>26.11</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>73.35</cell><cell>38.86</cell><cell>26.11</cell><cell>34.62</cell><cell>32.59</cell><cell>06.50</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>TCN [72]</cell><cell>19.33</cell><cell>19.95</cell><cell>20.43</cell><cell>20.82</cell><cell>21.82</cell><cell>23.03</cell><cell>23.35</cell><cell>24.40</cell><cell>73.93</cell><cell>36.75</cell><cell>21.82</cell><cell>28.95</cell><cell>30.28</cell><cell>05.28</cell><cell>01.54</cell><cell>00.88</cell><cell>00.56</cell></row><row><cell>ED* [8]</cell><cell>21.45</cell><cell>22.37</cell><cell>23.26</cell><cell>24.51</cell><cell>25.20</cell><cell>26.34</cell><cell>27.45</cell><cell>28.67</cell><cell>76.24</cell><cell>42.18</cell><cell>25.20</cell><cell>42.25</cell><cell>42.00</cell><cell>09.98</cell><cell>01.59</cell><cell>00.99</cell><cell>00.61</cell></row><row><cell>ED [8]</cell><cell>21.53</cell><cell>22.22</cell><cell>23.20</cell><cell>24.78</cell><cell>25.75</cell><cell>26.69</cell><cell>27.66</cell><cell>29.74</cell><cell>75.46</cell><cell>42.96</cell><cell>25.75</cell><cell>41.77</cell><cell>42.59</cell><cell>10.97</cell><cell>01.60</cell><cell>01.02</cell><cell>00.63</cell></row><row><cell>FN [51]</cell><cell>23.47</cell><cell>24.07</cell><cell>24.68</cell><cell>25.66</cell><cell>26.27</cell><cell>26.87</cell><cell>27.88</cell><cell>28.96</cell><cell>74.84</cell><cell>40.87</cell><cell>26.27</cell><cell>35.30</cell><cell>37.77</cell><cell>06.64</cell><cell>01.52</cell><cell>00.86</cell><cell>00.56</cell></row><row><cell>RL [7]</cell><cell>25.95</cell><cell>26.49</cell><cell>27.15</cell><cell>28.48</cell><cell>29.61</cell><cell>30.81</cell><cell>31.86</cell><cell>32.84</cell><cell>76.79</cell><cell>44.53</cell><cell>29.61</cell><cell>40.80</cell><cell>40.87</cell><cell>10.64</cell><cell>01.57</cell><cell>00.94</cell><cell>00.62</cell></row><row><cell>EL [14]</cell><cell>24.68</cell><cell>25.68</cell><cell>26.41</cell><cell>27.35</cell><cell>28.56</cell><cell>30.27</cell><cell>31.50</cell><cell>33.55</cell><cell>75.66</cell><cell>43.72</cell><cell>28.56</cell><cell>38.70</cell><cell>40.32</cell><cell>08.62</cell><cell>01.55</cell><cell>00.94</cell><cell>00.62</cell></row><row><cell>LSTM [66]</cell><cell>26.45</cell><cell>27.11</cell><cell>28.22</cell><cell>29.24</cell><cell>29.89</cell><cell>31.03</cell><cell>31.88</cell><cell>33.19</cell><cell>76.33</cell><cell>44.21</cell><cell>29.89</cell><cell>39.31</cell><cell>40.30</cell><cell>10.42</cell><cell>01.56</cell><cell>00.93</cell><cell>00.63</cell></row><row><cell>RU-LSTM</cell><cell>29.44</cell><cell>30.73</cell><cell>32.24</cell><cell>33.41</cell><cell>35.32</cell><cell>36.34</cell><cell>37.37</cell><cell>38.98</cell><cell>79.55</cell><cell>51.79</cell><cell>35.32</cell><cell>43.72</cell><cell>49.90</cell><cell>15.10</cell><cell>01.62</cell><cell>01.11</cell><cell>00.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+2.25</cell><cell>+7.26</cell><cell>+5.43</cell><cell>+1.47</cell><cell>+7.31</cell><cell>+4.13</cell><cell>+0.02</cell><cell>+0.09</cell><cell>+0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Egocentric action anticipation results on the EPIC-Kitchens test set.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy%</cell><cell cols="2">Top-5 Accuracy%</cell><cell cols="2">Avg Class Precision%</cell><cell cols="2">Avg Class Recall%</cell></row><row><cell></cell><cell></cell><cell cols="8">VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</cell></row><row><cell></cell><cell>DMR [9]</cell><cell>26.53 10.43</cell><cell>01.27</cell><cell>73.30 28.86</cell><cell>07.17</cell><cell>06.13 04.67</cell><cell>00.33</cell><cell>05.22 05.59</cell><cell>00.47</cell></row><row><cell></cell><cell>2SCNN [10]</cell><cell>29.76 15.15</cell><cell>04.32</cell><cell>76.03 38.56</cell><cell>15.21</cell><cell>13.76 17.19</cell><cell>02.48</cell><cell>07.32 10.72</cell><cell>01.81</cell></row><row><cell>S1</cell><cell>ATSN [10] MCE [11]</cell><cell>31.81 16.22 27.92 16.09</cell><cell>06.00 10.76</cell><cell>76.56 42.15 73.59 39.32</cell><cell>28.21 25.28</cell><cell>23.91 19.13 23.43 17.53</cell><cell>03.13 06.05</cell><cell>09.33 11.93 14.79 11.65</cell><cell>02.39 05.11</cell></row><row><cell></cell><cell>ED [8]</cell><cell>29.35 16.07</cell><cell>08.08</cell><cell>74.49 38.83</cell><cell>18.19</cell><cell>18.08 16.37</cell><cell>05.69</cell><cell>13.58 14.62</cell><cell>04.33</cell></row><row><cell></cell><cell cols="2">Miech et al. [73] 30.74 16.47</cell><cell>09.74</cell><cell>76.21 42.72</cell><cell>25.44</cell><cell>12.42 16.67</cell><cell>03.67</cell><cell>08.80 12.66</cell><cell>03.85</cell></row><row><cell></cell><cell>RU-LSTM</cell><cell>33.04 22.78</cell><cell>14.39</cell><cell>79.55 50.95</cell><cell>33.73</cell><cell>25.50 24.12</cell><cell>07.37</cell><cell>15.73 19.81</cell><cell>07.66</cell></row><row><cell></cell><cell>Imp. wrt best</cell><cell>+1.23 +6.31</cell><cell>+3.63</cell><cell>+2.99 +8.23</cell><cell>+5.52</cell><cell>+1.59 +4.99</cell><cell>+1.32</cell><cell>+0.94 +5.19</cell><cell>+2.55</cell></row><row><cell></cell><cell>DMR [9]</cell><cell>24.79 08.12</cell><cell>00.55</cell><cell>64.76 20.19</cell><cell>04.39</cell><cell>09.18 01.15</cell><cell>00.55</cell><cell>05.39 04.03</cell><cell>00.20</cell></row><row><cell></cell><cell>2SCNN [10]</cell><cell>25.23 09.97</cell><cell>02.29</cell><cell>68.66 27.38</cell><cell>09.35</cell><cell>16.37 06.98</cell><cell>00.85</cell><cell>05.80 06.37</cell><cell>01.14</cell></row><row><cell>S2</cell><cell>ATSN [10] MCE [11]</cell><cell>25.30 10.41 21.27 09.90</cell><cell>02.39 05.57</cell><cell>68.32 29.50 63.33 25.50</cell><cell>06.63 15.71</cell><cell>07.63 08.79 10.02 06.88</cell><cell>00.80 01.99</cell><cell>06.06 06.74 07.68 06.61</cell><cell>01.07 02.39</cell></row><row><cell></cell><cell>ED [8]</cell><cell>22.52 07.81</cell><cell>02.65</cell><cell>62.65 21.42</cell><cell>07.57</cell><cell>07.91 05.77</cell><cell>01.35</cell><cell>06.67 05.63</cell><cell>01.38</cell></row><row><cell></cell><cell cols="2">Miech et al. [73] 28.37 12.43</cell><cell>07.24</cell><cell>69.96 32.20</cell><cell>19.29</cell><cell>11.62 08.36</cell><cell>02.20</cell><cell>07.80 09.94</cell><cell>03.36</cell></row><row><cell></cell><cell>RU-LSTM</cell><cell>27.01 15.19</cell><cell>08.16</cell><cell>69.55 34.38</cell><cell>21.10</cell><cell>13.69 09.87</cell><cell>03.64</cell><cell>09.21 11.97</cell><cell>04.83</cell></row><row><cell></cell><cell>Imp. wrt best</cell><cell>-1.36 +2.76</cell><cell>+0.92</cell><cell>-0.41 +2.18</cell><cell>+1.81</cell><cell>-2.68 +1.08</cell><cell>+1.44</cell><cell>+1.41 +2.03</cell><cell>+1.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Egocentric action anticipation results on EGTEA Gaze+.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-5 ACTION Accuracy% @ different ?a(s)</cell><cell></cell><cell cols="3">Top-5 Acc.% @1s</cell><cell cols="3">M. Top-5 Rec.% @1s</cell><cell></cell><cell>Mean T tA(5)</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell><cell cols="2">VERB NOUN</cell><cell>ACT.</cell></row><row><cell>DMR [9]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>55.70</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>92.78</cell><cell>71.36</cell><cell>55.70</cell><cell>70.22</cell><cell>53.92</cell><cell>38.11</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ATSN [10]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>40.53</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>90.60</cell><cell>69.94</cell><cell>40.53</cell><cell>69.24</cell><cell>57.02</cell><cell>31.61</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MCE [11]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>56.29</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>90.73</cell><cell>70.02</cell><cell>56.29</cell><cell>72.38</cell><cell>58.67</cell><cell>43.75</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>TCN [72]</cell><cell>49.86</cell><cell>51.05</cell><cell>54.08</cell><cell>55.17</cell><cell>58.50</cell><cell>59.34</cell><cell>62.87</cell><cell>65.53</cell><cell>91.10</cell><cell>71.94</cell><cell>58.50</cell><cell>73.36</cell><cell>63.11</cell><cell>47.14</cell><cell>01.86</cell><cell>01.58</cell><cell>01.33</cell></row><row><cell>ED* [8]</cell><cell>52.91</cell><cell>54.16</cell><cell>56.22</cell><cell>58.31</cell><cell>60.18</cell><cell>62.57</cell><cell>64.77</cell><cell>67.05</cell><cell>91.12</cell><cell>73.50</cell><cell>60.18</cell><cell>78.19</cell><cell>68.33</cell><cell>54.61</cell><cell>01.87</cell><cell>01.57</cell><cell>01.34</cell></row><row><cell>ED [8]</cell><cell>45.03</cell><cell>46.22</cell><cell>46.86</cell><cell>48.36</cell><cell>50.22</cell><cell>51.86</cell><cell>49.99</cell><cell>49.17</cell><cell>86.79</cell><cell>64.35</cell><cell>50.22</cell><cell>69.66</cell><cell>56.62</cell><cell>42.74</cell><cell>01.84</cell><cell>01.40</cell><cell>01.24</cell></row><row><cell>FN [51]</cell><cell>54.06</cell><cell>54.94</cell><cell>56.75</cell><cell>58.34</cell><cell>60.12</cell><cell>62.03</cell><cell>63.96</cell><cell>66.45</cell><cell>91.05</cell><cell>71.64</cell><cell>60.12</cell><cell>76.73</cell><cell>63.59</cell><cell>49.82</cell><cell>01.83</cell><cell>01.39</cell><cell>01.26</cell></row><row><cell>RL [7]</cell><cell>55.70</cell><cell>56.45</cell><cell>58.65</cell><cell>60.69</cell><cell>62.74</cell><cell>64.37</cell><cell>67.02</cell><cell>69.33</cell><cell>91.54</cell><cell>74.51</cell><cell>62.74</cell><cell>78.55</cell><cell>67.10</cell><cell>52.17</cell><cell>01.84</cell><cell>01.43</cell><cell>01.29</cell></row><row><cell>EL [14]</cell><cell>55.05</cell><cell>56.75</cell><cell>58.81</cell><cell>61.00</cell><cell>63.76</cell><cell>66.37</cell><cell>69.12</cell><cell>72.33</cell><cell>91.77</cell><cell>75.68</cell><cell>63.76</cell><cell>79.63</cell><cell>69.93</cell><cell>55.11</cell><cell>01.85</cell><cell>01.47</cell><cell>01.32</cell></row><row><cell>LSTM [66]</cell><cell>56.88</cell><cell>58.23</cell><cell>59.87</cell><cell>61.83</cell><cell>63.87</cell><cell>65.84</cell><cell>67.70</cell><cell>70.65</cell><cell>91.56</cell><cell>75.30</cell><cell>63.87</cell><cell>78.27</cell><cell>68.43</cell><cell>53.35</cell><cell>01.85</cell><cell>01.54</cell><cell>01.33</cell></row><row><cell>RU-LSTM</cell><cell>56.82</cell><cell>59.13</cell><cell>61.42</cell><cell>63.53</cell><cell>66.40</cell><cell>68.41</cell><cell>71.84</cell><cell>74.28</cell><cell>93.11</cell><cell>77.48</cell><cell>66.40</cell><cell>82.07</cell><cell>73.30</cell><cell>58.64</cell><cell>01.88</cell><cell>01.61</cell><cell>01.41</cell></row><row><cell>Improv.</cell><cell>-0.06</cell><cell cols="7">+0.90 +1.55 +1.70 +2.53 +2.04 +2.72 +1.95</cell><cell>+0.33</cell><cell>+1.80</cell><cell>+2.53</cell><cell>+2.44</cell><cell>+3.37</cell><cell>+3.53</cell><cell>+0.01</cell><cell>+0.03</cell><cell>+0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 70.20 70.45 71.04 71.75 72.93 72.95 72</head><label>4</label><figDesc>Anticipation results on ActivityNet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Top-5 Accuracy% @ different ?a(s)</cell><cell></cell><cell></cell><cell cols="3">Top-1% M.T-5 Rec.% M.T tA(5)</cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell><cell>1</cell><cell>1</cell><cell>/</cell></row><row><cell>DMR [9]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>52.39</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>24.13</cell><cell>39.55</cell><cell>/</cell></row><row><cell>ATSN [10]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>48.08</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>29.09</cell><cell>45.94</cell><cell>/</cell></row><row><cell>TCN [72]</cell><cell cols="6">52.71 53.55 55.21 56.69 58.39 59.42</cell><cell>60.85</cell><cell>62.06</cell><cell>34.15</cell><cell>57.21</cell><cell>01.27</cell></row><row><cell>ED* [8]</cell><cell cols="6">62.89 63.71 64.23 65.27 65.84 67.09</cell><cell>68.16</cell><cell>69.79</cell><cell>42.83</cell><cell>64.98</cell><cell>01.40</cell></row><row><cell>ED [8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.52</cell><cell>71.43</cell><cell>48.58</cell><cell>72.32</cell><cell>01.54</cell></row><row><cell>FN [51]</cell><cell cols="6">59.44 60.01 60.79 61.38 62.29 63.32</cell><cell>64.16</cell><cell>65.34</cell><cell>37.57</cell><cell>60.65</cell><cell>01.27</cell></row><row><cell>RL [7]</cell><cell cols="6">65.11 65.66 66.56 67.51 68.71 69.78</cell><cell>71.15</cell><cell>72.48</cell><cell>45.27</cell><cell>67.68</cell><cell>01.40</cell></row><row><cell>EL [14]</cell><cell cols="6">63.21 63.99 65.15 66.03 67.05 68.25</cell><cell>69.49</cell><cell>71.15</cell><cell>43.06</cell><cell>66.17</cell><cell>01.37</cell></row><row><cell>LSTM [66]</cell><cell cols="6">61.61 62.92 63.72 64.31 65.70 66.71</cell><cell>67.87</cell><cell>69.24</cell><cell>40.37</cell><cell>64.44</cell><cell>01.34</cell></row><row><cell>RU-LSTM</cell><cell cols="6">65.53 66.67 67.59 69.13 70.23 71.66</cell><cell>72.73</cell><cell>73.97</cell><cell>46.49</cell><cell>69.41</cell><cell>01.45</cell></row><row><cell>Imp</cell><cell>-4.67</cell><cell>-3.78</cell><cell>-3.45</cell><cell>-2.62</cell><cell>-2.70</cell><cell>-1.29</cell><cell cols="2">+0.21 +1.49</cell><cell>-2.09</cell><cell>-2.91</cell><cell>-0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 29.10 29.77 31.72 33.09 34.23 35.28 36.10 37.61 0.73 Imp29.44 30.73 32.24 33.41 35.32 36.34 37.37 38.98 0.7630.73 32.24 33.41 35.32 36.34 37.37 38.98 0.76</head><label>5</label><figDesc>Ablation study on EPIC-KITCHENS. Late) 27.96 28.76 29.99 31.09 32.02 33.09 34.13 34.92 0.66 RU (Late) . +1.14 +1.01 +1.73 +2.00 +2.21 +2.19 +1.97 +2.69 +0.07 (a) Rolling-Unrolling Mechanism. 21.10 21.24 21.84 23.05 23.93 25.00 26.11 26.45 0.57 RGB+Flow 26.75 27.43 29.20 30.15 32.16 33.49 34.37 35.46 0.70 RGB+OBJ 28.04 29.51 31.48 32.22 34.27 35.36 36.89 37.79 0.74 RGB+Flow+OBJ Imp. (Fusion) +1.48 +1.97 +2.25 +2.32 +3.30 +3.25 +3.24 +4.06 +0.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-5 ACTION Accuracy% @ different ?a(s)</cell><cell>T tA</cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell></row><row><cell>BL (RU (RGB)</cell><cell cols="8">25.44 26.89 28.32 29.42 30.83 32.00 33.31 34.47</cell><cell>0.69</cell></row><row><cell>RU (Flow)</cell><cell cols="8">17.38 18.04 18.91 19.97 21.42 22.37 23.49 24.18</cell><cell>0.51</cell></row><row><cell>RU (OBJ)</cell><cell cols="8">24.56 25.60 26.61 28.32 29.89 30.85 31.82 33.39</cell><cell>0.67</cell></row><row><cell>Early Fusion</cell><cell cols="8">25.58 27.25 28.58 29.59 31.88 32.78 33.99 35.62</cell><cell>0.72</cell></row><row><cell>Late Fusion</cell><cell cols="8">29.10 29.77 31.72 33.09 34.23 35.28 36.10 37.61</cell><cell>0.73</cell></row><row><cell>MATT</cell><cell cols="8">29.44 30.73 32.24 33.41 35.32 36.34 37.37 38.98</cell><cell>0.76</cell></row><row><cell>Imp.</cell><cell cols="8">+0.34 +0.96 +0.52 +0.32 +1.09 +1.06 +1.27 +1.37 +0.03</cell></row><row><cell></cell><cell cols="7">(b) Modality Attention Fusion Mechanism.</cell></row><row><cell></cell><cell cols="7">(c) MATT fusion with different modalities.</cell></row><row><cell>w/o SCP</cell><cell cols="8">29.22 30.43 32.34 33.37 34.75 35.84 36.79 37.93</cell><cell>0.75</cell></row><row><cell>with SCP</cell><cell cols="8">29.44 30.73 32.24 33.41 35.32 36.34 37.37 38.98</cell><cell>0.76</cell></row><row><cell>Imp. of SCP</cell><cell cols="8">+0.22 +0.30 -0.10 +0.04 +0.57 +0.50 +0.58 +1.05 +0.01</cell></row><row><cell></cell><cell cols="7">(d) Sequence-Completion Pre-training.</cell></row><row><cell>BL (Fusion)</cell><cell cols="8">27.96 28.76 29.99 31.09 32.02 33.09 34.13 34.92 0.66</cell></row><row><cell>RU (Fusion)</cell><cell>29.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Flow+OBJ1 (e) Overall comparison wrt strong baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>(e) compares RU with the strong baseline of TABLE 5(a). The comparison shows the cumula-</figDesc><table><row><cell>2.00</cell><cell>1.75</cell><cell>1.50</cell><cell>1.25 Anticipation time ( a) 1.00</cell><cell>0.75</cell><cell>0.50</cell><cell>0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Early recognition results on EPIC-KITCHENS.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-1 ACTION Accuracy% @ different observation rates</cell><cell></cell><cell></cell><cell>M OR%</cell><cell></cell></row><row><cell></cell><cell cols="2">12.5% 25.0%</cell><cell cols="2">37.5% 50.0%</cell><cell>62.5%</cell><cell>75.0%</cell><cell>87.5%</cell><cell cols="4">100% VERB NOUN ACT.</cell></row><row><cell>TCN [72]</cell><cell>16.93</cell><cell>18.42</cell><cell>19.27</cell><cell>19.05</cell><cell>20.62</cell><cell>20.53</cell><cell>19.45</cell><cell>17.12</cell><cell>53.89</cell><cell>63.49</cell><cell>76.29</cell></row><row><cell>FN [51]</cell><cell>19.61</cell><cell>23.85</cell><cell>25.66</cell><cell>26.85</cell><cell>27.47</cell><cell>28.34</cell><cell>28.26</cell><cell>28.38</cell><cell>51.12</cell><cell>65.12</cell><cell>74.49</cell></row><row><cell>RL [7]</cell><cell>22.53</cell><cell>25.08</cell><cell>27.19</cell><cell>28.64</cell><cell>29.57</cell><cell>30.13</cell><cell>30.45</cell><cell>30.47</cell><cell>51.29</cell><cell>63.59</cell><cell>73.13</cell></row><row><cell>EL [14]</cell><cell>19.69</cell><cell>23.27</cell><cell>26.03</cell><cell>27.49</cell><cell>29.06</cell><cell>29.97</cell><cell>30.91</cell><cell>31.46</cell><cell>51.89</cell><cell>64.00</cell><cell>74.10</cell></row><row><cell>LSTM [66]</cell><cell>22.16</cell><cell>25.78</cell><cell>27.80</cell><cell>28.98</cell><cell>29.87</cell><cell>31.13</cell><cell>31.28</cell><cell>30.93</cell><cell>50.64</cell><cell>63.27</cell><cell>72.60</cell></row><row><cell>RU-LSTM</cell><cell>24.48</cell><cell>27.63</cell><cell>29.44</cell><cell>30.93</cell><cell>32.16</cell><cell>33.09</cell><cell>33.63</cell><cell>34.07</cell><cell>47.91</cell><cell>59.15</cell><cell>69.34</cell></row><row><cell>Imp.</cell><cell>+1.95</cell><cell>+1.85</cell><cell>+1.64</cell><cell>+1.95</cell><cell>+2.29</cell><cell>+1.96</cell><cell>+2.35</cell><cell>+2.61</cell><cell>-2.73</cell><cell>-4.12</cell><cell>-3.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>Early recognition results on EGTEA Gaze+.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-1 ACTION Accuracy% @ different observation rates</cell><cell></cell><cell></cell><cell>M OR%</cell><cell></cell></row><row><cell></cell><cell cols="2">12.5% 25.0%</cell><cell cols="2">37.5% 50.0%</cell><cell>62.5%</cell><cell>75.0%</cell><cell>87.5%</cell><cell cols="4">100% VERB NOUN ACT.</cell></row><row><cell>TCN [72]</cell><cell>49.61</cell><cell>52.88</cell><cell>55.47</cell><cell>56.24</cell><cell>56.90</cell><cell>57.58</cell><cell>57.62</cell><cell>56.11</cell><cell>34.58</cell><cell>33.54</cell><cell>45.13</cell></row><row><cell>FN [51]</cell><cell>44.02</cell><cell>50.32</cell><cell>53.34</cell><cell>55.10</cell><cell>56.58</cell><cell>57.31</cell><cell>57.95</cell><cell>57.72</cell><cell>37.07</cell><cell>37.80</cell><cell>48.94</cell></row><row><cell>RL [7]</cell><cell>45.42</cell><cell>51.00</cell><cell>54.20</cell><cell>56.54</cell><cell>58.09</cell><cell>58.93</cell><cell>59.29</cell><cell>59.50</cell><cell>37.80</cell><cell>36.86</cell><cell>48.43</cell></row><row><cell>EL [14]</cell><cell>40.31</cell><cell>48.08</cell><cell>51.84</cell><cell>54.71</cell><cell>56.93</cell><cell>58.45</cell><cell>59.55</cell><cell>60.18</cell><cell>38.91</cell><cell>37.48</cell><cell>49.93</cell></row><row><cell>LSTM [66]</cell><cell>50.22</cell><cell>53.82</cell><cell>55.73</cell><cell>57.20</cell><cell>58.01</cell><cell>58.79</cell><cell>59.09</cell><cell>59.32</cell><cell>36.20</cell><cell>35.46</cell><cell>46.89</cell></row><row><cell>RU-LSTM</cell><cell>45.94</cell><cell>51.84</cell><cell>54.39</cell><cell>57.05</cell><cell>58.15</cell><cell>59.31</cell><cell>60.10</cell><cell>60.20</cell><cell>34.91</cell><cell>34.64</cell><cell>45.57</cell></row><row><cell>Imp.</cell><cell>-4.28</cell><cell>-1.98</cell><cell>-1.34</cell><cell>-0.15</cell><cell>0.06</cell><cell>0.38</cell><cell>0.55</cell><cell>0.02</cell><cell>0.33</cell><cell>1.10</cell><cell>0.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Early recognition results on ActivityNet.Top-1 ACTION Accuracy% @ different observation rates M OR% 12.5% 25.0% 37.5% 50.0% 62.5% 75.0% 87.5%</figDesc><table><row><cell>100%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. The reader is referred to Appendix A for the implementation and training details of the proposed approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. The reader is referred to Appendix B for the implementation details of the considered methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. See Appendix C and https://iplab.dmi.unict.it/rulstm/ for additional examples and videos.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trafficpredict: Trajectory prediction for heterogeneous traffic-agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6120" to="6127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating notifications for missing actions: Don&apos;t forget to turn the lights off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4669" to="4677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">First-person vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2442" to="2453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RED: Reinforced encoderdecoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3118" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robot-centric activity prediction from first-person videos: What will they do to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper into first-person activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1894" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Trajectory aligned features for first person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lsta: Long short-term attention for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recognize human activities from partially observed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2658" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sequential maxmargin event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Am I done? predicting action progress in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications in Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Car that knows before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3182" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What will happen next? forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3342" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint prediction of activity labels and starting times in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5773" to="5782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visual forecasting by imitating dynamics in natural sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal perception and prediction in ego-centric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4498" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Egocentric future localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4697" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Nextactive-object prediction from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Forecasting hand and object locations in future frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno>abs/1705.07328</idno>
		<ptr target="http://arxiv.org/abs/1705.07328" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">First-person activity forecasting with online inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Choosing smartly: Adaptive multimodal fusion for object detection in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<ptr target="http://ais.informatik.uni-freiburg.de/publications/papers/mees16iros.pdf" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Deajeon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Epicfusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Detectron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
