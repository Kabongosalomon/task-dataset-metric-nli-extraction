<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNDER REVIEW FOR IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoning</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Liang</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jingwen</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jinwei</forename><forename type="middle">Gu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Weisi</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">UNDER REVIEW FOR IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fragments</term>
					<term>Sampling</term>
					<term>Quality-Sensitive Neighbourhood Representatives</term>
					<term>Video Quality Assessment !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increased resolution of real-world videos presents a dilemma between efficiency and accuracy for deep Video Quality Assessment (VQA). On the one hand, keeping the original resolution will lead to unacceptable computational costs. On the other hand, existing practices, such as resizing and cropping, will change the quality of original videos due to the loss of details and contents, and are therefore harmful to quality assessment. With the obtained insight from the study of spatial-temporal redundancy in the human visual system and visual coding theory, we observe that quality information around a neighbourhood is typically similar, motivating us to investigate an effective quality-sensitive neighbourhood representatives scheme for VQA. In this work, we propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS) to get a novel type of sample, named fragments. Full-resolution videos are first divided into mini-cubes with preset spatial-temporal grids, then the temporal-aligned quality representatives are sampled to compose the fragments that serve as inputs for VQA. In addition, we design the Fragment Attention Network (FANet), a network architecture tailored specifically for fragments. With fragments and FANet, the proposed efficient end-to-end FAST-VQA and FasterVQA achieve significantly better performance than existing approaches on all VQA benchmarks while requiring only 1/1612 FLOPs compared to the current state-of-the-art. Codes, models and demos are available at https://github.com/timothyhtimothy/FAST-VQA-and-FasterVQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>V ISUAL content with a large spatial resolution has always been the pursuit of humans. Indeed, with the proliferation of high-definition photographing devices and significant advancements in various technologies such as video compression and 4G/5G, the videos shot by most common users have greatly increased in resolution (e.g., 1080P, 4K, or even 8K), thereby largely enriching human perception and entertainment styles. Nevertheless, the increased size of real-world videos has posed a number of practical obstacles for machine algorithms in terms of capture, transmission, storage, analysis, and evaluation of those videos. Video Quality Assessment (VQA), also known as the quantification of human perception of video quality, severely suffers from the growing video sizes.</p><p>While classical shallow VQA algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> based on handcrafted features struggle to handle in-thewild videos with diverse contents and degradation types, the most recent and effective approaches on in-the-wild VQA are based on deep neural networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, the computational complexity of deep neural networks usually grows with the video size, i.e., quadratically with the resolution, making them intolerable on high-resolution videos. Taking a 10-second-long 1080P ? H. <ref type="bibr">Wu</ref>  video clip as an example, a plain ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as the network backbone will require 40,919GFLOPs computational cost for inference and 217GB graphic memory cost during training with a batch size of 1 ( <ref type="figure" target="#fig_0">Fig. 1)</ref>, which exceeds the memory limits of all GPUs at present. In order to alleviate computational resource and memory shortage issues on GPUs, the majority of deep VQA methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> choose to regress quality scores with fixed features extracted from pre-trained networks of classification tasks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> instead of end-to-end training, resulting in these methods lacking effective representation learning and essentially only training a shallow regressor for VQA. Meanwhile, some other video-related tasks employ various sampling strategies to avoid the high computational cost. Most of them obtained their insight from studies on the human visual system (HVS) <ref type="bibr" target="#b16">[17]</ref> or visual coding theories <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which proved that visual content tends to be similar around a local region, i.e., a neighbourhood. For example, image and video compression standards, e.g., JPEG <ref type="bibr" target="#b20">[21]</ref> and H264/AVC <ref type="bibr" target="#b21">[22]</ref>, and resizing algorithms, such as Bicubic <ref type="bibr" target="#b22">[23]</ref>, generally extract representatives for partitioned neighbourhoods to ensure that the resampled information can represent the original information. As a result, most high-level video recognition (e.g., classification, detection) methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>   ing to reduce the video dimensions. However, as illustrated in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, resizing corrupts quality-related local textures such as blurs and artifacts in video 1&amp;2 which is significant in VQA and other low-level tasks. On the other hand, in order to preserve these local textures, several works <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> attempt to crop a single continuous patch. Nevertheless, these samples lose a large proportion of quality information, e.g., video 2&amp;3 in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, thus also not suitable for the VQA task. To build good samples for VQA, we need to ensure that they are representative of global quality information while also preserving the sensitivity to quality information on local textures and temporal variations. In this paper, we propose a new sampling paradigm to tackle with VQA, quality-sensitive neighbourhood representatives, that only requires sampling representatives from partitioned neighbourhoods but also selects texture-sensitive raw continuous patches as representatives. Specifically, we design a unified spatial-temporal sampling scheme, Spatialtemporal Grid Mini-cubes Sampling (St-GMS). Spatially, it cuts video frames into uniform non-overlapping grids, and samples a mini-patch randomly from each grid. Temporally, it cuts videos into uniform segments and samples multiple continuous frames within each segment. To better preserve temporal continuity between frames, we also constrain that mini-patches in each spatial grid and temporal segment should be aligned to form a mini-cube. Finally, all the mini-cubes are stitched to an integrated sample specially designed for VQA, termed fragments <ref type="figure" target="#fig_1">(Fig. 2</ref>). <ref type="figure" target="#fig_1">Fig. 2</ref>(a) illustrates the spatial view of fragments. First, they preserve the local texture-related quality information (e.g., spot blurs happened in video 1&amp;2) by retaining the patches in original resolution. Second, benefiting from the globally uniformly partitioned grids, fragments cover the global quality even though different regions have different qualities (e.g., video 2&amp;3). Third, by splicing the mini-cubes, fragments retain contextual relations among them so that the model can learn global scene information and rough semantic information of the original frames. As for the temporal view of fragments, as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, with the continuous frames and aligned mini-patches in each segment, fragments can also spot temporal variations in videos, e.g., distinguish between severely shaking videos (e.g., video 5) from relatively stable shots (e.g., video 6). The segment-wise sampling on the temporal dimension also ensures temporally uniform coverage of quality information.</p><p>It is non-trivial to design deep networks for fragments, as the mini-cubes are actually independent and the edges in between may be misinterpreted as quality defects. To avoid uncontrolled fusion of pixels in different mini-cubes, we propose a rule for building networks on fragments, the match constraint, to align the pooling operations with sampled mini-cubes. Specifically, we choose Video Swin Transformer <ref type="bibr" target="#b23">[24]</ref> as the backbone and improve the Relative Position Biases in the backbone into Gated Relative Position Biases (GRPB) to correctly represent the positions of pixels in fragments. Based on the characteristic of fragments that quality is diverse among mini-cubes, we further replace the pool-first head that is usually used in high-level tasks with a pool-last Intra-Patch Non-linear Regression (IP-NLR) head, to get better performance and predict local quality maps beyond quality scores. In general, with a Tiny Swin Transformer (abbr. as Swin-T) as baseline backbone and the proposed GRPB &amp; IP-NLR modules as modifications, we propose the Fragment Attention Network (FANet) that best extracts the quality-sensitive information in fragments.</p><p>This work is a substantial extension to our earlier conference version FAST-VQA <ref type="bibr" target="#b29">[30]</ref> which proposes a spatial-only sampling scheme and the accommodated network structure (FANet). In comparison to the conference version, we include a significant amount of improvements: 1) To further improve efficiency, we extend spatial-only sampling in to the spatial-temporal sampling scheme (St-GMS), based on which we improve FAST-VQA into the Fragment spatialtemporal Video Quality Assessment (FasterVQA) that performs comparable to FAST-VQA with only 25% of FLOPs 2) We propose the Adaptive Multi-scale Inference (AMI) on FANet for adaptively inferring on different scales with one model trained on a fixed scale while keeping competitive performance. 3) We add extensive ablation studies to further analyze the effects of sampling granularity, end-to-end training and semantic pre-training in the proposed methods. The main contributions of this work are listed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose the quality-sensitive neighbourhood representatives, a novel sampling paradigm for VQA, and design a unified Spatial-temporal Grid Mini-cube Sampling (St-GMS) scheme to sample fragments. The fragments enable deep VQA methods to efficiently and effectively evaluate videos of any resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose and evaluate the match constraint for pooling layers as guidance for building networks for fragments. Based on this constraint, we propose the Fragment Attention Network (FANet) with newly designed GRPB and IP-NLR modules to best accommodate the characteristics of fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The proposed FAST-VQA and FasterVQA outperform existing VQA methods by a large margin (up to 7%) with unprecedented efficiency (up to 1612?). Our efficient version can even infer at 13.6? faster than real-time on CPU with competitive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Classical VQA Methods. Classical VQA methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> employ handcraft features to evaluate video quality. Some methods hypothesize <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> that natural videos follow specific statistical rules, while the defect videos do not, and compute quality scores only from statistical evidence without regression from any subjective labels. In recent years, several methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b36">[37]</ref> choose to first handcraft quality-sensitive features and then regress them to subjective mean opinion scores (MOS), in order to better fit the human perception. Among them, TLVQM <ref type="bibr" target="#b2">[3]</ref> uses a combination of two levels of handcraft features, including high-complexity spatial features computed on sparse frames for measuring spatial distortions, and low-complexity temporal features computed for each frame for assessing temporal variations. VIDEVAL <ref type="bibr" target="#b3">[4]</ref> ensembles various handcraft features to model the diverse authentic distortions and also reduces the feature dimensions to reduce the computational burden. Spatial-temporal chips are sampled in a recent work called ChipQA <ref type="bibr" target="#b37">[38]</ref> for more efficient handcraft feature extraction. These classical approaches suggest that it is possible to reduce the size of videos while retaining their quality information. Nevertheless, since the factors affecting the in-the-wild video quality are quite complicated and usually cannot be concluded by finite handcraft features, the performance of these classical methods are constrained. Deep VQA Methods. Benefiting from the semantic awareness of deep neural network features, deep VQA methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref> are becoming predominant. For example, VSFA <ref type="bibr" target="#b4">[5]</ref> uses the features extracted by pre-trained ResNet-50 <ref type="bibr" target="#b10">[11]</ref> from ImageNet-1k dataset <ref type="bibr" target="#b39">[40]</ref> and adopts Gate Recurrent Unit (GRU) <ref type="bibr" target="#b40">[41]</ref> for quality regression. However, due to the extremely high memory cost of deep networks on high-resolution videos (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>), most existing deep VQA methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> can only extract fixed features instead of updating them. Without end-toend training, existing methods generally improve features in the three following ways. 1) Introducing heavier backbones, e.g., MLSP-FF <ref type="bibr" target="#b7">[8]</ref> includes heavier Inception-ResNet-V2 <ref type="bibr" target="#b14">[15]</ref> for feature extraction. 2) Using multiple backbone networks instead of one, e.g., PVQ <ref type="bibr" target="#b6">[7]</ref> uses an additional ResNet-3d-18 <ref type="bibr" target="#b15">[16]</ref> network to extract temporal quality features.</p><p>3) Including frame-wise pre-training <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> from IQA databases <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. A most recent method, BVQA-TCSVT-2022 <ref type="bibr" target="#b12">[13]</ref>, combines all these three ways to reach better performance, while it requires up to 26 minutes on CPU to assess the quality for an 8-second-long video, 200? slower than video playback. While improving performance, these practices significantly sacrifice the final computational efficiency. These practices further highlight the value of the proposed method with effective end-to-end training via efficient quality-retained sampling, so as to improve performance in an efficient manner for training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we introduce the proposed FAST-VQA and FasterVQA. We first define the paradigm of sampling quality-sensitive neighbourhood representatives (Sec. 3.1), and introduce the corresponding Spatial-temporal Grid Mini-cube Sampling (St-GMS, Sec. 3.2) scheme to resample the videos into fragments. After sampling, the fragments are fed into the Fragment Attention Network (FANet, discussed in Sec. 3.3) which is designed based on the match constraint. We also propose an Adaptive Multi-scale Inference (AMI, Sec. 3.4) strategy for adaptive-scale inference on the model trained at a single scale. Lastly, we present the associated objective functions (Sec. 3.5) for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling Representatives from Neighbourhoods</head><p>In visual tasks, sampling is widely applied. Specifically, uniform sampling schemes, such as spatial nearest/bicubic downsampling and temporal uniform sampling, are widely applied in high-level recognition tasks. In general, these methods can be concluded by two steps: 1) segmenting the image/video into various local areas (referred to as neighbourhoods), and 2) sampling a representative from each neighbourhood. We conclude the overall unified paradigm as neighbourhood representatives (R) which can be specified to either spatial or temporal dimensions. Given a target sampled size S and a single representative size S r , the paradigm first divides the visual contents into neighbourhoods N = {n i |i = 0, 1, 2, . . . , S Sr ? 1}, and then the neighbourhood representatives R can be formulated as,</p><formula xml:id="formula_0">R = {r(n i )|i = 0, 1, 2, . . . , S S r ? 1}<label>(1)</label></formula><p>where r(n i ) denotes the function that samples a representative from neighbourhood n i . Spatial Grid Partition: grids</p><formula xml:id="formula_1">G f ? G f</formula><p>Spatial Patch Splicing Spatial Patch Sampling:  As neighbourhood redundancy also occurs for qualityrelated information, the neighbourhood representatives can also be applied to quality tasks. Nevertheless, according to many widely acknowledged studies <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b37">[38]</ref>, continuous local textures and local temporal variations are significant while evaluating video quality, which will be corrupted if we apply resizing or uniform frame sampling (S r = 1). With deep thinking of the requirements of VQA task, we propose to sample quality-sensitive neighbourhood representatives (R q ), which should satisfy: 1) they should contain raw pixels in videos instead of pooled or averaged results; and 2) the raw pixels in one representative r(n i ) should form a continuous patch or clip that is large enough to distinguish spatial or temporal local quality information. As a result, these representatives R q can represent both the unbiased global quality information and the sensitive local quality information (e.g., spatial local textures, temporal variations among adjacent frames) that are vital for VQA.</p><formula xml:id="formula_2">Patch Size S f ? S f Frame t G f ? S f G f ? S f (a) spatial view of fragments in GMS/St-GMS segment 0 segment 1 segment G t ? 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial-temporal Grid Mini-cube Sampling</head><p>We propose the uniform Spatial-temporal Grid Mini-cube Sampling (St-GMS) scheme which follows the principle of quality-sensitive neighbourhood representatives in both spatial and temporal dimensions. The pipeline for St-GMS is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref> and discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spatial sampling: Grid Mini-patch Sampling (GMS)</head><p>In the first part, we discuss the Grid Mini-patch Sampling (GMS, <ref type="figure" target="#fig_2">Fig. 3(a)</ref>), i.e., the spatial sampling operations in St-GMS, together with the corresponding principles. Representing global quality: uniform grid partition. To include each region for quality assessment and uniformly assess quality in different areas, we design the grid partition to cut each video frame into uniform grids with each grid having the same size (as shown in <ref type="figure" target="#fig_2">Fig 3(a)</ref>). In particular, we cut the video frame</p><formula xml:id="formula_3">I with size H ? W into G f ? G f uniform grids with the same sizes, denoted as {g i,j |0 &lt; i &lt; G f , 0 &lt; j &lt; G f }, where g i,j</formula><p>refers to the grid in i-th row and j-th column. The partition is formalized as follows. 1</p><formula xml:id="formula_4">g i,j = I [ i?H G f : (i+1)?H G f , j?W G f : (j+1)?W G f ]<label>(2)</label></formula><p>Sensitive to local quality: raw patch sampling. To preserve the local textures (e.g., blurs, noises, artefacts) that are vital in VQA, we select raw resolution patches without any resizing operations to represent local textural quality in grids. To keep sensitivity to local textures, we employ uniform random patch sampling to select one mini-patch MP i,j of the size of S f ? S f from each grid g i,j . The spatial patch sampling (S s ) is formulated as follows.</p><formula xml:id="formula_5">MP i,j = S i,j s (g i,j ), 0 ? i, j &lt; G f<label>(3)</label></formula><p>Preserving contextual relations: patch splicing. Existing works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b46">[47]</ref> have shown that global scene information notably affects quality-related perception, that even the same textures under different semantic background can relate to different quality <ref type="bibr" target="#b47">[48]</ref>. To preserve the background information about the global scene, we retain the contextual relations among mini-patches by splicing them together:</p><formula xml:id="formula_6">F i,j = F [i?S f :(i+1)?S f ,j?S f :(j+1)?S f ] = MP i,j , 0 ? i, j &lt; G f<label>(4)</label></formula><p>where F denotes the spliced mini-patches from frame I after spatial GMS pipeline, as in our conference version <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b0">1</ref>. In this section, all square brackets ( [ ] ) denote the slicing operations, and all superscripts (e.g. i ) denote position indices. We also extend GMS into the temporal dimension for more efficient quality evaluation, discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Patch Attention Pair</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Extending GMS into the temporal dimension</head><p>We extend the GMS into the temporal dimension based on unified quality-sensitive neighbourhood representatives, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>(b). We discuss the detailed principles and operations in the temporal dimension as follows. Temporal representative: uniform segment partition. Similar to the spatial case, an accurate VQA method also need to uniformly assess quality along the temporal dimension. For uniformity, TSN <ref type="bibr" target="#b48">[49]</ref> proposed general segment-wise sampling for videos which had been applied by many existing VQA methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Thus, we divide the video V with T total frames into G t uniform non-overlapping temporal segments (as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). Overall, we extend the uniform grid partition as defined in Eq. 2 into spatialtemporal uniform grid partition, as follows.</p><formula xml:id="formula_7">g k,i,j = V [ k?T G t : (k+1)?T G t , i?H G f : (i+1)?H G f , j?W G f : (j+1)?W G f ]<label>(5)</label></formula><p>where g k,i,j denotes the spatial-temporal grid in k-th temporal segment, i-th row and j-th column. Sensitive to inter-frame variations: continuous frames. It is widely recognized by early works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref> that interframe temporal variations are influential to video quality. To retain the raw temporal variations in videos, we would like the frames sampled in each segment to be continuous and the corresponding mini-patches to be aligned so that the temporal variation inside the segment can be reflected by these samples. Thus, we apply temporal continuous frame sampling (S t ) before the raw-patch sampling (S s , Eq. 3) to sample a continuous mini-cube MC k,i,j of size T f ?S f ?S f from each spatial-temporal grid g k,i,j as follows:</p><formula xml:id="formula_8">MC k,i,j = S i,j s (S k t (g k,i,j )), 0 ? i, j &lt; G s , 0 ? k &lt; G t (6)</formula><p>Long-term dependencies: temporal splicing. Although there are no consensus on explanations of the long-term temporal dependencies in VQA, plenty of existing methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> have proved that they are practically influential to the video quality. Therefore, we include temporal splicing into the whole splicing operation as follows:</p><formula xml:id="formula_9">F k,i,j 3D = F 3D[k?T f :(k+1)?T f ,i?S f :(i+1)?S f ,j?S f :(j+1)?S f ] = MC k,i,j 0 ? i, j &lt; G s , 0 ? k &lt; G t<label>(7)</label></formula><p>where F 3D denotes the spliced spatial-temporal mini-cubes after the St-GMS pipeline, as space-time-unified fragments.</p><p>The GMS and the following FANet (Sec. 3.3, <ref type="figure" target="#fig_5">Fig. 5</ref>) together constitute the proposed FAST-VQA, which only includes the proposed spatial sampling operations and selects dense frames in the temporal dimension for inference. With unified spatial and temporal sampling strategies, we improve FAST-VQA into FasterVQA by replacing the GMS with the St-GMS. FasterVQA has 4X efficiency than FAST-VQA yet comparable accuracy. Both FAST-VQA and Faster-VQA include the FANet structure, discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quality Regression Network for fragments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Motivation: match constraint for pooling layers</head><p>It is non-trivial to build a network using the proposed fragments as inputs. Like most quality assessment networks, it should be able to effectively extract the quality information preserved in fragments, including the local textures inside mini-cubes and the contextual relationships between them. Moreover, it should specifically avoid misinterpreting the discontinuity between mini-cubes (resulted by artificial splicing) for local textures, which calls for more careful network design, especially for the pooling layers which decide the values of subsequent feature pixels and are not learnable. As a result, we impose the match constraint, which constrains that each pooling kernel should only include pixels inside of an individual mini-cube as green boxes in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>), but not between parts of mini-cubes (red boxes), before each mini-cube is finally downsampled as a single pixel. Formally, take any pooling kernel at any layer (before mini-cubes have been downsampled as single pixels), denote the set of original pixels that falls into the area of the kernel as P, the constraint can be formulated as:</p><formula xml:id="formula_10">? k, i, j, s.t. P ? MC k,i,j<label>(8)</label></formula><p>To follow the match constraint, we require the networks that use non-overlapping pooling kernels. Many backbone structures can meet this requirement, including transformerbased structures <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b49">[50]</ref> and part of modern convolution-based structures such as ConvNeXt <ref type="bibr" target="#b50">[51]</ref>, while it is possible to match their pooling kernels with mini-cubes. Our experiments show that either 1) using conventional backbones (i.e., ResNet <ref type="bibr" target="#b10">[11]</ref> and MobileNet <ref type="bibr" target="#b51">[52]</ref>) with overlapping pooling kernels or 2) failing to align mini-cubes with pooled pixels leads to a notable performance drop, suggesting the significance of match constraint for pooling  layers. Finally, we choose the Video Swin Transformer Tiny (Swin-T) backbone which follows the match constraint as the backbone of the quality regression network for fragments. We also make several modification on the Swin-T to better accommodate it for fragments, discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Fragment Attention Network (FANet)</head><p>The Overall Framework. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the overall framework of Fragment Attention Network (FANet), the proposed end-to-end quality regression network for fragments. It includes a four-layer Swin-T with first three window selfattention layers modified by GRPB as the backbone (abbr. as Swin-GRPB), and an IP-NLR quality-regression head. Gated Relative Position Biases (GRPB). In Swin-T, the window self-attention layers are built across mini-cubes to learn contextual relations between them. However, in these window self-attention layers, representing the positions of pixels of fragments differs from those of normal inputs. While original Swin-T proposes relative position bias (RPB) that uses learnable Relative Bias Table (T) to represent the relative positions of pixels in attention pairs (QK T ), they cannot well represent the relative positions of different pixels in fragments. Specifically, considering that some pairs in the same attention window might have the same relative position (e.g., <ref type="figure" target="#fig_3">Fig. 4</ref>(b) A-C, D-E, A-B), but the cross-patch attention pairs (A-C, D-E, two pixels from different minicubes) are in far actual distances while intra-patch attention pairs (A-B, two pixels from the same mini-cube). Therefore, we distinguish the two type of attention pairs and propose the gated relative position biases (GRPB) as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>(b) that uses two learnable real position bias table (T real ) and pseudo position bias table (T pseudo ) to replace T. Denote any two pixels in positions (p,p) (p ? MC k,i,j ,p ? MCk ,?,? ), the GRPB between them (B(p,p)) can be formulated as</p><formula xml:id="formula_11">G(p,p) = 1, i =? ? j =? ? k =k, 0, else<label>(9)</label></formula><formula xml:id="formula_12">B(p,p) = G(p,p)T p?p real + (1 ? G(p,p))T p?p pseudo<label>(10)</label></formula><p>where p ?p is the vector difference between the two positions, and used to index the two position bias tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Patch Non-Linear Regression (IP-NLR) Head.</head><p>Several recent quality assessment methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b45">[46]</ref> apply patchindependent regression heads to obtain local quality. Based on the match constraint (Eq. 8), feature pixels are aligned with mini-cubes, so it is also possible to regress qualities for each mini-cube to obtain local quality maps. Furthermore, as shown in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>, the quality-related features in different mini-cubes should be diverse even in the same video as their original positions are far apart. Therefore, averaging them before regression as commonly practised in video recognition may have the potential risk to lose the sensitivity to the diverse quality information, while regressing them independently can avoid this problem. Based on the two reasons above, we design the Intra-Patch Non-Linear Regression (IP-NLR, <ref type="figure" target="#fig_5">Fig. 5(c)</ref>) to regress the features via a twolayer MLP first and perform pooling on the regressed local quality scores. Denote final backbone features as f final , local quality map as l pr , the global quality scores (final output of FANet) as g pr , linear layers as L 1 , L 2 , the IP-NLR can be expressed as follows:</p><formula xml:id="formula_13">l t,h,w pr = L 2 (GeLU(L 1 (f t,h,w final )))<label>(11)</label></formula><p>g pr = l pr (12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adaptive Multi-scale Inference</head><p>The proposed models can adapt to various computing resources by changing the sampling densities (scales) of fragments. However, our conference version <ref type="bibr" target="#b29">[30]</ref> (FAST-VQA) still requires training different models for different scales of fragments. This could be inefficient when the input scale needs to be changed frequently, or adaptively. Therefore, with the objective of training at only one scale (least cost) and infer at any different scale (most flexible), we propose the Adaptive Multi-scale Inference (AMI) for FasterVQA.</p><p>To perform AMI, we adaptively modify the backbone structure of FANet with respect to different sizes of inference inputs. Generally, we keep all the linear and pooling layers unchanged as they mainly focus on local textures. For the window-based self-attention layers, we adaptively rescale the attention windows to ensure that the proportion of the window size to the global size is conserved when the input scale changes, which simulates self-attention-based approaches <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b52">[53]</ref> in dealing with variable-length inputs.</p><p>Formally, the attention window sizes given new scales of fragments are computed as follows:</p><formula xml:id="formula_14">W = W 0 ?? G 0<label>(13)</label></formula><p>where? and W 0 are the rescaled and base window sizes, and? and G 0 are the actual and preset base number of grids (to meet the match constraint, the sizes of mini-cubes are kept the same). For GRPB, we also lookup from the shared T real and T pseudo as defined in Eq. 10, and the gates G are computed from partitions of actual inputs. Our experiments demonstrate that the proposed FasterVQA with AMI can still infer with high accuracy at a certain scale even without training on fragments on the corresponding scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Functions</head><p>Many existing works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> have pointed out that the linearity and monotonicity of quality predictions to ground truth scores are more important objectives than the predictions themselves in quality assessment tasks. Therefore, we define a fusion loss function as the weighted sum of monotonicity loss L mono and linearity loss L lin as follows:</p><formula xml:id="formula_15">L mono = i,j max((s i pred ? s j pred ) sgn (s j gt ? s i gt ), 0) (14) L lin = (1 ? &lt; s pred ? s pred , s gt ? s gt &gt; s pred ? s pred 2 s gt ? s gt 2 )/2 (15) L f usion = L lin + ?L mono<label>(16)</label></formula><p>where sgn(?) denotes the sign function, &lt;&gt; denotes the inner product of two vectors, and s pred and s gt are vectors that refer to predictions and ground truth labels in a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In the experiment part, we conduct experiments for the proposed concepts and methods in the following aspects:</p><p>? Benchmark comparison with existing approaches (Sec. 4.2), in terms of both accuracy and efficiency.</p><p>? Detailed evaluation on sampling (Sec. 4.3), compared to naive sampling approaches and different variants.</p><p>? Ablation studies on match constraint, FANet structure, training and inference strategies, e.g. AMI (Sec. 4.4).</p><p>? Extra justifications to our methods: irreplaceable role of semantics (Sec. 4.5), evaluation on high-resolution cases (Sec. 4.6) and stability analysis (Sec. 4.7).</p><p>? Quantitative studies for local quality maps (Sec. 4.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation Details</head><p>We use the Swin-T <ref type="bibr" target="#b23">[24]</ref> as the backbone of our FANet, which is initialized by pretraining on Kinetics-400 dataset <ref type="bibr" target="#b55">[56]</ref>.</p><p>For FAST-VQA, we implement two sampling densities for fragments and adjust the window sizes in FANet to the input sizes: FAST-VQA (better accuracy) and FAST-VQA-M (mobile-friendly), as listed in Tab. 1. For FasterVQA, as we practice Adaptive Multi-scale Inference (AMI), we unify different sample densities in one single model. Still, we benchmark the performance of FasterVQA on two mobile-friendly  is set as 0.3, with initial learning rate set as 0.001 for IP-NLR head and 0.0001 for the Swin-GRPB backbone respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We use three metrics, including Pearson Linear Correlation Coefficient (PLCC), Spearman Rank-order Correlation Coefficient (SRCC), and Kendall Rank-order Correlation Coefficient (KRCC), for evaluating the accuracy of quality predictions. PLCC computes the linear correlation between a series of predicted scores and ground truth scores. SRCC will first rank the labels in both series and computes the linear correlation between the two rank series. KRCC computes the rank-pair accuracy, measuring the proportion of correctly predicted relative relations between score pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training &amp; Benchmark Sets</head><p>We use the large-scale LSVQ train <ref type="bibr" target="#b6">[7]</ref>  We directly evaluate the generalization ability of proposed models on cross-dataset evaluations on KoNViD-1k <ref type="bibr" target="#b56">[57]</ref> and LIVE-VQC <ref type="bibr" target="#b57">[58]</ref>, two widely-recognized in-the-wild VQA benchmark datasets composed of natural videos. We also discuss the fine-tuning results on several non-natural VQA datasets, including lab-collected datasets <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> and datasets with computer-generated videos <ref type="bibr" target="#b60">[61]</ref>, in Sec. 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Accuracy</head><p>Benchmarking FAST-VQA. In Tab. 3, we compare FAST-VQA with existing classical and deep VQA methods and our baseline, the full-resolution Swin-T with feature regression instead of end-to-end training (denoted as 'Full-res Swin-T feat.') while it notably outperforms state-of-the-arts with almost "negligible" cost. FAST-VQA also shows significant improvement to Full-res Swin-T feat., demonstrating that the proposed end-to-end learning via quality-retained sampling is not only much more efficient (with only 1/42.5 FLOPs required on 1080P videos) but also notably more accurate (with 8.10% improvement on PLCC metric for LSVQ 1080p ) than the existing fixed-feature-based paradigm.   Benchmarking FasterVQA. We also benchmark the variants of FasterVQA. The base version of FasterVQA achieves performance comparable to FAST-VQA while requiring 75% fewer FLOPs. As FAST-VQA and FasterVQA share the same network structure, the comparison proves the effectiveness of reducing temporal redundancy in VQA in general. The MS and MT versions of FasterVQA also show notably better performance than FAST-VQA-M, with up to 24% fewer FLOPs. FasterVQA-MT can be more competitive than the recently-published BVQA-TCSVT-2022 <ref type="bibr" target="#b12">[13]</ref> (existing stateof-the-art) in six of eight metrics, while up to 2,600? faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Efficiency</head><p>To benchmark efficiency, we compare the FLOPs and running times on CPU/GPU (average of ten runs per sample) of the proposed methods with existing approaches on different resolutions in Tab. 4. We also draw the respective performance-FLOPs curves in <ref type="figure" target="#fig_6">Fig. 6</ref>. Note that we remove video loading latency for all methods. Efficiency of base models. Even the base models of FAST-VQA and FasterVQA reach unprecedented efficiency. FAST-VQA reduces up to 210? FLOPs and 70? CPU running time than PVQ <ref type="bibr" target="#b6">[7]</ref> while obtaining notably better performance, while FasterVQA can reduce up to 840? FLOPs and 284? CPU running time. FasterVQA is also 3.3? faster than FAST-VQA and obviously faster than real-time. Efficiency of mobile-friendly variants. Prior to our submission, the fastest in-the-wild VQA method (including classical methods) on CPU with relatively good accuracy was the RAPIQUE [62] model with 17.3s CPU inference time. However, all three of our efficient versions can infer in less than one second on the Apple M1 CPU, which is the processor for several iPad modules. They enable the implementation of more accurate VQA methods on devices with limited computing resources, and we hope the proposed methods can help contribute to green computing on VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fine-tuning on Small Datasets</head><p>End-to-end Pre-train&amp;Fine-tune for VQA. With fragments, we are able to enable the pre-train&amp;fine-tune scheme for VQA with affordable computational resources, which pre-  trains on large VQA datasets to learn quality-related representations and fine-tunes on smaller datasets. This scheme is important as many VQA datasets <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> in specific scenarios are with much smaller scale than datasets for other video tasks <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> and it is relatively hard to learn robust quality representations on these small VQA datasets alone. Moreover, the following fine-tuning stage can also be done in an end-to-end manner, which allows the network to learn additional quality-related representations on videos out of the pre-training distributions. Results on public datasets. Practically, we use LSVQ as the large dataset and choose five small datasets representing diverse scenarios, including not only natural video datasets, i.e. LIVE-VQC (from real-world mobile photography, 240P-1080P) and KoNViD-1k (from online social media contents, all 540P), but also non-natural datasets: CVD2014 (lab-collected in-capture distortions, 480P-720P), LIVE-Qualcomm (lab-collected videos with specific degradations, all 1080P) and YouTube-UGC (user-generated contents, including computer-generated contents, 360P-2160P 2 ). We divide each dataset into random splits for 10 times and report the average result on the test splits. As Tab. 5 shows, with the pre-train&amp;fine-tune scheme, the proposed FAST-VQA and FasterVQA outperforms the existing state-of-thearts on all these five scenarios with a very large margin, while obtaining much higher efficiency. Note that YouTube-UGC contains 4K(2160P) videos with 600-frame long but even the FasterVQA still performs well.</p><p>2. The current available version of YouTube-UGC is incomplete and only with 1147 videos. The peer comparison is only for reference.  Results on ICME2021 UGC-VQA Challenge. We also evaluated the fine-tune performance of the proposed FAST-VQA on the ICME2021 UGC-VQA challenge <ref type="bibr" target="#b63">[64]</ref>, where the ground truths are hidden and all the methods are fairly evaluated by the challenge server. As shown in Tab. 6, while the top methods show very similar performance, FAST-VQA is notably better than all of them. As we are not able to pick our model on a hidden-GT database, the result further demonstrates the robustness of FAST-VQA with effective video quality representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Sampling Approaches</head><p>We specifically discuss the effects of the proposed sampling paradigm, quality-sensitive neighbourhood representatives, and the St-GMS (Sec. 3.2) scheme to get fragments. We first show the effectiveness of spatial GMS by comparing it to different spatial sampling variants (Tab. 7), and the effectiveness of unified St-GMS by comparing it to different temporal sampling variants (Tab. 8). We also discuss the sampling granularity ( <ref type="figure" target="#fig_7">Fig. 7)</ref> to support the general paradigm of selecting quality-sensitive neighbourhood representatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effects of GMS: in the spatial dimension</head><p>Comparing with resizing &amp; cropping. In Group 1 of Tab. 7, we compare the proposed fragments with spatial GMS with two common sampling approaches: bilinear resizing and random cropping. The proposed fragments are notably superior to  bilinear resizing on high-resolution (LSVQ 1080p ) (+4%) and cross-resolution (LIVE-VQC) scenarios (+4%). Fragments still lead to non-trivial 2% improvements over resizing on lower-resolution scenarios where the problems of resizing are not that severe. This proves that keeping local textures is vital for VQA. Fragments also largely outperform single random crops as well as ensembles of multiple crops, suggesting that retaining uniform global quality is also critical to VQA. We additionally compare with Swin-T's original inference samples for video recognition, resizing+cropping with three crops, which need 3? computational cost but still perform notably worse than fragments.</p><p>Comparing with spatial variants of fragments. We also compare with three variants of fragments in Tab. 7, Group 2. We prove the effectiveness of uniform grid partition by comparing with random mini-patches (ignore grids while sampling), and the importance of retaining contextual relations by comparing with shuffled mini-patches (sample minipatches in grids but shuffle them while splicing). The proposed GMS is markedly superior to both variants. Moreover, it shows much better performance than the variant without temporal alignment especially on high-resolution videos, indicating that preserving inter-frame temporal variations is necessary for fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effects of St-GMS: in the temporal dimension.</head><p>Comparing with uniform &amp; short-clip sampling. In Group 1 of Tab. 8, we compare the proposed spatial-temporal fragments with St-GMS in the temporal dimension with two prevalent temporal sampling strategies: sampling a short clip and uniform sampling. A short clip leads to a notable performance drop on KoNViD-1k <ref type="bibr" target="#b56">[57]</ref>, where a non-uniform sample is insufficient to account for the changing content over time. Uniform sampling lacks continuous frames and is especially inaccurate on LIVE-VQC <ref type="bibr" target="#b57">[58]</ref>, where inter-frame variations are very complicated. The proposed FasterVQA with St-GMS is representative and sensitive to temporal quality and performs better in a variety of situations.</p><p>Comparing with temporal variants of fragments. Similar to the spatial situation, we also discussed random (ignore segments while sampling) and shuffled mini-cubes. The results suggest that preserving contextual relations is still important in the temporal dimension and leads to a performance gap of around 1% across all datasets. However, the gap is notably smaller than in the spatial dimension, indicating that the temporal contextual relations may be less influential on quality than their spatial counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Discussion on Sampling Granularity</head><p>We sample the fragments based on the paradigm of qualitysensitive neighbourhood representatives, where we stress two important factors: 1) partitioned neighbourhoods (the more, the better representative); 2) continuous representatives (the larger, the better textural sensitivity). They have to be balanced during practical sampling. We discuss the two important factors by evaluating the spatial and temporal granularity of sampling given a fixed total sample size. Spatial Granularity: G f &amp;S f in GMS. We discuss different combinations of number of grids (G f ) and size of mini-patches (S f ) for GMS, including combinations that follow (solid curves) or not follow (dashed curves) the match constraint (Eq. 8). We notice that setting S f = 32 shows best performance and is better than smaller patches which gradually becomes insensitive to local textures and degenerates into resizing), or larger patches which gradually cedes to be representative to global quality and degenerates into cropping. (Results of cropping are in Tab. 7). Temporal Granularity: G t &amp;T f in St-GMS. We also discuss the combinations of number of G t and T f for St-GMS given the same total frames. As no temporal pooling is operated   in FANet, we only have the matched group, as shown in <ref type="figure" target="#fig_7">Fig. 7(b)</ref>. The T f = 4 shows best performance on both datasets which is comparable to dense temporal sampling (FAST-VQA), which follows our observation that a few continuous frames can be sensitive to temporal variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies II on FANet, Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effects of the Match Constraint</head><p>Effects of Appropriate Backbones. In the first part of our ablation studies on FANet, we discuss the effects of different backbone structures by dividing them into two groups: those with non-overlapping pooling layers and can comply with the match constraint (Swin-T, inflated ConvNeXt-Tiny) and others (I3D <ref type="bibr" target="#b24">[25]</ref> with ResNet-50 backbone under a modern initialization <ref type="bibr" target="#b67">[68]</ref>). The IP-NLR is included in all variants, while the GRPB is excluded as it is particularly designed for Swin-T. As shown in Tab. 9, the matched backbones are significantly more effective at processing fragments as inputs given similar computational cost, demonstrating our analysis for the match constraint (Eq. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Matching Mini-cubes with Pooling.</head><p>We further discuss the match constraint by comparing the spatial matched (solid lines) vs mis-matched mini-cubes (dashed lines) with the same backbone structure. As <ref type="figure" target="#fig_7">Fig. 7(a)</ref> shows, the non-matched combinations of pooling kernels and minicubes show notably worse performance in all situations, again proving the importance of the match constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effects of GRPB and IP-NLR</head><p>In the second part of the ablation studies on FANet, we analyze the effects of two novel modifications in it: the proposed Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) Head as in Tab. 10. We compare the IP-NLR with two variants: the linear regression layer and the non-linear regression layers with pooling before regression (PrePool). Both modules lead to non-negligible improvements especially on high-resolution (LSVQ 1080p ) or cross-resolution (LIVE-VQC) scenarios. As the discontinuity between mini-patches is more obvious in high-resolution videos, this result suggests that the corrected position biases and regression head are helpful on solving the problems caused by such discontinuity.    FasterVQA. We also observed that the intra-dataset performance of the state-of-the-art classical VQA approaches is comparable to that of our variants without semantic pretraining. The results indicate the significant influence of semantics in VQA and suggest that there might exist an accuracy limit of all semantic-blind VQA methods. This further proves that semantic-aware deep VQA methods are irreplaceable, while FAST-VQA and FasterVQA fill in the blanks on improving their practical efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation on High-resolution Videos</head><p>As the base version of FAST-VQA only samples 5.44% and 2.42% spatial information from 720P and 1080P videos, respectively, it is worthwhile to evaluate its performance on high-resolution videos. We use two existing databases with 1080P videos: for cross-resolution LIVE-VQC, we split the videos according to their resolutions and test the performance of different variants; for LSVQ 1080p , we create variants by downsampling its 1080P videos before sampling fragments and compare between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Performance on Split Resolutions</head><p>We divide the cross-resolution VQA benchmark set LIVE-VQC into three resolution groups: (A) 1080P (110 videos); (B) 720P (316 videos); and (C) ?540P (159 videos) to evaluate the performance of FAST-VQA on different resolutions in comparison to other variants. As shown in Tab. 14, the proposed FAST-VQA achieves good performance on all resolution groups (?0.80 SRCC&amp;PLCC), with the most superior improvement over other variants on Group (A) with 1080P high-resolution videos, proving that FAST-VQA is robust and reliable on videos with different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Impacts of Video Downsampling</head><p>To demonstrate that keeping the raw-resolution textures is crucial in sampling fragments, we evaluate the proposed FAST-VQA/FasterVQA with multiple downsampled variants of LSVQ 1080p dataset. We resize these 1080P highresolution videos into 540P(2X?), 360P(3X?), 270P(4X?) and sample fragments from the resized videos. As shown in <ref type="figure">Fig. 8</ref>, although downsampling before sampling can preserve more information from these videos, the overall effect still significantly degrades the final accuracy, proving that keeping the original resolution is crucial to quality sensitivity. As the model is only trained on videos ?720P, the result further reveals the general importance of textures on different resolutions of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Stability and Reliability Analysis</head><p>Due to the randomness of fragment sampling, the proposed FAST-VQA may produce varying predictions for the same video. Therefore, we measure the stability and reliability of single random sampling in FAST-VQA using two metrics: 1) the assessment stability of multiple single samplings on the same video; 2) the relative accuracy of single sampling compared with multiple sample ensemble. As shown in Tab. 15, the normalized std. dev. of different sampling on the same video is only around 0.01, indicating that the sampled fragments are enough for making highly stable predictions. Compared with a six-sample ensemble, sampling only once can be 99.40% as accurate even on the pure high-resolution test set (LSVQ 1080P ). They prove that a single sample of fragments is sufficiently stable and reliable for quality assessment even though only a small proportion of information is kept during sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Visualizations of Local Quality Maps</head><p>The proposed IP-NLR head with patch-wise independent quality regression not only improves the performance of the proposed method but also enables the generation of spatial-temporal local quality maps as <ref type="bibr" target="#b6">[7]</ref> does. These quality maps allow us to qualitatively evaluate what can be  <ref type="figure">Fig. 9</ref>. Spatial-temporal patch-wise local quality maps, where red areas refer to low predicted quality and green areas refer to high predicted quality. This sample video is a 1080P video from LIVE-VQC <ref type="bibr" target="#b57">[58]</ref> dataset. Zoom in for clearer view.</p><p>learned during the end-to-end training for FAST-VQA. We show the patch-wise local quality maps and the re-projected frame quality maps for a 1080P video (from LIVE-VQC <ref type="bibr" target="#b57">[58]</ref> dataset) in <ref type="figure">Fig. 9</ref>. As the patch-wise quality maps and reprojected quality maps in <ref type="figure" target="#fig_1">Fig. 9 (column 2&amp;4)</ref> shows, FAST-VQA is sensitive to textural quality information and distinguishes between clear (Frame 0) and blurry textures (Frame 12/24). It demonstrates that FAST-VQA with fragments (column 3) as input is sensitive to local texture quality. Furthermore, the qualities of the action-related areas are notably different from those of the background areas, showing that FAST-VQA effectively learns the global contextual relations. It is aware of and influenced by semantic information in the video, thereby demonstrating our aforementioned claims. More visualizations of local quality maps are presented in our GitHub page, together with codes and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we have discussed sampling for video quality assessment (VQA) in order to tackle the difficulties as a result of high computing and memory requirements when evaluating high-resolution videos. We propose the principle of quality-sensitive neighbourhood representatives and conduct extensive experiments to demonstrate that the proposed samples, fragments, are effective samples for VQA that retain quality information in videos better than naive sampling approaches. Based on fragments, the proposed end-to-end FAST-VQA and FasterVQA refreshed state-ofthe-arts on all in-the-wild VQA benchmarks with up to 1612? efficiency than the existing state-of-the-art. The proposed methods can bring deep VQA methods into practical use regardless of video resolution or length. In our future work, we would like to further improve specific network structures with insights from the match constraint and design more effective sampling approaches based on the principle of quality-sensitive neighbourhood representatives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Inference cost (FLOPs, running time) and training memory cost of a vanilla ResNet-50 on a full 1080P, 10-second-long video (without any sampling), compared with our methods (FAST-VQA/FasterVQA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fragments, in spatial view (compared with resizing and cropping) (a) and temporal view (b). Zoom-in views of mini-patches show that fragments can retain spatial local quality information (a), and spot temporal variations such as shaking across frames (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The pipeline for sampling fragments with Spatial-temporal Grid Mini-Cube Sampling (St-GMS, Sec. 3.2), including spatial (a, discussed in Sec. 3.2.1) and temporal (b, discussed in Sec. 3.2.2) sampling operations. The sampled fragments are fed into the FANet(Fig. 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Motivation for GRPB: Distinguishing cross-patch &amp; intra-patch pixel pairs in Window Self-AttentionSelf-attention window (c) Motivation for IP-NLR Head: Qualities are diverse between mini-cubes colors denote they come from different mini-patches/mini-cubes.(a) the Match Constraint:Pooling kernels across mini-cubes are "bad" Motivation for the match constraint (a) and two proposed modules in FANet: (b) Gated Relative Position Biases (GRPB); (c) Intra-Patch Non-Linear Regression (IP-NLR) head. The structure for the whole FANet is illustrated inFig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 G</head><label>1</label><figDesc>Gated Relative Position Biases (GRPB) (a) Hierarchical Swin-T Backbone with GRPB (c) Intra-Patch Non-Linear Regression (IP-NLR) G(i, j) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The overall framework for FANet, including the Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) modules. The fragments come from Grid Mini-patch Sampling (for FAST-VQA) or Spatial-temporal Grid Mini-cube Sampling (for FasterVQA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FasterVQAFig. 6 .</head><label>6</label><figDesc>curve on cross-resolution videos (LIVE-VQC) The Performance-FLOPs curve of proposed FAST-VQA / Faster-VQA and baseline methods. X-Axis: GFLOPs (log scale); Y-Axis: PLCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Discussion on the spatial (in spatial-only GMS) and temporal (in St-GMS) sampling granularity. The dashed lines are for mis-matched combinations, with notably worse performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, C. Chen, L. Liao are with S-Lab, Nanyang Technological University (NTU), Singapore (email: haoning001@e.ntu.edu.sg, [chaofeng.chen, liang.liao]@ntu.edu.sg).</figDesc><table><row><cell cols="4">FLOPs (CPU running time) during Inference</cell><cell cols="5">End-to-end Training Memory Cost</cell></row><row><cell>FasterVQA (Ours) FAST-VQA (Ours) ResNet-50</cell><cell>279 (2.7s) 69 (8.7s)</cell><cell cols="2">1,750 Unit: GFLOPs (466s) 40919</cell><cell cols="2">Ours (1) Ours (16) ResNet-50 (1)</cell><cell>2.7</cell><cell>25.8</cell><cell>Tesla A100 (80GB) 100 Unit: GB 217</cell></row><row><cell>0.8</cell><cell cols="2">FAST-VQA</cell><cell>FasterVQA</cell><cell>0.84</cell><cell></cell><cell></cell><cell>FAST-VQA</cell><cell>FasterVQA</cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1X</cell><cell>2X</cell><cell>3X</cell><cell cols="2">4X</cell><cell>1X</cell><cell></cell><cell>2X</cell><cell>3X</cell><cell>4X</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Downsample</cell><cell></cell><cell></cell><cell></cell><cell>Downsample</cell></row><row><cell></cell><cell></cell><cell>SRCC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PLCC</cell></row></table><note>? J. Hou and W. Lin are with School of Computer Science and En- gineering, Nanyang Technological University (NTU), Singapore (jing- wen003@e.ntu.edu.sg, wslin@ntu.edu.sg).? W. Sun, Q. Yan and J. Gu are with Tetras. AI and Sensetime Research ([sunwx, yanqiong]@tetras.ai, gujinwei@sensebrain.ai).? Corresponding author: Weisi Lin.Preprint Edition. Under Review.*batch size noted in parenthesis.*Ours include both FAST-VQA and FasterVQA (same training cost).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>video 5 Score=38.24 Severely Shaking Video Original Frames video 6 Score=74.54 Relatively Stable Shot</head><label></label><figDesc>have adopted resiz-arXiv:2210.05357v1 [cs.CV] 11 Oct 2022 Spatial View of fragments, in comparison with samples via Resizing or Cropping</figDesc><table><row><cell>fragments</cell><cell></cell></row><row><cell>Score=37.38</cell><cell></cell></row><row><cell>video 1</cell><cell></cell></row><row><cell>Score=66.58</cell><cell></cell></row><row><cell>video 2</cell><cell></cell></row><row><cell>Score=71.71</cell><cell></cell></row><row><cell>video 3</cell><cell></cell></row><row><cell>Score=86.33</cell><cell></cell></row><row><cell>video 4</cell><cell></cell></row><row><cell>(a)</cell><cell>(b) Temporal View of fragments</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Variants for FAST-VQA with GMS sampling. Both variants require 4 clips at inference to cover whole video.</figDesc><table><row><cell>Methods</cell><cell>Number of Frames (T )</cell><cell>Size of Mini-patch (Sf , Sf )</cell><cell>Number of Grids (Gf )</cell><cell>Window Size in FANet</cell><cell>FLOPs (Infer)</cell></row><row><cell>FAST-VQA</cell><cell>32</cell><cell>(32, 32)</cell><cell>7</cell><cell>(8, 7, 7)</cell><cell>279G</cell></row><row><cell>FAST-VQA-M</cell><cell>16</cell><cell>(32, 32)</cell><cell>4</cell><cell>(4, 4, 4)</cell><cell>46G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Inference variants for FasterVQA with St-GMS via AMI. FasterVQA), as listed in Tab. 2. All S f and T f are selected to follow the match constraint (Eq. 8). The ? in Eq. 16</figDesc><table><row><cell>Methods</cell><cell>Size of Mini-Cube (T f , S f , S f )</cell><cell>Segments and Grids (Gt, Gs, Gs)</cell><cell>Rescaled Window Size in FANet (? )</cell><cell>FLOPs (Infer)</cell></row><row><cell>FasterVQA</cell><cell>(4, 32, 32)</cell><cell>(8, 7, 7)</cell><cell>(8, 7, 7)</cell><cell>69G</cell></row><row><cell>FasterVQA-MT</cell><cell>(4, 32, 32)</cell><cell>(4, 7, 7)</cell><cell>(4, 7, 7)</cell><cell>35G</cell></row><row><cell>FasterVQA-MS</cell><cell>(4, 32, 32)</cell><cell>(8, 5, 5)</cell><cell>(8, 5, 5)</cell><cell>36G</cell></row><row><cell cols="5">scales with reduced size on either spatial (FasterVQA-MS)</cell></row><row><cell cols="5">or temporal (FasterVQA-MT) dimensions together with the</cell></row><row><cell>base scale (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Comparison with existing methods (classical and deep) and our baseline (Full-res. Swin-T feat.). The 1st/2nd/3rd best scores are colored in red, blue and boldface, respectively. We infer FasterVQA with multiple scales via AMI.</figDesc><table><row><cell>Type/</cell><cell></cell><cell>FLOPs on 1080P/8-sec</cell><cell></cell><cell cols="2">Intra-dataset Test Sets</cell><cell></cell><cell></cell><cell cols="2">Cross-dataset Test Sets</cell><cell></cell></row><row><cell>Testing Set/</cell><cell></cell><cell></cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Groups</cell><cell>Methods</cell><cell>relative to FAST-VQA</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell></row><row><cell>Existing Classical</cell><cell>BRISQUE [35] TLVQM [3] VIDEVAL [4]</cell><cell>NA NA NA</cell><cell>0.569 0.772 0.794</cell><cell>0.576 0.774 0.783</cell><cell>0.497 0.589 0.545</cell><cell>0.531 0.616 0.554</cell><cell>0.646 0.732 0.751</cell><cell>0.647 0.724 0.741</cell><cell>0.524 0.670 0.630</cell><cell>0.536 0.691 0.640</cell></row><row><cell>Existing Fixed Deep</cell><cell>VSFA [5] PVQ wo/ patch [7] PVQ w/ patch [7]</cell><cell>147? 210? 210?</cell><cell>0.801 0.814 0.827</cell><cell>0.796 0.816 0.828</cell><cell>0.675 0.686 0.711</cell><cell>0.704 0.708 0.739</cell><cell>0.784 0.781 0.791</cell><cell>0.794 0.781 0.795</cell><cell>0.734 0.747 0.770</cell><cell>0.772 0.776 0.807</cell></row><row><cell></cell><cell>BVQA-TCSVT-2022 [13]</cell><cell>403?</cell><cell>0.852</cell><cell>0.854</cell><cell>0.771</cell><cell>0.782</cell><cell>0.834</cell><cell>0.837</cell><cell>0.816</cell><cell>0.824</cell></row><row><cell cols="2">Full-res Swin-T [24] feat., 32 ? 4 frames</cell><cell>42.5?</cell><cell>0.835</cell><cell>0.833</cell><cell>0.739</cell><cell>0.753</cell><cell>0.825</cell><cell>0.828</cell><cell>0.794</cell><cell>0.809</cell></row><row><cell>Ours,</cell><cell>FAST-VQA-M</cell><cell>0.165?</cell><cell>0.852</cell><cell>0.854</cell><cell>0.739</cell><cell>0.773</cell><cell>0.841</cell><cell>0.832</cell><cell>0.788</cell><cell>0.810</cell></row><row><cell>higher</cell><cell>FasterVQA-MS (AMI)</cell><cell>0.130?</cell><cell>0.846</cell><cell>0.850</cell><cell>0.758</cell><cell>0.798</cell><cell>0.852</cell><cell>0.854</cell><cell>0.791</cell><cell>0.818</cell></row><row><cell>efficiency</cell><cell>FasterVQA-MT (AMI)</cell><cell>0.125?</cell><cell>0.860</cell><cell>0.861</cell><cell>0.753</cell><cell>0.791</cell><cell>0.846</cell><cell>0.849</cell><cell>0.803</cell><cell>0.826</cell></row><row><cell>Ours,</cell><cell>FAST-VQA</cell><cell>1?</cell><cell>0.876</cell><cell>0.877</cell><cell>0.779</cell><cell>0.814</cell><cell>0.859</cell><cell>0.855</cell><cell>0.823</cell><cell>0.844</cell></row><row><cell>Accuracy</cell><cell>FasterVQA</cell><cell>0.25?</cell><cell>0.873</cell><cell>0.874</cell><cell>0.772</cell><cell>0.811</cell><cell>0.863</cell><cell>0.863</cell><cell>0.813</cell><cell>0.837</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 FLOPs</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>540P</cell><cell></cell><cell></cell><cell>720P</cell><cell></cell><cell></cell><cell>1080P</cell><cell></cell></row><row><cell>Method</cell><cell>FLOPs(G)</cell><cell cols="2">Time(GPU/s) Time(CPU/s)</cell><cell>FLOPs(G)</cell><cell cols="2">Time(GPU/s) Time(CPU/s)</cell><cell>FLOPs(G)</cell><cell cols="2">Time(GPU/s) Time(CPU/s)</cell></row><row><cell>VSFA [5]</cell><cell>1024936.7?</cell><cell>2.603</cell><cell>152.4</cell><cell>1818465.2?</cell><cell>3.571</cell><cell>233.9</cell><cell>40919147?</cell><cell>11.14</cell><cell>465.6</cell></row><row><cell>PVQ [7]</cell><cell>1464652.5?</cell><cell>3.091</cell><cell>149.5</cell><cell>2202979.0?</cell><cell>4.143</cell><cell>247.8</cell><cell>58501210?</cell><cell>13.79</cell><cell>538.4</cell></row><row><cell>BVQA-TCSVT-2022 [13]</cell><cell>28176101?</cell><cell>5.392</cell><cell>378.3</cell><cell>50184180?</cell><cell>10.83</cell><cell>592.1</cell><cell>112537403?</cell><cell>27.64</cell><cell>1567</cell></row><row><cell>Full-res Swin-T [24] feat.</cell><cell>303210.9?</cell><cell>3.226</cell><cell>102.0</cell><cell>535719.2?</cell><cell>5.049</cell><cell>166.2</cell><cell>1185242.5?</cell><cell>8.753</cell><cell>234.9</cell></row><row><cell>FAST-VQA (Ours)</cell><cell>2791?</cell><cell>0.044</cell><cell>8.839</cell><cell>2791?</cell><cell>0.043</cell><cell>8.930</cell><cell>2791?</cell><cell>0.045</cell><cell>8.678</cell></row><row><cell>FasterVQA (Ours)</cell><cell>690.25?</cell><cell>0.023</cell><cell>2.754</cell><cell>690.25?</cell><cell>0.022</cell><cell>2.732</cell><cell>690.25?</cell><cell>0.023</cell><cell>2.697</cell></row><row><cell>FAST-VQA-M (Ours)</cell><cell>460.165?</cell><cell>0.019</cell><cell>0.598</cell><cell>460.165?</cell><cell>0.019</cell><cell>0.633</cell><cell>460.165?</cell><cell>0.019</cell><cell>0.602</cell></row><row><cell>FasterVQA-MS (Ours)</cell><cell>360.130?</cell><cell>0.016</cell><cell>0.594</cell><cell>360.130?</cell><cell>0.018</cell><cell>0.587</cell><cell>360.130?</cell><cell>0.018</cell><cell>0.609</cell></row><row><cell>FasterVQA-MT (Ours)</cell><cell>350.125?</cell><cell>0.018</cell><cell>0.647</cell><cell>350.125?</cell><cell>0.020</cell><cell>0.621</cell><cell>350.125?</cell><cell>0.017</cell><cell>0.645</cell></row></table><note>and running time (avg. of 20 runs) on GPU Server (Tesla V100) and CPU (Apple M1) comparison of FAST-VQA, state-of-the-art methods and our baseline on 8-sec videos different resolutions. We boldface FLOPs ? 500G, green FLOPs ? 100G and running time ? 1s.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>The finetune results on LIVE-VQC, KoNViD, CVD2014, LIVE-Qualcomm and YouTube-UGC datasets, compared with existing classical and fixed-backbone deep VQA methods, and ensemble of classical (C) and deep (D) branches.</figDesc><table><row><cell cols="2">Finetune Dataset/</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">CVD2014</cell><cell cols="2">LIVE-Qualcomm</cell><cell cols="2">YouTube-UGC</cell></row><row><cell cols="2">resolution range in the dataset</cell><cell cols="2">(240P -1080P)</cell><cell cols="2">(540P)</cell><cell cols="2">(480P -720P)</cell><cell cols="2">(1080P)</cell><cell cols="2">(360P -2160P(4K))</cell></row><row><cell>Groups</cell><cell>Methods</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell></row><row><cell>Existing Classical</cell><cell>TLVQM [3] VIDEVAL [4] RAPIQUE [62]</cell><cell>0.799 0.752 0.755</cell><cell>0.803 0.751 0.786</cell><cell>0.773 0.783 0.803</cell><cell>0.768 0.780 0.817</cell><cell>0.83 NA NA</cell><cell>0.85 NA NA</cell><cell>0.77 NA NA</cell><cell>0.81 NA NA</cell><cell>0.669 0.779 0.759</cell><cell>0.659 0.773 0.768</cell></row><row><cell></cell><cell>VSFA [5]</cell><cell>0.773</cell><cell>0.795</cell><cell>0.773</cell><cell>0.775</cell><cell>0.870</cell><cell>0.868</cell><cell>0.737</cell><cell>0.732</cell><cell>0.724</cell><cell>0.743</cell></row><row><cell>Existing</cell><cell>PVQ [7]</cell><cell>0.827</cell><cell>0.837</cell><cell>0.791</cell><cell>0.786</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Fixed</cell><cell>GST-VQA [42]</cell><cell>NA</cell><cell>NA</cell><cell>0.814</cell><cell>0.825</cell><cell>0.831</cell><cell>0.844</cell><cell>0.801</cell><cell>0.825</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Deep</cell><cell>CoINVQ [63]</cell><cell>NA</cell><cell>NA</cell><cell>0.767</cell><cell>0.764</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>0.816</cell><cell>0.802</cell></row><row><cell></cell><cell>BVQA-TCSVT-2022 [13]</cell><cell>0.831</cell><cell>0.842</cell><cell>0.834</cell><cell>0.836</cell><cell>0.872</cell><cell>0.869</cell><cell>0.817</cell><cell>0.828</cell><cell>0.831</cell><cell>0.819</cell></row><row><cell>Ensemble</cell><cell>CNN+TLVQM [10]</cell><cell>0.825</cell><cell>0.834</cell><cell>0.816</cell><cell>0.818</cell><cell>0.863</cell><cell>0.880</cell><cell>0.810</cell><cell>0.833</cell><cell>NA</cell><cell>NA</cell></row><row><cell>C+D</cell><cell>CNN+VIDEVAL [4]</cell><cell>0.785</cell><cell>0.810</cell><cell>0.815</cell><cell>0.817</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>0.808</cell><cell>0.803</cell></row><row><cell cols="2">Full-res Swin-T [24] feat.</cell><cell>0.799</cell><cell>0.808</cell><cell>0.841</cell><cell>0.838</cell><cell>0.868</cell><cell>0.870</cell><cell>0.788</cell><cell>0.803</cell><cell>0.798</cell><cell>0.796</cell></row><row><cell cols="2">FAST-VQA-M (Ours)</cell><cell>0.803</cell><cell>0.828</cell><cell>0.873</cell><cell>0.872</cell><cell>0.877</cell><cell>0.892</cell><cell>0.804</cell><cell>0.838</cell><cell>0.768</cell><cell>0.765</cell></row><row><cell cols="2">standard deviation</cell><cell>?.031</cell><cell>?.030</cell><cell>?.012</cell><cell>?.012</cell><cell>?.035</cell><cell>?.019</cell><cell>?.039</cell><cell>?.026</cell><cell>?.019</cell><cell>?.022</cell></row><row><cell cols="2">FAST-VQA (ours)</cell><cell>0.849</cell><cell>0.865</cell><cell>0.891</cell><cell>0.892</cell><cell>0.891</cell><cell>0.903</cell><cell>0.819</cell><cell>0.851</cell><cell>0.855</cell><cell>0.852</cell></row><row><cell cols="2">standard deviation</cell><cell>?.024</cell><cell>?.019</cell><cell>?.008</cell><cell>?.008</cell><cell>?.030</cell><cell>?.019</cell><cell>?.036</cell><cell>?.024</cell><cell>?.008</cell><cell>?.011</cell></row><row><cell cols="2">FasterVQA (ours) with 4X efficiency than FAST-VQA</cell><cell>0.843</cell><cell>0.858</cell><cell>0.895</cell><cell>0.898</cell><cell>0.896</cell><cell>0.904</cell><cell>0.826</cell><cell>0.844</cell><cell>0.863</cell><cell>0.859</cell></row><row><cell cols="2">standard deviation</cell><cell>?.032</cell><cell>?.027</cell><cell>?.010</cell><cell>?.010</cell><cell>?.029</cell><cell>?.018</cell><cell>?.038</cell><cell>?.027</cell><cell>?.014</cell><cell>?.017</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Comparsion on ICME2021 UGC-VQA Challenge<ref type="bibr" target="#b63">[64]</ref> (Test Set). The results are evaluated by the leaderboard.</figDesc><table><row><cell>Methods</cell><cell cols="2">Challenge Rank SRCC PLCC KRCC RMSE</cell></row><row><cell>QA-FTE</cell><cell>1</cell><cell>0.9477 0.9831 0.8127 0.2251</cell></row><row><cell>GVSP</cell><cell>2</cell><cell>0.9472 0.9809 0.8097 0.2389</cell></row><row><cell>FMISZU</cell><cell>3</cell><cell>0.9471 0.9800 0.8078 0.2441</cell></row><row><cell>CENSEO</cell><cell>4</cell><cell>0.9428 0.9802 0.8020 0.2432</cell></row><row><cell>FAST-VQA (Ours)</cell><cell>-</cell><cell>0.9552 0.9878 0.8266 0.1929</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7</head><label>7</label><figDesc>Ablation study for GMS in spatial dimension: comparison with naive approaches and variants.</figDesc><table><row><cell>Testing Set/</cell><cell></cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Video Resolutions</cell><cell></cell><cell cols="2">240p to 720p</cell><cell cols="2">1080p</cell><cell cols="2">540p</cell><cell cols="2">240p to 1080p</cell></row><row><cell>Methods/Metric</cell><cell cols="9">Relative FLOPs SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell>Group 1: Naive Sampling Approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bilinear resizing</cell><cell>1?</cell><cell>0.857</cell><cell>0.859</cell><cell>0.752</cell><cell>0.786</cell><cell>0.841</cell><cell>0.840</cell><cell>0.772</cell><cell>0.814</cell></row><row><cell>random cropping</cell><cell>1?</cell><cell>0.807</cell><cell>0.812</cell><cell>0.643</cell><cell>0.677</cell><cell>0.734</cell><cell>0.776</cell><cell>0.740</cell><cell>0.773</cell></row><row><cell>-test with 3 crops</cell><cell>3?</cell><cell>0.838</cell><cell>0.835</cell><cell>0.727</cell><cell>0.754</cell><cell>0.841</cell><cell>0.827</cell><cell>0.785</cell><cell>0.809</cell></row><row><cell>-test with 6 crops</cell><cell>6?</cell><cell>0.843</cell><cell>0.844</cell><cell>0.734</cell><cell>0.761</cell><cell>0.845</cell><cell>0.834</cell><cell>0.796</cell><cell>0.817</cell></row><row><cell>resizing+cropping with 3 crops, as in [24]</cell><cell>3?</cell><cell>0.860</cell><cell>0.862</cell><cell>0.758</cell><cell>0.793</cell><cell>0.845</cell><cell>0.846</cell><cell>0.783</cell><cell>0.817</cell></row><row><cell cols="2">Group 2: Variants of fragments in the spatial dimension</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>random mini-patches</cell><cell>1?</cell><cell>0.857</cell><cell>0.861</cell><cell>0.754</cell><cell>0.790</cell><cell>0.844</cell><cell>0.845</cell><cell>0.792</cell><cell>0.818</cell></row><row><cell>shuffled mini-patches</cell><cell>1?</cell><cell>0.858</cell><cell>0.863</cell><cell>0.761</cell><cell>0.799</cell><cell>0.849</cell><cell>0.847</cell><cell>0.796</cell><cell>0.821</cell></row><row><cell>w/o temporal alignment</cell><cell>1?</cell><cell>0.850</cell><cell>0.853</cell><cell>0.736</cell><cell>0.779</cell><cell>0.823</cell><cell>0.816</cell><cell>0.764</cell><cell>0.802</cell></row><row><cell>GMS (FAST-VQA, Ours)</cell><cell>1?</cell><cell>0.876</cell><cell>0.877</cell><cell>0.779</cell><cell>0.814</cell><cell>0.859</cell><cell>0.855</cell><cell>0.823</cell><cell>0.844</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8</head><label>8</label><figDesc>Ablation study for St-GMS on the temporal dimension: comparison with naive approaches and variants.</figDesc><table><row><cell>Testing Set/</cell><cell></cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Inter-frame Variations</cell><cell></cell><cell cols="2">weak to medium</cell><cell cols="2">medium</cell><cell cols="2">weak</cell><cell cols="2">strong</cell></row><row><cell>Temporal Content Changes</cell><cell></cell><cell cols="2">medium</cell><cell cols="2">medium</cell><cell cols="2">strong</cell><cell cols="2">weak</cell></row><row><cell>Methods/Metric</cell><cell cols="2">Relative FLOPs SRCC</cell><cell>PLCC</cell><cell cols="6">SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell>Group 1: Naive Sampling Approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sampling a continuous short clip</cell><cell>0.25?</cell><cell>0.853</cell><cell>0.856</cell><cell>0.750</cell><cell>0.785</cell><cell>0.833</cell><cell>0.834</cell><cell>0.782</cell><cell>0.812</cell></row><row><cell>uniform sampling (sparse, no continuous frames)</cell><cell>0.25?</cell><cell>0.859</cell><cell>0.858</cell><cell>0.753</cell><cell>0.790</cell><cell>0.843</cell><cell>0.842</cell><cell>0.774</cell><cell>0.808</cell></row><row><cell cols="2">Group 2: Variants of fragments in the temporal dimension</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>temporally random mini-cubes</cell><cell>0.25?</cell><cell>0.865</cell><cell>0.866</cell><cell>0.758</cell><cell>0.797</cell><cell>0.851</cell><cell>0.852</cell><cell>0.803</cell><cell>0.827</cell></row><row><cell>temporally shuffled mini-cubes</cell><cell>0.25?</cell><cell>0.864</cell><cell>0.866</cell><cell>0.756</cell><cell>0.793</cell><cell>0.853</cell><cell>0.854</cell><cell>0.807</cell><cell>0.828</cell></row><row><cell>St-GMS (FasterVQA, ours)</cell><cell>0.25?</cell><cell>0.873</cell><cell>0.874</cell><cell>0.772</cell><cell>0.811</cell><cell>0.864</cell><cell>0.863</cell><cell>0.813</cell><cell>0.837</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9</head><label>9</label><figDesc>Ablation study on backbones: networks that follow the Match Constraint are significantly better. All backbones have similar FLOPs (&lt;300G).</figDesc><table><row><cell>Testing Set/</cell><cell>LSVQtest</cell><cell>LSVQ1080p</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell>Variants/Metric</cell><cell cols="4">SRCC/PLCC SRCC/PLCC SRCC/PLCC SRCC/PLCC</cell></row><row><cell cols="4">"non-matched" backbone (with overlapping pooling kernels):</cell><cell></cell></row><row><cell>I3D-ResNet-50</cell><cell>0.847/0.846</cell><cell>0.717/0.764</cell><cell>0.828/0.829</cell><cell>0.776/0.808</cell></row><row><cell cols="4">"matched" backbones (with non-overlapping pooling kernels):</cell><cell></cell></row><row><cell>ConvNext-Tiny</cell><cell>0.869/0.870</cell><cell>0.765/0.802</cell><cell>0.851/0.852</cell><cell>0.811/0.833</cell></row><row><cell cols="2">Swin-T (w/o GRPB) 0.873/0.872</cell><cell>0.769/0.805</cell><cell>0.854/0.853</cell><cell>0.808/0.832</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 10</head><label>10</label><figDesc>Ablation study on GRPB and IP-NLR.</figDesc><table><row><cell>Testing Set/</cell><cell>LSVQtest</cell><cell>LSVQ1080p</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell>Variants/Metric</cell><cell cols="4">SRCC/PLCC SRCC/PLCC SRCC/PLCC SRCC/PLCC</cell></row><row><cell>Variants of GRPB:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o GRPB (baseline)</cell><cell>0.873/0.872</cell><cell>0.769/0.805</cell><cell>0.854/0.853</cell><cell>0.808/0.832</cell></row><row><cell cols="2">GRPB on Layers 1&amp;2 0.873/0.875</cell><cell>0.772/0.809</cell><cell>0.856/0.851</cell><cell>0.812/0.838</cell></row><row><cell>remove T pseudo</cell><cell>0.868/0.869</cell><cell>0.763/0.802</cell><cell>0.849/0.847</cell><cell>0.806/0.831</cell></row><row><cell>Variants of IP-NLR:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>linear (baseline)</cell><cell>0.872/0.873</cell><cell>0.768/0.803</cell><cell>0.847 /0.849</cell><cell>0.810/0.835</cell></row><row><cell>non-linear, pool-first</cell><cell>0.873/0.874</cell><cell>0.771/0.805</cell><cell>0.851/0.850</cell><cell>0.813/0.834</cell></row><row><cell>FANet (ours)</cell><cell>0.876/0.877</cell><cell>0.779/0.814</cell><cell>0.859/0.855</cell><cell>0.823/0.844</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11</head><label>11</label><figDesc>Ablation study on the Adaptive Multi-scale Inference (AMI) to help inference on different scales.</figDesc><table><row><cell>Testing Set/</cell><cell>LSVQtest</cell><cell>LSVQ1080p</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell cols="2">Variants of FasterVQA-MS:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>without AMI</cell><cell>0.838/0.844</cell><cell>0.739/0.772</cell><cell>0.845/0.842</cell><cell>0.782/0.807</cell></row><row><cell>with AMI</cell><cell>0.846/0.850</cell><cell>0.758/0.798</cell><cell>0.852/0.854</cell><cell>0.791/0.818</cell></row><row><cell cols="2">Variants of FasterVQA-MT:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>without AMI</cell><cell>0.853/0.854</cell><cell>0.746/0.782</cell><cell>0.841/0.838</cell><cell>0.782/0.811</cell></row><row><cell>with AMI</cell><cell>0.861/0.860</cell><cell>0.753/0.791</cell><cell>0.846/0.849</cell><cell>0.803/0.826</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>4.4.3 Effects of Adaptive Multi-scale Inference (AMI)In the third part, we evaluate the importance of Adapive Multi-scale Inference (AMI) to allow inference of FasterVQA on different scales with only training on one base scale. In Tab. 11, we evaluate the inference accuracy on MT and MS scales with or without AMI. The results have demonstrated the effectiveness of AMI, which allows robust inference on multiple scales for different test sets. The large-scale pre-training contributes to the performance by up to 11%, and are especially effective on cross-resolution scenarios, e.g. LIVE-VQC and YouTube-UGC. The end-to-end fine-tune also lead to up to 8% improvements, especially on non-natural videos (CVD2014, LIVE-Qualcomm, YouTube-UGC) which may contain specific quality-related issues. Both stages are undoubtedly effective and made affordable via the proposed fragments. In our discussions in Sec. 3.2.2, one question remains unclear: can the fragments retain aware to semantic video contents that can still be recognized by deep neural networks? This can hardly be answered as for a 10-sec-long 720P video, fragments sampled by St-GMS contain only 0.58% original information. Thus, we measure the ability by experiments: we use fragments as classification inputs for videos in Kinetics-400<ref type="bibr" target="#b55">[56]</ref> action recognition dataset, and the results prove that simply finetuning the Swin-T backbone with fragments can reach 68.6% top-1 accuracy (87.4 % relative to original Swin-T which needs 12 samples and requires 12? FLOPs) and 88.7% top-5 accuracy (94.8% relative to original), which has been on par with several deep VQA approaches under similar computational cost. The absolute accuracy also suggests that the fragments still contain rough scene-level semantics and can be recognized by the backbone in FANet.</figDesc><table><row><cell>4.4.4 Effects of End-to-end Pre-train&amp;Fine-tune Scheme</cell></row><row><cell>We discuss the effects of pre-train&amp;fine-tune scheme</cell></row><row><cell>(Sec. 4.2.3) in Tab. 12 in comparison with direct training</cell></row><row><cell>on these small datasets (w/o end-to-end pre-train) and only</cell></row><row><cell>linear regression on pre-trained features (w/o end-to-end</cell></row><row><cell>finetune).</cell></row></table><note>4.5 Role of Semantics in FAST-VQA/FasterVQA Can fragments preserve semantics?Effects of Semantic Pre-training. We further discuss the significance of semantic pre-training by training FAST- VQA/FasterVQA models from scratch (w/o semantics) as their semantic-blind variants, and the proposed models are regarded as semantic-aware (w/ semantics) variants based on discussions above. As shown in Tab. 13, semantic pre- training has significantly contributed to the performance on FAST-VQA (avg. 8%) and FasterVQA (avg. 10%), especially</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 12</head><label>12</label><figDesc>Effects of end-to-end pre-training and fine-tuning processes on downstream small VQA datasets.</figDesc><table><row><cell>Finetune Dataset/</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">CVD2014</cell><cell cols="2">LIVE-Qualcomm</cell><cell cols="2">YouTube-UGC</cell></row><row><cell>Metric</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell><cell>SRCC</cell><cell>PLCC</cell></row><row><cell>w/o end-to-end pre-train</cell><cell>0.765</cell><cell>0.782</cell><cell>0.842</cell><cell>0.844</cell><cell>0.871</cell><cell>0.888</cell><cell>0.756</cell><cell>0.778</cell><cell>0.794</cell><cell>0.784</cell></row><row><cell>w/o end-to-end fine-tune</cell><cell>0.818</cell><cell>0.838</cell><cell>0.869</cell><cell>0.868</cell><cell>0.822</cell><cell>0.840</cell><cell>0.740</cell><cell>0.787</cell><cell>0.814</cell><cell>0.811</cell></row><row><cell>FAST-VQA (ours)</cell><cell>0.849</cell><cell>0.865</cell><cell>0.891</cell><cell>0.892</cell><cell>0.891</cell><cell>0.903</cell><cell>0.819</cell><cell>0.851</cell><cell>0.855</cell><cell>0.852</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 13</head><label>13</label><figDesc>Effects of semantic pre-training on Kinetics-400.</figDesc><table><row><cell>Testing Set/</cell><cell>LSVQtest</cell><cell>LSVQ1080p</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell>Variants/Metric</cell><cell>SRCC/PLCC</cell><cell>SRCC/PLCC</cell><cell>SRCC/PLCC</cell><cell>SRCC/PLCC</cell></row><row><cell cols="2">Existing Classical Methods:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIDEVAL [4]</cell><cell>0.794/0.783</cell><cell>0.545/0.554</cell><cell>0.751/0.741</cell><cell>0.630/0.640</cell></row><row><cell>TLVQM [3]</cell><cell>0.772/0.774</cell><cell>0.589/0.616</cell><cell>0.734/0.724</cell><cell>0.670/0.690</cell></row><row><cell>FAST-VQA:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o semantics</cell><cell>0.788/0.791</cell><cell>0.662/0.707</cell><cell>0.802/0.793</cell><cell>0.737/0.766</cell></row><row><cell>w/ semantics</cell><cell>0.876/0.877</cell><cell>0.779/0.814</cell><cell>0.859/0.855</cell><cell>0.823/0.844</cell></row><row><cell>FasterVQA:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o semantics</cell><cell>0.763/0.760</cell><cell>0.634/0.685</cell><cell>0.770/0.778</cell><cell>0.720/0.739</cell></row><row><cell>w/ semantics</cell><cell>0.873/0.874</cell><cell>0.772/0.811</cell><cell>0.863/0.864</cell><cell>0.813/0.837</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 14</head><label>14</label><figDesc>Performance on split resolutions of LIVE-VQC.</figDesc><table><row><cell>Resolution</cell><cell>(A): 1080P</cell><cell>(B): 720P</cell><cell>(C): ?540P</cell></row><row><cell>Variants</cell><cell cols="3">SRCC/PLCC/KRCC SRCC/PLCC/KRCC SRCC/PLCC/KRCC</cell></row><row><cell cols="2">Full-res Swin features 0.771/0.774/0.584</cell><cell>0.796/0.811/0.602</cell><cell>0.810/0.853/0.625</cell></row><row><cell>bilinear resizing</cell><cell>0.758/0.773/0.573</cell><cell>0.790/0.822/0.599</cell><cell>0.835/0.878/0.650</cell></row><row><cell>random cropping</cell><cell>0.765/0.768/0.565</cell><cell>0.774/0.787/0.581</cell><cell>0.730/0.809/0.535</cell></row><row><cell>w/o GRPB</cell><cell>0.796/0.785/0.598</cell><cell>0.802/0.820/0.608</cell><cell>0.834/0.883/0.649</cell></row><row><cell>FAST-VQA (Ours)</cell><cell>0.807/0.806/0.610</cell><cell>0.803/0.825/0.610</cell><cell>0.840/0.885/0.654</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Impacts of downsampling 1080P videos in LSVQ 1080P . TABLE 15 Stability and reliability of single sampling of fragments .</figDesc><table><row><cell cols="5">FLOPs (CPU running time) during Inference</cell><cell cols="4">Training Memory Cost (Batch Size)</cell></row><row><cell>FasterVQA (Ours) FAST-VQA (Ours) ResNet-50</cell><cell>69</cell><cell>279 (2.7s) (8.7s)</cell><cell>2,000</cell><cell>(7min46s) 40919</cell><cell>Ours (1) Ours (16) ResNet-50 (1)</cell><cell>2.7</cell><cell>25.8</cell><cell>Tesla A100 (80GB, best at present) 120 217</cell></row><row><cell></cell><cell></cell><cell cols="2">Unit: GFLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unit: GB</cell></row><row><cell>0.8</cell><cell></cell><cell>FAST-VQA</cell><cell cols="2">FasterVQA</cell><cell>0.84</cell><cell cols="2">FAST-VQA</cell><cell>FasterVQA</cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1X</cell><cell></cell><cell>2X</cell><cell>3X</cell><cell>4X</cell><cell>1X</cell><cell>2X</cell><cell>3X</cell><cell>4X</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Downsample</cell><cell></cell><cell></cell><cell></cell><cell>Downsample</cell></row><row><cell></cell><cell></cell><cell>SRCC</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PLCC</cell><cell></cell></row><row><cell cols="3">Fig. 8. Testing Set/</cell><cell></cell><cell>LSVQtest</cell><cell>LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell cols="3">Score Range</cell><cell></cell><cell>0-100</cell><cell>0-100</cell><cell>1-5</cell><cell></cell><cell>0-100</cell></row><row><cell cols="4">std. dev. of Single Samplings</cell><cell>0.65</cell><cell>0.79</cell><cell cols="2">0.046</cell><cell>1.07</cell></row><row><cell cols="3">Normalized std. dev.</cell><cell></cell><cell>0.0065</cell><cell>0.0079</cell><cell cols="2">0.0115</cell><cell>0.0107</cell></row><row><cell cols="4">Avg. KRCC on Single Sampling</cell><cell>0.6918</cell><cell>0.5862</cell><cell cols="2">0.6693</cell><cell>0.6296</cell></row><row><cell cols="4">KRCC on 6-sample ensemble</cell><cell>0.6947</cell><cell>0.5897</cell><cell cols="2">0.6730</cell><cell>0.6326</cell></row><row><cell cols="3">Relative Accuracy</cell><cell></cell><cell>99.59%</cell><cell>99.40%</cell><cell cols="2">99.45%</cell><cell>99.52%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ugc-vqa: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, ser. MM &apos;19</title>
		<meeting>ACM Int. Conf. Multimedia, ser. MM &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1238" to="1257" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Patch-vq: &apos;patching up&apos; the video quality problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="19" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Konvid-150k: A dataset for no-reference video quality assessment of videos in-thewild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?tz-Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Access 9</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">160</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blind natural video quality prediction via statistical temporal features and deep spatial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2020</title>
		<meeting>ACM Int. Conf. Multimedia, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term convolutional transformer for noreference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, ser. MM &apos;21</title>
		<meeting>ACM Int. Conf. Multimedia, ser. MM &apos;21</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2112" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blindly assess quality of in-the-wild videos via quality-aware pre-training and motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5944" to="5958" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discovqa: Temporal distortion-content transformers for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09853</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Workshops</title>
		<meeting>Int. Conf. Comput. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progress and challenges in probing the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Farah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">526</biblScope>
			<biblScope unit="issue">7573</biblScope>
			<biblScope unit="page" from="371" to="379" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting spatial redundancy in pixel domain wynerziv video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trapanese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The mpeg video compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eva 2 : Exploiting temporal redundancy in live computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bedoukian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasuriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="533" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The jpeg still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="1991-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Draft itu-t recommendation and final draft international standard of joint video specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="684" to="694" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the effectiveness of video perceptual representation in blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ST-GREED: Space-time generalized entropic differences for frame rate dependent video quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ChipQA: No-reference video quality prediction via spacetime chips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sethuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8059" to="8074" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014. ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning generalized spatial-temporal deep feature representation for noreference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rirnet: Recurrent-inrecurrent network for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient video quality assessment with deeper spatiotemporal feature extraction and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using a deep bilinear convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="47" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Which has better visual quality: The clear blue sky or a blurry animal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multim</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1221" to="1234" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process</title>
		<meeting>Adv. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data in cnns by self-supervised learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Norm-in-norm loss with faster convergence and better performance for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia. ACM</title>
		<meeting>ACM Int. Conf. Multimedia. ACM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The konstanz natural video database (konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cvd2014-a database for evaluating noreference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Subjective quality assessment for youtube ugc dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rapique: Rapid and accurate video quality prediction of user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="440" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rich features for perceptual quality assessment of ugc videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="435" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Icme 2021 ugc-vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<ptr target="http://ugcvqa.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
