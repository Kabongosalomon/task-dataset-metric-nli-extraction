<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VOCALSOUND: A DATASET FOR IMPROVING HUMAN VOCAL SOUNDS RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
							<email>yuangong@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
							<email>jin.yu@signify.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Signify Research</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VOCALSOUND: A DATASET FOR IMPROVING HUMAN VOCAL SOUNDS RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-vocal sounds</term>
					<term>audio classification</term>
					<term>corpus</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing human non-speech vocalizations is an important task and has broad applications such as automatic sound transcription and health condition monitoring. However, existing datasets have a relatively small number of vocal sound samples or noisy labels. As a consequence, state-of-the-art audio event classification models may not perform well in detecting human vocal sounds. To support research on building robust and accurate vocal sound recognition, we have created a VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Experiments show that the vocal sound recognition performance of a model can be significantly improved by 41.9% by adding VocalSound dataset to an existing dataset as training material. In addition, different from previous datasets, the VocalSound dataset contains meta information such as speaker age, gender, native language, country, and health condition. Dataset and code available at https://github.com/yuangongnd/vocalsound.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic human vocal sound recognition is an important task and has a wide range of applications, e.g., it can help the automatic speech recognition system transcribe both speech and non-speech vocalizations. Recognizing health-related sounds like cough and sneeze could also provide insights into the general well-being of occupants in the office, at home, or other public or private spaces, e.g., the detection of coughing and sneezing and their density, intensity, and other features could be used as an indicator of group health <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To build an accurate and robust non-speech vocal sounds recognizer, a dataset with reasonable volume and variety, and accurate annotation is crucial. However, to our knowledge, currently, there is no such large-scale publicly available vocal sound dataset. Moreover, it has been found that a generic audio event classification model trained with existing datasets such as AudioSet <ref type="bibr" target="#b3">[4]</ref> does not perform well in classifying human vocal sounds, e.g., the average precision of state-of-the-art models on cough and sneeze classes are only around 0.5 on the AudioSet evaluation set <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Potential reasons include the fact that corpora such as ESC-50 <ref type="bibr" target="#b6">[7]</ref>, FSD50K <ref type="bibr" target="#b7">[8]</ref>, and AudioSet <ref type="bibr" target="#b3">[4]</ref> have a relatively small number of human vocal sound samples (summarized in <ref type="table">Table 1</ref>) and the AudioSet annotation quality for these sounds may be lacking due to the challenge of annotating with a large sound vocabulary <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. To address this limitation, in this paper we introduce the VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs, collected via Amazon Mechanical Turk. The VocalSound dataset is classbalanced, collected from 3,365 speakers from 60 countries, with their ages ranging from 18 to 80. To the best of our knowledge, the VocalSound dataset has the largest number of human vocal sound samples. While one potential limitation of VocalSound dataset is the audio samples are not produced spontaneously but acted by the subjects, our experiments show that the model vocal sound recognition performance on an evaluation set consisting of real vocal sounds can be significantly improved by over 41.9% by adding VocalSound dataset to existing dataset as training material, demonstrating the usefulness of the VocalSound dataset. In addition, in contrast to previous datatsets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>, the VocalSound dataset contains meta information such as speaker age, gender, native language, country, and health condition to support research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There are a few existing datasets for generic audio event classification that also contain human vocal sound samples such as AudioSet <ref type="bibr" target="#b3">[4]</ref>, FSD50K <ref type="bibr" target="#b7">[8]</ref>, ESC-50 <ref type="bibr" target="#b6">[7]</ref>, and DCASE <ref type="bibr" target="#b10">[11]</ref>. Specifically, AudioSet is currently the largest publicly available dataset for generic audio classification consisting of over 2 million audio clips excised from YouTube and labeled with a set of 527 labels. The FSD50K dataset consists of 51,197 audio clips unequally distributed in 200 sound classes. While AudioSet and FSD50K consist of a large number of audio samples, they are class imbalanced and have a relatively small number of vocal sound samples (summarized in Table 1). Also, limited by the data acquisition scheme, they do not provide speaker information such as age, gender, native language, etc. Due to the difficulty of annotating YouTube videos with a large sound vocabulary, the noisy label issue has been found in AudioSet <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, which could also impact the performance of the model trained on it. Recently, there are a few efforts to collect cough samples for building COVID-19 classification models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In comparison with the proposed dataset, existing imitated vocal sound datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> are much smaller in size. The proposed VocalSound dataset differs from previous efforts in that 1) the VocalSound dataset is class-balanced and has more vocal sound samples collected from a large number of speakers with reasonable gender and age distributions. Due to the data acquisition scheme, the labels are also reliable. 2) the VocalSound dataset has rich meta information, including speaker gender, age, native language, country, and health condition, which broadens the application of the dataset, e.g., the metadata can be used to study the impact of gender, age, language on the performance of vocal sound classification models; the health label can potentially be used to build speechbased health classification system; the anonymous speaker label can potentially be used to build vocal sound-based speaker re-identification system, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VOCALSOUND DATA COLLECTION</head><p>We crowdsource the VocalSound recordings via Amazon Mechanical Turk (AMT). Subjects volunteer to complete our Human Intelligence Tasks (HITs) on AMT and get compensation after the HITs are reviewed and approved by us. Our HIT consists of seven subtasks. First, we ask the gender, age, country, native language, and health information of the subject. For the health condition, we ask the question "do you have a cold, allergy, or other health-related symptoms that might affect your speech today?". Then in subtasks 2-7, we ask the subject to record themselves laughing, sighing, coughing, clearing their throat, sneezing, and sniffing. We do not collect personally identifiable information from the subject or the recording de-vice, and the data collection is anonymous. We approve HITs according to the following three criteria: 1) the audio length is longer than 2 seconds; 2) we use Google Speech API to transcribe the audio to make sure no speech is contained, audios that can be transcribed as words (e.g., haha) are manually verified; 3) We use the model in <ref type="bibr" target="#b5">[6]</ref> to verify if the audio matches the corresponding class. As the prediction of the model might not be accurate, we only use a low threshold to exclude obvious unrelated samples. We apply these criteria during the data collection process, to provide immediate feedback to the Turker and to improve the overall quality of the recordings. We manually verified 600 samples from the dataset, with about 96% judged to be high quality recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DATA DISTRIBUTION</head><p>We collected 3,504 HITs completed by 3,365 unique subjects. Only a small number of subjects completed the HIT more than one time. Our goal was to encourage as much diversity across speakers as possible. Among the subjects, 45% are female, 55% are male. Therefore, the VocalSound dataset is roughly gender-balanced. We show the subject age, country, and native language distribution in <ref type="figure">Figure 1</ref>. The age of the subjects ranges from 18 to 80 while most subjects' ages fall between 20 to 40. Nevertheless, there are still 321 subjects that are older than 50, which are adequate for evaluating the model performance on the senior group. The subjects are from 60 countries, where the United States (60.3%), India (10.8%), and Brazil (8.3%) are the majority countries. English (67.2%), Portuguese (8.7%), and Italian (6.8%) are the corresponding dominant native languages of the subjects. 4.0% of the subjects report that they have healthrelated symptoms that might affect their speech during the data collection. The mean, median, and standard deviation of the audio length is 4.18s, 3.72s, and 1.81s, respectively. The audios are recorded at 44.1kHz in .wav format.</p><p>The data is split into training, validation, and evaluation sets with 15570 (74%), 1860 (9%), and 3594 (17%) samples, respectively. The three sets are speaker-independent. We pay special attention to the evaluation set and manually checked one sample from each speaker and removed lowquality recordings. This clean evaluation set makes the model evaluation fairer and more effective.</p><formula xml:id="formula_0">(? t / 32? X #Classes)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log fbank (t X 128)</head><p>EfficientNet-B0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-frequency Representation</head><p>Frequency Mean Pooling</p><formula xml:id="formula_1">(? t / 32? X 1280) 1X1 Conv Prediction (? t / 32? X 4 X 1280)</formula><p>Temporal Mean Pooling (#Classes) <ref type="figure">Fig. 2</ref>. The model architecture used in our experiments. Each audio waveform is first converted to a sequence of 128dimensional log Mel filterbank (fbank) features computed with a 25ms Hanning window every 10ms. The t ? 128 fbank feature vector is input to an EfficientNet-B0 model <ref type="bibr" target="#b20">[21]</ref>. The EfficientNet-B0 model effectively downsamples the time and frequency dimensions by a factor of 32, and the feature dimension is 1280. Thus, the penultimate output of the model is a t/32 ?4?1280 tensor. We apply mean pooling over the 4 frequency dimensions to produce a 33?1408 representation that is fed to a set of 1 ? 1 convolutional filters with a sigmoid activation function, where #class is the number of prediction classes. A temporal mean pooling is then performed to produce a final #class dimensional output for each class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BASELINE EXPERIMENTS</head><p>We conduct two baseline experiments using the proposed Vo-calSound dataset. First, in Section 5.1, we conduct a six-class (laughter, sigh, cough, throat clearing, sneeze, and sniff) classification experiment on the VocalSound dataset to show the model trained with VocalSound dataset can perform well on vocal sound classification. Second, in Section 5.2, we show the VocalSound dataset can help improve the vocal sound recognition from a wide variety of background sounds by combining it with the existing FSD50K dataset.</p><p>For both experiments, we use an EfficientNet-B0 <ref type="bibr" target="#b20">[21]</ref> based audio classifier (illustrated in <ref type="figure">Figure 2</ref>), which has a similar architecture with the state-of-the-art audio classification model in <ref type="bibr" target="#b5">[6]</ref>, but uses EfficientNet-B0 and mean temporal pooling instead of EfficientNet-B2 and attention pooling. As discussed in <ref type="bibr" target="#b5">[6]</ref>, such simplification can greatly improve the computational efficiency while only marginally reducing performance. For both experiments, we train the model using an Adam optimizer <ref type="bibr" target="#b21">[22]</ref>, an initial learning rate of 1e-4, a batch size of 100, and cross-entropy loss for 50 epochs and select the best model using the development set and evaluate the model on the evaluation set. SpecAugment <ref type="bibr" target="#b22">[23]</ref> is used during training. We repeat each experiment 3 times and report the mean and standard deviation of the results.  <ref type="table">Table 2</ref>. Six-class Vocal Sound Classification Results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Six-class Vocal Sound Classification</head><p>In this experiment, we train a six-class (laughter, sigh, cough, throat clearing, sneeze, and sniff) classifier using the Vocal-Sound dataset. We train, validate, and evaluate the model using the training, validation, and evaluation sets mentioned in Section 4. We downsample the sampling rate to 16kHz, and truncate or pad all audio samples to 5 seconds. As shown in <ref type="table">Table 2</ref>, the accuracy on the evaluation set is 90.5?0.2% (on the validation set: 90.1?0.2%), demonstrating the proposed VocalSound dataset can be used as training material to effectively train a vocal sound classifier. Interestingly, we find the classification accuracy varies with the speaker groups. As shown in <ref type="table">Table 2</ref>, the model achieves an accuracy of 91.5?0.3%, 90.1?0.2%, 90.9?1.6% on the age group of 18-25, 26-48, 49-80, respectively; and 89.2?0.5% and 91.9?0.1% on male and female subjects, respectively. The performance does not solely depend on the number of training samples of each group as the age group of 26-48 and the male group have the largest number of samples but do not have the highest accuracy. Since the VocalSound dataset contains speaker meta information, it can be used to support future research on removing such model bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Vocal Sound Recognition from Background Sounds</head><p>While the model trained with just the VocalSound dataset achieves good accuracy on the 6-class vocal sound classification task, recognizing vocal sounds from a wide variety of background natural sounds is a more important and challenging task. Even the state-of-the-art audio classification models in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> cannot achieve satisfactory results for the vocal sound classes, e.g., the average precision on cough and sneeze classes are only around 0.5 on the AudioSet evaluation set. In this experiment, we show how the proposed Vo-calSound dataset can help improve the performance for this task. Specifically, we show that combining the VocalSound dataset with the existing FSD50K dataset as training material can noticeably improve vocal sounds recognition from background sounds compared with only using FSD50K as training material. The reason why we use FSD50K rather than  <ref type="table">Table 3</ref>. Vocal Sound Recognition Results on FSD50K Evaluation Set. AudioSet as the base dataset is because FSD50K, especially its evaluation set, has more accurate labels <ref type="bibr" target="#b7">[8]</ref> while labels of AudioSet are relatively noisy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. The FSD50K consists of 51K audio clips distributed in 200 sound classes so a wide variety of background sounds are included. Since FSD50K only contains 4 vocal sound classes, we consider a 4+1-class (laughter, sigh, cough, and sneeze + background class) classification problem. For the FSD50K dataset, we relabel all samples that are not labeled as laughter, sigh, cough, and sneeze as a new "background" class. FSD50K is a multi-label dataset but there are only 5, 1, and 13 samples having more than one vocal sound label in the training, validation, and evaluation set, respectively. We randomly select one label for these samples, making the task a single-class classification problem. We compare two training set settings: 1) FSD50K only, we use the official 37k training split of FSD50K as the training set, among the 37k samples, only 1,241 samples are vocal sound samples and other samples are background sounds; 2) FSD50K + VocalSound, VocalSound dataset samples are combined with the FSD50K training set to form a new training set. It is worth mentioning that both datasets are severely class-imbalanced. The background class has 10? more samples than each vocal sound class even after VocalSound sam-ples are added. Therefore, we use a balanced sampling strategy <ref type="bibr" target="#b5">[6]</ref> to make the model see roughly the same number of samples of each class during training, specifically, we use the torch.utils.data.WeightedRandomSampler function. This also makes the comparison between the models trained with these two training sets fairer. In addition to balanced sampling, we also apply SpecAugment <ref type="bibr" target="#b22">[23]</ref> and random time shift to alleviate the class-imbalance issue.</p><p>We train the EfficientNet models with the aforementioned two training sets with the same setting, validate the models using the FSD50K validation set, and evaluate the models using the FSD50K evaluation set. Note that we intentionally evaluate on FSD50K (real sounds, independent from the Vo-calSound dataset) rather than the VocalSound dataset itself to more fairly show the advantage of adding VocalSound for training. Since the evaluation set is also class-imbalanced, we report average precision (AP) and f1-score (F1) rather than accuracy. As shown in <ref type="table">Table 3</ref>, training with FSD50K + Vo-calSound can significantly boost the vocal sound recognition performance by a relative f1-score improvement of 31.8% and an average precision improvement of 41.9%. In <ref type="figure" target="#fig_0">Figure 3</ref>, we compare the confusion matrix of the two models. We find that adding VocalSound in the training set can greatly improve the precision of the vocal sound classes. We run a McNemar's test and confirm the improvement is statistically significant (p &lt; 0.05). All these demonstrate that the proposed Vocal-Sound dataset, while consisting of non-spontaneous sounds, can be used as training material to effectively improve the vocal sound classification performance in realistic use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we introduce VocalSound, a new dataset consisting of over 21,000 audio recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs. Compared with existing generic audio event datasets, the proposed dataset has more vocal sound samples and richer speaker information. Our experiments show that the VocalSound dataset can noticeably improve vocal sound recognition performance. We hope the new dataset can contribute to future research on building accurate and robust vocal sound recognizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>This work is supported in part by Signify.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of the confusion matrixes of the model trained with only FSD50K (left) and with FSD50K + Vocal-Sound (right), evaluated on FSD50K evaluation set. Results averaged from three runs and rounded to integer. Adding Vo-calSound in the training set can improve the precision of the vocal sound classes (highlighted in bold numbers).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Continuous sound collection using smartphones and machine learning to measure cough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kvapilova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Majernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bogar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Goldsack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Biomarkers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A universal system for cough detection in domestic acoustic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Simou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stefanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zervas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate and privacy preserving cough sensing using a low-cost microphone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Addressing missing labels in largescale sound event recognition using a teacher-student framework with loss masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A closer look at weak label learning for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09288</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dcase 2017 challenge setup: Tasks, datasets and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The coughvid crowdsourcing dataset: A corpus for the study of large-scale cough analysis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Orlandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Teijeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Atienza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11644</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Covid-19 artificial intelligence diagnosis using only cough recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laguarta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hueto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Subirana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Engineering in Medicine and Biology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring automatic diagnosis of covid-19 from crowdsourced respiratory sound data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grammenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hasthanasombat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cicuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mascolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Novel coronavirus cough database: Nococoda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen-Mcfarlane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goubran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Knoefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ai4covid-19: Ai enabled preliminary diagnosis for covid-19 from cough samples via an app</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posokhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Informatics in Medicine Unlocked</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bergler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lefter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stappen</surname></persName>
		</author>
		<title level="m">The interspeech 2021 computational paralinguistics challenge: Covid-19 cough</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>covid-19 speech, escalation &amp; primates,&quot; in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cough against covid: Evidence of covid-19 signature in cough sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bagad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhamare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panicker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08790</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vocal imitation set: a dataset of vocally imitated sound events using the audioset ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>DCASE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vocalsketch: Vocally imitating audio concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual ACM Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
