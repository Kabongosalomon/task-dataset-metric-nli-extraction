<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Training under Label Noise by Over-parameterization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-04">August 4, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of EECS</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">University of Denver</orgName>
								<orgName type="institution" key="instit3">University of Michigan ? Google Research</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of EECS</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">University of Denver</orgName>
								<orgName type="institution" key="instit3">University of Michigan ? Google Research</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of EECS</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">University of Denver</orgName>
								<orgName type="institution" key="instit3">University of Michigan ? Google Research</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of EECS</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">University of Denver</orgName>
								<orgName type="institution" key="instit3">University of Michigan ? Google Research</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Training under Label Noise by Over-parameterization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-04">August 4, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, over-parameterized deep networks, with increasingly more network parameters than training samples, have dominated the performances of modern machine learning. However, when the training data is corrupted, it has been well-known that over-parameterized networks tend to overfit and do not generalize. In this work, we propose a principled approach for robust training of over-parameterized deep networks in classification tasks where a proportion of training labels are corrupted. The main idea is yet very simple: label noise is sparse and incoherent with the network learned from clean data, so we model the noise and learn to separate it from the data. Specifically, we model the label noise via another sparse over-parameterization term, and exploit implicit algorithmic regularizations to recover and separate the underlying corruptions. Remarkably, when trained using such a simple method in practice, we demonstrate state-of-the-art test accuracy against label noise on a variety of real datasets. Furthermore, our experimental results are corroborated by theory on simplified linear models, showing that exact separation between sparse noise and low-rank data can be achieved under incoherent conditions. The work opens many interesting directions for improving over-parameterized models by using sparse over-parameterization and implicit regularization 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most important factors for the success of deep models is their large model size and high expressive power, which enable them to learn complicated input-output relations. As such, over-parametrized deep networks or large models, with more parameters than the size of training data, have dominated the performance in computer vision, natural language processing, and so on. The adoption of large models is justified by the recent discovery that deep models exhibit a "double descent" <ref type="bibr" target="#b0">[1]</ref> and "uni-modal variance" <ref type="bibr" target="#b1">[2]</ref> generalization behavior, where their performance continues to improve beyond the interpolation point, extending the classical learning theory of bias-variance trade-off. While there are infinitely many global solutions that overfit to training data, the choice of optimization algorithm imposes certain implicit regularization <ref type="bibr" target="#b2">[3]</ref> so that over-parameterized models converge to those that are generalizable.</p><p>Nonetheless, the success of over-parameterization of deep networks critically depends on the availability of clean training data, while overfitting inevitably occurs when training data is corrupted. Consider the task of image classification with a training dataset {(x i , y i )} N i=1 , with x i being an input image and y i being the corresponding one-hot label. With an over-parameterized deep network f (?; ?), model training is achieved by solving an optimization problem with respect to (w.r.t.) the network parameter ? as follows:</p><formula xml:id="formula_0">min ? L(?) = 1 N N i=1 f (x i ; ?), y i ,<label>(1)</label></formula><p>where (?, ?) is a loss function that measures the distance between network prediction f (x i ; ?) and the label y i . If a proportion of the images in the training set is mislabelled <ref type="bibr" target="#b3">[4]</ref>, it is well-known that the network will be optimized to zero training error hence produce f (x i ; ?) ? y i for all i ? {1, ? ? ? , N }, even for y i 's that are incorrect <ref type="bibr" target="#b4">[5]</ref>. Overfitting to wrong labels inevitably leads to poor generalization performance (see <ref type="figure">Fig. 1</ref>). <ref type="figure">Figure 1</ref>: Sparse over-parameterization prevents overfitting to label noise. Training and test accuracy of a PreActResNet18 network trained with a standard cross entropy (CE) loss (dashed lines) and our Sparse Over-parameterization (SOP) (solid lines) for image classification on the CIFAR-10 dataset with 0%, 20%, and 40% of the labels flipped at random. SOP prevents overfitting to the wrong training labels, obtaining near 100%, 80%, 60% training accuracy respectively, therefore achieves better generalization on the test set without an accuracy drop at the end of training.</p><p>In this paper, we introduce a principled method to address the challenges of overfitting over-parameterized deep networks in the presence of training data corruptions. We focus on the task of classification trained with noisy label, a ubiquitous problem in practice due to the extreme complexity of data annotation even for experienced domain experts <ref type="bibr" target="#b5">[6]</ref>. Our idea leverages the property that the label noise is sparse, namely only a fraction of the labels are corrupted and the rest are intact. Principled methods for dealing with sparse corruption have a rich history, which can be retraced back to compressed sensing <ref type="bibr" target="#b6">[7]</ref>, robust subspace recovery <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and even earlier <ref type="bibr" target="#b9">[10]</ref>. Such methods are based on using a robust loss function, such as the 1 norm which is less sensitive to large outlying entries. While it is tempting to use sparse modeling for the label noise problem by setting the loss () in (1) as the 1 loss, such an approach cannot solve the overfitting issue since all global solutions are still given by those that satisfy f (x i ; ?) ? y i for all i ? {1, ? ? ? , N }. Hence, handling sparse corruptions with over-parameterized models requires the development of techniques beyond the classical 1 loss for sparse modeling.</p><p>Overview of our method and contribution. To handle sparse corruption with over-parameterized models, our idea is simply to use an extra variable s i to model the unknown label noise s i , which is the difference between the observed label y i and the corresponding clean label. Hence, the goal is to minimize the discrepancy between f (x i ; ?) + s i and y i . Inspired by a line of recent work <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, we enforce sparsity of s i by the overparameterization s i = u i u i ? v i v i and optimize the following training loss</p><formula xml:id="formula_1">min ?,{ui,vi} N i=1 L ?, {u i , v i } N k=1 , where L ?, {u i , v i } N k=1 ) . = 1 N N i=1 (f (x i ; ?) + u i u i ? v i v i , y i )),<label>(2)</label></formula><p>with denoting an entry-wise Hadamard product. We term our method "Sparse Over-Parameterization" (SOP). At the first glance, our SOP approach is seemingly problematic, because adding more learnable parameters</p><formula xml:id="formula_2">{u i , v i } N i=1</formula><p>to an over-parameterized network f (?, ?) would aggravate rather than alleviate the overfitting issue. Indeed, a global solution to (2) is given by u i ? v i ? 0 and f (x i , ?) ? y i for all i ? {1, ? ? ? , N } where the network overfits to noisy labels. Here, we leverage the choice of a particular training algorithm to enforce an implicit bias towards producing the desired solutions. Technically, we run gradient descent on the objective in (2) starting from a small initialization for</p><formula xml:id="formula_3">{u i , v i } N i=1 : ? ? ? ? ? ? ?L(?, {u i , v i }) ?? , u i ? u i ? ?? ? ?L(?, {u i , v i }) ?u i , i = 1, . . . , N, v i ? v i ? ?? ? ?L(?, {u i , v i }) ?v i , i = 1, . . . , N,<label>(3)</label></formula><p>where ? &gt; 0 is the ratio of learning rates for different training variables. Such a simple algorithm enables our method of SOP to train a deep image classification networks without overfitting to wrong labels and obtain better generalization performance (see <ref type="figure">Fig. 1</ref>). A more comprehensive empirical study with a variety of datasets is presented in Section 2.</p><p>To rigorously justify our method, we theoretically investigate our method based upon a simplified overparameterized linear model with sparse corruptions. As justified by a line of recent work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, overparameterized linear models capture similar phenomena because they well approximate over-parameterized deep networks in a linearized regime around the initial points. Under sparse corruption and certain low-rank assumptions on the data, we show that the gradient descent (3) with an ? below a certain threshold recovers the underlying model parameters with sparse corruptions. Our result is obtained by explicitly characterizing the implicit regularization for the term u i u i ? v i v i . In particular, we explicitly show that it leads to an 1 -norm regularization on the sparse corruption, hence connecting our method to classical 1 loss approaches for model robustness. For more details, we refer readers to Section 3.</p><p>In summary, our contributions are two-folds:</p><p>? Method. We proposed a simple yet practical SOP method that can effectively prevent overfitting for learning over-parameterized deep networks from corrupted training data, demonstrated on a variety of datasets.</p><p>? Theory. Under a simplified over-parameterized linear model, we rigorously justify our approach for exactly separating sparse corruption from the data.</p><p>Moreover, we believe the methodology we developed here could be far beyond the label noise setting, with the potential for dealing with more challenging scenarios of preventing overfitting in learning modern overparametrized models of an ever-increasing size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Robust Classification with Label Noise</head><p>In this section, we show how our SOP method plays out on image classification problems with the noisy label.</p><p>In particular, we discuss extra implementation details of our method, followed by experimental demonstrations on a variety of datasets with synthetic and real label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Implementation Details of SOP</head><p>We train an over-parameterized deep neural network f (?; ?) from the noisy training data {(x i , y i )} N i=1 using the method briefly discussed in Section 1. Specifically, we train the network f (?; ?) using the objective (2) with stochastic gradient descent (SGD) (i.e. a batch version of (3)).</p><p>Notice that there is additional prior information on label noise s i associated with a sample {x i , y i }, namely, the positive and negative entries of s i must correspond to nonzero entry and zero entries of y i , respectively. Moreover, all entries of s i must lie in the range of [?1, 1]. To leverage such information, we optimize a variant of (2) given by</p><formula xml:id="formula_4">min ?,{ui,vi} N i=1 1 N N i=1 f (x i ; ?) + s i , y i ,<label>(4)</label></formula><formula xml:id="formula_5">s.t. s i . = u i u i y i ? v i v i (1 ? y i ), and u i ? [?1, 1] K , v i ? [?1, 1] K ,<label>(5)</label></formula><p>where K is the number of classes. In above, constraints on u i , v i are realized by performing a projection step after each gradient descent update.</p><p>Choice of the loss function (?, ?) in <ref type="bibr" target="#b3">(4)</ref>. The most commonly used loss function for classification tasks is the cross-entropy loss CE (?, ?) <ref type="bibr" target="#b15">[16]</ref>. Because the CE (?, ?) loss requires a probability distribution as an input, we define a mapping</p><formula xml:id="formula_6">?(w) . = max{w, 1} max{w, 1} 1 ,<label>(6)</label></formula><p>Algorithm 1 Image classification under label noise by the method of Sparse Over-Parameterization (SOP).</p><formula xml:id="formula_7">1: Input: Training data {(x i , y i )} N i=1 , network backbone f (?, ?), variables {u i , v i } N i=1</formula><p>, number of epochs T , learning rate ? , learning rate ratio ? u , ? v , batch size ? 2: Initialization: Draw entries of u i , v i from i.i.d. Gaussian distribution with zero-mean and s.t.d. 1e ? 8 3: for each t ? {1, ? ? ? , T } do <ref type="bibr">4:</ref> # Train network f (?, ?) with SGD <ref type="bibr">5:</ref> for each b ? {1, ? ? ? , N/?} do <ref type="bibr">6:</ref> Sample a batch B ? {1, . . . , N } with |B| = ? 7:</p><formula xml:id="formula_8">Set ? ? ? ? ? ? i?B ? CE ?(f (xi; ?)+si), y i ?? 8:</formula><p>end for <ref type="bibr" target="#b8">9</ref>:</p><formula xml:id="formula_9"># Update {u i , v i } 10:</formula><p>for each i ? {1, ? ? ? , N } do <ref type="bibr">11:</ref> Set</p><formula xml:id="formula_10">s i ? u i u i y i ? v i v i (1 ? y i ) 12: Set u i ? P [?1,1] u i ? ? u ? ? ? CE ?(f (xi; ?)+si), y i ?ui 13: Set v i ? P [?1,1] v i ? ? v ? ? ? MSE f (xi; ?)+si, y i ?vi 14:</formula><p>end for 15: end for <ref type="bibr">16:</ref> Output: Network parameters ? and</p><formula xml:id="formula_11">{u i , v i } N i=1</formula><p>and set the loss (?, ?) in (4) to be</p><formula xml:id="formula_12">f (x i ; ?) + s i , y i = CE ? f (x i ; ?) + s i , y i<label>(7)</label></formula><p>On the other hand, the cross-entropy loss cannot be used to optimize the variables {v i } (see Section A.1 for an explanation). Hence, we use the mean squared error loss MSE and set the loss in (4) to be</p><formula xml:id="formula_13">f (x i ; ?) + s i , y i = MSE f (x i ; ?) + s i , y i ,<label>(8)</label></formula><p>when optimizing {v i } 2 . We summarize our training method in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiments</head><p>We experimentally demonstrate the effectiveness of our proposed SOP method on datasets with both synthetic (i.e., CIFAR-10 and CIFAR-100) and realistic (i.e., CIFAR-N, Clothing-1M, and WebVision) label noise. In addition to the SOP described in Algorithm 1, we also implement an improved version, termed SOP+, which incorporates two commonly used regularization techniques in the literature of label noise, namely the consistency regularization and the class-balance regularization. We explain SOP+ in more detail in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset descriptions.</head><p>We use datasets with synthetic label noise generated from CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b16">[17]</ref>. Each dataset contains 50k training images and 10k test images, all with clean labels, where each image is of size 32 ? 32. Following previous works <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, we generate symmetric label noise by uniformly flipping labels for a percentage of the training set for all classes, as well as asymmetric label noise by flipping labels for particular pairs of classes. For datasets with realistic label noise, we test on CIFAR-10N/CIFAR-100N <ref type="bibr" target="#b20">[21]</ref> which contains a re-annotation of CIFAR-10/CIFAR-100 with human workers. Specifically, each image in CIFAR-10N contains three submitted labels (i.e., Random 1, 2, 3) which are further combined to have an Aggregate and a Worst label. Each image in CIFAR-100N contains a single submitted label for the fine classes. We also test on Clothing-1M <ref type="bibr" target="#b21">[22]</ref> which is a large-scale dataset with images clawed from online shopping websites and labels generated based on surrounding texts. Clothing-1M contains 1 million training images, 15k validation images, and 10k test images with clean labels. Finally, we also test on the mini WebVision dataset <ref type="bibr" target="#b22">[23]</ref> which contains the top 50 classes from the Google image subset of WebVision (approximately 66 thousand images). Models  ? CIFAR-10/100 and CIFAR-10N/100N. We follow <ref type="bibr" target="#b18">[19]</ref> to use ResNet-34 and PreActResNet18 architectures trained with SGD using a 0.9 momentum. The initial learning rate is 0.02 decayed with a factor of 10 at the 40th and 80th epochs for CIFAR-10/CIFAR-10N and at the 80th and 120th epochs for CIFAR-100/CIFAR-100N, respectively. Weight decay for network parameters ? is set to 5 ? 10 ?4 . No weight decay is used</p><formula xml:id="formula_14">for parameters {u i , v i } N i=1 . ? Clothing-1M.</formula><p>We follow the previous work <ref type="bibr" target="#b18">[19]</ref> to use a ResNet-50 <ref type="bibr" target="#b23">[24]</ref> pre-trained on ImageNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>The network is trained with batch size 64 and an initial learning rate 0.001, which is reduced by a factor of 10 after 5th epoch (10 training epochs in total). Optimization is performed using SGD with a momentum 0.9. Weight decay is 0.001 for parameters ? and is zero for</p><formula xml:id="formula_15">parameters {u i , v i } N i=1 . ? Mini Webvision.</formula><p>We use InceptionResNetV2 as the backbone architecture. All other optimization details are the same as for CIFAR-10, except that we use weight decay 0.0005 and batch size 32.</p><p>Experimental results. We compare with methods based on estimation of the transition matrix (Forward <ref type="bibr" target="#b24">[25]</ref>), design of loss functions (GCE <ref type="bibr" target="#b25">[26]</ref> and SL <ref type="bibr" target="#b26">[27]</ref>), training two networks (Co-teaching <ref type="bibr" target="#b17">[18]</ref> and DivideMix <ref type="bibr" target="#b27">[28]</ref>), and label noise correction (ELR <ref type="bibr" target="#b18">[19]</ref> and CORES 2 <ref type="bibr" target="#b28">[29]</ref>). <ref type="table" target="#tab_0">Table 1</ref> reports the performance of our method on synthetically generated symmetric label noise using CIFAR-10 and CIFAR-100. To compare with state-of-the-art methods, we also report the performance of SOP+ which contains additional regularization on both symmetric and asymmetric label noise and report the results <ref type="table" target="#tab_5">Table 3</ref>: Test accuracy with realistic label noise on Clothing1M and WebVision. We use a pre-trained ResNet50 for Clothing1M and an InceptionResNetV2 for WebVision dataset. The results of the comparing methods are taken from their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Clothing1M WebVision ILSVRC12  in <ref type="table" target="#tab_1">Table 2</ref>. It can be observed that our method is robust to a fairly large amount of label noise, and compares favorably to existing techniques. We further demonstrate that our method can effectively handle datasets with realistic label noise by reporting its performance on Clothing1M &amp; WebVision (see <ref type="table" target="#tab_5">Table 3</ref>) and CIFAR-N (see <ref type="table" target="#tab_3">Table 4</ref>) datasets. We can observe a performance gain over all comparing methods.</p><p>Finally, we compare the training time (on a single Nvidia V100 GPU) of our method to the baseline methods in <ref type="table" target="#tab_4">Table 5</ref>. We observe that our algorithm SOP/SOP+ achieves the fastest speed across all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Insights with Simplified Models</head><p>This section provides theoretical insights into our SOP method by studying structured data recovery with sparse corruption in the context of over-parameterized linear models. We will start with model simplification, followed by our main theoretical results and experimental verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup &amp; Main Result</head><p>Given a highly overparameterized network f (?; ?), recent work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> suggests that the parameter ? ? R p may not change much from its initialization ? 0 before obtaining zero training error. Hence, a nonlinear network f (?; ?) : R n ? R can be well approximated by its first-order Taylor expansion:</p><formula xml:id="formula_16">f (x; ?) ? f (x; ? 0 ) + ? ? f (x; ? 0 ), ? ? ? 0 ,<label>(9)</label></formula><p>where we consider f (?; ?) as a scalar function for simplicity. Since the bias term f (</p><formula xml:id="formula_17">x; ? 0 ) ? ? ? f (x; ? 0 ), ? 0 is constant w.r.t.</formula><p>?, for simplicity we may further assume that </p><formula xml:id="formula_18">f (x; ?) ? ? ? f (x; ? 0 ), ? .<label>(10)</label></formula><formula xml:id="formula_19">} N i=1 of N points, collectively ? ? ? f (x 1 ; ?) . . . f (x N ; ?) ? ? ? ? ? ? ? ? ? f (x 1 ; ? 0 ) . . . ? ? f (x N ; ? 0 ) ? ? ? ? ? = J ? ?,<label>(11)</label></formula><p>where J ? IR N ?p is a Jacobian matrix. This observation motivates us to consider the following problem setup.</p><p>Problem setup. Based upon the above linearization, we assume that our corrupted observation y ? R N (e.g., noisy labels) is generated by</p><formula xml:id="formula_20">y = J ? ? + s ,<label>(12)</label></formula><p>where ? ? R p is the underlying groundtruth parameter, and the noise s ? R N is sparse so that only a subset of observation (e.g., labels) is corrupted. Given J and y generated from <ref type="formula" target="#formula_0">(12)</ref>, our goal is to recover both ? and s . However, as we are considering the problem in an over-parameterized regime with p &gt; N , the underdetermined system <ref type="bibr" target="#b11">(12)</ref> implies that there are infinite solutions for ? even if s is given. Nonetheless, recent work showed that the implicit bias of gradient descent for overparameterized linear models and deep networks tend to find minimum 2 -norm solutions <ref type="bibr" target="#b4">[5]</ref>. To make our problem more well-posed, motivated by these results, we would like to find an ? with minimum 2 -norm, namely,</p><formula xml:id="formula_21">? = arg min ? ? 2 2 s.t. y = J ? + s .<label>(13)</label></formula><p>Analogous to (2), we will show that ? and s can be provably recovered by solving the problem</p><formula xml:id="formula_22">min ?,u,v h(?, u, v) . = 1 2 J ? + u u ? v v ? y 2 2 ,<label>(14)</label></formula><p>using the gradient descent algorithm with learning rates ? and ?? on ? and {u, v}, respectively:</p><formula xml:id="formula_23">? k+1 = ? k ? ? ? J r k , u k+1 = u k ? 2?? ? u k r k , v k+1 = v k + 2?? ? v k r k ,<label>(15)</label></formula><p>where r k .</p><formula xml:id="formula_24">= J ? k + u k u k ? v k v k ? y.</formula><p>Based on these, our result can be summarized as follows. We state our result at a high level with more technical details in Section 3.2 and Section 3.3. The overall idea of the proof can be sketched through the following two steps.</p><p>? First, although the problem <ref type="formula" target="#formula_0">(14)</ref> is nonconvex, in Section 3.2 we show that it has benign global landscape, and that the gradient descent (15) converges to particular global solutions that are the same as solutions to a convex problem with explicit regularizations on ? and s.</p><p>? Building upon above results, in Section 3.3 we complete our analysis by showing that ? and s can be exactly recovered by the convex problem with a small enough value for ?.</p><p>Throughout the analysis, we corroborate our findings with numerical simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Landscapes &amp; Implicit Sparse Regularization</head><p>Benign global landscape. We start by characterizing the nonconvex landscape of <ref type="bibr" target="#b13">(14)</ref>, showing the following result. <ref type="formula" target="#formula_0">(14)</ref> is either a global minimizer, or it is a strict saddle <ref type="bibr" target="#b30">[31]</ref> with its Hessian having at least one negative eigenvalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3.2. Any critical point of</head><p>For a strict saddle function, recent work <ref type="bibr" target="#b31">[32]</ref> showed that gradient descent with random initialization almost surely escapes saddle points and converges to a local minimizer. Thus, Proposition 3.2 ensures that the algorithm in <ref type="bibr" target="#b14">(15)</ref> almost surely converges to a global solution of <ref type="bibr" target="#b13">(14)</ref>.</p><p>However, because there are infinite many global solutions for the overparameterized model <ref type="bibr" target="#b13">(14)</ref> and not all global solutions are of equal quality, <ref type="bibr" target="#b2">3</ref> convergence to a global solution alone is not sufficient for us to establish the correctness of our method. Nonetheless, as we will show in the following, the particular choice of the algorithm in (15) enables it to converge to a particular regularized global solution.</p><p>Implicit sparse regularization. To understand which solution the algorithm (15) converges to, we study its gradient flow counterpart by taking the stepsize ? ? 0 in <ref type="bibr" target="#b14">(15)</ref>. Thus, the dynamics of such a gradient flow is governed by the following differential equation?</p><formula xml:id="formula_25">? t (?, ?) = ?J r t (?, ?), u t (?, ?) = ?2? ? u t (?, ?) r t (?, ?), v t (?, ?) = 2? ? v t (?, ?) r t (?, ?),<label>(16)</label></formula><p>where we define</p><formula xml:id="formula_26">r t (?, ?) = J ? t (?, ?) + u t (?, ?) u t (?, ?) ? v t (?, ?) v t (?, ?) ? y.<label>(17)</label></formula><p>Here, we assume that ?, u, and v are initialized at</p><formula xml:id="formula_27">? 0 (?, ?) = 0, u 0 (?, ?) = ?1, v 0 (?, ?) = ?1,<label>(18)</label></formula><p>with some small ? &gt; 0. Solving the differential equations in <ref type="bibr" target="#b15">(16)</ref> gives the gradient flow</p><formula xml:id="formula_28">? t (?, ?) = J ? t (?, ?), u t (?, ?) = ? exp(2?? t (?, ?)), v t (?, ?) = ? exp(?2?? t (?, ?)),<label>(19)</label></formula><p>where we define</p><formula xml:id="formula_29">? t (?, ?) . = ? t 0 r ? (?, ?)d?.<label>(20)</label></formula><p>The following result shows that the solution that the gradient flow ? t (?, ?), u t (?, ?), v t (?, ?) in <ref type="bibr" target="#b18">(19)</ref> converges to at t ? ? is a global solution to <ref type="bibr" target="#b13">(14)</ref> that is regularized with a particular of (?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3.3.</head><p>Consider the gradient flow in <ref type="bibr" target="#b18">(19)</ref> with the initialization in <ref type="bibr" target="#b17">(18)</ref>.</p><p>? (Global convergence) For any (?, ?), if the limit</p><formula xml:id="formula_30">? ? (?, ?), u ? (?, ?), v ? (?, ?) . = lim t?? ? t (?, ?), u t (?, ?), v t (?, ?)<label>(21)</label></formula><p>of the gradient flow exists, then ? ? (?, ?), u ? (?, ?), v ? (?, ?) is a global solution to <ref type="bibr" target="#b13">(14)</ref>.</p><p>? (Implicit regularization) Fix any ? &gt; 0 and let ? be a function of ? as</p><formula xml:id="formula_31">?(?) = ? log ? 2? .<label>(22)</label></formula><p>If the limit</p><formula xml:id="formula_32">?, u, v . = lim ??0 ? ? (?, ?(?)), u ? (?, ?(?)), v ? (?, ?(?))<label>(23)</label></formula><p>exists, then ?, u, v is a global solution to <ref type="bibr" target="#b13">(14)</ref>. In particular, let</p><formula xml:id="formula_33">s . = u u ? v v,<label>(24)</label></formula><p>then ( ?, s) is an optimal solution to the following convex program min ?, s</p><formula xml:id="formula_34">1 2 ? 2 2 + ? s 1 , s.t. y = J ? + s.<label>(25)</label></formula><p>As we observe from the above result, because ( ?, s) that the gradient flow <ref type="bibr" target="#b15">(16)</ref> converges to is also an optimal solution of (25), it implies that ( ?, s) is regularized. In particular, the 1 -norm regularization on s Given a tuple (N, p, r, k) of model parameters, we generate simulation data (J , ? , s , y) as follows. The matrix J ? R N ?p is generated by multiplying two randomly generated matrices of shape N ? r and r ? p, respectively with entries drawn i.i.d. from a standard Gaussian distribution. The sparse vector s ? R N is generated by randomly choosing k entries to be i.i.d. standard Gaussian, with the rest of entries zero. Then, we generate a vector ? ? R p with all entries drawn i.i.d. from a standard Gaussian distribution, and let y = J ? +s . Finally, we set ? as the minimum 2 -norm solution according to <ref type="bibr" target="#b12">(13)</ref>.</p><p>In this experiment, we choose and fix (N, p, r, k) = (20, 40, 3, 3) for the data generation described above. With a varying learning rate ? ? [4, 4000], we compute (? ? , s ? ) as the solution provided by gradient descent in <ref type="bibr" target="#b14">(15)</ref> with an initialization by <ref type="bibr" target="#b17">(18)</ref> with ? = e ?8 . With a varying regularization ? ? [0.0001, 1] in (25), we compute (? ? , s ? ) as the solution provided by the convex problem in (25) with weight parameter ?. <ref type="bibr" target="#b3">4</ref>  <ref type="figure" target="#fig_10">Figure 2</ref> provides a visualization of the relative difference ? = ???? ? 2 max{ ?? 2, ? ? 2} between ? ? and ? ? (and likewise for s), across all pairs of (?, ?). We can observe that as long as (?, ?) satisfies the relationship in (22), the relative difference ? is small for ?, which is also true for s. On the other hand, the relative differences can be large if <ref type="bibr" target="#b21">(22)</ref> is not satisfied, corroborating Proposition 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exact Recovery under Incoherence Conditions</head><p>Given the overparameterized model (12) with y ? R N , ? ? R p , and p N , there is no enough information from y to recover ? and s even with the prior information that s is sparse -any given vector y ? R p can be decomposed as a summation of an arbitrary sparse vector s and a vector ? cooked up from the column space of J as long as J has full row rank.</p><p>For the solution ? and s to be identified, first, we assume that J is low-rank, where it has been empirically observed in practical deep neural network f ? that the Jacobian matrix J of f ? is approximately low-rank <ref type="bibr" target="#b34">[35]</ref>. <ref type="bibr" target="#b4">5</ref> However, the low-rank condition of J alone does not guarantee identifiability, because it cannot address the separability between J ? and s -following a similar argument as that in <ref type="bibr" target="#b7">[8]</ref>, if any column of J has a single nonzero entry, then any s that is supported on the same entry cannot be recovered without ambiguities. Hence, we further assume that the column space of J and the standard basis [e 1 , . . . , e N ] . = diag{1, . . . , 1} ? IR N ?N are incoherent, defined as follows. </p><p>It should be noted that the low-rank and incoherence assumptions are common for matrix recovery <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Based upon the above assumptions on J and s , we show the following. </p><formula xml:id="formula_36">k 2 r &lt; N 4?(J ) ,<label>(27)</label></formula><p>then the solution to <ref type="bibr" target="#b24">(25)</ref> is (? , s ) for any ? &gt; ? 0 , where ? 0 &gt; 0 is a scalar depending on (J , ? , k).</p><p>Thus, combining this result with Proposition 3.3, the gradient flow in <ref type="bibr" target="#b15">(16)</ref> with initialization (18) converges to (? , s ) when the choice of learning rate ratio ? in <ref type="formula" target="#formula_0">(16)</ref> is smaller than a certain threshold, justifying our claim in Theorem 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical verification.</head><p>To corroborate Proposition 3.5, we numerically solve <ref type="bibr" target="#b24">(25)</ref> under varying conditions of ?, r, and k. The simulated data (J , ? , s , y) is generated the same way as the experimental part in Section 3.2 with N = 100 and p = 150, and for an obtained solution (?, s) via solving <ref type="formula" target="#formula_1">(25)</ref> ? Relationships between the rank r and sparsity k. Here, we fix ? = 0.1 and plot the phase transition with respect to r and k. For each (r, k), the simulation is repeated for 20 random instances, and for each instance we declare the recovery to be successful if ? &lt; 0.001 and s &lt; 0.001. As shown in <ref type="figure" target="#fig_8">Figure 4</ref>, the phase transition is consistent with Proposition 3.5 that successful recovery is achieved only when both k and r are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prior Arts on Implicit Regularization</head><p>Since overparameterized deep neural networks do not overfit (in the absence of data corruption) even without any explicit regularization <ref type="bibr" target="#b4">[5]</ref>, it is argued that there are implicit regularizations pf learning algorithms that enable the models to converge to desired solutions. Under the assumption of linear or deep linear models, many work characterized the mathematics of such implicit bias via explicit regularizations <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>. Among those, the closest related to ours include <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>, which studied the implicit sparse regularization induced by a term of the form u u ? v v. While all the above works aim to understand implicit regularization by studying linear models, the practical benefits of such studies are unclear. Our work provides an inspiring result showing that principled design   with implicit regularization leads to robust learning of over-parameterized models. In particular, our model in <ref type="formula" target="#formula_1">(2)</ref> is motivated by existing studies on the implicit sparse regularization, but adds such a regularization to an (already) implicitly regularized model for handling sparse corruptions. In other words, two forms of implicit regularization are involved in our model which poses new problems in the design of the optimization algorithm and in mathematical analysis. To the best of our knowledge, the only prior works that use implicit sparse regularization for robust learning are <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> which studied the robust recovery of low-rank matrices and images. Among them, our work extends <ref type="bibr" target="#b12">[13]</ref> to the problem of image classification with label noise, demonstrates its effectiveness, and provides dedicated theoretical analyses. Additionally, methods in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> require a particular learning rate schedule that may not be compatible with commonly used schedules such as cosine annealing <ref type="bibr" target="#b52">[53]</ref> in image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relationship to Existing Work on Label Noise</head><p>Deep neural networks are over-parameterized hence prone to overfitting to the label noises. While many popular regularization techniques for alleviating overfitting, such as label smoothing <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> and mixup <ref type="bibr" target="#b56">[57]</ref>, are useful for mitigating the impact of label noise, they do not completely solve the problem due to a lack of precise noise modeling. In the following, we discuss three of the most popular line of work dedicated to the label noise problem; we refer the reader to the survey papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58]</ref> for a comprehensive review.</p><p>Loss design. Robust loss function, such as the 1 loss <ref type="bibr" target="#b58">[59]</ref>, is one of the most popular approaches to the label noise problem which has many recent extensions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref>. The method is based on reducing the loss associated with large outlying entries, hence the impact of label noise. A similar idea is also explored in gradient clipping <ref type="bibr" target="#b64">[65]</ref> and loss reweighting <ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref> methods. While robust loss enables the model to learn faster from correct labels, the global solution still overfits to corrupted labels with over-parameterized models.</p><p>Label transition probability. Another popular line of work for label noise is based on the assumption that the noisy label is drawn from a probability distribution conditioned on the true label. Here, the main task is to estimate the underlying transition probabilities. The early work <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref> encapsulates the transition probabilities as a noise adaptation layer that is stacked on top of a classification network and trained jointly in an end-to-end fashion. Recent work <ref type="bibr" target="#b24">[25]</ref> uses separated procedures to estimate the transition probabilities, the success of which requires either the availability of a clean validation data <ref type="bibr" target="#b72">[73]</ref> or additional data assumptions <ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref>. Even if the underlying transition probabilities can be correctly recovered, overfitting is only prevented asymptotically, requiring sufficiently many samples of corrupted labels for each input <ref type="bibr" target="#b24">[25]</ref>, which is not practical.</p><p>Label correction. In contrast to the above methods, our method completely avoids overfitting even with finite training samples. This is achieved by the over-parameterization term u u ? v v in (2) which recovers the clean labels. Hence, our method is related to techniques based on noisy label detection and refurbishment.</p><p>Nonetheless, existing techniques are based on heuristic argument about different behaviors of clean and corrupted samples in the training process, such as properties of learned representations <ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref>, prediction consistency <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>, learning speed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84]</ref>, margin <ref type="bibr" target="#b84">[85]</ref>, confidence <ref type="bibr" target="#b28">[29]</ref>. They often need to be combined with engineering tricks such as moving average <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b85">86]</ref> and burning-in <ref type="bibr" target="#b86">[87]</ref> to make them work well. Finally, the work <ref type="bibr" target="#b87">[88]</ref> introduces a variable to estimate the label noise in a way similar to <ref type="bibr" target="#b1">(2)</ref>. However, the variable is not over-parameterized to induce sparsity, and their method does not have competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sparsity in Deep Learning</head><p>Our method is broadly related to existing efforts on introducing sparsity into deep learning <ref type="bibr" target="#b88">[89]</ref>, but is notably different in both the objective of introducing sparsity, the origin of sparsity, and how sparsity is enforced. First, previous exploration of sparsity primarily aims to improve training and inference efficiency with large-scale models, while our paper focuses on robust training under label noise. Second, previous introduction of sparsity is often motivated by its presence in biological brains, but there is still a lack of clean understanding of how sparsity helps with learning. In contrast, sparsity in our method has the clear mathematically meaning that the percentage of corrupted labels is small. Finally, while pruning <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref> is a dominant approach for obtaining sparsity, our method leverages the implicit bias of gradient descent associated with a particular sparse over-parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations and Future Directions</head><p>Choice of optimization algorithms.</p><p>Our SOP method is based on introducing more parameters to an already over-parameterized model, hence relies critically on the choice of the optimization algorithm to induce the desired implicit regularization. For vanilla gradient descent, our analysis in Section 3 shows that it has the desired implicit regularization by design. In practical deep network training, it is more common to use the stochastic gradient descent with momentum. While not theoretically justified, experiments in Section 2 show that our method works with such practical variants. This may not come as a surprise, because existing studies already show that stochastic gradient descent <ref type="bibr" target="#b91">[92]</ref> and momentum acceleration <ref type="bibr" target="#b92">[93]</ref> have the same implicit bias as the vanilla gradient decent under certain models. We leave the extension of such results to our method as future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling of label noise.</head><p>Our method is based on the assumption that the label noise matrix S = [s 1 , . . . , s N ], where s i is the difference between the observed label y i and the underlying true label, is a sparse matrix. We made no additional assumption on the sparsity pattern of S , other than the non-negative and non-positive constraints discussed in <ref type="bibr" target="#b3">(4)</ref>. In practice, it is usually the case that certain pairs of classes are more similar hence more easily confusing with each other than other pairs. As a result, certain blocks of S tend to have more non-zero entries than the others. When there is a prior on which blocks may have more non-zero entries, our method may be adapted by using a weighted sparse regularization for the corresponding blocks. When there is no such prior, our method may be adapted by using a group sparse regularization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b93">94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of learned representations against label noise.</head><p>Recently, a line of work showed an intriguing and universal phenomenon of learned deep representations under natural setting <ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref>, that the last-layer representations of each class collapse to a single dimension. However, the collapsed representation loses the variability of the data and is vulnerable to corruptions such as label noise. Another line of recent work <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b98">99]</ref> empirically showed and argued that mapping each class to a linearly separable subspace with maximum dimension (instead of collapsing them to a vertex of Simplex ETF) can improve robustness against label noise and random data corruptions. Based upon the proposed implicit regularizations of the network, it would be interesting to study and further justify the robustness of the proposed methods in terms of the learned last-layer representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>This appendix is organized as follows. In Section A we provide additional details for reproducing experimental results presented in Section 2. In Section B we provide proofs for the theoretical results presented in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details for Robust Classification with Label Noise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Choice of Loss Function</head><p>The cross-entropy loss in <ref type="bibr" target="#b6">(7)</ref> cannot be used to optimize {v i } as we explain below. Consider a data point x with a one-hot label y. With the CE loss in <ref type="bibr" target="#b6">(7)</ref> rewritten below for convenience:</p><formula xml:id="formula_37">L CE (?, u, v; x, y) . = CE ? f (x, ?) + s , y , with s . = u u y ? v v (1 ? y), (A.1)</formula><p>we may compute its gradient with respect to (w.r.t.) v as</p><formula xml:id="formula_38">?L CE (?, u, v; x, y) ?v = 2v (1 ? y) 1 (f (x, ?) + s)</formula><p>.</p><formula xml:id="formula_39">(A.2)</formula><p>This shows that the gradient w.r.t. different entries of v does not depend on the output f (x, ?) of the model at all modulo the divider shared by all entries. Hence, v cannot correctly learn the label noise. We now consider the MSE loss in (8) rewritten below for convenience:</p><formula xml:id="formula_40">L MSE (?, u, v; x, y) . = MSE f (x, ?) + s, y , with s . = u u y ? v v (1 ? y). (A.3)</formula><p>The gradient w.r.t. v can be computed as</p><formula xml:id="formula_41">?L MSE (?, u, v; x, y) ?v = 4(f (x, ?) + s ? y) v (1 ? y). (A.4)</formula><p>Here the gradient w.r.t. different entries of v varies depending on how well the model prediction f (x, ?) + s matches the given label y at the corresponding entry. Hence, when the model prediction deviates from the given label which may occur when the label is corrupted, v is able to learn the underlying corruption to the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Definition of Label Noise</head><p>In this paper, we consider two types of widely existed label noise, namely symmetric label noise and asymmetric label noise. For symmetric noise with noise level ?, the labels are generated as follows: y = y GT with the probability of 1 ? ? random one hot vector with the probability of ?.</p><p>We consider noise level ? ? {0.2, 0.4, 0.5, 0.6, 0.8}. For asymmetric noise, following <ref type="bibr" target="#b24">[25]</ref>, we flip labels between TRUCK ?AUTOMOBILE, BIRD ? AIRPLANE, DEER ? HORSE, and CAT ? DOG. We randomly choose 40% training data with their labels to be flipped according to this asymmetric labeling rule. For real world datasets, Clothing1M has noise level estimated at around 38.5% <ref type="bibr" target="#b99">[100]</ref>, and for WebVision, the noise level is estimated to be at around 20% <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details of SOP+</head><p>We considered two separate regularization terms to further boost the results and stabilize training. We will describe the definitions and roles of them below: Consistency regularizer L C .. We use a regularizer L C to encourage consistency of network prediction on a original image and the corresponding augmented image. Such a regularizer is commonly used in semisupervised learning and label noise learning literature, see e.g., <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102]</ref>. Specifically, the consistency regularizer L C is defined as the Kullback-Leibler (KL)-divergence between the softmax predictions from the images with augmentations (described in Section A.4) and the softmax predictions for the corresponding  images generated with Unsupervised Data Augmentation (UDA) <ref type="bibr" target="#b102">[103]</ref>:</p><formula xml:id="formula_42">L c (?) = 1 N N i=1 D KL (f (x i ; ?) f (UDA(x i ); ?)) .</formula><p>Class-balance regularizer L B .. We use a regularizer L B to prevent the network from assigning all data points to the same class. Following <ref type="bibr" target="#b103">[104]</ref>, we use the prior information on the probability distribution p of class labels and minimize its distance in terms of KL-divergence to the mean prediction of each batch B:</p><formula xml:id="formula_43">L b (?) = K k=1 p k log p k f k (x, ?) = ? K k=1 p k log f k (x; ?), where f k (x; ?) ? 1 |B| x?B f (x; ?)</formula><p>, and p k stands for the prior probability of the kth class. The final loss function for SOP+ is therefore constructed by three terms as follows</p><formula xml:id="formula_44">L(?, {u i , v i }) + ? C L C (?) + ? B L B (?),</formula><p>where ? c , ? B &gt; 0 are the hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Experimental Settings</head><p>Data processing:. For experiments on CIFAR10/100 <ref type="bibr" target="#b16">[17]</ref> without extra techniques, we use simple data augmentations including random crop and horizontal flip following previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. For SOP+, we use the default setting from unsupervised data augmentation <ref type="bibr" target="#b102">[103]</ref> to apply efficient data augmentation to create another view of the data for consistency training. For Clothing-1M <ref type="bibr" target="#b21">[22]</ref>, we first resize images to 256 ? 256, and then random crop to 224 ? 224, following a random horizontal flip. For WebVision <ref type="bibr" target="#b22">[23]</ref>, we randomly crop the images into size of 227 ? 227. All images are standardized by their means and variances. Hyper-parameters of SOP:. We adopt a SGD optimizer without weight decay for U and V . We keep all the hyper-parameters fixed for different levels of noise. For fair comparison, we adopt two settings of hyperparameters and architectures for SOP and SOP+. More details of hyper-parameters can be found in <ref type="table" target="#tab_5">Table A</ref>.4. Note that the method is not very sensitive to hyper-parameters ? C and ? B .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs for Theoretical Analysis with Linear Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Proposition 3.2</head><p>We first present a simple but useful lemma.</p><p>Lemma B.1. Let (?, u, v) be a critical point to <ref type="bibr" target="#b13">(14)</ref> that is not a global minimum, i.e.,</p><formula xml:id="formula_45">r := J ? + u u ? v v ? y = 0.</formula><p>Then there exists an index i such that</p><formula xml:id="formula_46">u i = v i = 0, r i = 0, (B.1)</formula><p>where u i , v i , and r i denote the i-th elements of u, v and r, respectively.</p><p>Proof. We may compute the gradient of the objective function h in <ref type="formula" target="#formula_0">(14)</ref> as</p><formula xml:id="formula_47">? ? h(?, u, v) = J r, ? u h(?, u, v) = 2r u, ? v h(?, u, v) = ?2r v. Since r = 0 but ? u h(?, u, v) = ? v h(?, u, v) = 0,</formula><p>we must have u i = v i = 0 and r i = 0 for some i.</p><p>We now prove Proposition 3.2 as follows.</p><p>Proof of Proposition 3.2. We compute the hessian ? 2 h of the objective function h in <ref type="formula" target="#formula_0">(14)</ref> as</p><formula xml:id="formula_48">? 2 h(?, u, v) = ? ? J J 2J diag(u) ?2J diag(v) 2 diag(u)J 2 diag (r + 2u u) ?4 diag(v u) ?2 diag(v)J ?4 diag(v u) ?2 diag (r ? 2v v) ? ? .</formula><p>For any direction d = d ? d u d v , the quadratic form of the Hessian ? 2 h along this direction is given by</p><formula xml:id="formula_49">d ? 2 h(?, u, v)d = J d ? 2 2 + 4 u d u 2 2 + 4 v d v 2 2 + 2 r, d u d u ? d v d v + 4 J d ? , u d u ? v d v ? 8 u d u , v d v . (B.2)</formula><p>We now consider an arbitrary critical point (?, u, v) of (14) that is not a global minimum. By Lemma B.1, there exists an i such that r i = 0 while u i = v i = 0. We divide the discussion into two cases.</p><p>? Case 1: r i &gt; 0. We set d ? = 0, d u = 0, and d v to be such that all of its entries are zero except for the i-th entry which is given by</p><formula xml:id="formula_50">d i v = 1. Plugging this direction into (B.2), we obtain d ? 2 h(?, u, v)d = 4 [v i ] 2 v i =0 [d i v ] 2 ? 2r i [d i v ] 2 d i v =1 = ?2r i &lt; 0</formula><p>? Case 2: r i &lt; 0. We set d ? = 0, d v = 0, and d u to be such that all of its entries are zero except for the i-th entry which is given by d i u = 1. Plugging this direction into (B.2), we obtain</p><formula xml:id="formula_51">d ? 2 h(?, u, v)d = 4 [u i ] 2 u i =0 [d i u ] 2 + 2r i [d i u ] 2 d i u =1 = 2r i &lt; 0</formula><p>In both cases above we have constructed a direction of negative curvature, hence (?, u, v) is a strict saddle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 3.3</head><p>The proof is based on the following lemma which follows trivially from KKT conditions: then ( ?, s) is an optimal solution to <ref type="bibr" target="#b24">(25)</ref>. In above, sign(z) is defined entrywise on z as</p><formula xml:id="formula_52">sign(z) = z/|z| if z = 0, [?1, 1] if z = 0. (B.4)</formula><p>Proof of Proposition 3.3. We divide the proof into two parts.</p><p>Global convergence. In this part we show that ? ? (?, ?), u ? (?, ?), v ? (?, ?) is a global solution to <ref type="bibr" target="#b13">(14)</ref> for any fixed (?, ?). Denote</p><formula xml:id="formula_53">r ? (?, ?) . = lim t?? r t (?, ?). (B.5)</formula><p>It follows from <ref type="formula" target="#formula_0">(17)</ref> and <ref type="formula" target="#formula_0">(21)</ref> that the limit r ? (?, ?) exists and can be written as</p><formula xml:id="formula_54">r ? (?, ?) = J ? ? (?, ?) + u ? (?, ?) u ? (?, ?) ? v ? (?, ?) v ? (?, ?). (B.6)</formula><p>Suppose for the purpose of obtaining a contradiction that ? ? (?, ?), u ? (?, ?), v ? (?, ?) is not a global solution to <ref type="bibr" target="#b13">(14)</ref>. It follows from Lemma B.1 that there exists an i such that</p><formula xml:id="formula_55">u i ? (?, ?) = v i ? (?, ?) = 0, and r i ? (?, ?) = 0. (B.7)</formula><p>Without loss of generality we assume that C .</p><formula xml:id="formula_56">= r i ? (?, ?) &gt; 0 so that r i t (?, ?) ? C with t ? ?.</formula><p>For any ? (0, C), there exists a t 0 &gt; 0 such that</p><formula xml:id="formula_57">C ? ? r i t (?, ?) ? C + , ?t &gt; t 0 . (B.8)</formula><p>It follows from (B.8) and <ref type="formula" target="#formula_1">(20)</ref> that</p><formula xml:id="formula_58">? i t (?, ?) = ? t 0 r i ? (?, ?)d? = ? i t0 (?, ?) ? t t0 r i ? (?, ?)d? ? ? i t0 (?, ?) ? (C + )(t ? t 0 ), ? i t0 (?, ?) ? (C ? )(t ? t 0 ) , ?t &gt; t 0 . (B.9) Using this bound on ? i t (?, ?) in (19), we obtain v i t (?, ?) = ? exp ? 2?? i t (?, ?) ? ? exp ? 2?? i t0 (?, ?) exp 2?(C ? )(t ? t 0 ) , ?t &gt; t 0 . (B.10) Taking the limit of t ? ?, we obtain v i ? (?, ?) = ? which contradicts v i ? (?, ?) = 0 in (B.7). Therefore, we conclude that ? ? (?, ?), u ? (?, ?), v ? (?, ?)</formula><p>is a global solution to <ref type="bibr" target="#b13">(14)</ref>. Implicit regularization. In this part we prove that ( ?, s) is an optimal solution to the regularized convex optimization problem in <ref type="bibr" target="#b24">(25)</ref>. Let ? ? (?, ?) be the limit of ? t (?, ?) in <ref type="formula" target="#formula_1">(20)</ref>  For each entry of s = lim ??0 s ? (?, ?(?)) (recall that ?(?) is defined in <ref type="formula" target="#formula_1">(22)</ref>), we may have three cases:</p><p>? Case 1: s i &gt; 0. From (B.15), we must have ?(?)? i ? (?, ?(?)) ? +? as ? ? 0 so that lim Hence, for any small ? (0, 1), there exists ? 0 &gt; 0 such that for all ? ? (0, ? 0 ), we have ? 2 ? max exp 4?(?)? i ? (?, ?(?)) , exp ? 4?(?)? i ? (?, ?(?)) &lt; =? 2 log ? + 4?(?) ? |? i ? (?, ?(?))| &lt; log =? |? i ? (?, ?(?))| &lt; log 4?(?) ? log ? 2?(?) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B.21)</head><p>Now, plugging ?(?) = ? log ? 2? in, we have |? i ? (?, ?(?)))| &lt; ? ? log 2 log ? + ? &lt; ?.</p><p>Therefore, we have lim ??0 ? i ? (?, ?(?)) &lt; ?.</p><p>Synthesizing all the above three cases, we obtain:</p><p>? ? ?sign( s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Proposition 3.5</head><p>We begin with introducing the null space property that is widely used for providing necessary and sufficient conditions for exact recovery of sparse signals in compressive sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition B.3 ( [105]</head><p>). We say a matrix A ? R m?n satisfies the null space property with constant ? ? (0, 1) relative to S ? [n] if v S 1 ? ? v S c 1 for all v ? ker A, where ker A is the null space of A.</p><p>which is further equivalent to min s g(s) := 1 2 V ? ?1 U (y ? s) 2 2 + ? s 1 s.t. As = As.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B.24)</head><p>Assume A satisfies the stable null space property with constant ? ? (0, 1) relative to the support of s . Now for any s with As = As, by Theorem B.6, we have</p><formula xml:id="formula_59">s 1 ? s 1 ? 1 ? ? 1 + ? s ? s 1 ,</formula><p>which ensures s = s if we only minimize s 1 . The first term in (B.24) can be written as</p><formula xml:id="formula_60">V ? ?1 U (y ? s) 2 2 = V ? ?1 U (s ? s + J ? ) 2 2 , where ? = V ? ?1 U (y ? s ).</formula><p>This together with the previous equation gives</p><formula xml:id="formula_61">g(s) ? g(s ) ? ? 1 ? ? 1 + ? s ? s 1 + V ? ?1 U (s ? s + J ? ) 2 2 ? V ? ?1 U J ? 2 2 = ? 1 ? ? 1 + ? s ? s 1 + V ? ?1 U (s ? s) 2 2 + 2 s ? s, U ? ?1 V ? ? ? 1 ? ? 1 + ? s ? s 1 ? 2 U ? ?1 V ? ? s ? s 1 = ? 1 ? ? 1 + ? ? 2 U ? ?1 V ? ? s ? s 1 .</formula><p>Thus, if ? &gt; ? 0 with </p><formula xml:id="formula_62">? 0 = 2 1 + ? 1 ? ? U ? ?1 V ? ? ,<label>(</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3 . 1 (</head><label>31</label><figDesc>Main result, informal). Suppose J is rank-r and ?-incoherent defined in Section 3.3, and s is k-sparse. If k 2 r &lt; N/(4?), with ? ? 0 and a proper choice of ? depending on (J , ? , k), the gradient dynamics of (15) converges to the ground truth solution (? , s ) in (12) starting from a small initialization of (?, u, v).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :? ? 2 max{ ?? 2 ,</head><label>222</label><figDesc>The gradient descent in<ref type="bibr" target="#b14">(15)</ref> and the convex problem in<ref type="bibr" target="#b24">(25)</ref> produce the same solutions with ? = ? log ? 2? . For fixed data (J , y), left figure shows the relative difference ???? ? 2} between the solution ? ? to<ref type="bibr" target="#b14">(15)</ref> with varying values of ? (in y-axis) and the solution ? ? computed from(25)with varying values of ? (in x-axis). Likewise, right figure shows the relative difference for s. Blue line shows the curve ? = ? log ? 2? where ? is fixed to exp (?8) in all experiments. comes as a result of implicit regularization on overparameterization s = u u ? v v, leading to a sparse solution on s as we desired. On the other hand, the 2 regularization on ? leads to the desired minimum Numerical verification. While Proposition 3.3 is proved for gradient flow with both learning rate ? ? 0 and initialization scale ? ? 0, we numerically show that such a result also holds non-asymptotically with finitely small ? and ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 3 . 4 (</head><label>34</label><figDesc><ref type="bibr" target="#b7">[8]</ref>). Let J = U ?V ? IR N ?p be the compact SVD of J and r be the rank of J . The coherence of J (w.r.t. the standard basis) is defined as ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 3 . 5 .</head><label>35</label><figDesc>Let r be the rank of J and k be the number of nonzero entries of s . If we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we measure the relative recovery error ? = ??? 2 ? 2 and s = s?s 2 s 2 . ? Effects of the parameter ?. Here, we consider the recovery with varying ? ? [0.0001, 1]. First, we fix r = 20 and vary k ? {10, 20, 40, 60, 80}, showing the relative recovery errors ? and s in Figure 3(a). Second, we fix k = 20 and vary r ? {10, 20, 40, 60, 80}, showing the results inFigure 3(b). The results show a clear phase transition that correct recovery is obtained only when ? is greater than a particular threshold ? 0 . Moreover, ? 0 varies depending on k and r, consistent with Proposition 3.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Varying k with fixed r = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) Varying r with fixed k = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Effect of model parameter ? for exact recovery by<ref type="bibr" target="#b24">(25)</ref>. The y-axis is the relative error of ? (left) and s (right) defined as ??? 2 ? 2 and s?s 2 s 2 , respectively, where (?, s) is the solution to<ref type="bibr" target="#b24">(25)</ref>. The curves are averages over 10 independent trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Phase transition for solving (25) over 20 trials, with fixed ? = 0.1 and varying k, r. Recovery is declared success if ??? 2 ? 2 &lt; 0.001 (left) and s?s 2 s 2 &lt; 0.001 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Sym: ?u = 10, ?v = 10 Sym: ?u = 10, ?v = 10 Sym: ?u = 1, ?v = 10 Sym: ?u = 1, ?v = 10 ?u = 0.1 , ?v = 1 ?u = 0.1 , ?v = 1 Asym: ?u = 10, ?v = 100 Asym: ?u = 10, ?v = 100 Asym: ?u = 1, ?v = 100 Asym: ?u = 1, ?v = 100 wd for {ui,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma B. 2 (</head><label>2</label><figDesc>KKT condition). Given any J and y, if there exists ( ?, s, ?) satisfying y = J ? + s, ? = J ?, and ? ? ?sign( s), (B.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 . 2 . 3 .</head><label>123</label><figDesc>at t ? ?, and let ? . = lim ??0 ? ? (?, ?(?)), (B.11)with ?(?) defined in<ref type="bibr" target="#b21">(22)</ref>. We only need to show that the triplet ( ?, s, ?) with ? defined in<ref type="bibr" target="#b22">(23)</ref>, s defined in<ref type="bibr" target="#b23">(24)</ref> and ? defined in (B.11) satisfies the KKT conditions in (B.3). Because ? ? (?, ?), u ? (?, ?), v ? (?, ?) is a global solution to<ref type="bibr" target="#b13">(14)</ref>, we haveJ ? ? (?, ?) + u ? (?, ?) u ? (?, ?) ? v ? (?, ?) v ? (?, ?) = y, ?? &gt; 0, ? &gt; 0. (B.12)Taking the limit of ? ? 0 with ? = ?(?) and noting the assumption that all limits in (23) exist, we obtainJ ? + u u ? v v = y. (B.13)Plugging in the definition of s in(24), we obtain y = J ? + s. By taking the limit of the relation ? t (?, ?) = J ? t (?, ?) in<ref type="bibr" target="#b18">(19)</ref> and noting the assumptions that all relevant limits exist, we obtain? = lim ??0 lim t?? ? t (?, ?(?)) = lim ??0 lim t?? J ? t (?, ?(?)) = J ?. (B.14) Denote s ? (?, ?) . = u ? (?, ?) u ? (?, ?) ? v ? (?, ?) v ? (?, ?). By (19), we have s i ? (?, ?) . = u i ? (?, ?) 2 ? v i ? (?, ?) 2 = ? 2 exp(4?? i ? (?, ?)) ? ? 2 exp(?4?? i ? (?, ?)). (B.15)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>? 2 ? Case 2 :?? 2 ? Case 3 :? 2</head><label>22232</label><figDesc>??0 exp 4?(?)? i ? (?, ?(?)) = ?, and lim ??0 exp ? 4?(?)? i ? (?, ?(?)) = 0. (B.16)Hence,lim ??0 exp 4?(?)? i ? (?, ?(?)) = s i =? lim ??0 2 log ? + 4?(?)? i ? (?, ?(?)) = log s i relation ?(?) = ? log ? 2? in (22), we have lim ??0 ? i ? (?, ?(?)) = ? lim ??0 ? log s i 2 log ? + ? = ?. s i &lt; 0. Similar to case 1, from (B.15) we must have lim ??0 exp 4?(?)? i ? (?, ?(?)) = 0, and lim ??0 exp ? 4?(?)? i ? (?, ?(?)) = ?. exp ? 4?(?)? i ? (?, ?(?)) = s i =? lim ??0 2 log ? ? 4?(?)? i ? (?, ?(?)) = log(? s i ) =? lim ??0 ? i ? (?, ?(?)) = lim ??0 ? log(? s i ) 4?(?) + log ? 2?(?) . (B.19)Plugging in the relation ?(?) = ? log ? 2? in(22), we have lim??0 ? i ? (?, ?(?)) = ??. s i = 0. From (B.15), we must have lim ??0 exp 4?(?)? i ? (?, ?(?)) = 0, and lim ??0 ? 2 exp ? 4?(?)? i ? (?, ?(?)) = 0. (B.20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>B. 25 )U e i 2 ? k a 2</head><label>2522</label><figDesc>we have g(s) ? g(s ) &gt; 0 whenever s = s .Proof of Lemma B.5.Proof. Let J = U ?V be the compact SVD of J . From (B.22) we have(? + 1)k r N ?(J ) ? ?. (B.26) Let S ? [N ]with |S| = k and a ? IR r be an arbitrary vector. We have[U a] S 1 = i?S |e i U a| = i?S | U e i , a | ? k a 2 ?max i?S inequality is obtained from Definition 3.4. In addition, we have U a 1 ? U a 2 = a 2 . (B.28) Combining (B.26), (B.27) and (B.28), we get (? + 1) [U a] S 1 ? (? + 1)k a 2 r N ?(J ) ? ? a 2 ? ? U a 1 , (B.29) hence, [U a] S 1 ? ? [U a] S c 1 . (B.30) Noting that {U a|a ? IR r } = ker A, this finishes the proof by Definition B.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Test accuracy with synthetic label noise on</head><label>1</label><figDesc>CIFAR-10 and CIFAR-100 with {20%, 40%, 60%, 80%} percent of labels for training data randomly flipped uniformly to another class. All methods use ResNet34 as the architecture. Mean and standard deviation over 5 independent runs are reported. ?0.<ref type="bibr" target="#b17">18</ref> 82.65 ?0.16 76.15 ?0.32 59.28 ?0.97 51.43 ?0.58 45.23 ?0.53 36.31 ?0.39 20.23 ?0.82 Forward 87.99 ?0.36 83.25 ?0.38 74.96 ?0.65 54.64 ?0.44 39.19 ?2.61 31.05 ?1.44 19.12 ?1.95 8.99 ?0.58 GCE 89.83 ?0.20 87.13 ?0.22 82.54 ?0.23 64.07 ?1.38 66.81 ?0.42 61.77 ?0.24 53.16 ?0.78 29.16 ?0.74 SL 89.83 ?0.32 87.13 ?0.26 82.81 ?0.61 68.12 ?0.81 70.38 ?0.13 62.27 ?0.22 54.82 ?0.57 25.91 ?0.44 ELR 91.16 ?0.08 89.15 ?0.17 86.12 ?0.49 73.86 ?0.61 74.21 ?0.22 68.28 ?0.31 59.28 ?0.67 29.78 ?0.56 SOP (ours) 93.18 ?0.57 90.09 ?0.27 86.76 ?0.22 68.32 ?0.77 74.67 ?0.30 70.12 ?0.57 60.26 ?0.41 30.20 ?0.63</figDesc><table><row><cell>Methods</cell><cell>20%</cell><cell>CIFAR-10 40% 60%</cell><cell>80%</cell><cell>20%</cell><cell>CIFAR-100 40% 60%</cell><cell>80%</cell></row><row><cell>CE</cell><cell>86.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Comparison with the state-of-the-art methods that</head><label>2</label><figDesc></figDesc><table><row><cell>use two network ensembles and semi-supervised</cell></row></table><note>SOP+ (ours) 96.3 95.5 94.0 93.8 78.8 75.9 63.3 78.0 trained on mini WebVision are evaluated on both WebVision and ImageNet ILSVRC12 validation set. Details on the label noise for these datasets is provided in Section A.2. Network structures &amp; hyperparameters. We implement our method with PyTorch v1.7. For each dataset, the choices of network architectures and hyperparameters for SOP are as follows. Additional details, as well as hyper-parameters for both SOP and SOP+, can be found in Appendix A.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>?0.11 85.02 ?0.65 86.46 ?1.79 85.16 ?0.61 87.77 ?0.38 77.69 ?1.55 76.70 ?0.74 55.50 ?0.66 Forward 93.02 ?0.12 86.88 ?0.50 86.14 ?0.24 87.04 ?0.35 88.24 ?0.22 79.79 ?0.46 76.18 ?0.37 57.01 ?1.03 Co-teaching 93.35 ?0.14 90.33 ?0.13 90.30 ?0.17 90.15 ?0.18 91.20 ?0.13 83.83 ?0.13 73.46 ?0.09 60.37 ?0.27 ELR+ 95.39 ?0.05 94.43 ?0.41 94.20 ?0.24 94.34 ?0.22 94.83 ?0.10 91.09 ?1.60 78.57 ?0.12 66.72 ?0.07 CORES</figDesc><table><row><cell>Methods</cell><cell>Clean</cell><cell>CIFAR-10N Random 1 Random 2 Random 3 Aggregate</cell><cell>Worst</cell><cell>CIFAR-100N Clean Noisy</cell></row><row><cell>CE</cell><cell>92.92</cell><cell></cell><cell></cell><cell></cell></row></table><note>Test accuracy with realistic label noise on CIFAR-N. Mean and standard deviation over 5 independent runs are reported. The results of the baseline methods are taken from [21] which all use ResNet34 as the architecture. For SOP+, we use PreActResNet18.* 94.16 ?0.11 94.45 ?0.14 94.88 ?0.31 94.74 ?0.03 95.25 ?0.09 91.66 ?0.09 73.87 ?0.16 55.72 ?0.42 SOP+(ours) 96.38 ?0.31 95.28 ?0.13 95.31 ?0.10 95.39 ?0.11 95.61 ?0.13 93.24 ?0.21 78.91 ?0.43 67.81 ?0.23</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of total training time in hours on CIFAR-10 with 50% symmetric label noise. CE Co-teaching+ DivideMix ELR+ SOP (ours) SOP+ (ours)</figDesc><table><row><cell>0.9h</cell><cell>4.4h</cell><cell>5.4h</cell><cell>2.3h</cell><cell>1.0h</cell><cell>2.1h</cell></row><row><cell>Thus, for a dataset {x i</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc>1: Hyper-parameters for SOP on CIFAR-10/100, Clothing-1M and Webvision datasets.</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell>Clothing-1M</cell><cell>Webvision</cell></row><row><cell>architecture</cell><cell>ResNet34</cell><cell>PreAct PresNet18</cell><cell>ResNet34</cell><cell>PreAct PresNet18</cell><cell cols="2">ResNet-50 (pretrained) InceptionResNetV2</cell></row><row><cell>batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>32</cell></row><row><cell>learning rate (lr)</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell>0.0 2</cell><cell>0.002</cell><cell>0.02</cell></row><row><cell>lr decay</cell><cell>40th &amp; 80th</cell><cell>Cosine Annealing</cell><cell>40th &amp; 80th</cell><cell>Cosine Annealing</cell><cell>5th</cell><cell>50th</cell></row><row><cell>weight decay (wd)</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>1 ? 10 ?3</cell><cell>5 ? 10 ?4</cell></row><row><cell>training epochs</cell><cell>120</cell><cell>300</cell><cell>150</cell><cell>300</cell><cell>10</cell><cell>100</cell></row><row><cell>training examples</cell><cell>45,000</cell><cell>50,000</cell><cell>45,000</cell><cell>50,000</cell><cell>1,000,000</cell><cell>66,000</cell></row><row><cell>lr for {ui, vi}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at: https://github.com/shengliu66/SOP. 1 arXiv:2202.14026v2 [cs.LG] 2 Aug 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also project f (x i ; ?) to a one-hot vector when using MSE loss which is empirically found to accelerate convergence of {v i }.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In other words, not all global solutions recover the underlying ? and s</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">-norm solution as we discussed in<ref type="bibr" target="#b12">(13)</ref>. Thus, the only question remains is whether the ground truth (? , s ) in (12) can be identified through solving the convex problem<ref type="bibr" target="#b24">(25)</ref>, which we will discuss in the following Section 3.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the ECOS solver<ref type="bibr" target="#b32">[33]</ref> provided in CVXPY<ref type="bibr" target="#b33">[34]</ref> for solving<ref type="bibr" target="#b24">(25)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our low-rank assumption is an idealization of the approximate low-rank property of the Jacobian, which simplifies our analysis but at the cost that our model is not able to overfit to any corrupted labels as a deep neural network. We leave the study under approximate low-rank assumption to future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lemma B.4. Given matrix J and a matrix A that annihilates J on the left (i.e. such that AJ = 0). If A satisfies the stable null space property with constant ? ? (0, 1) relative to the support of s , then the solution to <ref type="bibr" target="#b24">(25)</ref> is (? , s ) for any ? &gt; ? 0 where ? 0 is a scalar that depends only on (J , ? , ?).</p><p>The second lemma shows that the null space property is satisfied under the incoherent condition in <ref type="bibr" target="#b26">(27)</ref>. Lemma B.5. Given matrix J and a matrix A that annihilates J on the left, if</p><p>then A satisfies null space property with constant ? relative to any S that satisfies |S| = k.</p><p>Proof of Proposition 3.5. Assume that the condition in <ref type="formula">(27)</ref> is satisfied. Then there exists a ? ? (0, 1) such that the condition in (B.22) holds. Hence, A satisfies null space property with constant ? relative to any S that satisfies |S| = k. Since s is k-sparse, we have that A satisfies null space property with constant ? relative to the support of s . Then the conclusion of Proposition 3.5 follows from applying Lemma B.4. Finally, from Lemma B.4 we have that ? 0 is a function of (J , ? , ?), wherein ? is determined by A (hence J ) and the associated sparsity k.</p><p>Hence ? 0 can be determined with a given (J , ? , k).</p><p>In the rest of this section we prove Lemma B.4 and Lemma B.5.</p><p>Proof of Lemma B.4. We first introduce the following result on a useful property of the stable null space property.</p><p>Theorem B.6 (Useful property of stable null space property). Suppose a matrix A ? R m?n satisfies the null space property with constant ? ? (0, 1) relative to S ? [n]. Then for every vector x supported on S, we have</p><p>for any z with Az = Ax.</p><p>Proof of Theorem B.6. Since A(z ? x) = 0, i.e., z ? x ? ker A, the null space property of A implies</p><p>We now use these properties to prove the main result as</p><p>where the first inequality follows because x is only supported on S.</p><p>We are now ready to prove Lemma B.4. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical biasvariance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking bias-variance trade-off for generalization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10767" to="10777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6614</idno>
		<title level="m">search of the real inductive bias: On the role of implicit regularization in deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="210" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust modeling with erratic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Claerbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="826" to="844" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit regularization for optimal sparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaskevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rebeschini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2968" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Implicit regularization via hadamard product over-parametrization in highdimensional linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-C</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09367</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust recovery via implicit bias of discrepant learning rates for double over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17733" to="17744" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2937" to="2947" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with noisy labels revisited: A study using real-world human annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12088</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with instance-dependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sgd on neural networks learns functions of increasing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalimeris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3496" to="3506" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Escaping from saddle points-online stochastic gradient for tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient descent only converges to minimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1246" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ECOS: An SOCP solver for embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Domahidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Control Conference (ECC)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3071" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CVXPY: A Python-embedded modeling language for convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">83</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05392</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of low-rank matrix recovery from incomplete observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="622" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonconvex optimization meets low-rank matrix factorization: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5239" to="5269" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Implicit regularization in matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Information Theory and Applications Workshop (ITA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overparameterized nonlinear learning: Gradient descent takes the shortest path?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4951" to="4960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7411" to="7422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep learning may not be explainable by norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradient descent follows the regularization path for general losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dud?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2109" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>St?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15933</idno>
		<title level="m">Deep linear networks dynamics: Low-rank biases induced by initialization scale and l2 regularization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Kernel and rich regimes in overparametrized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3635" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Implicit sparse regularization: The impact of depth and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">More is less: Inducing sparsity via overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rauhut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Implicit regularization of sub-gradient method in robust matrix recovery: Don&apos;t be afraid of outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fattahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02969</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rank overspecified robust matrix recovery: Subgradient method and exact recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Understanding (generalized) label smoothing whenlearning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04149</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image classification with deep learning in the presence of noisy labels: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Algan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ulusoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page">106771</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15013" to="15022" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning diverse and discriminative representations via the principle of maximal coding rate reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9422" to="9434" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">When optimizing f -divergence is robust with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Blessing of nonconvexity in deep linear models: Depth flattens the optimization landscape around the true solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fattahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07612</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Can gradient clipping mitigate label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multiclass learning with partially corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2568" to="2580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1002" to="1012" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dualgraph: A graph-based method for reasoning about label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9654" to="9663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Robust neural network classification via double regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zetterqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>J?rnsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jonasson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08102</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10456" to="10465" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6838" to="6849" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Clusterability as an alternative to anchor points when learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05291</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Provably end-to-end label-noise learning without anchor points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02400</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Learning noise transition matrix from only noisy labels via total variation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02414</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fine samples for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4804" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4313" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Adaptive early-learning correction for segmentation from noisy annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<idno>abs/2110.03740</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Learning to combat noisy labels via classification margins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00751</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Self-adaptive training: beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10319</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Error-bounded correction of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11447" to="11457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Simple and effective regularization methods for training on noisily labeled data with generalization guarantee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">241</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Do we actually need dense over-parameterization? in-time over-parameterization in sparse training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Sparsity winning twice: Better robust generalization from more efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3051" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Momentum doesn&apos;t change the implicit bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03891</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Equivalences between sparse models and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Prevalence of neural collapse during the terminal phase of deep learning training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="24652" to="24663" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Neural collapse under mse loss: Proximity to and dynamics on the central path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02073</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A geometric analysis of neural collapse with unconstrained features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">43</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Redunet: A white-box deep network from the principle of maximizing rate reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10446</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Prestopping: How does early stopping help generalization against label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Compressed sensing and best k-term approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American mathematical society</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">With Definition B.3, we prove Proposition 3.5 by using the following two lemmas. The first lemma establishes correct recovery of (? , s ) from (25) under the null space property</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
