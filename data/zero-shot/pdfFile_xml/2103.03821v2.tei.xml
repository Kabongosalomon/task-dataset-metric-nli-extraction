<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Interactive Video Object Segmentation with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Varga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">E?tv?s Lor?nd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>L?rincz</surname></persName>
							<email>lorincz@inf.elte.hu</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Informatics E?tv?s</orgName>
								<orgName type="institution">Lor?nd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Interactive Video Object Segmentation with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pixelwise annotation of image sequences can be very tedious for humans. Interactive video object segmentation aims to utilize automatic methods to speed up the process and reduce the workload of the annotators. Most contemporary approaches rely on deep convolutional networks to collect and process information from human annotations throughout the video. However, such networks contain millions of parameters and need huge amounts of labeled training data to avoid overfitting. Beyond that, label propagation is usually executed as a series of frame-by-frame inference steps, which is difficult to be parallelized and is thus time consuming. In this paper we present a graph neural network based approach for tackling the problem of interactive video object segmentation. Our network operates on superpixel-graphs which allow us to reduce the dimensionality of the problem by several magnitudes. We show, that our network possessing only a few thousand parameters is able to achieve state-of-the-art performance, while inference remains fast and can be trained quickly with very little data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video object segmentation (VOS) aims to segment multiple objects in videos from possibly unseen semantic categories, without identifying the category itself. The most popular ways to solve VOS are fully automatic (unsupervised) foreground segmentation and semi-supervised object segmentation methods <ref type="bibr" target="#b0">[1]</ref>. Both approaches are inherently limited and can be rarely applied in practice when high quality segmentation is needed.</p><p>While unsupervised VOS solutions do not require human supervision, they easily fail in cases when the objects are not moving or are not clearly separated (e.g., a person riding a bicycle). Also, by definition <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, it is not allowed for human annotators to indicate objects of interest in the scene, ?2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p><p>Viktor Varga was supported by the Hungarian Government and co-financed by the European Social Fund (EFOP-3.6.3-VEKOP-16-2017-00002: EFOP-3.6.3-VEKOP-16-2017-00002, Integrated Program for Training New Generation of Scientists in the Fields of Computer Science. Andr?s L?rincz was supported by the Thematic Excellence Programme (Project no. ED 18-1-2019-0030 titled Application-specific highly reliable IT solutions) of the National Research, Development and Innovation Fund of Hungary. The authors thank Robert Bosch, Ltd. Budapest, Hungary for their generous support to the Department of Artificial Intelligence or to make corrections in the label predictions, therefore the maximum accuracy is limited.</p><p>Semi-supervised VOS receives full, pixelwise GT annotation in one or several frames throughout the video. However, producing high quality annotation even for a single frame can be very slow <ref type="bibr" target="#b3">[4]</ref> and again, by definition <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> no corrections can be made in the label predictions of unannotated frames.</p><p>On the other hand, interactive VOS suits applications requiring 90+% accuracy in segmentation labeling. In this case, precise high quality full frame annotations are replaced by much quicker, although less precise annotation means: the annotator iteratively marks mistakes in label predictions. Following each interaction step, the method updates the label predictions for the whole video, using the information provided by the annotator.</p><p>Acuna et al. <ref type="bibr" target="#b4">[5]</ref> predict the outline of objects as polygons, which can be efficiently corrected by the dragging of their vertices. Song et al. <ref type="bibr" target="#b5">[6]</ref> aim to learn automatic seed point generation in image segmentation and also enable users to intervene by injecting seed points manually. Nagaraja et al. <ref type="bibr" target="#b6">[7]</ref> employ scribbles -a crude, but fast way to input user annotation.</p><p>Caelles et al. <ref type="bibr" target="#b3">[4]</ref> put together the first and only database so far to support evaluation of interactive VOS methods with the use of scribbles and a simulated annotator agent. The benchmark automatically generates scribbles from the ground-truth label masks to correct submitted dense label predictions, in one frame at a time. In our paper, we use the same annotation technique to be able to derive quantitative comparisons between our works and other ones.</p><p>A large portion of recently published methods in interactive VOS exploit deep Convolutional Neural Networks (CNNs) and are implemented in two main steps. First, the scribble input of each interaction step and predictions obtained from the previous interaction steps are used to estimate the mask and the pixelwise labeling. Second, the estimated mask is propagated in both directions along the video and the label predictions for each frame are updated. This approach has several disadvantages. The mask propagation and the repeated estimation in consecutive frames are computationally intensive and such tasks are restricted to sequential execution since propagation must move frame-by-frame through the video, <ref type="figure">Fig. 1</ref>. Overview of our method. First, the user submits a scribble annotation in one of the frames of the video. Seed points are sampled from the scribble. In the first interaction round, negative (background) seeds are generated as well. The global label model is trained from feature vectors extracted in these points. Then, the seed points are propagated into other frames of the video by our seed propagation algorithm. We annotate the nodes of the superpixel graph based on the location of the seed points. The propagated and the actual annotations form the input of the GNN together with the predictions of the label model, and the precomputed image and optical flow based features in all superpixel nodes. The GNN predicts the labeling of the superpixel nodes throughout the video. The prediction is then returned to the user for correction and it is also used as an input to the GNN in the next inference phase.</p><p>preventing parallelization and giving rise to slow execution and long waiting times for the user. In addition, information propagation is unidirectional within a single interaction step. Finally, both mask estimation and mask propagation utilize deep CNNs having millions of parameters and enormous amounts of labeled data may be required for the proper training of these networks to avoid overfitting. Further review of the literature follows later.</p><p>By contrast, our method is based on a superpixel segmentation, from which a graph is constructed. Labeling of the graph is executed by a Graph Neural Network (GNN). The dimensionality reduction provided by the superpixel segmentation allows us to build models with several magnitudes fewer parameters (in the order of a few thousands only). Accordingly, our model is much less sensitive to overfitting and can be trained with a few videos of the DAVIS training set, while it still achieves high accuracy. Furthermore, the network operates on the whole image sequence at once and executes an iteration of information passing between nodes in each layer, enabling bidirectional information propagation.</p><p>In this paper we present the first GNN based solution to interactive VOS to our best knowledge, and reach state-of-theart results. Our method can be trained with small amounts of data, without overfitting. Additionally, our method is fast, both in terms of training and inference. We release the software code and the trained model at https://github.com/vvarga90/ gnn annot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Neural Networks</head><p>The extension of neural networks to process graphstructured data was first conceived by Gori et al. <ref type="bibr" target="#b7">[8]</ref>. Scarselli et al. improved the method in 2009 <ref type="bibr" target="#b8">[9]</ref>. Since then, many architectural variants have been proposed <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and customized for graph-, edge-, or node classification and regression <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. In recent years, graph neural networks (GNNs) became highly successful in computer vision applications, too. Gao et al. <ref type="bibr" target="#b14">[15]</ref> integrate GNNs into a Siamese framework to solve single object tracking. Bras? and Leal-Taix? <ref type="bibr" target="#b15">[16]</ref> utilize GNNs to find target correspondences in multi-object tracking. Wang et al. <ref type="bibr" target="#b16">[17]</ref> proposes a somewhat similar approach for action recognition. Sarlin et al. <ref type="bibr" target="#b17">[18]</ref> build their GNNs from alternating layers of self-attention and cross-attention and significantly improve accuracy and speed of feature matching in a pair of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Object Segmetation</head><p>Before the dominance of deep learning techniques, approaches to video object segmentation often included the handling of graphically structured data. Many solutions <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> utilize superpixel (or supervoxel) segmentation algorithms as a preprocessing step to reduce the dimensionality of the image data. Superpixels <ref type="bibr" target="#b21">[22]</ref> provide a convenient way to identify spatio-temporally coherent regions of images and to reduce the dimensionality of the VOS problem.</p><p>In the past few years, end-to-end trained deep CNNs significantly raised the bar in the quality of segmentation. Introduction of larger scale and high resolution datasets, e.g., DAVIS <ref type="bibr" target="#b1">[2]</ref> and Youtube-VOS <ref type="bibr" target="#b2">[3]</ref> further helped research. Both datasets released benchmarks in unsupervised and semisupervised categories. In semi-supervised VOS, true pixelwise annotation is given in specific frames (usually in the first frame only) to define semantic categories of importance. Most solutions utilize CNNs in their pipelines. Khoreva et al. <ref type="bibr" target="#b22">[23]</ref> GT 1st step 2nd step 4th step 8th step  <ref type="bibr" target="#b23">[24]</ref> which maintain a memory module to store feature information across time. Seong et al. refine <ref type="bibr" target="#b24">[25]</ref> this method by prioritizing temporally local matches over more distant ones.</p><p>In unsupervised VOS, no indication is given about the semantic categories, resulting in many of the developed methods to seriously overfit to the dataset. Many solutions heavily rely on optical flow estimation to differentiate foreground objects from the background. Wang et al. utilize superpixel segmentation and graph-based saliency estimation <ref type="bibr" target="#b25">[26]</ref> to solve unsupervised VOS. Goel et al. integrate the training of their video segmentation networks with that of a reinforcement learning agent to focus on segmentation of relevant objects <ref type="bibr" target="#b26">[27]</ref>. Oh et al. adapt <ref type="bibr" target="#b27">[28]</ref> Space-Time Memory networks to unsupervised VOS as well.</p><p>Graph-based solutions had been intermittently abandoned in the state-of-the-art literature, as GNN research lagged behind due to the lack of efficient software implementations. In the past two years, however, the field observed a dramatic growth that can be attributed partly to the introduction of graphically structured neural networks in popular deep-learning libraries <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Relatively few results combine GNNs with VOS as of yet. Wang et al. <ref type="bibr" target="#b30">[31]</ref> propose a GNN variant and successfully matches object proposals in different frames of a video setting the new state-of-the-art in unsupervised VOS. Johnander et al. <ref type="bibr" target="#b31">[32]</ref> applies a combination of recurrent and graph neural networks to perform real-time video instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interactive Video Object Segmentation</head><p>Interactive image segmentation has a long history. Here, the human annotator and the machine take turns to refine the dense labeling of the image in cooperation. Well known algorithms, like GrabCut <ref type="bibr" target="#b32">[33]</ref> continue to be in use as part of many annotation pipelines. A natural extension of the problem is interactive VOS, where the temporal propagation of information can drastically increase annotation efficiency.</p><p>Most interactive VOS methods follow a turn based annotation protocol: the task of annotation or correction of the predicted labeling in a certain frame or region is given. This task can be selected by either the user or the algorithm. Next, the user solves the task and these outputs are used in the improvement of the predicted segmentation labeling. Some methods <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref> intend to learn to select optimal annotation tasks for the user (sometimes called active learning). A majority of approaches, including ours, focus on the efficient extraction of information from user input and try to maximize the improvement of the label predictions. Notable examples from the pre-deep learning era include <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Recently, deep learning based tools, such as deep CNNs have successfully been utilized for the task of interactive VOS as well. Quantitative analysis of published methods either relied on human experiments or used simulated annotators with unique annotation protocols. Both cases lead to difficulties in reproduction and comparison to other works. In 2018, Caelles et al. <ref type="bibr" target="#b3">[4]</ref> added an automatic interactive benchmark to the DAVIS video segmentation dataset. The benchmark follows the turn based scheme and for task selection the default evaluation algorithm is: the frame with the poorest quality labeling is selected for annotation. User input takes the form of one or several scribbles restricted to the frame being annotated.</p><p>Almost all top contenders of the DAVIS Interactive Challenges use deep CNNs. Training deep CNNs for mask estimation or propagation has multiple disadvantages. First, the number of trainable parameters is usually in the order of tens of millions, leading to serious overfitting for pixelwise labeling when only small amounts of data are available. Second, if deep CNNs propagate masks between adjacent frames then waiting time between interaction steps may be large since label prediction for each consecutive frame requires a full feedforward pass of the network. Oh et al. solve <ref type="bibr" target="#b37">[38]</ref> inveractive VOS with the help of two networks: an interaction network estimates segmentation masks from user scribbles, while a propagation network approximates labeling in consecutive frames. A slight deficiency of their method is the lack of weight sharing between the two networks in the convolutional layers. Heo et al. achieve superior results with their feature information transfer modules <ref type="bibr" target="#b38">[39]</ref>. A drawback of their method is the need to use multiple additional segmentation datasets for their training process. Miao et al. proposes a more efficient solution by computing all feature representations during the preprocessing stage and only utilizing shallow networks during prediction <ref type="bibr" target="#b39">[40]</ref>. They implement global and local memory modules in a somewhat similar fashion to <ref type="bibr" target="#b38">[39]</ref>.</p><p>To the best of our knowledge, our work is the first to apply GNNs to solve label propagation in interactive VOS. While GNN implementations are less efficient than grid based convolutional layers due to the irregularities of the graph structure, advantages of reduced dimensionality overcome this deficiency and provide a speedy solution for label propagation. Our graph-based solution requires the extraction of node and edge level features which takes up additional computation time. However, most of the feature extraction is a preprocessing step that takes place before the interactive phase and can be easily parallelized, thus it adds a minimal contribution to the waiting time of the annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The outline of our method is depicted in <ref type="figure">Fig. 1</ref>. The procedure is consistent with the interactive protocol of the DAVIS interactive benchmark. Two steps are repeated in an alternating manner. First, the (simulated) user returns a set of annotation scribbles in one of the frames. Then, our method estimates a full, pixelwise labeling of the video and submits it to the benchmark. After that the two steps are repeated until the end of the session. Starting from the second step of the process, the user provides correctional scribbles in reaction to our estimations in the frame where the error rate of our estimation was the worst. The benchmark measures the time taken by our method to generate the label estimation. The benchmark metrics are computed as a function of accuracy and elapsed time.</p><p>Our method relies on a graph constructed over the superpixel segmentation of the video frames. We utilize a GNN to estimate the labeling of the superpixel nodes. In what follows, we describe our approach in detail. First, we provide details about the superpixel segmentation algorithm and the construction of the graph over the superpixel segmentation. Following that, we define our GNN based label propagation model and list the node and edge features used as the GNN input. Finally, we describe the training and inference procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Superpixel graph</head><p>One of our main priorities is to construct a quick and efficient method for interactive VOS. For this reason, we stick with a simple and fast superpixel algorithm, the wellknown SLIC <ref type="bibr" target="#b40">[41]</ref>, which can be easily applied recursively. However, the basic algorithm does not perform that well in terms of boundary recall and undersegmentation error <ref type="bibr" target="#b21">[22]</ref>. Nonetheless, objects to be segmented typically move compared to their background or to other objects and optical flow estimation of the direction and magnitude of object movement can be invoked. We use the FlowNet 2 [42] model. We apply the Canny edge detection algorithm <ref type="bibr" target="#b42">[43]</ref> to find object border candidates. We split superpixel segments with the object border candidates and further apply recursive refinement of the segmentation in the vicinity of these border candidates. This way, the undersegmentation error of the segmentation is significantly reduced. We limit the number of superpixel segments in each frame to have the whole graph fit into GPU memory. Approximate metrics of the graphs can be found in Sect. IV.</p><p>We construct a graph from the superpixel segmentation, where the segments are interpreted as nodes of the graph. Spatially adjacent or causally connected nodes are linked with edges. Causal connections are, again, extracted from optical flow estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Neural Network architecture</head><p>We model the task of video segmentation as a node classification task over the superpixel graph. We learn to estimate label probabilities for each node with a graph neural network (GNN). Our method extract node and edge level features and use them as inputs to our network. We utilize a variant of the Gated GCN <ref type="bibr" target="#b13">[14]</ref> architecture, motivated by both of its performance and its ability to maintain an edge representation beyond the ordinary node representations. We denote the input node feature vectors for node i with x i and the input edge feature vectors for the directed edge i ? j with z ij . More details about the individual input features follow later (in Sect. III-D).</p><p>First, we use linear embeddings to transform the input node and edge feature vectors to the node and edge representations of the GNN.</p><formula xml:id="formula_0">h 1 i = W n x i + b n , e 1 ij = W e z ij + b e</formula><p>Here, h l i denotes the node representation of node i and e l ij designates the edge representation of edge i ? j in layer</p><formula xml:id="formula_1">l. W n ? R H?N , W e ? R H?E , b n ? R H , b e ? R H where N = |x i |, E = |z ij | and H = |h i |.</formula><p>Next, we define the GNN layer based on <ref type="bibr" target="#b13">[14]</ref>. The layer relies on a LSTM-like gating mechanism to construct the updated node representation from a weighted mixture of the representation of the adjacent nodes. Individual weights are estimated for each feature.</p><formula xml:id="formula_2">e l ij = ?(C l e l ij + D l h l i + F l h l j ) h l i = A l h l i + j? l ij j ? l ij + ? B l h l j ,</formula><p>where A l , B l , C l , D l and F l are all square matrices with a size of H ? H. In each layer, a different set of parameters are learned. We apply batch normalization <ref type="bibr" target="#b43">[44]</ref> and ReLU activations on both the node and edge representations. The input of the layer is added through residual connections <ref type="bibr" target="#b44">[45]</ref> and is followed by the dropout operation:</p><formula xml:id="formula_3">h l+1 i = Dropout(BatchNorm(ReLU(h l i )) + h l i ) e l+1 ij = Dropout(BatchNorm(ReLU(? l ij )) + e l ij )</formula><p>We stack L GNN layers on top of each other. Finally, we learn a linear classification from the node representations of the last layer to acquire {? i }, the binary label probabilities for each node:?</p><formula xml:id="formula_4">i = ?(w y h L i + b y )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Seed propagation and global label model</head><p>Each layer of the GNN is responsible for the propagation of information from each node to all of the adjacent nodes. In view of this, the range of information propagation within a single inference phase equals the number of layers in the network. In case of static or slow moving objects in images, this range might not be sufficient to connect distant frame pairs of relatively similar contents. One can tackle this problem by introducing long distance edges into the graph. We applied a simple, optical flow based seed propagation algorithm and found that it was sufficient for propagating scribble annotations to distant frames of the video.</p><p>As already noted, annotations are given by the user in the form of scribbles. To facilitate the handling of the annotations, we randomly sample seed points from the scribbles. Our seed propagation algorithm propagates seed points to consecutive frames in both directions, as long as optical flow estimations at the location of the seed point are consistent with optical flow estimations from the reversed video. Specifically, the algorithm propagates a seed point p to the consecutive frame if ||g(f (p))?p|| 2 &lt; ?, where f, g are the forward and backward optical flow transformations and thus f (p) is the propagated seed point. The backward optical flow is estimated from the consecutive frames of the reversed video. The algorithm tries to propagate each point to both the subsequent and the preceding frames. In the latter case, f and g are swapped. We chose ? to be 5 pixels and found that this value results in a very low false positive rate. While the recall of this method is not very high, the precision is almost perfect: when the propagation of a seed point is allowed to a consecutive frame, results are almost always correct.</p><p>Although we successfully extended the range of user annotation propagation, it is not possible to connect intermittently occluded objects before and after the point of occlusion (e.g., a person walking behind trees). To solve this, we utilize a global label model to implicitly store image feature distributions for each label category. We implement the global label model by means of a logistic regression model, which is trained from extracted image feature vectors of randomly selected annotated superpixel segments. Feature vectors are extracted from feature maps estimated by a pretrained MobileNet v2 <ref type="bibr" target="#b45">[46]</ref> model. The logistic regression model handles multiclass labels using the One-versus-Rest strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Node and edge features</head><p>We encode information in the node and edge features about the video itself and the present state of the annotation process. The individual features included into the node representations x i are as follows:</p><p>? x i,0 , x i,1 encode the occlusion computed from the forward and backward optical flow estimations. A pixel is considered that it got occluded when no optical flow vectors point to it from the adjacent frame in the reverse direction.</p><p>? x i,2 to x i,7 encode the mean and variance of the color distributions regarding the pixels of node i in the CIELAB color space. ? x i,8 and x i,9 store whether user annotations concerning any foreground or background have been given such that they intersect superpixel i. ? x i,10 and x i,11 store whether any seeds were propagated into superpixel i with foreground/background labels. ? x i,12 encodes the foreground/background probability prediction of the global label model. ? x i,13 encodes the foreground/background probability prediction of the GNN in the previous interaction round. The z ij edge representations consist of the following features:</p><p>? z ij,0 to z ij,2 hold the mean L 2 color distance of superpixels i and j, computed in the CIELAB color space. ? z ij,3 to z ij,6 hold the L 2 distance of the mean optical flow vectors of superpixels i and j. Both forward and backward optical flow directions are considered. ? z ij,7 and z ij,8 encode whether i ? j is a spatial edge or a causal edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training procedure</head><p>In contrast to semantic segmentation, in VOS, the model is trained to be able to predict boundaries of objects from previously unseen categories, ignoring the semantics. As we must be able to handle previously unseen label configurations with an indefinite number of categories, we choose to approximate the multiclass label estimation task with the combination of several binary label estimation tasks, following the Oneversus-Rest strategy. This way, the GNN can be trained to output a single foreground/background probability in each node instead of a probability vector varying in length. Therefore, during prediction of a multiclass label of k categories, k binary prediction rounds must be executed, where each category is selected exactly once as the foreground, while the rest is assigned to be background. The estimated individual binary probabilities are summed and normalized to acquire the final multiclass probability estimations. During training, we randomly partition the label set of the current training sample into two disjoint non-empty subsets. The scribble annotations and the target labels are transformed accordingly.</p><p>We train our GNN model against the simulated user provided by the DAVIS interactive benchmark. We start and maintain the state of multiple interactive sessions simultaneously and randomly sample from them to avoid high correlations between consecutive training examples taken from a single session. When preparing a training example, we generate a correctional scribble with the help of the simulated user in response to the previously predicted labels. We sample seed points from the scribbles and perform seed propagation as already described in the beginning of this section and depicted in <ref type="figure">Fig. 1</ref>. In the first interaction step, only foreground objects are annotated by the simulated user. Consequently, we sample background seed points from pixels farther away from foreground annotations. We input all node and edge level features as listed in the previous subsection to the network and infer the binary target label probabilities. We train the network with a binary cross-entropy loss averaged over the nodes. To move the current interactive session forward, we must submit a prediction to it. However, when the current label set consists of more than two categories, the original multiclass prediction cannot be restored from a single binary estimation. In this case a full One-versus-Rest prediction round must be executed. We note, that seed points must be sampled randomly from scribbles with high enough variance, as a fully deterministic procedure could result in the generation of the same interactions and predictions over and over that can jeopardize the success of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND EVALUATION A. Details of the training and prediction processes</head><p>We implement the training process by means of the Py-Torch <ref type="bibr" target="#b28">[29]</ref> and DGL <ref type="bibr" target="#b29">[30]</ref> libraries. We scale the superpixel segmentation to be able to fit the resulting graph into the GPU memory using a 12 layer GatedGCN network with a hidden representation size of H = 20 and a batch size of 1. We apply dropout to the node representations in each layer with a probability of 0.1. The mean graph node count is around 60k (about 800 segments per frame), while the mean edge count is around 1.1M directed edges. We use the first 45 videos of the DAVIS 2017 training set as our training set and the remaining 15 videos as our validation set. We use the DAVIS 2017 validation set for testing purposes. We note, that the network can be trained by using much fewer videos with a relatively small performance degradation of the final model. We attribute this to the small number of parameters of our network (the 12 layer network contains a total of 25700 trained parameters). The network can be trained quite quickly; the accuracy saturates after seeing approximately 4000 samples.</p><p>We predict the multiclass labels following the One-versus-Rest strategy as already mentioned in Sect. III-E. During the prediction, one binary inference step is executed for each category in the label set. This approach is significantly faster than that of most contenders of the DAVIS interactive benchmark. On the average, our method takes 45 seconds to finish a full interactive session, consisting of 8 interaction rounds. For the evaluation we use a GTX 1080 Ti graphics card to be able to compare our execution times with other methods. Papers <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b39">[40]</ref> report similar graphics cards: paper <ref type="bibr" target="#b37">[38]</ref> completes approximately 5 interactions in a minute, while paper <ref type="bibr" target="#b39">[40]</ref> finishes 7 interactions during this time. It seems that paper <ref type="bibr" target="#b39">[40]</ref> includes preprocessing time into their measurements being taken before revealing the first user interactions for their method. Our preprocessing phase includes the estimation of feature maps, optical flow, and the extraction of node and edge level features from them, which takes approximately 40 seconds for an average video from the dataset. Thus, if we include preprocessing time in the measurement, our method cannot finish 8 interaction rounds within 60 seconds. However, as we have argued, timing measurements should start after the annotator submits their first scribble and a preprocessing phase should be allowed to take place since it does not require the supervision of an annotator at all. At once, our hardware configuration is able to handle approximately 150 frames. Longer sequences may be simply cut to shorter slices. However, seed propagation can be continued across the boundaries of the slices and the state of the global label model can be maintained indefinitely. These two techniques speed up the annotation of consecutive slices significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interactive VOS results</head><p>We evaluate our approach using the DAVIS 2017 validation set and the simulated user of the interactive benchmark. The benchmark uses (a) the Jaccard-score J , also known as the mean intersection-over-union score and F-score, the measure of contour accuracy <ref type="bibr" target="#b1">[2]</ref> in order to compute J &amp;F scores, the arithmetic mean of the two averaged over all objects in all frames of the dataset.</p><p>Since J and J &amp;F are measured for each object individually and then the object scores are averaged across the whole dataset, videos with many objects are highly responsible for the overall outcome. We summarize the results in <ref type="table" target="#tab_0">Table I</ref>. We use the standard metrics of the benchmark, AUC J , and AUC J &amp;F, short for area-under-curve for the J and J &amp;F curves taken over elapsed time, respectively. We split the table into two parts: we list methods with and without extra training data separately. Oh et al. <ref type="bibr" target="#b37">[38]</ref> measure a 44% increase in error rate when extra training data is omitted, indicating that methods with many parameters are data-hungry and underperform without the help of additional data. Method AUC J AUC J &amp;F Extra data Heo et al. <ref type="bibr" target="#b38">[39]</ref> 0.771 0.809 Heo et al. <ref type="bibr" target="#b46">[47]</ref> 0.704 -Oh et al. <ref type="bibr" target="#b37">[38]</ref> 0.691 -Miao et al. <ref type="bibr" target="#b39">[40]</ref> 0.749 --Najafi et al. <ref type="bibr" target="#b47">[48]</ref> 0.702 --Oh et al. <ref type="bibr" target="#b37">[38]</ref> 0.555 --Ours 0.735 0.764 -Our method is superior to most of the contemporary literature, both with or without extra data, except for Heo et al., who use several image and video segmentation datasets for training and surpasses our solution by a margin of 3.5 percents considering the AUC J metric. Miao et al., is the only one surpassing our solution without extra training data by 1.4 percents. Both papers surpassing ours were published in the past 12 months.</p><p>In order to gain some insight into the data structure, we show the error rate of prediction in case of individual videos in <ref type="figure" target="#fig_2">Fig. 4</ref> after the second and the eighth interaction steps. It is clear from the figure, that the majority of the videos can be annotated to reach high label accuracy with less than half a minute of total waiting and annotation time from the user. Failures include the kite-surf and the paragliding-launch sequences, where the segment based solution is unable to correctly segment the cords of the parachutes. We note that most state-of-the-art approaches are also incapable of handling similarly shaped segments. The lab-coat and the bmx-trees sequences contain object categories where the corresponding segments are relatively small.</p><p>To the best of our knowledge, no other graph-based, high performance solutions have been published in interactive VOS. However, until a few years ago, many of the published interactive VOS approaches exploited graph-structures and graphical models, e.g., Markov Random Field (MRF), Bayesian Networks, as cited in Sect. II-C. While the cited publications all use different annotation protocols, we try to compare the performance of our solution to their work by replacing only the GNN of our method to a MRF based node classification algorithm.</p><p>We implement this task by using of the gco library [49]- <ref type="bibr" target="#b50">[51]</ref>, which optimizes an energy function by means of graph cuts. We define the energy of our graph labeling prediction in the following way:</p><formula xml:id="formula_5">E(L) = i U i (l i ) + ? i,j V i,j (l i , l j ),</formula><p>where U i is the unary potential for node i and V i,j is the pairwise potential for the undirected edge i ?? j. L is a complete labeling of the graph nodes and l i is the label assigned to node i within the MRF model. We derive U i from p i,li , the probability predicted by the global label model for the specific node to be assigned with label l i :</p><formula xml:id="formula_6">U i (l i ) = ? log p i,li</formula><p>Our pair-wise smoothing term is formulated as:</p><formula xml:id="formula_7">V i,j (l i , l j ) = 0 l i = l j w i,j otherwise<label>(1)</label></formula><p>Here, w i,j are computed from the strength of the causal connection between nodes i and j, which is estimated by optical flow. In other words, when different labels are assigned to nodes with strong causal relation, the energy of the system increases significantly. The weight of the pair-wise term in the energy function was optimized as a hyperparameter.</p><p>One advantage of the MRF framework is the capability to handle unseen label sets naturally without the need of the Oneversus-Rest strategy. We show the results of this comparison in <ref type="table" target="#tab_0">Table II</ref>. We rely on the same benchmark, dataset and protocol as in the previous case of <ref type="table" target="#tab_0">Table I</ref>. We measure the average quality of the predicted labeling after the 2nd interaction round and at the end of the interactive sessions, i.e., after the 8th interaction round. The superiority of the neural network approach over classic approaches is convincing. Next, we analyze our method qualitatively. We show the progression of two interactive labeling sessions in <ref type="figure" target="#fig_0">Fig. 2</ref>. Both displayed sessions were chosen from the worst quartile considering the individual error rates of the sequences. In both cases, one or multiple objects appear to be very small in the frame. Although, in some frames the predicted object boundaries are rough, even very small objects happen to be tracked adequately.</p><p>The prediction quality after the 8th interaction round in case of five other sequences are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>We conduct an ablation study to analyze how the removal of key components in our method affect the results. We train and evaluate our approach after disabling the seed propagation algorithm and the global label model individually, then finally both at the same time. We measure the J metric of the label predictions after 2 and 8 interaction rounds. The results can be seen in <ref type="table" target="#tab_0">Table III</ref>. Following only two interaction steps, relatively long, unannotated sections still exist in the videos. Under these circumstances, the GNN, limited in range by the number of its layers, simply cannot propagate annotations to major parts of the video on its own. The addition of the seed propagation algorithm results in significant improvement in the case of sequences that are mostly static or contain only slow moving objects. Nonetheless, seed propagation tends to fail in more dynamic scenes. The application of a global label model ensures that even after a single piece of annotation was given by the user, an estimation of the label probabilities are available for each superpixel in the video. However, in several sequences (e.g., goldfish, pigs, lab-coat), multiple instances of the same semantic categories need to be tracked and segmented. The global label model on its own cannot solve ambiguities between different objects of similar appearance, thus the method fails to tell them apart in frames or regions beyond the information propagation range of the GNN. In conclusion, the seed propagation and the global label model appear to complement each other and the GNN model. Despite the severe deterioration experienced after the 2nd interaction round with the removal of components, at the end of the session, the reduction in performance is much more subtle. We attribute this to the fact, that after 8 interaction rounds, the direct propagation of annotation information by the GNN reaches most parts of the video and in this case, the seed propagation and the global label model cease to be the exclusive sources of label information.</p><p>We also analyze the robustness of our method to the scarcity of training data. We evaluate our approach trained on only the first 15 and the first 5 sequences (taking sequence names in alphabetical order) of the DAVIS 2017 training set. The results are shown in <ref type="table" target="#tab_0">Table IV</ref>. We observe, that our model reaches a relatively high J score of 0.702 at the end of the session, even when only 5 videos, a total of 405 frames are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have shown that a graph neural network based approach is able to achieve state-of-the-art results in interactive VOS with a significantly smaller number of trained parameters and less training data than most existing solutions. Our method utilizes GNNs trained for binary classification with the One-versus-Rest strategy in order to implement multiclass classification of superpixels. We further improve the accuracy by utilizing seed propagation to overcome the limited propagation range of the GNN and a global label model to be able to associate isolated appearances of temporarily occluded objects as well.</p><p>A known limitation of our method is the quality of the superpixel segmentation. We implement the splitting of superpixel segments when annotations with multiple different labels intersect a specific segment. We use the Watershed algorithm <ref type="bibr" target="#b51">[52]</ref> for this purpose. While the Watershed algorithm is far behind the state-of-the-art in image segmentation, in case of splitting small segments, its performance is comparable to contemporary methods and execution is fast. In principle, using Watershed-based segment splitting, our method can reach high segmentation quality, nonetheless this component of our solution could be replaced by other methods. For example, one may consider the refinement of superpixel segmentation based on the GNN label predictions after each interaction round. In spite of the fact that this addition may slightly increase the waiting time of the user, it offers high quality labeling and the decrease of the number of interaction steps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative results with error rates from the worst quartile of the DAVIS 2017 validation set. The top four rows and the bottom four rows show results on the horsejump-high and the lab-coat sequences, respectively. The first row for each video depicts the scribbles given by the simulated user in the corresponding interaction round. The 2nd to 4th rows show label predictions in color for frames 0, 20, and 40 in this order. The 1st column contains the ground truth masks of the respective rows, whereas the 2nd to 5th columns show the predicted masks after the 1st, 2nd, 4th, and 8th iterations of user annotation inputs. Note the decrease of scribble sizes as time progresses due to the decrease of the erroneous regions. Best viewed in color. present a refined data augmentation procedure to completely avoid pretraining of their models. Oh et al. propose the attention based Space-Time Memory networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results after 8 interaction steps taken from videos of the DAVIS 2017 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Quantitative results on individual videos of the DAVIS 2017 validation set. Mean Jaccard-scores after the second and eight interaction steps are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR METHOD WITH STATE-OF-THE-ART INTERACTIVE METHODS ON THE DAVIS 2017 VALIDATION SET. THE RIGHTMOST COLUMN INDICATES WHETHER EXTRA TRAINING DATA WERE USED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF OUR METHOD WITH THE MARKOV RANDOM FIELD BASELINE.</figDesc><table><row><cell>Method</cell><cell cols="2">J @ 2nd step J @ 8th step</cell></row><row><cell>Ours -GNN</cell><cell>0.622</cell><cell>0.741</cell></row><row><cell>Ours -MRF</cell><cell>0.295</cell><cell>0.419</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY TO ANALYZE THE EFFECT OF THE REMOVAL OF CERTAIN COMPONENTS OF OUR METHOD. SP AND GLM ARE ABBREVIATIONS FOR SEED PROPAGATION AND GLOBAL LABEL MODEL, RESPECTIVELY.</figDesc><table><row><cell cols="2">SP GLM</cell><cell cols="2">J @ 2nd step J @ 8th step</cell></row><row><cell></cell><cell></cell><cell>0.622</cell><cell>0.741</cell></row><row><cell>-</cell><cell></cell><cell>0.570</cell><cell>0.703</cell></row><row><cell></cell><cell>-</cell><cell>0.485</cell><cell>0.712</cell></row><row><cell>-</cell><cell>-</cell><cell>0.209</cell><cell>0.639</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF OUR METHOD TRAINED WITH DIFFERENT SUBSETS OF THE DAVIS 2017 TRAINING SET.</figDesc><table><row><cell>Training data</cell><cell cols="2">J @ 2nd step J @ 8th step</cell></row><row><cell>Original (first 45 videos)</cell><cell>0.622</cell><cell>0.741</cell></row><row><cell>First 15 videos</cell><cell>0.615</cell><cell>0.728</cell></row><row><cell>First 5 videos</cell><cell>0.604</cell><cell>0.702</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video object segmentation and tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seednet: Automatic seed generation with deep reinforcement learning for robust interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1760" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Residual gated graph convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4649" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superpixelbased video object segmentation using perceptual organization and location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murabito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4814" to="4822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="696" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Superpixel segmentation: An evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
	<note>in German conference on pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A kernel-based approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised video object segmentation for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07780</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A unified model for semisupervised and interactive video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9236" to="9245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning video instance segmentation with recurrent graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03911</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="496" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Active frame, location, and detector selection for automated and manual video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2123" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reducing human efforts in video segmentation annotation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="247" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Livecut: Learning-based interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5247" to="5256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation using global and local transfer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08139</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Slic superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transac</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using sparse-to-dense networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Similarity learning for dense label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="147" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An experimental comparison of mincut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Use of watersheds in contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Image Processing. CCETT</title>
		<meeting>the International Workshop on Image Processing. CCETT</meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
