<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ITERATIVE ENERGY-BASED PROJECTION ON A NOR- MAL DATA MANIFOLD FOR ANOMALY LOCALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Combrexelle</surname></persName>
							<email>sebastien@anotherbrain.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Eline</surname></persName>
							<email>pierre@anotherbrain.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ITERATIVE ENERGY-BASED PROJECTION ON A NOR- MAL DATA MANIFOLD FOR ANOMALY LOCALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder's loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-defined optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset. * Equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Automating visual inspection on production lines with artificial intelligence has gained popularity and interest in recent years. Indeed, the analysis of images to segment potential manufacturing defects seems well suited to computer vision algorithms. However these solutions remain data hungry and require knowledge transfer from human to machine via image annotations. Furthermore, the classification in a limited number of user-predefined categories such as non-defective, greasy, scratched and so on, will not generalize well if a previously unseen defect appears. This is even more critical on production lines where a defective product is a rare occurrence. For visual inspection, a better-suited task is unsupervised anomaly detection, in which the segmentation of the defect must be done only via prior knowledge of non-defective samples, constraining the issue to a two-class segmentation problem.</p><p>From a statistical point of view, an anomaly may be seen as a distribution outlier, or an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism <ref type="bibr" target="#b10">(Hawkins, 1980)</ref>. In this setting, generative models such as Variational Au-toEncoders <ref type="bibr">(VAE, Kingma &amp; Welling (2014)</ref>), are especially interesting because they are capable to infer possible sampling mechanisms for a given dataset. The original autoencoder (AE) jointly learns an encoder model, that compresses input samples into a low dimensional space, and a decoder, that decompresses the low dimensional samples into the original input space, by minimizing the distance between the input of the encoder and the output of the decoder. The more recent variant, VAE, replaces the deterministic encoder and decoder by stochastic functions, enabling the modeling of the distribution of the dataset samples as well as the generation of new, unseen samples. In both models, the output decompressed sample given an input is often called the reconstruction, and is used as some sort of projection of the input on the support of the normal data distribution, which we will call the normal manifold. In most unsupervised anomaly detection methods based on VAE, models are trained on flawless data and defect detection and localization is then performed using a One fundamental issue in this approach is that the models learn on the normal manifold, hence there is no guarantee of the generalization of their behavior outside this manifold. This is problematic since it is precisely outside the dataset distribution that such methods intend to use the VAE for anomaly localization. Even in the case of a model that always generates credible samples from the dataset distribution, there is no way to ensure that the reconstruction will be connected to the input sample in any useful way. An example illustrating this limitation is given in <ref type="figure">figure 1</ref>, where a VAE trained on regular grid images provides a globally poor reconstruction despite a local perturbation, making the anomaly localization challenging.</p><p>In this paper, instead of using the VAE reconstruction, we propose to find a better projection of an input sample on the normal manifold, by optimizing an energy function defined by an autoencoder architecture. Starting at the input sample, we iterate gradient descent steps on the input to converge to an optimum, simultaneously located on the data manifold and closest to the starting input. This method allows us to add prior knowledge about the expected anomalies via regularization terms, which is not possible with the raw VAE reconstruction. We show that such an optimum is better than previously proposed autoencoder reconstructions to localize anomalies on a variety of unsupervised anomaly localization datasets <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref> and present its inpainting capabilities on the CelebA dataset <ref type="bibr" target="#b15">(Liu et al., 2015)</ref>. We also propose a variant of the standard gradient descent that uses the pixel-wise reconstruction error to speed up the convergence of the energy.  <ref type="figure">Figure 1</ref>: Even though an anomaly is a local perturbation in the image (b), the whole VAEreconstructed image can be disturbed (c). Our gradient descent-based method gives better quality reconstructions (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GENERATIVE MODELS</head><p>In unsupervised anomaly detection, the only data available during training are samples x from a non-anomalous dataset X ? R d . In a generative setting, we suppose the existence of a probability function of density q, having its support on all R d , from which the dataset was sampled. The generative objective is then to model an estimate of density q, from which we can obtain new samples close to the dataset. Popular generative architectures are Generative Adversarial Networks (GAN, <ref type="bibr" target="#b8">Goodfellow et al. (2014)</ref>), that concurrently train a generator G to generate samples from random, low-dimensional noise z ? p, z ? R l , l d, and a discriminator D to classify generated samples and dataset samples. This model converges to the equilibrium of the expectation over both real and generated datasets of the binary cross entropy loss of the classifier</p><formula xml:id="formula_0">min G max D [ E x?q [log(D(x))] + E z?p [log(1 ? D(G(z)))] ].</formula><p>Disadvantages of GANs are that they are notoriously difficult to train <ref type="bibr" target="#b9">(Goodfellow, 2017)</ref>, and they suffer from mode collapse, meaning that they have the tendency to only generate a subset of the original dataset. This can be problematic for anomaly detection, in which we do not want some subset of the normal data to be considered as anomalous <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>. Recent works such as Thanh-Tung et al. (2019) offer simple and attractive explanations for GAN behavior and propose substantial upgrades, however Ravuri &amp; Vinyals (2019) still support the point that GANs have more trouble than other generative models to cover the whole distribution support.</p><p>Another generative model is the VAE (Kingma &amp; Welling (2014)), where, similar to a GAN generator, a decoder model tries to approximate the dataset distribution with a simple latent variables prior p(z), with z ? R l , and conditional distributions output by the decoder p(x|z). This leads to the estimate p(x) = p(x|z)p(z)dz, that we would like to optimize using maximum likelihood estimation on the dataset. To render the learning tractable with a stochastic gradient descent (SGD) estimator with reasonable variance, we use importance sampling, introducing density functions q(z|x) output by an encoder network, and Jensen's inequality to get the variational lower bound :</p><formula xml:id="formula_1">log p(x) = log E z?q(z|x) p(x|z)p(z) q(z|x) ? E z?q(z|x) log p(x|z) ? D KL (q(z|x) p(z)) = ?L(x)<label>(1)</label></formula><p>We will use L(x) as our loss function for training. We define the VAE reconstruction, per analogy with an autoencoder reconstruction, as the deterministic sample f V AE (x) that we obtain by encoding x, decoding the mean of the encoded distribution q(z|x), and taking again the mean of the decoded distribution p(x|z).</p><p>VAEs are known to produce blurry reconstructions and generations, but <ref type="bibr" target="#b7">Dai &amp; Wipf (2019)</ref> show that a huge enhancement in image quality can be gained by learning the variance of the decoded distribution p(x|z). This comes at the cost of the distribution of latent variables produced by the encoder q(z) being farther away from the prior p(z), so that samples generated by sampling z ? p(z), x ? p(x|z) have poorer quality. The authors show that using a second VAE learned on samples from q(z), and sampling from it with ancestral sampling u ? p(u), z ? p(z|u), x ? p(x|z), allows to recover samples of GAN-like quality. The original autoencoder can be roughly considered as a VAE whose encoded and decoded distributions have infinitely small variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ANOMALY DETECTION AND LOCALIZATION</head><p>We will consider that an anomaly is a sample with low probability under our estimation of the dataset distribution. The VAE loss, being a lower bound on the density, is a good proxy to classify samples between the anomalous and non-anomalous categories. To this effect, a threshold T can be defined on the loss function, delimiting anomalous samples with L(x) ? T and normal samples L(x) &lt; T . However, according to <ref type="bibr" target="#b16">Matsubara et al. (2018)</ref>, the regularization term L KL (x) = D KL (q(z|x) p(z)) has a negative influence in the computation of anomaly scores. They propose instead an unregularized score L r (x) = ?E z?q(z|x) log p(x|z) which is equivalent to the reconstruction term of a standard autoencoder and claim a better anomaly detection.</p><p>Going from anomaly detection to anomaly localization, this reconstruction term becomes crucial to most of existing solutions. Indeed, the inability of the model to reconstruct a given part of an image is used as a way to segment the anomaly, using a pixel-wise threshold on the reconstruction error. Actually, this segmentation is very often given by a pixel-wise <ref type="bibr" target="#b1">(An &amp; Cho, 2015;</ref><ref type="bibr" target="#b2">Baur et al., 2018;</ref><ref type="bibr" target="#b16">Matsubara et al., 2018)</ref> or patch-wise comparison of the input image, and some generated image, as in <ref type="bibr" target="#b3">Bergmann et al. (2018;</ref>, where the structural dissimilarity (DSSIM, <ref type="bibr" target="#b23">Wang et al. (2004)</ref>) between the input and its VAE reconstruction is used.</p><p>Autoencoder-based methods thus provide a straightforward way of generating an image conditioned on the input image. In the GAN original framework, though, images are generated from random noise z ? p(z) and are not conditioned by an input. <ref type="bibr" target="#b19">Schlegl et al. (2017)</ref> propose with AnoGAN to get the closest generated image to the input using gradient descent on z for an energy defined by:</p><formula xml:id="formula_2">E AnoGAN = ||x ? G(z)|| 1 + ? ? ||f D (x) ? f D (G(z))|| 1<label>(2)</label></formula><p>The first term ensures that the generation G(z) is close to the input x. The second term is based on a distance between features of the input and the generated images, where f D (x) is the output of an intermediate layer of the discriminator. This term ensures that the generated image stays in the vicinity of the original dataset distribution.</p><p>Figure 2: Illustration of our method. We perform gradient descent on E(x t ) to iteratively correct x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ADVERSARIAL PROJECTIONS</head><p>According to <ref type="bibr" target="#b24">Zimmerer et al. (2018)</ref>, the loss gradient with respect to x gives the direction towards normal data samples, and its magnitude could indicate how abnormal a sample is. In their work on anomaly identification, they use the loss gradient as an anomaly score.</p><p>Here we propose to use the gradient of the loss to iteratively improve the observed x. We propose to link this method to the methodology of computing adversarial samples in <ref type="bibr" target="#b21">Szegedy et al. (2014)</ref>.</p><p>After training a VAE on non-anomalous data, we can define a threshold T on the reconstruction loss L r as in <ref type="bibr" target="#b16">(Matsubara et al., 2018)</ref>, such that a small proportion of the most improbable samples are identified as anomalies. We obtain a binary classifier defined by</p><formula xml:id="formula_3">A(x) = 1 if L r (x) ? T 0 otherwise (3)</formula><p>Our method consists in computing adversarial samples of this classifier <ref type="bibr" target="#b21">(Szegedy et al., 2014)</ref>, that is to say, starting from a sample x 0 with A(x 0 ) = 1, iterate gradient descent steps over the input x, constructing samples x 1 , . . . x N , to minimize the energy E(x), defined as</p><formula xml:id="formula_4">E(x t ) = L r (x t ) + ? ? ||x t ? x 0 || 1<label>(4)</label></formula><p>An iteration is done by calculating x t+1 as</p><formula xml:id="formula_5">x t+1 = x t ? ? ? ? x E(x t ),<label>(5)</label></formula><p>where ? is a learning rate parameter, and ? is a parameter trading off the inclusion of x t in the normal manifold, given by L r (x t ), and the proximity between x t and the input x 0 , assured by the regularization term ||x t ? x 0 || 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REGULARIZATION TERM</head><p>We model the anomalous images that we encounter as normal images in which a region or several regions of pixels are altered but the rest of the pixels are left untouched. To recover the best segmentation of the anomalous pixels from an anomalous image x a , we want to recover the closest image from the normal manifold x g . The term closest has to be understood in the sense that the smallest number of pixels are modified between x a and x g . In our model, we therefore would like to use the L 0 distance as a regularization distance of the energy. Since the L 0 distance is not differentiable, we use the L 1 distance as an approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OPTIMIZATION IN INPUT SPACE</head><p>While in our method the optimization is done in the input space, in the previously mentioned AnoGAN, the search for the optimal reconstruction is done by iterating over z samples with the energy defined in equation 2. Following the aforementioned analogy between a GAN generator G and a VAE decoder Dec, a similar approach in the context of a VAE would be to use the energy</p><formula xml:id="formula_6">||x ? Dec(z)|| 1 ? ? ? log p(z)<label>(6)</label></formula><p>where the ? log p(z) term has the same role as AnoGAN's ||f D (x) ? f D (G(z))|| 1 term, to ensure that Dec(z) stays within the learned manifold. We chose not to iterate over z in the latent space for two reasons. First, because as noted in <ref type="bibr" target="#b7">Dai &amp; Wipf (2019)</ref> and Hoffman &amp; Johnson <ref type="formula" target="#formula_1">(2016)</ref>, the prior p(z) is not always a good proxy for the real image of the distribution in the latent space q(z). Second, because the VAE tends to ignore some details of the original image in its reconstruction, considering that these details are part of the independent pixel noise allowed by the modeling of p(x|z) as a diagonal Gaussian, which causes its infamous blurriness. An optimization in latent space would have to recreate the high frequency structure of the image, whereas iterating over the input image space, and starting the descent on the input image x 0 , allows us to keep that structure and thus to obtain projections of higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">OPTIMIZING GRADIENT DESCENT</head><p>We observed that using the Adam optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref> is beneficial for the quality of the optimization. Moreover, to speed up the convergence and further preserve the aforementioned high frequency structure of the input, we propose to compute our iterative samples using the pixel-wise reconstruction error of the VAE. To explain the intuition behind this improvement, we will consider the inpainting task. In this setting, as in anomaly localization, a local perturbation is added on top of a normal image. However, in the classic inpainting task, the localization of the perturbation is known beforehand, and we can use the localization mask ? to only change the value of the anomalous pixels in the gradient descent:</p><formula xml:id="formula_7">x t+1 = x t ? ? ? ( ? x E(x t ) ? ) (7) where is the Hadamard product.</formula><p>For anomaly localization and blind inpainting, where this information is not available, we compute the pixel-wise reconstruction error which gives a rough estimate of the mask. The term ?</p><formula xml:id="formula_8">x E(x t ) is therefore replaced with ? x E(x t ) (x t ? f V AE (x t )) 2 ) in equation 5: x t+1 = x t ? ? ? ( ? x E(x t ) (x t ? f V AE (x t )) 2 )<label>(8)</label></formula><p>where f V AE (x) is the standard reconstruction of the VAE. Optimizing the energy this way, a pixel where the reconstruction error is high will update faster, whereas a pixel with good reconstruction will not change easily. This prevents the image to update its pixels where the reconstruction is already good, even with a high learning rate. As can be seen in appendix B, this method converges to the same performance as the method of equation 5, but with fewer iterations. An illustration of our method can be found in figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">STOP CRITERION</head><p>A standard stop criterion based on the convergence of the energy can efficiently be used. Using the adversarial setting introduced in section 3.1, we also propose to stop the gradient descent when a certain predefined threshold on the VAE loss is reached. For example, such a threshold can be chosen to be a quantile of the empirical loss distribution computed on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed method for two different applications: anomaly segmentation and image inpainting. Both applications are interesting use cases of our method, where we search to reconstruct partially corrupted images, correcting the anomalies while preserving the uncorrupted image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UNSUPERVISED ANOMALY SEGMENTATION</head><p>In order to evaluate the proposed method for the task of anomaly segmentation, we perform experiments with the recently proposed MVTec dataset <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>. This collection of datasets <ref type="table">Table 1</ref>: Results for anomaly segmentation on MVTec datasets, expressed in AUROC (Area Under the Receiver Operating Characteristics). Four different baselines are trained on normal samples and are augmented by our proposed gradient based reconstruction (grad) for comparison: A deterministic autoencoder trained with L 2 loss (L 2 AE) as in <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>; A deterministic autoencoder trained with DSSIM loss (DSAE) as in <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>; A variational autoencoder (VAE); And a variational autoencoder with a learned decoder variance (?-VAE) as in <ref type="bibr" target="#b7">(Dai &amp; Wipf, 2019)</ref>. For each result a green or red background denotes respectively an improvement or a decrease in performance compared to the baseline. It can be seen that the proposed gradient-based reconstruction achieves the best segmentation for most datasets, with a mean improvement rate of 9.52% over all baselines. We perform experiments with three different baseline autoencoders: A "vanilla" variational autoencoder with decoder covariance matrix fixed to identity (Kingma &amp; Welling, 2014), a variational autoencoder with learned decoder variance <ref type="bibr" target="#b7">(Dai &amp; Wipf, 2019)</ref>, a "vanilla" deterministic autoencoder trained with L 2 as reconstruction loss (L 2 AE) and a deterministic autoencoder trained with DSSIM reconstruction loss (DSAE), as proposed by <ref type="bibr" target="#b3">Bergmann et al. (2018)</ref>. For the sake of a fair comparison, all the autoencoder models are parameterized by convolutional neural networks with the same architecture, latent space dimensionality (set to 100), learning rate (set to 0.0001) and number of epochs (set to 300). The architecture details (layers, paddings, strides) are the same as described in <ref type="bibr" target="#b3">Bergmann et al. (2018)</ref> and <ref type="bibr" target="#b4">Bergmann et al. (2019)</ref>. Similarly to the authors in <ref type="bibr" target="#b4">Bergmann et al. (2019)</ref>, for the textures datasets, we first subsample the original dataset images to 512 ? 512 and then crop random patches of size 128 ? 128 which are used to train and test the different models. For the object datasets, we directly subsample the original dataset images to 128 ? 128 unlike in <ref type="bibr" target="#b4">Bergmann et al. (2019)</ref> who work on 256 ? 256 images, then we perform rotation and translation data augmentations. For all datasets we train on 10000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Anomaly segmentation is then computed by reconstructing the anomalous image and comparing it with the original. We perform the comparison between reconstructed and original with the DSSIM metric as it has been observed in <ref type="bibr" target="#b3">Bergmann et al. (2018)</ref> that it provides better anomaly localization than L 2 or L 1 distances. For the gradient descent, we set the step size ? := 0.5, L 1 regularization weight ? := 0.05 and the stop criterion is achieved when a sample reconstruction loss is inferior to the minimum reconstruction loss over the training set.</p><p>In table 1 we show the AUROC (Area Under the Receiver Operating Characteristics) for different autoencoder methods, with different thresholds applied to the DSSIM anomaly map computed be-tween original and reconstructed images. Note that an AUROC of 1 expresses the best possible segmentation in terms of normal and anomalous pixels. For each autoencoder variant we compare the baseline reconstruction with the proposed gradient-based reconstruction (grad.). As in <ref type="bibr" target="#b4">Bergmann et al. (2019)</ref> we observe that an overall best model is hard to identify, however we show that our method increases the AUC values for almost all autoencoder variants. Aggregating the results over all datasets and baselines, we report a mean improvement rate of 9.52%, with a median of 4.33%, a 25th percentile of 1.86%, and a 75th percentile of 15.86%. The histogram of the improvement rate for all datasets and baselines is provided in appendix F, as well as a short analysis.</p><p>In <ref type="figure" target="#fig_1">figure 3</ref> we compare our anomaly segmentation with a baseline L 2 autoencoder Bergmann et al.  <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>; Fourth row: our proposed anomaly segmentation with L 2 autoencoder augmented with gradient-based iterative reconstruction. Ground truth is represented by red contour, and each estimated segmentation by a green overlay. It can be seen that anomaly segmentation is refined by our proposed method, with a tendency of detecting less false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INPAINTING</head><p>Image inpainting is a well known image reconstruction problem which consists of reconstructing a corrupted or missing part of an image, where the region to be reconstructed is usually given by a known mask. Many different approaches for inpainting have been proposed in the literature, such as anisotropic diffusion <ref type="bibr" target="#b5">(Bertalmio et al., 2000)</ref>, patch matching <ref type="bibr" target="#b6">(Criminisi et al., 2004)</ref>, context autoencoders <ref type="bibr" target="#b17">(Pathak et al., 2016)</ref> and conditional variational autoencoders <ref type="bibr" target="#b12">(Ivanov et al., 2019)</ref>. If we consider that the region to be reconstructed is not known beforehand, the problem is sometimes called blind inpainting <ref type="bibr" target="#b0">(Altinel et al., 2018)</ref>, and the corrupted part can be seen as an anomaly to be corrected.</p><p>We performed experiments with image inpainting on the CelebA dataset <ref type="bibr" target="#b15">(Liu et al., 2015)</ref>, which consists of celebrity faces. In <ref type="figure" target="#fig_2">figure 4</ref> we compare the inpainting results obtained with a baseline VAE with learned variance (?-VAE) and Resnet architecture, as described by <ref type="bibr" target="#b7">Dai &amp; Wipf (2019)</ref>, with the same VAE model, augmented by our proposed gradient-based iterative reconstruction. Note that for the regular inpainting task, gradients are multiplied by the inpainting mask at each iteration (equation 7), while for the blind inpainting task, the mask is unknown. See appendix D for a comparison with a recent method based on variational autoencoders, proposed by <ref type="bibr" target="#b12">Ivanov et al. (2019)</ref>. <ref type="bibr" target="#b2">Baur et al. (2018)</ref> have used autoencoder reconstructions to localize anomalies in MRI scans, and have compared several variants using diverse per-pixel distances as well as perceptual metrics derived from a GAN-like architecture. <ref type="bibr" target="#b3">Bergmann et al. (2018)</ref> use the structural similarity metric <ref type="bibr" target="#b23">(Wang et al., 2004)</ref> to compare the original image and its reconstruction to achieve better anomaly localization, and also presents the SSIM autoencoder, which is trained directly with this metric. <ref type="bibr" target="#b24">Zimmerer et al. (2018)</ref> use the derivative of the VAE loss function with respect to the input, called the score. The amplitude of the score is supposed to indicate how abnormal a pixel is. While we agree that the gradient of the loss is an indication of an anomaly, we think that we have to integrate this gradient over the path from the input to the normal manifold to obtain meaningful information. We compare our results to score-based results for anomaly localization in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The work that is the most related to ours is AnoGAN <ref type="bibr" target="#b19">(Schlegl et al., 2017)</ref>. We have mentioned above the differences between the two approaches, which, apart from the change in underlying architectures, boil down to the ability in our method to update directly the input image instead of searching for the optimal latent code. This enables the method to converge faster and above all to keep higher-frequency structures of the input, which would have been deteriorated if it were passed through the AE bottleneck. <ref type="bibr" target="#b4">Bergmann et al. (2019)</ref> compare standard AE reconstructions techniques to AnoGAN, and observes that AnoGAN's performances on anomaly localizations tasks are poorer than AE's due to the mode collapse tendency of GAN architectures. Interestingly, updates on AnoGAN such as fast AnoGAN <ref type="bibr" target="#b20">(Schlegl et al., 2019)</ref> or AnoVAEGAN <ref type="bibr" target="#b2">(Baur et al., 2018)</ref> replaced the gradient descent search of the optimal z with a learned encoder model, yielding an approach very similar to the standard VAE reconstruction-based approaches, but with a reconstruction loss learned by a discriminator, which is still prone to mode collapse <ref type="bibr" target="#b22">(Thanh-Tung et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed a novel method for unsupervised anomaly localization, using gradient descent of an energy defined by an autoencoder reconstruction loss. Starting from a sample under test, we iteratively update this sample to reduce its autoencoder reconstruction error. This method offers a way to incorporate human priors into what is the optimal projection of an out-of-distribution sample into the normal data manifold. In particular, we use the pixel-wise reconstruction error to modulate the gradient descent, which gives impressive anomaly localization results in only a few iterations. Using gradient descent in the input data space, starting from the input sample, enables us to overcome the autoencoder tendency to provide blurry reconstructions and keep normal high frequency structures. This significantly reduces the number of pixels that could be wrongly classified as defects when the autoencoder fails to reconstruct high frequencies.</p><p>We showed that this method, which can easily be added to any previously trained autoencoder architecture, gives state-of-the-art results on a variety of unsupervised anomaly localization datasets, as well as qualitative reconstructions on an inpainting task. Future work can focus on replacing the L 1 -based regularization term with a Bayesian prior modeling common types of anomalies, and on further improving the speed of the gradient descent.</p><p>A COMPARISON WITH ZIMMERER ET AL. (2019)  <ref type="figure">(x)</ref>), gradient of the loss |? x L(x)|, combination of both, gradient of the KL divergence |D KL (q(z|x) p(z)) (denoted |? x L KL (x)|) as well as combination of KL derivative and error reconstruction as suggested in <ref type="bibr" target="#b24">Zimmerer et al. (2018;</ref>.  <ref type="bibr" target="#b25">Zimmerer et al. (2019)</ref> proposed to perform anomaly localization using different scores derived from the gradient of the VAE loss. In particular, it has been shown that the product of the VAE reconstruction error with the gradient of the KL divergence was very informative for medical images.</p><formula xml:id="formula_9">Category L r (x) |? x L(x)| |? x L(x)| L r (x) |? x L KL (x)| |? x L KL (x)| L r (x) VAE-</formula><p>In table 2 we compare the pixel-wise anomaly detection AUROC of these different scores with our method. For all experiments, we use the same "vanilla" VAE as described in section 4.1.</p><p>It can be seen that other VAE-based methods using a single evaluation of the gradient are constantly outperformed by our method. B CONVERGENCE SPEED <ref type="figure">Figure 5</ref>: Evolution of pixel-wise anomaly detection AUROC performance.</p><p>In <ref type="figure">figure 5</ref> we compare the number of iterations needed to reach convergence with our two proposals for gradient descent: Standard update as in equation 5 and Tuned update using a gradient mask computed with the VAE reconstruction error, as in equation 8. The model is a VAE with learned decoder variance <ref type="bibr" target="#b7">(Dai &amp; Wipf, 2019)</ref>, trained on the Grid dataset <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>. We compute the mean pixel-wise anomaly detection AUROC after each iteration on the test set.</p><p>We can see that the tuned method converges to the same performance as the standard method, with far fewer iterations.</p><p>C ADDITIONAL ANOMALY SEGMENTATION RESULTS <ref type="figure">Figure 6</ref>: From left to right: Normal; Anomalous; Anomaly segmentation with baseline L 2 autoencoder <ref type="bibr" target="#b4">(Bergmann et al., 2019)</ref>; Our proposed anomaly segmentation with L 2 autoencoder augmented with gradient-based iterative reconstruction. The quality of the reconstructions is comparable, even though our VAE is trained without any assumptions over the mask's properties. <ref type="figure">Figure 9</ref>: Principle of the energy optimization to project anomalous sample on the normal manifold <ref type="figure">Figure 9</ref> illustrates our method principle. We start with a defective input x 0 whose reconstruction x 0 does not necessarily lie on the normal data manifold. As the optimization process carries on, the optimized sample x 0 and its reconstruction look more similar and get closer to the manifold. The regularization term of the energy function makes sure that the optimized sample stays close to the original sample.</p><p>F DISTRIBUTION OF THE IMPROVEMENT RATE ON MVTEC AD <ref type="figure">Figure 10</ref>: Distribution of the improvement rate over all presented baselines and all datasets in MVTec AD. <ref type="figure">Figure 10</ref> shows the distribution of the AUC improvement rate over all presented baselines and all datasets in MVTec AD using our gradient-based projection method.</p><p>improvement rate = AU C grad ? AU C base AU C base ? 8.3% of data points are under the 0 value delimiting an increase or decrease in AUC due to our method, and 91.7% data points are over this value. Our method increases the AUC in a vast majority of cases. ? The median is at 4.33%, the 25th percentile at 1.86%, and the 75th percentile at 15.86%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(2019) (L 2 AE) for a number of image categories. For all results in figure 3, we set the same threshold to 0.2 to the anomaly detection map given by the DSSIM metric. The visual results infigure 3highlights an overall improvement of anomaly localization by our proposed iterative reconstruction (L 2 AE-?). See appendix C for additional visual results of anomaly segmentation on remaining categories of MVTec dataset, and on remaining baseline models. First row: Normal samples of hazelnut, grid, cable, wood, carpet and bottle categories in MVTec dataset; Second row: anomalous samples from the aforementioned dataset categories; Third row: Anomaly segmentation with baseline L 2 autoencoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Inpainting experiment performed on CelebA dataset, where the test face images are masked with uniform noise. The baseline VAE reconstruction is disturbed by the noise mask, providing a poor inpainting. The proposed gradient-based VAE provides a more convincing inpainting by an iterative process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Illustration of anomaly localization comparison over four baselines (L 2 AE, DSAE, VAE, ?-VAE). Ground truth is represented by red contour, and each estimated segmentation by a green overlay. It can be seen that anomaly segmentation is overall improved when different baselines are augmented by our proposed gradient descent. D INPAINTING COMPARISON Inpainting comparison. Each batch is made of four rows, from top to bottom: Masked input image; VAE with arbitrary conditioning (VAEAC, Ivanov et al. (2019)); Ours; Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Complementary results for anomaly segmentation on MVTec datasets, expressed in AU-ROC for different pixel-wise scores derived from a baseline VAE: from left to right, squared error reconstruction ||x ? f V AE (x)|| 2 (denoted L r</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep structured energy-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fazil</forename><surname>Altinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2018.8546025</idno>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>SNU Data Mining Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno>abs/1804.04488</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno>abs/1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
		<idno type="DOI">10.1145/344779.344972</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;00</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2004.833105</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Diagnosing and enhancing VAE models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1701.00160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Identification of outliers. Monographs on applied probability and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hawkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Chapman and Hall</publisher>
			<biblScope unit="volume">041221900</biblScope>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational autoencoder with arbitrary conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Anomaly machine component detection by deep generative model with unregularized score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
		<idno>abs/1807.05800</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Classification accuracy score for conditional generative models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.01.010</idno>
		<idno>1361-8415. doi</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.01.010" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving generalization and stability of generative adversarial networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Hoang Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A case for the score: Identifying image anomalies using variational autoencoder gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly localization using variational auto-encoders. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
