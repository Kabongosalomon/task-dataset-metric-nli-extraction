<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hunting Group Clues with Transformers for Social Group Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
							<email>masato.tamura.sf@hitachi.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hitachi America</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Vishwakarma</surname></persName>
							<email>rahul.vishwakarma@hal.hitachi.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hitachi America</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravigopal</forename><surname>Vennelakanti</surname></persName>
							<email>ravigopal.vennelakanti@hal.hitachi.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hitachi America</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hunting Group Clues with Transformers for Social Group Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>social group activity recognition</term>
					<term>group activity recogni- tion</term>
					<term>social scene understanding</term>
					<term>attention mechanism</term>
					<term>transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel framework for social group activity recognition. As an expanded task of group activity recognition, social group activity recognition requires recognizing multiple sub-group activities and identifying group members. Most existing methods tackle both tasks by refining region features and then summarizing them into activity features. Such heuristic feature design renders the effectiveness of features susceptible to incomplete person localization and disregards the importance of scene contexts. Furthermore, region features are sub-optimal to identify group members because the features may be dominated by those of people in the regions and have different semantics. To overcome these drawbacks, we propose to leverage attention modules in transformers to generate effective social group features. Our method is designed in such a way that the attention modules identify and then aggregate features relevant to social group activities, generating an effective feature for each social group. Group member information is embedded into the features and thus accessed by feed-forward networks. The outputs of feedforward networks represent groups so concisely that group members can be identified with simple Hungarian matching between groups and individuals. Experimental results show that our method outperforms stateof-the-art methods on the Volleyball and Collective Activity datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social group activity recognition is a task of recognizing multiple sub-group activities and identifying group members in a scene. This task is derived from group activity recognition, which needs to recognize only one group activity in a scene. Both tasks have gained tremendous attention in recent years for potential applications such as sports video analysis, crowd behavior analysis, and social scene understanding <ref type="bibr">[1-5, 12-14, 16-18, 21, 23-27, 32, 33, 36, 40-43]</ref>. In the context of these tasks, the term "action" denotes an atomic movement of a single person, and the term "activity" refers to a more complex relation of movements performed by a group of people. Although our framework can recognize both actions and activities, we focus on group activities.  Most existing methods decompose the recognition process into two independent parts; person localization and activity recognition (See <ref type="figure" target="#fig_0">Fig. 1a</ref>) <ref type="bibr">[5, 12-14, 16, 18, 21, 26, 32, 33, 36, 40-43]</ref>. Person localization identifies regions where people are observed in a scene with bounding boxes. These boxes are used to extract region features from feature maps. The region features are further refined to encode spatio-temporal relations with feature refinement modules such as recurrent neural networks (RNNs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, graph neural networks (GNNs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>, and transformers <ref type="bibr" target="#b37">[38]</ref>. The refined features are summarized for the purpose of activity recognition.</p><p>While these methods have demonstrated significant improvement, they have several drawbacks attributed to the heuristic nature of feature design. Since region features are extracted from bounding box regions in feature maps, the effectiveness of the features is affected by the localization performance. Most existing methods ignore this effect and evaluate their performances with region features of ground truth boxes. However, several works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> show that the recognition performance is slightly degraded when using predicted boxes instead of ground truth boxes. Moreover, substantial scene contexts are discarded by using region features because they are typically dominated by features of the people in the boxes. Scene contexts such as object positions and background situations are sometimes crucial to recognize group activities. For instance, positions of sports balls are informative to recognize group activities in sports games. These features should be leveraged to enhance recognition performance.</p><p>Another challenge specific to social group activity recognition is that utilizing region features is sub-optimal to identify group members. Ehsanpour et al . <ref type="bibr" target="#b12">[13]</ref> use region features as node features of graph attention networks (GATs) <ref type="bibr" target="#b38">[39]</ref> and train them to output adjacency matrices that have low probabilities for people in different groups and high probabilities for those in the same groups. During inference, spectral clustering <ref type="bibr" target="#b30">[31]</ref> is applied to the adjacency matrices to divide people into groups. Because adjacency matrices reflect semantic similarities of node features, this method may not work if region features of people in the same group have different semantics such as doing different actions.</p><p>To address these challenges, we propose a novel social group activity recognition method that can be applied to both social group activity recognition and group activity recognition. We leverage a transformer-based object detection framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref> to obviate the need for the heuristic feature design in existing methods (See <ref type="figure" target="#fig_0">Fig. 1b</ref>). Attention modules in transformers play crucial roles in our method. We design our method in such a way that the attention modules identify and then aggregate features relevant to social group activities, generating an effective feature for each social group. Because activity and group member information is embedded into the generated features, the information can be accessed by feed-forward networks (FFNs) in the detection heads. The outputs of the detection heads are designed so concisely that group member identification can be performed with simple Hungarian matching between groups and individuals. This identification method differs from Ehsanpour et al .'s method <ref type="bibr" target="#b12">[13]</ref> in that their method relies on individuals' features to divide people into groups, while our method generates features that are embedded with clues for grouping people, enabling effective group identification.</p><p>To summarize, our contributions are three-fold:</p><p>-We propose a novel social group activity recognition method that leverages the attention modules in transformers to generate effective social group features. The group member information extracted from the features is designed to be concise and can be used to identify group members with a simple matching process. -Our method achieves better or competitive performance to state-of-the-art methods on both group activity recognition and social group activity recognition in two challenging benchmarks. -We perform comprehensive analyses to reveal how our method works with activities under various conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Group Activity Recognition</head><p>Deep-neural-network-based methods have become dominant in group activity recognition due to the learning capability of the networks. Ibrahim et al . <ref type="bibr" target="#b17">[18]</ref> proposed an RNN-based method that uses convolutional neural networks to extract features of person bounding box regions and long short-term memories to refine region features. This architecture captures the temporal dynamics of each person between frames and spatial dynamics of people in a scene. After their work, several RNN-based methods were proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>GNNs are also utilized to model the spatio-temporal context and relationships of people in a scene. Wu et al . <ref type="bibr" target="#b40">[41]</ref> used graph convolutional networks (GCNs) <ref type="bibr" target="#b19">[20]</ref> to capture spatio-temporal relations of people's appearances and positions between frames. Ehsanpour et al . <ref type="bibr" target="#b12">[13]</ref> adopted GATs <ref type="bibr" target="#b38">[39]</ref> to learn underlying interactions and divide people into social groups with adjacency matrices. Hu et al . <ref type="bibr" target="#b15">[16]</ref> utilized both RNNs and GNNs with reinforcement learning to refine features. Yuan et al . <ref type="bibr" target="#b41">[42]</ref> used person-specific dynamic graphs that dynamically change connections of GNNs for each node.</p><p>With the rapid application of transformers <ref type="bibr" target="#b37">[38]</ref> to vision problems, several works introduced transformers into group activity recognition. Gavrilyuk et al . <ref type="bibr" target="#b13">[14]</ref> used transformer encoders to refine region features. Li et al . <ref type="bibr" target="#b25">[26]</ref> proposed spatial-temporal transformers that can encode spatio-temporal dependence and decode the group activity information. Zhou et al . <ref type="bibr" target="#b42">[43]</ref> proposed multi-scale spatio-temporal stacked transformers for compositional understanding and relational reasoning in group activities.</p><p>Our method differs from existing methods in that they rely on region features, while our method generates social group features with the attention modules in transformers, resulting in improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detection Transformer</head><p>Carion et al . <ref type="bibr" target="#b5">[6]</ref> proposed a transformer-based object detector called DETR, which regards object detection as a set prediction and achieves end-to-end object detection. One significant difference between conventional object detectors and DETR is that conventional ones need heuristic detection points whose features are used to predict object classes and bounding boxes, while DETR obviates such heuristic components by letting queries in transformer decoders aggregate features for their target objects with the attention mechanisms. DETR shows competitive performance compared with conventional state-of-the-art detectors even without such heuristic components.</p><p>To further improve the performance of DETR, several methods have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Zhu et al . <ref type="bibr" target="#b44">[45]</ref> proposed Deformable DETR that replaces standard transformers with deformable ones. Deformable attention modules in the transformers combine a sparse sampling of deformable convolution <ref type="bibr" target="#b9">[10]</ref> and dynamic weighting of standard attention modules, which significantly reduces the computational complexity of the attention weight calculation. This reduction allows Deformable DETR to use multi-scale feature maps from backbone networks. To leverage non-heuristic designs and multi-scale feature maps, we use deformable transformers to generate social group features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We leverage a deformable-transformer-based object detection framework <ref type="bibr" target="#b44">[45]</ref> to recognize multiple group activities and identify group members without the heuristic feature design. We first explain the overall architecture in Sec. <ref type="bibr" target="#b2">3</ref> 3.1 Overall Architecture <ref type="figure">Figure 2</ref> shows the overall architecture of the proposed method. Given a frame sequence x ? R 3?T ?H?W , a feature extractor extracts a set of multi-scale feature</p><formula xml:id="formula_0">maps Z f = {z (f ) i | z (f ) i ? R Di?T ?H ? i ?W ? i } L f i=1 ,</formula><p>where T is the length of the sequence, H and W are the height and width of the frame, H ? i and W ? i are those of the output feature maps, D i is the number of channels, and L f is the number of scales. We adopt the inflated 3D (I3D) network <ref type="bibr" target="#b6">[7]</ref> as a feature extractor to embed local spatio-temporal context into feature maps. Note that we use only the RGB stream of I3D because group members are identified by their positions, which cannot be predicted with the optical flow stream. To reduce the computational costs of transformers, each feature map z (f ) i is mean-pooled over the temporal dimension and input to a projection convolution layer that reduces the channel dimension from D i to D p . One additional projection convolution layer with a kernel size of 3 ? 3 and stride of 2 ? 2 is applied to the smallest feature map to further add the scale.</p><p>Features in the modified feature maps are refined and aggregated with deformable transformers. Given a set of the modified multi-scale feature maps Z p = {z</p><formula xml:id="formula_1">(p) i | z (p) i ? R Dp?H ? i ?W ? i } L f +1 i=1 , a set of refined feature maps Z e = {z (e) i | z (e) i ? R Dp?H ? i ?W ? i } L f +1 i=1 is obtained as Z e = f enc (Z p , P ), where f enc (?, ?) is stacked deformable transformer encoder layers and P = {p i | p i ? R Dp?H ? i ?W ? i } L f +1 i=1</formula><p>is a set of multi-scale position encodings <ref type="bibr" target="#b44">[45]</ref>, which supplement the attention modules with position and scale information to identify where each feature lies in the feature maps. The encoder helps features to acquire rich social group context by exchanging information in a feature map and between multi-scale feature maps. These enriched feature maps are fed into the deformable transformer decoder to aggregate features. Given a set of refined feature maps Z e and learnable query embeddings</p><formula xml:id="formula_2">Q = {q i | q i ? R 2Dp } Nq i=1 , a set of feature em- beddings H = {h i | h i ? R Dp } Nq i=1 is obtained as H = f dec (Z e , Q),</formula><p>where N q is the number of query embeddings and f dec (?, ?) is stacked deformable transformer decoder layers. Each decoder layer predicts locations that contain features relevant to input embeddings and aggregates the features from the locations with the dynamic weighting. We design queries in such a way that one query captures at most one social group. This design enables each query to aggregate features of its target social group from the refined feature maps.</p><p>The feature embeddings are transformed into prediction results with detection heads. Here we denote the localization results in normalized image coordinates. Social group activities are recognized by predicting activities and identifying group members. The identification is performed with a group size head and group member point head. The size head predicts the number of people in a target social group, and the point head indicates group members by localizing the centers of group members' bounding boxes. This design enables our method to identify group members with simple point matching during inference as described in Sec. <ref type="bibr" target="#b2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.3. The predictions of activity class probabilities {v</head><formula xml:id="formula_3">i |v i ? [0, 1] Nv } Nq i=1 , group sizes {? i |? i ? [0, 1]} Nq i=1 , and sequences of group member points {? i } Nq i=1 are obtained asv i = f v (h i ),? i = f s (h i ), and? i = f u (h i , r i ), where N v is the number of activity classes,? i = (? (i) j |? (i) j ? [0, 1] 2 ) M</formula><p>j=1 is a sequence of points that indicate centers of group members' bounding boxes, M is a hyperparameter that defines the maximum group size, f v (?), f s (?), and f u (?, ?) are the detection heads for each prediction, and r i ? [0, 1] 2 is a reference point, which is used in the same way as the localization in Deformable DETR <ref type="bibr" target="#b44">[45]</ref>. The predicted group sizes are values normalized with M . All the detection heads are composed of FFNs with subsequent sigmoid functions. We describe the details of the detection heads in the supplementary material.</p><p>Individual recognition can be performed by replacing the group recognition heads with individual recognition heads. We empirically find that using different parameters of deformable transformers for individual recognition and social group recognition does not show performance improvement and thus use shared parameters to reduce the computational costs. The details of the individual recognition heads are described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Calculation</head><p>We view social group activity recognition as a direct set prediction problem and match predictions and ground truths with the Hungarian algorithm <ref type="bibr" target="#b21">[22]</ref> during training following the training procedure of DETR <ref type="bibr" target="#b5">[6]</ref>. The optimal assignment is determined by calculating the matching cost with the predicted activity class probabilities, group sizes, and group member points. Given a ground truth set of social group activity recognition, the set is first padded with ? (gr) (no activity) to change the set size to N q . With the padded ground truth set, the matching cost of i-th element in the ground truth set and j-th element in the prediction set is calculated as follows:</p><formula xml:id="formula_4">H (gr) i,j = 1 {i? ?? (gr) } ? v H (v) i,j + ? s H (s) i,j + ? u H (u) i,j ,<label>(1)</label></formula><formula xml:id="formula_5">H (v) i,j = ? v T iv j + (1 ? v i ) T (1 ?v j ) N v ,<label>(2)</label></formula><formula xml:id="formula_6">H (s) i,j = |s i ?? j | ,<label>(3)</label></formula><formula xml:id="formula_7">H (u) i,j = Si k=1 u (i) k ?? (j) k 1 S i ,<label>(4)</label></formula><p>where ? (gr) is a set of ground-truth indices that correspond to </p><formula xml:id="formula_8">? (gr) , v i ? {0, 1} Nv is a ground truth activity label, s i ? [0, 1] is a ground truth group size normalized with M , S i is an unnormalized ground truth group size, u (i) k ? [0, 1] 2 is</formula><formula xml:id="formula_9">U i = (u (i) k ) Si k=1</formula><p>are sorted in ascending order along X coordinates as seen from the image of the group recognition result in <ref type="figure">Fig. 2</ref>. We use this arrangement because group members are typically seen side by side at the same vertical positions in an image, and the order of group member points is clear from their positions, which makes the prediction easy. We evaluate the performances with other arrangements and compare the results in Sec. 4.4. Using Hungarian algorithm, the optimal assignment is calculated as? (gr) = arg min ??? Nq</p><formula xml:id="formula_10">Nq i=1 H (gr) i,?(i) ,</formula><p>where ? Nq is the set of all possible permutations of N q elements.</p><p>The training loss for social group activity recognition L gr is calculated between matched ground truths and predictions as follows:</p><formula xml:id="formula_11">L v = 1 |? (gr) | Nq i=1 1 {i? ?? (gr) } l f v i ,v?(gr) (i) + 1 {i?? (gr) } l f 0,v?(gr) (i) , (5) L s = 1 |? (gr) | Nq i=1 1 {i? ?? (gr) } s i ???(gr) (i) ,<label>(6)</label></formula><formula xml:id="formula_12">L u = 1 |? (gr) | Nq i=1 Si j=1 1 {i? ?? (gr) } u (i) j ?? (? (gr) (i)) j 1 ,<label>(7)</label></formula><p>where ? {v,s,u} are hyper-parameters and l f (?, ?) is the element-wise focal loss function <ref type="bibr" target="#b27">[28]</ref> whose hyper-parameters are described in <ref type="bibr" target="#b43">[44]</ref>. Individual recognition is jointly learned by matching ground truths and predictions of person class probabilities, bounding boxes, and action class probabilities and calculating the losses between matched ground truths and predictions.</p><p>The matching and loss calculations are performed by slightly modifying the original matching costs and losses of Deformable DETR <ref type="bibr" target="#b44">[45]</ref>. We describe the details of these matching and loss calculations in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Group Member Identification</head><p>The outputs of the detection heads represent groups in group sizes and group member points that indicate centers of group members' bounding boxes. These values have to be transformed into values that indicate individuals. We transform the predicted values into indices that refer to the elements in the individual prediction set with the following simple process during inference. To match the group member points and individual predictions, the Hungarian algorithm <ref type="bibr" target="#b21">[22]</ref> is used instead of just calculating the closest center of a bounding box for each group member point. Hungarian algorithm can prevent multiple group member points from matching the same individuals and thus slightly improve the performance. The matching cost between i-th group member point of k-th social group prediction and j-th individual prediction is calculated as follows:</p><formula xml:id="formula_13">H (gm,k) i,j = ? (k) i ? f cent b j 2 c j ,<label>(8)</label></formula><p>whereb j ? [0, 1] 4 is a predicted bounding box of an individual,? j ? [0, 1] is a detection score of the individual, and f cent (?) is a function that calculates the center of a bounding box. By applying the Hungarian algorithm to this matching cost, the optimal assignment is calculated as? (gm,k) = arg min ??? Nq</p><formula xml:id="formula_14">?M ?? k ? i=1 H (gm,k)</formula><p>i,?(i) , where ??? rounds an input value to the nearest integer. Finally, the index set of individuals for k-th social group prediction is obtained as</p><formula xml:id="formula_15">G k = {? (gm,k) (i)} ?M ?? k ? i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We evaluate the performance of our method on two publicly available benchmark datasets: Volleyball dataset <ref type="bibr" target="#b17">[18]</ref> and Collective Activity dataset <ref type="bibr" target="#b8">[9]</ref>. The Volleyball dataset contains 4,830 videos of 55 volleyball matches, which are split into 3,493 training videos and 1,337 test videos. The center frame of each video is annotated with bounding boxes, actions, and one group activity. The number of action and activity classes are 9 and 8, respectively. Because the original annotations do not contain group member information, we use an extra annotation set provided by Sendo and Ukita <ref type="bibr" target="#b34">[35]</ref>. We combine the original annotations with the group annotations in the extra set and use them for our experiments. Note that annotations other than the group annotations in the extra set are not used for a fair comparison. The Collective Activity dataset contains 44 videos of life scenes, which are split into 32 training videos and 12 test videos. The videos are annotated every ten frames with bounding boxes and actions. The group activity is defined as the action with the largest number in the scenes. The number of action classes is 6. Because the original annotations do not have group member information, Ehsanpour et al . <ref type="bibr" target="#b12">[13]</ref> annotated group labels. We use their annotations for our experiments.</p><p>We divide the evaluation into two parts: group activity recognition and social group activity recognition. In the evaluation of group activity recognition, we follow the detection-based settings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> and use classification accuracy as an evaluation metric. Because our method is designed to predict multiple group activities, we need to select one from them for group activity recognition. We choose the predicted activity of the highest probability and compare it with the ground truth activity. In the evaluation of social group activity recognition, different metrics are used for each dataset because each scene in the Volleyball dataset contains only one social group activity, while that in the Collective Activity dataset contains multiple social group activities. For the Volleyball dataset, group identification accuracy is used as an evaluation metric. One group prediction is first selected in the same way as group activity recognition, and then the predicted bounding boxes of the group members are compared with the ground truth boxes. The selected prediction results are correct if the predicted activity is correct and the predicted boxes have IoUs larger than 0.5 with the corresponding ground truth boxes. For the Collective Activity dataset, mAP is used as an evaluation metric. Prediction results are judged as true positives if the predicted activities are correct, and all the predicted boxes of the group members have IoUs larger than 0.5 with the corresponding ground truth boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use the RGB stream of I3D <ref type="bibr" target="#b6">[7]</ref> as a backbone feature extractor and input features from Mixed 3c, Mixed 4f, and Mixed 5c layers into the deformable transformers. The hyper-parameters of the deformable transformers are set in accordance with the setting of Deformable DETR <ref type="bibr" target="#b44">[45]</ref>, where L f = 3, D p = 256, and N q = 300. We initialize I3D with the parameters trained on the Kinetics dataset <ref type="bibr" target="#b18">[19]</ref> and deformable transformers with the parameters trained on the COCO dataset <ref type="bibr" target="#b28">[29]</ref>. We use the AdamW <ref type="bibr" target="#b29">[30]</ref> optimizer with the batch size of 16, the initial learning rate of 10 ?4 , and the weight decay of 10 ?4 . Training epochs are set to 120, and the learning rate is decayed after 100 epochs. We set the length of the sequence T to 9. Ground truth labels of the center frame are used to calculate the losses. To augment the training data, we randomly shift frames in the temporal direction and use bounding boxes from visual trackers as ground truth boxes when a non-annotated frame is at the center. We also augment the training data by random horizontal flipping, scaling, and cropping. Following the DETR's training <ref type="bibr" target="#b5">[6]</ref>, auxiliary losses are used to boost the performance. The maximum group size M is set to 12. The hyper-parameters are set as ? v = ? v = 2, ? s = ? s = 1, and ? u = ? u = 5.</p><p>While evaluating performances with the Collective Activity dataset, some specific settings are used. For the evaluation of group activity recognition, training epochs are set to 10, and the learning rate is decayed after 5 epochs because the losses converge in a few epochs due to the limited diversity of the scenes in the dataset. For the evaluation of social group activity recognition, the length of the sequence T is set to 17 following the setting of Ehsanpour et al . <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Group Activity Recognition</head><p>Comparison against State-of-the-Art. We compare our method against state-of-the-art methods on group activity recognition. <ref type="table" target="#tab_3">Table 1</ref> shows the comparison results. The values without the brackets demonstrate the detection-based performances, while those inside the brackets indicate the performances with ground truth bounding boxes. We show the performances of individual action recognition for future reference. Several detection-based performances are not reported because existing works typically use ground-truth boxes for the evaluation. To compare the effectiveness with these methods, we evaluate Group-Former <ref type="bibr" target="#b25">[26]</ref>, which is the strongest baseline of group activity recognition, with predicted boxes of Deformable DETR <ref type="bibr" target="#b44">[45]</ref>. Note that Deformable DETR is finetuned on each dataset for a fair comparison, which demonstrates 90.8 and 90.2 mAP on the Volleyball and Collective Activity datasets, respectively. As seen from the table, our method outperforms state-of-the-art methods in the detection-based setting. We confirm that GroupFormer shows the performance degradation as well as the previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> when predicted bounding boxes are used. These results indicate that the latest region-featurebased method still suffers from incomplete person localization and that our feature generation has advantages over these methods. Even compared to the ground-truth-based performances, our method shows the best performance. It is worth noting that our method uses only RGB images as inputs, while Group-Former utilizes optical flows and pose information in addition to RGB data. These results suggest that features generated by our method are more effective than region features and that it is not optimal to restrict regions of features to bounding boxes.</p><p>Analysis on Group Annotations. As described in Sec. 4.1, we use the additional group annotations to fully leverage our social group activity recognition capability. We analyze the effect of the group annotations on group activity recognition by investigating the performances of both GroupFormer <ref type="bibr" target="#b25">[26]</ref> and our method with and without the group annotations. Note that hereinafter we use the Volleyball dataset for analyses because the diversity of the scenes in the Collective Activity dataset is limited. To evaluate GroupFormer with the group annotations in the detection-based setting, we trained Deformable DETR <ref type="bibr" target="#b44">[45]</ref> with bounding boxes of only group members, which is intended to detect only people involved in activities. The detector shows the performance of 87.1 mAP. Among all the results, GroupFormer with the group annotations in the ground-truthbased setting demonstrates the best performance. However, the performance is substantially degraded when the predicted boxes are used. This is probably because group member detection underperforms and degrades the recognition performance. As our method does not rely on bounding boxes to predict group activities, the performance does not degrade even if group members cannot be identified correctly. Accordingly, our method demonstrates the best performance in the detection-based setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Social Group Activity Recognition</head><p>Comparison against State-of-the-Art. To demonstrate the effectiveness of our method on social group activity recognition, we compare our method against  Ehsanpour et al .'s method <ref type="bibr" target="#b12">[13]</ref>, which is a state-of-the-art method that tackles social group activity recognition, and GroupFormer <ref type="bibr" target="#b25">[26]</ref>, which is the strongest baseline on group activity recognition. Due to the unavailability of both Ehsanpour et al .'s source codes and their performance report on the Volleyball dataset, we implemented their algorithm based on our best understanding and evaluated the performance on the dataset. For the evaluation of GroupFormer, we trained Deformable DETR <ref type="bibr" target="#b44">[45]</ref> in the same way as described in the group annotation analysis section for detecting group members. Because this group member detection cannot be applied to multiple social groups, we evaluate GroupFormer only on the Volleyball dataset. <ref type="table" target="#tab_5">Table 3</ref> shows the results on the Volleyball dataset. As shown in the table, our method yields significant performance gains over the other methods, which demonstrates the improvement on group member identification as well as on activity recognition. Our method aggregates features that are embedded with clues for grouping people from feature maps. It is highly likely that this feature aggregation contributes to the high accuracy of identifying activities with different distributions of group members in an image. We qualitatively analyze how features are aggregated depending on the distribution of group members and discuss the analysis results towards the end in the qualitative analysis section.</p><p>The comparison results on the Collective Activity dataset are listed in <ref type="table" target="#tab_6">Table 4</ref>. As seen from the table, Ehsanpouret al .'s method shows better performance than our method. We find that our method demonstrates relatively low performance on the activity "Talking". This low performance is probably attributed to the number of samples in training data. In the test data, 86 % of  samples with the activity "Talking" have the group sizes of four, while the training data has only 57 samples whose group sizes are four, which is 0.8 % of the training data. As our method learns to predict group sizes, the number of samples in training data for each group size affects the performance. We analyze this effect in the subsequent section.</p><p>Analysis on Group Sizes. The group size prediction is one of the key factors to identify group members and thus affects social group activity recognition performance. To analyze this effect, we evaluate the performance on each group size and compare the results with Ehsanpouret al .'s method <ref type="bibr" target="#b12">[13]</ref> and Group-Former <ref type="bibr" target="#b25">[26]</ref>. <ref type="table" target="#tab_7">Table 5</ref> shows the results. As shown in the table, the performances of our method are moderately correlated to the training data ratios, while the other two methods do not show the correlation. This is the drawback of our method that relies on group size learning. However, our method shows the competitive performances on both small and large group sizes if there are a certain amount of training data. In contrast, each of the other two methods shows the competitive performances only on either large or small group sizes. These results imply that our method does not have the performance dependence on group sizes and thus can achieve high performance with large-scale training data.</p><p>Analysis on Order of Group Member Points. As described in Sec. 3.2, group member points in a ground truth point sequence are sorted in ascending order along X coordinates. To confirm the effectiveness of this arrangement, we compare the performances with two arrangements.  The higher probability implies that the order of group member points changes more frequently when group members move. These results suggest that the order changes more frequently when group member points are sorted in ascending order along Y coordinates and that the order is difficult to predict with slight differences of box positions.</p><p>Qualitative Analysis. The deformable attention modules are the critical components to aggregate features relevant to social group activity recognition and generate social group features. To analyze how the attention modules aggregate features for various social group activities, we visualize the attention locations of the transformer decoder in <ref type="figure" target="#fig_1">Fig. 3</ref>. We show locations with the top four attention weights in the last layer of the decoder. The purple bounding boxes show the group members, the red circles show the predicted group member points, and the yellow circles show the attention locations. The small and large yellow circles mean that the locations are in the high and low-resolution feature maps, respectively, showing a rough range of image areas affecting the generated features. The figure shows that features are typically aggregated from low-resolution feature maps if group members are located in broad areas, and vice versa. These results indicate that the attention modules can effectively aggregate features depending on the distribution of group members and contribute to improving the performance of social group activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a novel social group activity recognition method that leverages deformable transformers to generate effective social group features. This feature generation obviates the need for region features and hence makes the effectiveness of the social group features person-localization-agnostic. Furthermore, the group member information extracted from the features is represented so concisely that our method can identify group members with simple Hungarian matching, resulting in high-performance social group activity recognition. We perform extensive experiments and show significant improvement over existing methods.</p><p>The training loss for individual recognition L id is calculated between matched ground truths and predictions as follows:</p><formula xml:id="formula_16">L id = ? c L c + ? b L b + ? o L o + ? a L a ,<label>(14)</label></formula><formula xml:id="formula_17">L c = 1 |? (id) | Nq i=1 1 {i? ?? (id) } l f 1 , ??(id) (i) + 1 {i?? (id) } l f 0 , ??(id) (i) ,<label>(15)</label></formula><formula xml:id="formula_18">L b = 1 |? (id) | Nq i=1 1 {i? ?? (id) } b i ?b?(id) (i) 1 ,<label>(16)</label></formula><formula xml:id="formula_19">L o = 1 |? (id) | Nq i=1 1 {i? ?? (id) } 1 ? f GIoU b i ,b?(id) (i) ,<label>(17)</label></formula><formula xml:id="formula_20">L a = 1 |? (id) | Nq i=1 1 {i? ?? (id) } l f a i ,??(id) (i) ,<label>(18)</label></formula><p>where ? {c,b,o,a} are hyper-parameters and l f (?, ?) is the element-wise focal loss function <ref type="bibr" target="#b27">[28]</ref> whose hyper-parameters are described in <ref type="bibr" target="#b43">[44]</ref>.</p><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details of Detection Heads</head><p>In our method, all the detection heads are constituted by feed-forward networks with the subsequent sigmoid functions. The details of the detection heads are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person class head</head><p>This head has 1 linear layer with the subsequent sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box head</head><p>This head has 3 linear layers with the ReLU activation between the layers and the subsequent sigmoid function. A reference point is added to each corresponding box position before applying the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action head</head><p>This head has 1 linear layer with the subsequent sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity head</head><p>This head has 1 linear layer with the subsequent sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group size head</head><p>This head has 3 linear layers with the ReLU activation between the layers and the subsequent sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Member point head</head><p>This head has 3 linear layers with the ReLU activation between the layers and the subsequent sigmoid function. 2 ? M values are output from the last linear layer and then split into M group member points, where M denotes the maximum group size. A reference point is added to each corresponding group member point before applying the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Group Annotations in The Volleyball Dataset</head><p>Group annotations are critical components to fully leverage the learning capability of our method. In the evaluation of the Volleyball dataset <ref type="bibr" target="#b17">[18]</ref>, we use the original annotation set combined with the extra annotation set provided by Sendo and Ukita <ref type="bibr" target="#b34">[35]</ref> because the original annotations do not contain group information. The group annotations in the extra set are transferred to the original set by matching bounding boxes from each set with intersection over union (IoU). IoU is first calculated for each pair of a box from the original set and that from the extra set in the same frame. The calculated IoU values are then used as costs for the Hungarian algorithm <ref type="bibr" target="#b21">[22]</ref> to match the boxes. If a box from the extra set has a label indicating that the person in the box is involved in an activity, we assign a group member flag to the matched box from the original set.</p><p>The players involved in each group activity are defined by Sendo and Ukita <ref type="bibr" target="#b34">[35]</ref> as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pass</head><p>Players who are trying an underhand pass independently of whether or not they successfully do it. Set A player who is doing an overhand pass and those who will spike the ball whether they are trying or faking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spike</head><p>Players who are spiking and blocking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Winpoint</head><p>All players in the team scoring a point. This group activity is observed for a few seconds right after scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Qualitative Analysis</head><p>We further analyze the recognition results qualitatively with our method's success and failure cases on the Volleyball dataset <ref type="bibr" target="#b17">[18]</ref>. The results of the successful cases and failure cases are shown in <ref type="figure">Fig. 4a and 4b</ref>, respectively. The purple bounding boxes show the ground truth group members, the red circles show the predicted group member points, and the yellow circles show the attention locations. The small and large yellow circles mean that the locations are in the high and low-resolution feature maps, respectively, offering a rough range of image areas affecting the features used for the predictions. As seen from the figures, features are successfully aggregated from the areas around the group members in the successful cases, while those are aggregated from the regions around the non-group members, backgrounds, and part of the group members in the failure cases. It is worth noting that our method successfully recognizes social group activities even when one group member is apart from the other members such as the cases of "Right spike" and "Left winpoint" in <ref type="figure">Fig. 4a</ref>, demonstrating the effectiveness of our feature aggregation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right pass</head><p>Right spike Left set Left winpoint (a) Successful cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left pass Left spike</head><p>Right set Right winpoint (b) Failure cases. <ref type="figure">Fig. 4</ref>: Visualization of the social group activity recognition results. The purple bounding boxes, red circles, and yellow circles show the ground truth group members, predicted group member points, and attention locations in the deformable transformer decoder, respectively.</p><p>In the failure case of "Left pass", a non-group member is falsely recognized as a group member probably because the non-group member has the pose of the underhand pass, which is quite similar to the group member. In the failure case of "Left spike", a group member cannot be identified due to the occlusion. To correctly identify group members in these cases, long-term temporal context should be leveraged effectively. In the failure cases of "Right set" and "Right winpoint", the group members are widely distributed especially in the vertical direction. As discussed in the main manuscript, the group member point prediction is designed on the assumption that group members are seen side by side at the same vertical positions in an image. This design might affect the performance of the failure cases. These observations present opportunities for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overviews of conventional and proposed social group activity recognition methods. The labels in the right image show predicted social group activities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the attention locations in the deformable transformer decoder. We show the locations of the top four attention weights. The large circles mean that the locations are in the low-resolution feature maps. than in ascending order along Y coordinates. The probabilities in the table indicate the ratio of the changes of the point order when small perturbations are added to ground-truth bounding box positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>our training, the hyper-parameters ? {c,b,o,a} and ? {c,b,o,a} are set as ? c = ? c = 1, ? b = ? b = 5, ? o = ? o = 2, and ? a = ? a = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a ground truth group member point normalized with the image size, and ? {v,s,u} are hyper-parameters. Group member points in the sequence</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison against state-of-the-art methods on group activity recognition. The values with and without the brackets demonstrate the performances in the ground-truth-based and detection-based settings, respectively. The performances of individual action recognition are shown for future reference.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Volleyball</cell><cell></cell><cell></cell><cell cols="2">Collective Activity</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Activity</cell><cell cols="2">Action</cell><cell></cell><cell>Activity</cell><cell cols="2">Action</cell></row><row><cell>SSU [5]</cell><cell cols="2">86.2 (90.6)</cell><cell>-</cell><cell>(81.8)</cell><cell>-</cell><cell>( -)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>stagNet [33]</cell><cell cols="2">87.6 (89.3)</cell><cell>-</cell><cell cols="2">( -) 87.9</cell><cell>(89.1)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>ARG [41]</cell><cell cols="5">91.5 (92.5) 39.8 (83.0) 86.1</cell><cell>(88.1)</cell><cell cols="2">49.6 (77.3)</cell></row><row><cell>CRM [4]</cell><cell>-</cell><cell>(93.0)</cell><cell>-</cell><cell>( -)</cell><cell>-</cell><cell>(85.8)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>PRL [16]</cell><cell>-</cell><cell>(91.4)</cell><cell>-</cell><cell>( -)</cell><cell>-</cell><cell>( -)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>Actor-Transformers [14]</cell><cell>-</cell><cell>(94.4)</cell><cell>-</cell><cell>(85.9)</cell><cell>-</cell><cell>(92.8)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>Ehsanpour et al . [13]</cell><cell cols="5">93.0 (93.1) 41.8 (83.3) 89.4</cell><cell>(89.4)</cell><cell cols="2">55.9 (78.3)</cell></row><row><cell>Pramono et al . [32]</cell><cell>-</cell><cell>(95.0)</cell><cell>-</cell><cell>(83.1)</cell><cell>-</cell><cell>(95.2)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>DIN [42]</cell><cell>-</cell><cell>(93.6)</cell><cell>-</cell><cell>( -)</cell><cell>-</cell><cell>(95.9)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>GroupFormer [26]</cell><cell cols="2">95.0 * (95.7)</cell><cell>-</cell><cell cols="3">(85.6) 85.2 * (87.5  ? /96.3)</cell><cell>-</cell><cell>( -)</cell></row><row><cell>Ours</cell><cell cols="5">96.0 ( -) 65.0 ( -) 96.5</cell><cell>( -)</cell><cell cols="2">64.9 ( -)</cell></row></table><note>* We evaluated the performance with the publicly available source codes.? We evaluated but were not able to reproduce the reported accuracy because the configuration file for the Collective Activity dataset is not publicly available.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Analysis on the effect of the group annotations with the Volleyball dataset. The values with and without the brackets demonstrate the performances in the ground-truth-based and detection-based settings, respectively.</figDesc><table><row><cell>Method</cell><cell>Annotation type</cell><cell>Activity</cell></row><row><cell>GroupFormer [26]</cell><cell>Original Group</cell><cell>95.0 * (95.7) 93.2  ? (96.1 * )</cell></row><row><cell>Ours</cell><cell>Original Group</cell><cell>95.0 ( -) 96.0 ( -)</cell></row></table><note>* We evaluated the performance with the publicly available source codes.? We trained a group member detector and evaluated the performance with publicly available source codes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison against state-of-the-art social group activity recognition methods with the Volleyball dataset. Because the source codes are not publicly available, we implemented their algorithm based on our best understanding and evaluated the performance. We trained a group member detector and evaluated the performance with publicly available source codes.</figDesc><table><row><cell>Right</cell><cell>Left</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison against a state-of-the-art social group activity recognition method with the Collective Activity dataset.</figDesc><table><row><cell>Method</cell><cell cols="6">mAP Crossing Waiting Queueing Walking Talking</cell></row><row><cell cols="2">Ehsanpour et al . [13] 51.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>46.0</cell><cell>49.2</cell><cell>64.5</cell><cell>54.1</cell><cell>55.6</cell><cell>6.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Analysis on group sizes with Volleyball dataset. Because the source codes are not publicly available, we implemented their algorithm based on our best understanding and evaluated the performance. We trained a group member detector and evaluated the performance with publicly available source codes.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Group size (Training data ratio)</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">1 (36 %) 2 (21 %) 3 (19 %) 4 (6 %) 5 (5 %) 6 (12 %)</cell></row><row><cell>Ehsanpour et al . [13]  ?</cell><cell>45.3</cell><cell>48.2</cell><cell>61.2</cell><cell>27.3</cell><cell>15.8</cell><cell>32.5</cell></row><row><cell>GroupFormer [26]  ?</cell><cell>57.3</cell><cell>29.6</cell><cell>58.4</cell><cell>28.4</cell><cell>44.7</cell><cell>54.4</cell></row><row><cell>Ours</cell><cell>83.6</cell><cell>42.9</cell><cell>52.4</cell><cell>26.1</cell><cell>39.5</cell><cell>63.8</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Analysis on the order of member points with the Volleyball dataset.</figDesc><table><row><cell>Order of the group member points</cell><cell>Probability of changes in order</cell><cell>Accuracy</cell></row><row><cell>Ascending order in X coordinates</cell><cell>7.4 %</cell><cell>60.6</cell></row><row><cell>Ascending order in Y coordinates</cell><cell>13 %</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>shows the comparison results. As shown in the table, our method demonstrates better performance when group member points are sorted in ascending order along X coordinates</figDesc><table><row><cell>Right pass</cell><cell>Right spike</cell></row><row><cell>Left set</cell><cell>Left winpoint</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Individual Recognition</head><p>In our method, individuals are recognized by simply adding an action classification head to the detection heads in Deformable DETR <ref type="bibr" target="#b44">[45]</ref>. Given a set of feature embedding</p><p>from the deformable transformer decoder, the predictions of person class probabilities</p><p>, and action class probabilities</p><p>where N q is the number of query embeddings, N a is the number of action classes, f c (?), f b (?, ?), and f a (?) are the detection heads for the predictions, and r i ? [0, 1] 2 is a reference point, which is used in the same way as the localization in Deformable DETR. Note that the localization results are denoted in the normalized image coordinates.</p><p>We view individual recognition as a direct set prediction problem and match predictions and ground truths with the Hungarian algorithm <ref type="bibr" target="#b21">[22]</ref> during training. The optimal assignment of ground truths and predictions is determined by calculating the matching cost with the predicted person class probabilities, bounding boxes, and action class probabilities. Given a ground truth set of individual recognition, the set is first padded with ? (id) (no person) to change the size of the set to N q . Using the padded ground truth set, the matching cost of i-th element in the ground truth set and j-th element in the prediction set for individual recognition is calculated as follows:</p><p>where ? (id) is a set of ground-truth indices that correspond to ? (id) , b i ? [0, 1] 4 is a ground truth bounding box normalized with the image size, a i ? {0, 1} Na is a ground truth action label, f GIoU (?, ?) is a function that calculates generalized IoU <ref type="bibr" target="#b33">[34]</ref>, and ? {c,b,o,a} are the hyper-parameters. The Hungarian algorithm is applied to the matching cost to find the optimal assignment? (id) = arg min ??? Nq </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">HiRF: Hierarchical random field for collective activity recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sum product networks for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monte carlo tree search for scheduling activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Social scene understanding: End-to-end multi-person action localization and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-11" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">What are they doing? : Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-09" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic DETR: End-to-end object detection with dynamic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint learning of social groups, individuals action and sub-group activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehsanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-08" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Actor-transformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Progressive relation learning for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical relational networks for group activity recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical attention and context modeling for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Social roles in hierarchical models for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond actions: Discriminative models for contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GroupFormer: Group activity recognition with clustered spatial-temporal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chuah</surname></persName>
		</author>
		<title level="m">SBGAR: Semantics based group activity recognition. In: ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Empowering relational network by self-attention augmented conditional random fields for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R A</forename><surname>Pramono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-08" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">stagNet: An attentive semantic rnn for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Heatmapping of people involved in group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sendo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">CERN: Confidence-energy recurrent network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recurrent modeling of interaction context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning actor relation graphs for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatio-temporal dynamic inference network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05892</idno>
		<title level="m">COMPOSER: Compositional learning of group activity in videos</title>
		<imprint>
			<date type="published" when="2021-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-05" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
