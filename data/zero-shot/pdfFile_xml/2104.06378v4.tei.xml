<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
							<email>antoineb@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, Open-BookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions. Our code and data are available at https: //github.com/michiyasunaga/qagnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering systems must be able to access relevant knowledge and reason over it. Typically, knowledge can be implicitly encoded in large language models (LMs) pre-trained on unstructured text <ref type="bibr" target="#b33">(Petroni et al., 2019;</ref><ref type="bibr" target="#b9">Bosselut et al., 2019)</ref>, or explicitly represented in structured knowledge graphs (KGs), such as Freebase <ref type="bibr" target="#b6">(Bollacker et al., 2008)</ref> and ConceptNet <ref type="bibr" target="#b41">(Speer et al., 2017)</ref>, where entities are represented as nodes and relations between them as edges. Recently, pre-trained LMs have demonstrated remarkable success in many question answering tasks <ref type="bibr" target="#b34">Raffel et al., 2020)</ref>. However, while LMs have a broad coverage of knowledge, they do not empirically perform well on structured reasoning (e.g., handling negation) <ref type="bibr" target="#b18">(Kassner and Sch?tze, 2020)</ref>. On the other hand, KGs are more suited for structured reasoning    <ref type="figure">Figure 1</ref>: Given the QA context (question and answer choice; purple box), we aim to derive the answer by performing joint reasoning over the language and the knowledge graph (green box).</p><p>explainable predictions e.g., by providing reasoning paths <ref type="bibr" target="#b22">(Lin et al., 2019)</ref>, but may lack coverage and be noisy <ref type="bibr" target="#b7">(Bordes et al., 2013;</ref><ref type="bibr" target="#b14">Guu et al., 2015)</ref>. How to reason effectively with both sources of knowledge remains an important open problem. Combining LMs and KGs for reasoning (henceforth, LM+KG) presents two challenges: given a QA context (e.g., question and answer choices; <ref type="figure">Figure 1</ref> purple box), methods need to (i) identify informative knowledge from a large KG (green box); and (ii) capture the nuance of the QA context and the structure of the KGs to perform joint reasoning over these two sources of information. Previous works <ref type="bibr" target="#b2">(Bao et al., 2016;</ref><ref type="bibr" target="#b44">Sun et al., 2018;</ref><ref type="bibr" target="#b22">Lin et al., 2019)</ref> retrieve a subgraph from the KG by taking topic entities (KG entities mentioned in the given QA context) and their few-hop neighbors. However, this introduces many entity nodes that are semantically irrelevant to the QA context, especially when the number of topic entities or hops increases. Additionally, existing LM+KG methods for reasoning <ref type="bibr" target="#b22">(Lin et al., 2019;</ref><ref type="bibr" target="#b50">Wang et al., 2019a;</ref><ref type="bibr" target="#b13">Feng et al., 2020;</ref><ref type="bibr" target="#b26">Lv et al., 2020)</ref> treat the QA context and KG as two separate modalities. They <ref type="figure">Figure 2</ref>: Overview of our approach. Given a QA context (z), we connect it with the retrieved KG to form a joint graph (working graph; ?3.1), compute the relevance of each KG node conditioned on z ( ?3.2; node shading indicates the relevance score), and perform reasoning on the working graph ( ?3.3).</p><p>individually apply LMs to the QA context and graph neural networks (GNNs) to the KG, and do not mutually update or unify their representations. This separation might limit their capability to perform structured reasoning, e.g., handling negation.</p><p>Here we propose QA-GNN, an end-to-end LM+KG model for question answering that addresses the above two challenges. We first encode the QA context using an LM, and retrieve a KG subgraph following prior works <ref type="bibr" target="#b13">(Feng et al., 2020)</ref>. Our QA-GNN has two key insights: (i) Relevance scoring: Since the KG subgraph consists of all few-hop neighbors of the topic entities, some entity nodes are more relevant than others with respect to the given QA context. We hence propose KG node relevance scoring: we score each entity on the KG subgraph by concatenating the entity with the QA context and calculating the likelihood using a pretrained LM. This presents a general framework to weight information on the KG; (ii) Joint reasoning:</p><p>We design a joint graph representation of the QA context and KG, where we explicitly view the QA context as an additional node (QA context node) and connect it to the topic entities in the KG subgraph as shown in <ref type="figure">Figure 1</ref>. This joint graph, which we term the working graph, unifies the two modalities into one graph. We then augment the feature of each node with the relevance score, and design a new attention-based GNN module for reasoning. Our joint reasoning algorithm on the working graph simultaneously updates the representation of both the KG entities and the QA context node, bridging the gap between the two sources of information.</p><p>We evaluate QA-GNN on three question answering datasets that require reasoning with knowledge: CommonsenseQA <ref type="bibr" target="#b45">(Talmor et al., 2019)</ref> and Open-BookQA  in the commonsense domain (using the ConceptNet KG), and MedQA-USMLE <ref type="bibr" target="#b16">(Jin et al., 2021)</ref> in the biomedical domain (using the UMLS and DrugBank KGs). QA-GNN outperforms strong fine-tuned LM baselines as well as the existing best LM+KG model (with the same LM) by 4.7% and 2.3% respectively. In par-ticular, QA-GNN exhibits improved performance on some forms of structured reasoning (e.g., correctly handling negation and entity substitution in questions): it achieves 4.6% improvement over finetuned LMs on questions with negation, while existing LM+KG models are +0.6% over fine-tuned LMs. We also show that one can extract reasoning processes from QA-GNN in the form of general KG subgraphs, not just paths <ref type="bibr" target="#b22">(Lin et al., 2019)</ref>, suggesting a general method for explaining model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>We aim to answer natural language questions using knowledge from a pre-trained LM and a structured KG. We use the term language model broadly to be any composition of two functions, f head (f enc (x)), where f enc , the encoder, maps a textual input x to a contextualized vector representation h LM , and f head uses this representation to perform a desired task (which we discuss in ?3.2). In this work, we specifically use masked language models (e.g., RoBERTa) as f enc , and let h LM denote the output representation of a [CLS] token that is prepended to the input sequence x, unless otherwise noted. We define the knowledge graph as a multi-relational graph G = (V,E). Here V is the set of entity nodes in the KG; E ? V ?R?V is the set of edges that connect nodes in V, where R represents a set of relation types.</p><p>Given a question q and an answer choice a ? C, we follow prior work <ref type="bibr" target="#b22">(Lin et al., 2019)</ref> to link the entities mentioned in the question and answer choice to the given KG G. We denote V q ? V and V a ? V as the set of KG entities mentioned in the question (question entities; blue entities in Figure1) and answer choice (answer choice entities; red entities in Figure1), respectively, and use V q,a := V q ?V a to denote all the entities that appear in either the question or answer choice, which we call topic entities. We then extract a subgraph from G for a question-choice pair, G q,a sub = (V q,a sub , E q,a sub ), 1 which comprises all nodes on the k-hop paths between nodes in V q,a . <ref type="bibr">1</ref> We remove the superscript q,a if there is no ambiguity.  3 Approach: QA-GNN As shown in <ref type="figure">Figure 2</ref>, given a question and an answer choice a, we concatenate them to get the QA context [q; a]. To reason over a given QA context using knowledge from both the LM and the KG, QA-GNN works as follows. First, we use the LM to obtain a representation for the QA context, and retrieve the subgraph G sub from the KG. Then we introduce a QA context node z that represents the QA context, and connect z to the topic entities V q,a so that we have a joint graph over the two sources of knowledge, which we term the working graph, G W ( ?3.1). To adaptively capture the relationship between the QA context node and each of the other nodes in G W , we calculate a relevance score for each pair using the LM, and use this score as an additional feature for each node ( ?3.2). We then propose an attention-based GNN module that does message passing on the G W for multiple rounds ( ?3.3). Finally, we make the final prediction using the LM representation, QA context node representation and a pooled working graph representation ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint graph representation</head><p>To design a joint reasoning space for the two sources of knowledge, we explicitly connect them in a common graph structure. We introduce a new QA context node z which represents the QA context, and connect z to each topic entity in V q,a on the KG subgraph G sub using two new relation types r z,q and r z,a . These relation types capture the relationship between the QA context and the relevant entities in the KG, depending on whether the entity is found in the question portion or the answer portion of the QA context. Since this joint graph intuitively provides a reasoning space (working memory) over the QA context and KG, we term it working graph</p><formula xml:id="formula_0">G W = (V W ,E W ), where V W = V sub ? {z} and E W = E sub ?{(z,r z,q ,v) | v ? V q }?{(z,r z,a ,v) | v ? V a }.</formula><p>Each node in G W is associated with one of the four types: T = {Z,Q,A,O}, each indicating the context node z, nodes in V q , nodes in V a , and other nodes, respectively (corresponding to the node color, purple, blue, red, gray in Figure1 and 2). We denote the text of the context node z (QA context) and KG node v ? V sub (entity name) as text(z) and text(v).</p><p>We initialize the node embedding of z by the LM representation of the QA context (z LM = f enc (text(z))), and each node on G sub by its entity embedding ( ?4.2). In the subsequent sections, we will reason over the working graph to score a given (question, answer choice) pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KG node relevance scoring</head><p>Many nodes on the KG subgraph G sub (i.e., those heuristically retrieved from the KG) can be irrelevant under the current QA context. As an example shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the retrieved KG subgraph G sub with few-hop neighbors of the V q,a may include nodes that are uninformative for the reasoning process, e.g., nodes "holiday" and "river bank" are off-topic; "human" and "place" are generic. These irrelevant nodes may result in overfitting or introduce unnecessary difficulty in reasoning, an issue especially when V q,a is large. For instance, we empirically find that using the ConceptNet KG <ref type="bibr" target="#b41">(Speer et al., 2017)</ref>, we will retrieve a KG with |V sub | &gt; 400 nodes on average if we consider 3-hop neighbors.</p><p>In response, we propose node relevance scoring, where we use the pre-trained language model to score the relevance of each KG node v ? V sub conditioned on the QA context. For each node v, we concatenate the entity text(v) with the QA context text(z) and compute the relevance score:</p><formula xml:id="formula_1">? v = f head (f enc ([text(z); text(v)])), (1)</formula><p>where f head ?f enc denotes the probability of text(v) computed by the LM. This relevance score ? v captures the importance of each KG node relative to the given QA context, which is used for reasoning or pruning the working graph G W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNN architecture</head><p>To perform reasoning on the working graph G W , our GNN module builds on the graph attention framework (GAT) <ref type="bibr" target="#b47">(Veli?kovi? et al., 2018)</ref>, which induces node representations via iterative message passing between neighbors on the graph. Specifically, in a L-layer QA-GNN, for each layer, we update the representation h</p><formula xml:id="formula_2">( ) t ? R D of each node t ? V W by h ( +1) t = f n s?Nt?{t} ? st m st +h ( ) t ,<label>(2)</label></formula><p>where N t represents the neighborhood of node t, m st ? R D denotes the message from each neighbor node s to t, and ? st is an attention weight that scales each message m st from s to t. The sum of the messages is then passed through a 2-layer MLP, f n : R D ? R D , with batch normalization <ref type="bibr" target="#b15">(Ioffe and Szegedy, 2015)</ref>. For each node t ? V W , we set h (0) t using a linear transformation f h that maps its initial node embedding (described in ?3.1) to R D . Crucially, as our GNN message passing operates on the working graph, it will jointly leverage and update the representation of the QA context and KG. We further propose an expressive message (m st ) and attention (? st ) computation below.</p><p>Node type &amp; relation-aware message. As G W is a multi-relational graph, the message passed from a source node to the target node should capture their relationship, i.e., relation type of the edge and source/target node types. To this end, we first obtain the type embedding u t of each node t, as well as the relation embedding r st from node s to node t by</p><formula xml:id="formula_3">u t = f u (u t ), r st = f r (e st , u s , u t ),<label>(3)</label></formula><p>where u s ,u t ? {0,1} |T | are one-hot vectors indicating the node types of s and t, e st ? {0,1} |R| is a one-hot vector indicating the relation type of edge (s,t), f u : R |T | ? R D/2 is a linear transformation, and f r : R |R|+2|T | ? R D is a 2-layer MLP. We then compute the message from s to t as</p><formula xml:id="formula_4">m st = f m (h ( ) s , u s , r st ),<label>(4)</label></formula><p>where f m : R 2.5D ? R D is a linear transformation.</p><p>Node type, relation, and score-aware attention.</p><p>Attention captures the strength of association between two nodes, which is ideally informed by their node types, relations and node relevance scores. We first embed the relevance score of each node t by</p><formula xml:id="formula_5">? t = f ? (? t ),<label>(5)</label></formula><p>where f ? : R ? R D/2 is an MLP. To compute the attention weight ? st from node s to node t, we obtain the query and key vectors q, k by</p><formula xml:id="formula_6">q s = f q (h ( ) s , u s , ? s ),<label>(6)</label></formula><formula xml:id="formula_7">k t = f k (h ( ) t , u t , ? t , r st ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">f q : R 2D ? R D and f k : R 3D ? R D are linear transformations.</formula><p>The attention weight is then</p><formula xml:id="formula_9">? st = exp(? st ) t ?Ns?{s} exp(? st ) , ? st = q s k t ? D . (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference &amp; Learning</head><p>Given a question q and an answer choice a, we use the information from both the QA context and the KG to calculate the probability of it being the answer p</p><formula xml:id="formula_10">(a | q) ? exp(MLP(z LM , z GNN , g)), where z GNN = h (L) z and g denotes the pooling of {h (L) v | v ? V sub }.</formula><p>In the training data, each question has a set of answer choices with one correct choice. We optimize the model (both the LM and GNN components end-to-end) using the cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computation complexity</head><p>We analyze the time and space complexity of our method and compare with prior works, <ref type="bibr">KagNet (Lin et al., 2019)</ref> and MHGRN <ref type="bibr" target="#b13">(Feng et al., 2020)</ref> in Table 1. As we handle edges of different relation types using different edge embeddings instead of designing an independent graph networks for each relation as in RGCN <ref type="bibr" target="#b40">(Schlichtkrull et al., 2018)</ref> or MHGRN, the time complexity of our method is constant with respect to the number of relations and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN <ref type="bibr" target="#b13">(Feng et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate QA-GNN on three question answering datasets: CommonsenseQA <ref type="bibr" target="#b45">(Talmor et al., 2019)</ref>, OpenBookQA , and MedQA-USMLE <ref type="bibr" target="#b16">(Jin et al., 2021)</ref>.</p><p>CommonsenseQA is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, containing 12,102 questions. The test set of CommonsenseQA is not publicly available, and model predictions can only be evaluated once  <ref type="bibr">(2019)</ref>, and also report the score of our final system on the official test set.</p><formula xml:id="formula_11">Model Time Space G is a dense graph L-hop KagNet O |R| L |V| L+1 L O |R| L |V| L+1 L L-hop MHGRN O |R| 2 |V| 2 L O(|R||V|L) L-layer QA-GNN O |V| 2 L O(|R||V|L) G is a sparse graph with maximum node degree ? |V| L-hop KagNet O |R| L |V|L? L O |R| L |V|L? L L-hop MHGRN O |R| 2 |V|L? O(|R||V|L) L-layer QA-GNN O(|V|L?) O(|R||V|L)</formula><p>OpenBookQA is a 4-way multiple choice QA task that requires reasoning with elementary science knowledge, containing 5,957 questions. We use the official data splits from <ref type="bibr" target="#b29">Mihaylov and Frank (2018)</ref>.</p><p>MedQA-USMLE is a 4-way multiple choice QA task that requires biomedical and clinical knowledge. The questions are originally from practice tests for the United States Medical License Exams (USMLE). The dataset contains 12,723 questions. We use the original data splits from <ref type="bibr" target="#b16">Jin et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge graphs</head><p>For CommonsenseQA and OpenBookQA, we use ConceptNet <ref type="bibr" target="#b41">(Speer et al., 2017)</ref>, a general-domain knowledge graph, as our structured knowledge source G. It has 799,273 nodes and 2,487,810 edges in total. Node embeddings are initialized using the entity embeddings prepared by <ref type="bibr" target="#b13">Feng et al. (2020)</ref>, which applies pre-trained LMs to all triples in ConceptNet and then obtains a pooled representation for each entity. For MedQA-USMLE, we use a self-constructed knowledge graph that integrates the Disease Database portion of the Unified Medical Language System (UMLS; <ref type="bibr" target="#b5">Bodenreider, 2004)</ref> and <ref type="bibr">DrugBank (Wishart et al., 2018)</ref>. The knowledge graph contains 9,958 nodes and 44,561 edges. Node embeddings are initialized using the pooled representations of the entity name from SapBERT <ref type="bibr" target="#b23">(Liu et al., 2020a)</ref>.</p><p>Given each QA context (question and answer choice), we retrieve the subgraph G sub from G following the pre-processing step described in <ref type="bibr" target="#b13">Feng et al. (2020)</ref>, with hop size k = 2. We then prune G sub to keep the top 200 nodes according to the node relevance score computed in ?3.2. Henceforth, in this section ( ?4) we use the term "KG" to refer to G sub .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation &amp; training details</head><p>We set the dimension (D = 200) and number of layers (L = 5) of our GNN module, with dropout rate 0.2 applied to each layer <ref type="bibr" target="#b42">(Srivastava et al., 2014)</ref>. We train the model with the RAdam <ref type="bibr" target="#b24">(Liu et al., 2020b)</ref> optimizer using two GPUs (GeForce RTX 2080 Ti), which takes ?20 hours. We set the batch size from {32, 64, 128, 256}, learning rate for the LM module from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5}, and learning rate for the GNN module from {2e-4, 5e-4, 1e-3, 2e-3}. The above hyperparameters are tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>Fine-tuned LM. To study the role of KGs, we compare with a vanilla fine-tuned LM, which does not use the KG. We use RoBERTa-large  for CommonsenseQA, and RoBERTa-large and AristoRoBERTa 2 <ref type="bibr" target="#b10">(Clark et al., 2019)</ref> for Open-BookQA. For MedQA-USMLE, we use a state-of-theart biomedical LM, SapBERT <ref type="bibr" target="#b23">(Liu et al., 2020a)</ref>.</p><p>Existing LM+KG models. We compare with existing LM+KG methods, which share the same high-level framework as ours but use different modules to reason on the KG in place of QA-GNN ("yellow box" in MHGRN is the existing top performance model under this LM+KG framework. For fair comparison, we use the same LM in all the baselines and our model. The key differences between QA-GNN and these are that they do not perform relevance scoring or joint updates with the QA context ( ?3).  <ref type="bibr" target="#b40">(Schlichtkrull et al., 2018)</ref> 72.69 (?0.19) 68.41 (?0.66) + GconAttn <ref type="bibr" target="#b50">(Wang et al., 2019a)</ref> 72.61( ?0.39) 68.59 (?0.96) + KagNet <ref type="bibr" target="#b22">(Lin et al., 2019)</ref> 73.47 (?0.22) 69.01 (?0.76) + RN <ref type="bibr" target="#b39">(Santoro et al., 2017)</ref> 74.57 (?0.91) 69.08 (?0.21) + MHGRN <ref type="bibr" target="#b13">(Feng et al., 2020)</ref> 74.   <ref type="bibr" target="#b26">(Lv et al., 2020)</ref> 75.3 RoBERTa+MHGRN <ref type="bibr" target="#b13">(Feng et al., 2020)</ref> 75.4 Albert+PG <ref type="bibr" target="#b49">(Wang et al., 2020b)</ref> 75.6 Albert <ref type="bibr">(Lan et al., 2020) (ensemble)</ref> 76.5 UnifiedQA * <ref type="bibr" target="#b19">(Khashabi et al., 2020)</ref> 79.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Main results</head><p>RoBERTa + QA-GNN (Ours) 76.1 <ref type="table">Table 3</ref>: Test accuracy on CommonsenseQA's official leaderboard.</p><p>The top system, UnifiedQA (11B parameters) is 30x larger than our model.  We also achieve competitive results to other systems on the official leaderboards <ref type="table">(Table 3 and 5)</ref>. Notably, the top two systems, T5 <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref> and UnifiedQA <ref type="bibr" target="#b19">(Khashabi et al., 2020)</ref>, are trained with more data and use 8x to 30x more parameters than our model (ours has ?360M parameters). Excluding these and ensemble systems, our model is comparable in size and amount of data to other systems, and achieves the top performance on the two datasets. <ref type="table" target="#tab_8">Table 6</ref> shows the result on MedQA-USMLE. QA-GNN outperforms state-of-the-art fine-tuned LMs (e.g., SapBERT). This result suggests that our method is an effective augmentation of LMs and KGs across different domains (i.e., the biomedical</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa-large AristoRoBERTa</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Test</head><p>Careful Selection <ref type="bibr" target="#b1">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref type="bibr" target="#b0">(Banerjee and Baral, 2020)</ref> 80.0 AristoRoBERTa + PG <ref type="bibr" target="#b49">(Wang et al., 2020b)</ref> 80.2 AristoRoBERTa + MHGRN <ref type="bibr" target="#b13">(Feng et al., 2020)</ref> 80.6 Albert + KB 81.0 T5 * <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref> 83.2 UnifiedQA * <ref type="bibr" target="#b19">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + QA-GNN (Ours) 82.8 <ref type="table">Table 5</ref>: Test accuracy on OpenBookQA leaderboard. All listed methods use the provided science facts as an additional input to the language context. The top 2 systems, UnifiedQA (11B params) and T5 (3B params) are 30x and 8x larger than our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Test</head><p>BERT-base <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> 34.3 BioBERT-base  34.1 RoBERTa-large  35.0 BioBERT-large  36.7 SapBERT <ref type="bibr" target="#b23">(Liu et al., 2020a)</ref> 37.2 SapBERT + QA-GNN (Ours) 38.0   variant of the relevance scoring in Eq. 1, we also experimented with obtaining a contextual embedding w v for each node v ? V sub and adding to the node features: w v = f enc <ref type="bibr">([text(z)</ref>; text(v)]). However, we find that it does not perform as well (76.31%), and using both the relevance score and contextual embedding performs on par with using the score alone, suggesting that the score has a sufficient information in our tasks; hence, our final system simply uses the relevance score.</p><p>GNN architecture (bottom tables): We ablate the information of node type, relation, and relevance score from the attention and message computation in <ref type="figure" target="#fig_0">the GNN ( ?3.3)</ref>. The results suggest that all these features improve the model performance. For the number of GNN layers, we find L = 5 works the best on the dev set. Our intuition is that 5 layers allow various message passing or reasoning patterns between the QA context (z) and KG, such as "z ? 3 hops on KG nodes ? z".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Model interpretability</head><p>We aim to interpret QA-GNN's reasoning process by analyzing the node-to-node attention weights induced by the GNN. <ref type="figure">Figure 4</ref> shows two examples. In (a), we perform Best First Search (BFS) on the working graph to trace high attention weights from the QA context node (Z; purple) to Question entity nodes (blue) to Other (gray) or Answer choice entity nodes (orange), which reveals that the QA context z attends to "elevator" and "basement" in the KG, "elevator" and "basement" both attend strongly to "building", and "building" attends to "office building", which is our final answer. In (b), we use BFS to trace attention weights from two directions: Z ? Q ? O and Z ? A ? O, which reveals concepts ("sea" and "ocean") in the KG that are not necessarily mentioned in the QA context but bridge the reasoning between the question entity ("crab") and answer choice entity ("salt water"). While prior KG reasoning models <ref type="bibr" target="#b22">(Lin et al., 2019;</ref><ref type="bibr" target="#b13">Feng et al., 2020)</ref> enumerate individual paths in the KG for model interpretation, QA-GNN is not specific to paths, and helps to find more general reasoning structures (e.g., a KG subgraph with multiple anchor nodes as in example (a)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Structured reasoning</head><p>Structured reasoning, e.g., precise handling of negation or entity substitution (e.g., "hair" ? "art" in <ref type="figure">Figure 5b</ref>) in question, is crucial for making robust predictions. Here we analyze QA-GNN's ability to perform structured reasoning and compare with baselines (fine-tuned LMs and existing LM+KG models).</p><p>Quantitative analysis. <ref type="table" target="#tab_13">Table 9</ref> compares model performance on questions containing negation words (e.g., no, not, nothing, unlikely), taken from the CommonsenseQA IHtest set. We find that previous LM+KG models (KagNet, MHGRN) provide limited improvements over RoBERTa on questions with negation (+0.6%); whereas QA-GNN exhibits a bigger boost (+4.6%), suggesting its strength in structured reasoning. We hypothesize that QA-GNN's joint updates of the representations of the QA context and KG (during GNN message passing) allows the model to integrate semantic nuances expressed in language. To further study this hypothesis, we remove the connections between z and KG nodes from our QA-GNN <ref type="table" target="#tab_13">(Table 9</ref> bottom): now the performance on negation becomes close to the prior work, MHGRN, suggesting that the joint message passing helps for performing structured reasoning.</p><p>Qualitative analysis. <ref type="figure">Figure 5</ref> shows a case study to analyze our model's behavior for structured reasoning. The question on the left contains negation "not used for hair", and the correct answer is "B. art supply". We observe that in the 1st layer of QA-GNN, the attention from z to question entities ("hair", "round brush") is diffuse. After multiples rounds of message passing on the working graph, z attends strongly to "round brush" in the final layer of the GNN, but weakly to the negated entity "hair".  <ref type="figure">Figure 5</ref>: Analysis of QA-GNN's behavior for structured reasoning. Given an original question (left), we modify its negation (middle) or topic entity (right): we find that QA-GNN adapts attention weights and final predictions accordingly, suggesting its capability to handle structured reasoning.   The model correctly predicts the answer "B. art supply". Next, given the original question on the left, we (a) drop the negation or (b) modify the topic entity ("hair" ? "art"). In (a), z now attends strongly to "hair", which is not negated anymore. The model predicts the correct answer "A. hair brush". In (b), we observe that QA-GNN recognizes the same structure as the original question (with only the entity swapped): z attends weakly to the negated entity ("art") like before, and the model correctly predicts "A. hair brush" over "B. art supply". <ref type="table" target="#tab_12">Table 8</ref> shows additional examples, where we compare QA-GNN's predictions with the LM baseline (RoBERTa). We observe that RoBERTa tends to make the same prediction despite the modifications we make to the original questions (e.g., drop/insert negation, change an entity); on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work and discussion</head><p>Knowledge-aware methods for NLP. Various works have studied methods to augment natural language processing (NLP) systems with knowledge. Existing works <ref type="bibr" target="#b30">(Pan et al., 2019;</ref><ref type="bibr" target="#b58">Ye et al., 2019;</ref><ref type="bibr" target="#b33">Petroni et al., 2019;</ref><ref type="bibr" target="#b9">Bosselut et al., 2019</ref>) study pretrained LMs' potential as latent knowledge bases.</p><p>To provide more explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs <ref type="bibr" target="#b29">(Mihaylov and Frank, 2018;</ref><ref type="bibr" target="#b22">Lin et al., 2019;</ref><ref type="bibr" target="#b50">Wang et al., 2019a;</ref><ref type="bibr" target="#b49">Wang et al., 2020b;</ref><ref type="bibr" target="#b8">Bosselut et al., 2021)</ref>.  <ref type="formula" target="#formula_2">(2)</ref> language-conditioned KG node relevance scoring. Other works on scoring or pruning KG nodes/paths rely on graph-based metrics such as PageRank, centrality, and off-the-shelf KG embeddings <ref type="bibr" target="#b32">(Paul and Frank, 2019;</ref><ref type="bibr" target="#b12">Fadnis et al., 2019;</ref><ref type="bibr" target="#b3">Bauer et al., 2018;</ref><ref type="bibr" target="#b22">Lin et al., 2019)</ref>, without reflecting the QA context.</p><p>Other QA tasks. Several works study other forms of question answering tasks, e.g., passagebased QA, where systems identify answers using given or retrieved documents <ref type="bibr" target="#b35">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b17">Joshi et al., 2017;</ref>, and KBQA, where systems perform semantic parsing of a given question and execute the parsed queries on knowledge bases <ref type="bibr" target="#b4">(Berant et al., 2013;</ref><ref type="bibr" target="#b59">Yih et al., 2016;</ref><ref type="bibr" target="#b60">Yu et al., 2018)</ref>. Different from these tasks, we approach question answering using knowledge available in LMs and KGs.</p><p>Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) <ref type="bibr" target="#b38">(Riedel et al., 2013;</ref><ref type="bibr" target="#b46">Toutanova et al., 2015;</ref><ref type="bibr" target="#b53">Xiong et al., 2019;</ref><ref type="bibr" target="#b51">Wang et al., 2019b)</ref>. The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works.</p><p>Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text <ref type="bibr" target="#b57">(Yasunaga et al., 2017;</ref><ref type="bibr" target="#b56">Yasunaga and Liang, 2020)</ref> or KGs <ref type="bibr" target="#b48">(Wang et al., 2020a)</ref>. In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) <ref type="bibr" target="#b47">(Veli?kovi? et al., 2018)</ref> perform attention-based message passing to induce graph representations. We build on this framework, and further condition the GNN on the language input by introducing a QA context node ( ?3.1), KG node relevance scoring ( ?3.2), and joint update of the KG and language representations ( ?3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented QA-GNN, an end-to-end question answering model that leverages LMs and KGs.</p><p>Our key innovations include (i) Relevance scoring, where we compute the relevance of KG nodes conditioned on the given QA context, and (ii) Joint reasoning over the QA context and KGs, where we connect the two sources of information via the working graph, and jointly update their representations through GNN message passing. Through both quantitative and qualitative analyses, we showed QA-GNN's improvements over existing LM and LM+KG models on question answering tasks, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Relevance scoring of the retrieved KG: we use a pre-trained LM to calculate the relevance of each KG entity node conditioned on the QA context ( ?3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure2): (1) Relation Network (RN) (Santoro et al., 2017), (2) RGCN (Schlichtkrull et al., 2018), (3) GconAttn (Wang et al., 2019a), (4) KagNet (Lin et al., 2019), and (5) MHGRN (Feng et al., 2020). (1),(2),(3) are relation-aware GNNs for KGs, and (4),(5) further model paths in KGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Attention visualization direction: BFS from Q (b) Attention visualization direction: Q ? O and A ? O Figure 4: Interpreting QA-GNN's reasoning process by analyzing the node-to-node attention weights induced by the GNN. Darker and thicker edges indicate higher attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Question answering with LM+KG. In particular, a line of works propose LM+KG methods for question answering. Most closely related to ours are works by Lin et al. (2019); Feng et al. (2020); Lv et al. (2020). Our novelties are (1) the joint graph of QA context and KG, on which we mutually update the representations of the LM and KG; and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and enable If it is not used for hair, a round brush is an example of what? A. hair brush B. bathroom C. art supplies* If it is not used for hair, a round brush is an example of what? A. hair brush B. bathroom C. art supplies*</figDesc><table><row><cell>D. shower</cell><cell cols="2">E. hair salon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D. shower</cell><cell>E. hair salon</cell></row><row><cell>QA context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">QA context</cell></row><row><cell></cell><cell cols="3">QA context Node</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>QA context Node</cell></row><row><cell cols="2">Question Entity</cell><cell></cell><cell></cell><cell>Choice Entity</cell><cell></cell><cell></cell><cell cols="2">Question Entity</cell><cell>Choice Entity</cell></row><row><cell>hair</cell><cell></cell><cell cols="2">AtLocation</cell><cell>hair brush</cell><cell></cell><cell></cell><cell>hair</cell><cell>AtLocation</cell><cell>hair brush</cell></row><row><cell cols="2">round brush AtLocation</cell><cell></cell><cell>R e l a t e d T o</cell><cell>Answer art supply</cell><cell></cell><cell></cell><cell cols="2">round brush AtLocation</cell><cell>R e l a t e d T o</cell><cell>Answer art supply</cell></row><row><cell></cell><cell>Us ed Fo r</cell><cell cols="2">painting</cell><cell>Us ed Fo r</cell><cell></cell><cell></cell><cell></cell><cell>Us ed Fo r</cell><cell>painting</cell><cell>Us ed Fo r</cell></row><row><cell cols="2">Knowledge graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Knowledge graph</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Graph Connection ( ?3.1)</cell><cell>Dev Acc.</cell><cell>Contextualization ( ?3.2)</cell><cell>Dev Ac</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">No edge between Z and KG nodes</cell><cell>74.81</cell><cell>No contextualization</cell><cell>75.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Connect Z to all KG nodes</cell><cell>76.38</cell><cell>w/ contextual embedding</cell><cell>76.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Connect Z to QA entity nodes (final system)</cell><cell>76.54</cell><cell>w/ relevance score (final system)</cell><cell>76.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ both</cell><cell>76.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">GNN Attention &amp; Message ( ?3.3)</cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell>Dev Acc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Node type, relation, score-aware (final system) 76.54</cell><cell>L = 3</cell><cell>75.53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-type-aware -relation-aware -score-aware</cell><cell></cell><cell>75.41 75.61 75.56</cell><cell>L = 4 L = 5 (final system) L = 6 L = 7</cell><cell>76.34 76.54 76.21 75.96</cell></row><row><cell cols="2">Graph Connection ( ?3.1)</cell><cell></cell><cell>Dev Acc.</cell><cell cols="3">Relevance scoring ( ?3.2) Dev Acc.</cell><cell cols="2">Graph Connection ( ?3.1)</cell><cell>Dev Acc.</cell><cell>Relevance scoring ( ?3.2) Dev Acc.</cell></row><row><cell cols="2">No edge between Z and KG nodes</cell><cell></cell><cell>74.11</cell><cell>Nothing</cell><cell></cell><cell>75.15</cell><cell cols="2">No edge between Z and KG nodes</cell><cell>74.81</cell><cell>Nothing</cell><cell>75.56</cell></row><row><cell cols="2">Connect Z to all KG nodes</cell><cell></cell><cell>76.38</cell><cell cols="2">w/ contextual embedding</cell><cell>76.31</cell><cell cols="2">Connect Z to all KG nodes</cell><cell>76.38</cell><cell>w/ contextual embedding</cell><cell>76.31</cell></row><row><cell cols="3">Connect Z to QA entity nodes (final)</cell><cell>76.54</cell><cell cols="2">w/ relevance score (final)</cell><cell>76.54</cell><cell cols="2">Connect Z to QA entity nodes (final)</cell><cell>76.54</cell><cell>w/ relevance score (final)</cell><cell>76.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ both</cell><cell></cell><cell>76.52</cell><cell></cell><cell>w/ both</cell><cell>76.52</cell></row><row><cell cols="2">GNN Attention &amp; Message ( ?3.3)</cell><cell></cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell cols="2">Dev Acc.</cell><cell cols="2">GNN Attention &amp; Message ( ?3.3)</cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell>Dev Acc.</cell></row><row><cell cols="4">Node type, relation, score-aware (final) 76.54</cell><cell>L = 3</cell><cell></cell><cell>75.53</cell><cell cols="2">Node type, relation, score-aware (final) 76.54</cell><cell>L = 3</cell><cell>75.53</cell></row><row><cell>-type-aware -relation-aware -score-aware</cell><cell></cell><cell></cell><cell>75.11 75.23 75.15</cell><cell>L = 4 L = 5 (final) L = 6 L = 7</cell><cell></cell><cell>76.34 76.54 76.21 75.96</cell><cell cols="2">-type-aware -relation-aware -score-aware</cell><cell>75.41 75.61 75.56</cell><cell>L = 4 L = 5 (final) L = 6 L = 7</cell><cell>76.34 76.54 76.21 75.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Computation complexity of different L-hop reasoning models on a dense / sparse graph G = (V, E) with the relation set R.every two weeks via the official leaderboard. Hence, we perform main experiments on the in-house (IH) data splits used in Lin et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 and</head><label>2</label><figDesc>Table 4show the results on Common-senseQA and OpenBookQA, respectively. On both datasets, we observe consistent improvements over fine-tuned LMs and existing LM+KG models, e.g., on CommonsenseQA, +4.7% over RoBERTa, and +2.3% over the prior best LM+KG system, MHGRN. The boost over MHGRN suggests that QA-GNN makes a better use of KGs to perform joint reasoning than existing LM+KG methods.</figDesc><table><row><cell>Methods</cell><cell cols="2">IHdev-Acc. (%) IHtest-Acc. (%)</cell></row><row><cell>RoBERTa-large (w/o KG)</cell><cell>73.07 (?0.45)</cell><cell>68.69 (?0.56)</cell></row><row><cell>+ RGCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on Commonsense QA in-house split (controlled experiments). As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of<ref type="bibr" target="#b22">Lin et al. (2019)</ref>.</figDesc><table><row><cell>Methods</cell><cell>Test</cell></row><row><cell>RoBERTa (Liu et al., 2019)</cell><cell>72.1</cell></row><row><cell cols="2">RoBERTa+FreeLB (Zhu et al., 2020) (ensemble) 73.1</cell></row><row><cell>RoBERTa+HyKAS (Ma et al., 2019)</cell><cell>73.2</cell></row><row><cell>RoBERTa+KE (ensemble)</cell><cell>73.3</cell></row><row><cell>RoBERTa+KEDGN (ensemble)</cell><cell>74.4</cell></row><row><cell>XLNet+GraphReason</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy comparison on OpenBook QA (controlled experiments). Methods with Aris-toRoBERTa use the textual evidence by Clark et al.</figDesc><table /><note>(2019) as an additional input to the QA context.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy on MedQA-USMLE. it is not used for hair, a round brush is an example of what?A. hair brush B. bathroom C. art supplies*</figDesc><table><row><cell>If D. shower</cell><cell cols="2">E. hair salon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QA context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">QA context Node</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Question Entity</cell><cell></cell><cell></cell><cell>Choice Entity</cell><cell></cell><cell></cell><cell></cell></row><row><cell>hair</cell><cell></cell><cell cols="2">AtLocation</cell><cell>hair brush</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">round brush AtLocation</cell><cell></cell><cell>R e l a t e d T o</cell><cell>Answer art supply</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Us ed Fo r</cell><cell cols="2">painting</cell><cell>Us ed Fo r</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Knowledge graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Graph Connection ( ?3.1)</cell><cell>Dev Acc.</cell><cell>Contextualization ( ?3.2)</cell><cell>Dev Acc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">No edge between Z and KG nodes</cell><cell>74.81</cell><cell>No contextualization</cell><cell>75.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Connect Z to all KG nodes</cell><cell>76.38</cell><cell>w/ contextual embedding</cell><cell>76.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Connect Z to QA entity nodes (final system)</cell><cell>76.54</cell><cell>w/ relevance score (final system)</cell><cell>76.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ both</cell><cell>76.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GNN Attention &amp; Message ( ?3.3)</cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell>Dev Acc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Node type, relation, score-aware (final system) 76.54</cell><cell>L = 3</cell><cell>75.53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-type-aware -relation-aware -score-aware</cell><cell></cell><cell>75.41 75.61 75.56</cell><cell>L = 4 L = 5 (final system) L = 6 L = 7</cell><cell>76.34 76.54 76.21 75.96</cell></row><row><cell cols="2">Graph Connection ( ?3.1)</cell><cell></cell><cell>Dev Acc.</cell><cell cols="3">Relevance scoring ( ?3.2) Dev Acc.</cell><cell>Graph Connection ( ?3.1)</cell><cell>Dev Acc.</cell><cell>Relevance scoring ( ?3.2) Dev Acc.</cell></row><row><cell cols="2">No edge between Z and KG nodes</cell><cell></cell><cell>74.11</cell><cell>Nothing</cell><cell></cell><cell>75.15</cell><cell cols="2">No edge between Z and KG nodes</cell><cell>74.81</cell><cell>Nothing</cell><cell>75.56</cell></row><row><cell cols="2">Connect Z to all KG nodes</cell><cell></cell><cell>76.38</cell><cell cols="2">w/ contextual embedding</cell><cell>76.31</cell><cell>Connect Z to all KG nodes</cell><cell>76.38</cell><cell>w/ contextual embedding</cell><cell>76.31</cell></row><row><cell cols="3">Connect Z to QA entity nodes (final)</cell><cell>76.54</cell><cell cols="2">w/ relevance score (final)</cell><cell>76.54</cell><cell cols="2">Connect Z to QA entity nodes (final)</cell><cell>76.54</cell><cell>w/ relevance score (final)</cell><cell>76.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ both</cell><cell></cell><cell>76.52</cell><cell></cell><cell>w/ both</cell><cell>76.52</cell></row><row><cell cols="2">GNN Attention &amp; Message ( ?3.3)</cell><cell></cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell cols="2">Dev Acc.</cell><cell cols="2">GNN Attention &amp; Message ( ?3.3)</cell><cell>Dev Acc.</cell><cell>GNN Layers ( ?3.3)</cell><cell>Dev Acc.</cell></row><row><cell cols="4">Node type, relation, score-aware (final) 76.54</cell><cell>L = 3</cell><cell></cell><cell>75.53</cell><cell cols="2">Node type, relation, score-aware (final) 76.54</cell><cell>L = 3</cell><cell>75.53</cell></row><row><cell>-type-aware -relation-aware -score-aware</cell><cell></cell><cell></cell><cell>75.11 75.23 75.15</cell><cell>L = 4 L = 5 (final) L = 6 L = 7</cell><cell></cell><cell>76.34 76.54 76.21 75.96</cell><cell>-type-aware -relation-aware -score-aware</cell><cell>75.41 75.61 75.56</cell><cell>L = 4 L = 5 (final) L = 6 L = 7</cell><cell>76.34 76.54 76.21 75.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of our model components, using the CommonsenseQA IHdev set. domain besides the commonsense domain).Graph connection (top left table):The first key component of QA-GNN is the joint graph that connects the z node (QA context) to QA entity nodes V q,a in the KG ( ?3.1). Without these edges, the QA context and KG cannot mutually update their representations, hurting the performance: 76.5% ?74.8%, which is close to the previous LM+KG system, MHGRN. If we connected z to all the nodes in the KG (not just QA entities), the performance is comparable or drops slightly (-0.16%).Where would you find a basement that can be accessed with an elevator? A. closet B. church C. office building*</figDesc><table><row><cell>4.6 Analysis</cell></row><row><cell>4.6.1 Ablation studies</cell></row><row><cell>Table 7 summarizes the ablation study conducted</cell></row><row><cell>on each of our model components ( ?3.1,  ?3.2,  ?3.3),</cell></row><row><cell>using the CommonsenseQA IHdev set.</cell></row></table><note>KG node relevance scoring (top right table): We find the relevance scoring of KG nodes ( ?3.2) provides a boost: 75.56% ? 76.54%. As a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>If it is used for hair, a round brush is an example of what? A. hair brush B. art supply If it is not used for art, a round brush is an example of what? A. hair brush B. art supply</figDesc><table><row><cell cols="2">Original Question</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Negation Flipped</cell><cell></cell><cell></cell><cell cols="3">(b) Entity Changed (hair ? art)</cell></row><row><cell>Z</cell><cell></cell><cell>Z</cell><cell></cell><cell></cell><cell>Z</cell><cell></cell><cell></cell><cell></cell><cell>Z</cell><cell></cell></row><row><cell>hair</cell><cell>hair brush</cell><cell>hair</cell><cell>hair brush</cell><cell>A. hair brush (0.38) B. art supply (0.64)</cell><cell>hair</cell><cell>hair brush</cell><cell cols="2">A. hair brush (0.81) B. art supply (0.19)</cell><cell>art</cell><cell>hair brush</cell><cell>A. hair brush (0.72) B. art supply (0.28)</cell></row><row><cell>round brush</cell><cell>art supply</cell><cell>round brush</cell><cell>art supply</cell><cell></cell><cell>round brush</cell><cell>art supply</cell><cell></cell><cell></cell><cell>round brush</cell><cell>art supply</cell></row><row><cell>painting</cell><cell></cell><cell>painting</cell><cell></cell><cell></cell><cell>painting</cell><cell></cell><cell></cell><cell></cell><cell>painting</cell><cell></cell></row><row><cell cols="2">GNN 1st Layer</cell><cell cols="2">GNN Final Layer</cell><cell>Model Prediction</cell><cell cols="2">GNN Final Layer</cell><cell>Model Prediction</cell><cell></cell><cell cols="2">GNN Final Layer</cell><cell>Model Prediction</cell></row><row><cell></cell><cell></cell><cell cols="4">Example (Original taken from CommonsenseQA Dev)</cell><cell></cell><cell></cell><cell></cell><cell>RoBERTa Prediction</cell><cell></cell><cell>Our Prediction</cell></row><row><cell></cell><cell></cell><cell cols="5">[Original] If it is not used for hair, a round brush is an example of what? A. hair brush B. art supply</cell><cell></cell><cell></cell><cell>A. hair brush (?)</cell><cell></cell><cell>B. art supply (?)</cell></row><row><cell></cell><cell></cell><cell cols="5">[Negation flip] If it is used for hair, a round brush is an example of what?</cell><cell></cell><cell></cell><cell cols="3">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Entity change] If it is not used for art a round brush is an example of what?</cell><cell></cell><cell cols="3">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Original] If you have to read a book that is very dry you may become what? A. interested B. bored</cell><cell></cell><cell>B. bored (?)</cell><cell></cell><cell>B. bored (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 1] If you have to read a book that is very dry you may not become what?</cell><cell></cell><cell>B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 2] If you have to read a book that is not dry you may become what?</cell><cell></cell><cell>B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Double negation] If you have to read a book that is not dry you may not become what?</cell><cell></cell><cell cols="2">B. bored (? just no change?)</cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="4">Example (Original taken from CommonsenseQA Dev)</cell><cell></cell><cell></cell><cell cols="2">RoBERTa Prediction</cell><cell></cell><cell>Our Prediction</cell></row><row><cell></cell><cell></cell><cell cols="5">[Original] If it is not used for hair, a round brush is an example of what? A. hair brush B. art supply</cell><cell></cell><cell cols="2">A. hair brush (?)</cell><cell></cell><cell>B. art supply (?)</cell></row><row><cell></cell><cell></cell><cell cols="5">[Negation flip] If it is used for hair, a round brush is an example of what?</cell><cell></cell><cell cols="4">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Entity change] If it is not used for art a round brush is an example of what?</cell><cell cols="4">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Original] If you have to read a book that is very dry you may become what? A. interested B. bored</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>B. bored (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 1] If you have to read a book that is very dry you may not become what?</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 2] If you have to read a book that is not dry you may become what?</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Double negation] If you have to read a book that is not dry you may not become what?</cell><cell cols="3">B. bored (? just no change?)</cell><cell>A. interested (?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>If it is not used for hair, a round brush is an example of what? A. hair brush B. art supply A. hair brush (?) B. art supply (?) [Negation flip] If it is used for hair, a round brush is an example of what? A. hair brush (? just no change?) A. hair brush (?) [Entity change] If it is not used for art a round brush is an example of what? A. hair brush (? just no change?) A. hair brush (?) [Original] If you have to read a book that is very dry you may become what? A. interested B. bored B. bored (?) B. bored (?) If you have to read a book that is not dry you may not become what? B. bored (? just no change?)A. interested (?)</figDesc><table><row><cell>hair</cell><cell>hair brush</cell><cell>hair</cell><cell>hair brush</cell><cell>A. hair brush (0.38) B. art supply (0.64)</cell><cell>hair</cell><cell>hair brush</cell><cell cols="2">A. hair brush (0.81) B. art supply (0.19)</cell><cell>art</cell><cell>hair brush</cell><cell>A. hair brush (0.72) B. art supply (0.28)</cell></row><row><cell>round brush</cell><cell>art supply</cell><cell>round brush</cell><cell>art supply</cell><cell></cell><cell>round brush</cell><cell>art supply</cell><cell></cell><cell></cell><cell>round brush</cell><cell>art supply</cell></row><row><cell>painting</cell><cell></cell><cell>painting</cell><cell></cell><cell></cell><cell>painting</cell><cell></cell><cell></cell><cell></cell><cell>painting</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Example (Original taken from CommonsenseQA Dev)</cell><cell></cell><cell></cell><cell></cell><cell>RoBERTa Prediction</cell><cell></cell><cell>Our Prediction</cell></row><row><cell></cell><cell></cell><cell cols="6">[Original] [Negation ver 1] If you have to read a book that is very dry you may not become what?</cell><cell></cell><cell>B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 2] If you have to read a book that is not dry you may become what?</cell><cell></cell><cell>B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="2">[Double negation]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Example (Original taken from CommonsenseQA Dev)</cell><cell></cell><cell></cell><cell cols="2">RoBERTa Prediction</cell><cell></cell><cell>Our Prediction</cell></row><row><cell></cell><cell></cell><cell cols="5">[Original] If it is not used for hair, a round brush is an example of what? A. hair brush B. art supply</cell><cell></cell><cell cols="2">A. hair brush (?)</cell><cell></cell><cell>B. art supply (?)</cell></row><row><cell></cell><cell></cell><cell cols="5">[Negation flip] If it is used for hair, a round brush is an example of what?</cell><cell></cell><cell cols="4">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Entity change] If it is not used for art a round brush is an example of what?</cell><cell cols="4">A. hair brush (? just no change?) A. hair brush (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Original] If you have to read a book that is very dry you may become what? A. interested B. bored</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>B. bored (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 1] If you have to read a book that is very dry you may not become what?</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Negation ver 2] If you have to read a book that is not dry you may become what?</cell><cell cols="2">B. bored (?)</cell><cell></cell><cell>A. interested (?)</cell></row><row><cell></cell><cell></cell><cell cols="6">[Double negation] If you have to read a book that is not dry you may not become what?</cell><cell cols="3">B. bored (? just no change?)</cell><cell>A. interested (?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Case study of structured reasoning, comparing predictions by RoBERTa and our model (RoBERTa + QA-GNN). Our model correctly handles changes in negation and topic entities.</figDesc><table><row><cell>Methods</cell><cell>IHtest-Acc. (Overall)</cell><cell>IHtest-Acc. (Question w/ negation)</cell></row><row><cell>RoBERTa-large (w/o KG)</cell><cell>68.7</cell><cell>54.2</cell></row><row><cell>+ KagNet</cell><cell>69.0 (+0.3)</cell><cell>54.2 (+0.0)</cell></row><row><cell>+ MHGRN</cell><cell>71.1 (+2.4)</cell><cell>54.8 (+0.6)</cell></row><row><cell>+ QA-GNN (Ours)</cell><cell>73.4 (+4.7)</cell><cell>58.8 (+4.6)</cell></row><row><cell>+ QA-GNN (no edge between Z and KG)</cell><cell>71.5 (+2.8)</cell><cell>55.1 (+0.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Performance on questions with negation</figDesc><table><row><cell>in CommonsenseQA. () shows the difference with</cell></row><row><cell>RoBERTa. Existing LM+KG methods (KagNet, MH-</cell></row><row><cell>GRN) provide limited improvements over RoBERTa</cell></row><row><cell>(+0.6%); QA-GNN exhibits a bigger boost (+4.6%),</cell></row><row><cell>suggesting its strength in structured reasoning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Performance on questions with fewer/more entities in CommonsenseQA. () shows the difference with MHGRN (LM+KG baseline). KG node relevance scoring ( ?3.2) boosts the performance on questions containing more entities (i.e. larger retrieved KG). the other hand, QA-GNN adapts predictions to the modifications correctly (except for double negation in the table bottom, which is a future work). due to the size and noisiness of retrieved KGs: 70.1% accuracy vs 71.5% accuracy on questions with fewer entities. KG node relevance scoring mitigates this bottleneck, reducing the accuracy discrepancy: 73.5% and 73.4% accuracy on questions with more/fewer entities, respectively.</figDesc><table><row><cell>4.6.4 Effect of KG node relevance scoring</cell></row><row><cell>We find that KG node relevance scoring ( ?3.2)</cell></row><row><cell>is helpful when the retrieved KG (G sub ) is large.</cell></row><row><cell>Table 10 shows model performance on questions</cell></row><row><cell>containing fewer (?10) or more (&gt;10) entities in</cell></row><row><cell>the CommonsenseQA IHtest set (on average, the</cell></row><row><cell>former and latter result in 90 and 160 nodes in G sub ,</cell></row><row><cell>respectively). Existing LM+KG models such as</cell></row><row><cell>MHGRN achieve limited performance on questions</cell></row><row><cell>with more entities</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">OpenBookQA provides an extra corpus of scientific facts in a textual form. AristoRoBERTa uses the facts corresponding to each question, prepared by<ref type="bibr" target="#b10">Clark et al. (2019)</ref>, as an additional input to the QA context.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Rok Sosic, Weihua Hu, Jing Huang, Michele Catasta, members of the Stanford SNAP, P-Lambda and NLP groups and Project MOWGLI team, as well as our anonymous reviewers for valuable feedback. We gratefully acknowledge the support of DARPA under Nos. N660011924033 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>Code and data are available at https://github.com/michiyasunaga/qagnn. Experiments are available at https://worksheets. codalab.org/worksheets/ 0xf215deb05edf44a2ac353c711f52a25f.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge fusion and semantic knowledge ranking for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03101</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Careful selection of knowledge to solve open book question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Kumar Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constraint-based question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commonsense for generative multi-hop question answering tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
		<title level="m">The unified medical language system (UMLS): Integrating biomedical terminology. Nucleic acids research</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>?elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">From&apos;f&apos;to&apos;a&apos;on the ny regents science exams: An overview of the aristo project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Bhavana Dalvi Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tandon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01958</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Fadnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haque</forename><surname>Ishfaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02085</idno>
		<title level="m">Heuristics for interpretable knowledge graph contextualization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable multi-hop relational reasoning for knowledge-aware question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11784</idno>
		<title level="m">Self-alignment pretraining for biomedical entity representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-based reasoning over heterogeneous external knowledge for commonsense question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards generalizable neuro-symbolic systems for commonsense question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Oltramari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14087</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving question answering with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00993</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ranking and selecting multi-hop knowledge paths to better predict human needs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debjit</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beta embeddings for multi-hop logical reasoning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Entity context and relational paths for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Connecting the dots: A knowledgeable path generator for commonsense question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00691</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving natural language inference using external knowledge in the science questions domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassem</forename><surname>Makni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Mattei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pretrained language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David S Wishart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yannick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><forename type="middle">C</forename><surname>Feunang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvir</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sajed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinat</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sayeeda</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2018. Drugbank 5.0: a major update to the drugbank database for 2018. Nucleic acids research</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving question answering over incomplete kbs with knowledgeaware reader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Enhancing pre-trained language representations with rich knowledge for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graphbased, self-supervised program repair from diagnostic feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06725</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
