<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Robert</surname></persName>
							<email>damien.robert@ign.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ENGIE Lab CRIGEN</orgName>
								<orgName type="institution">CSAI</orgName>
								<address>
									<settlement>Stains</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">ENSG</orgName>
								<address>
									<postCode>F-77454</postCode>
									<settlement>Marne-la-Vallee</settlement>
									<region>IGN</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Vallet</surname></persName>
							<email>bruno.vallet@ign.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">ENSG</orgName>
								<address>
									<postCode>F-77454</postCode>
									<settlement>Marne-la-Vallee</settlement>
									<region>IGN</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
							<email>loic.landrieu@ign.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">ENSG</orgName>
								<address>
									<postCode>F-77454</postCode>
									<settlement>Marne-la-Vallee</settlement>
									<region>IGN</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works on 3D semantic segmentation propose to exploit the synergy between images and point clouds by processing each modality with a dedicated network and projecting learned 2D features onto 3D points. Merging large-scale point clouds and images raises several challenges, such as constructing a mapping between points and pixels, and aggregating features between multiple views. Current methods require mesh reconstruction or specialized sensors to recover occlusions, and use heuristics to select and aggregate available images. In contrast, we propose an end-to-end trainable multi-view aggregation model leveraging the viewing conditions of 3D points to merge features from images taken at arbitrary positions. Our method can combine standard 2D and 3D networks and outperforms both 3D models operating on colorized point clouds and hybrid 2D/3D networks without requiring colorization, meshing, or true depth maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic segmentation on S3DIS (74.7 mIoU 6-Fold) and on . Our full pipeline is accessible at https: //github.com/drprojects/DeepViewAgg, and only requires raw 3D scans and a set of images and poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The fast-paced development of dedicated neural architectures for 3D data has led to significant improvements in the automated analysis of large 3D scenes <ref type="bibr" target="#b27">[28]</ref>. All top-performing methods operate on colorized point clouds, which requires either specialized sensors <ref type="bibr" target="#b70">[71]</ref>, or running a colorization step which is often closed-source <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> and sensor-dependent <ref type="bibr" target="#b40">[41]</ref>. However, while colorized point clouds carry some radiometric information, images combined with dedicated 2D architectures are better suited for learning textural and contextual cues. A promising line of work sets out to leverage the complementarity between 3D point clouds and images by projecting onto 3D points <ref type="bibr">Figure 1</ref>. Combining 2D and 3D Information. We propose to merge the complementary information between point clouds and a set of co-registered images. Using a simple visibility model, we can project 2D features onto the 3D points and use viewing conditions to select features from the most relevant images. We represent images at their position with the symbol and color the 3D points according to the image they are seen in. the 2D features learned from real <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> or virtual images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44]</ref> Combining point clouds and images with arbitrary poses (i.e. in the wild) as represented in <ref type="figure">Figure 1</ref>, involves recovering occlusions and computing a point-pixel mapping, which is typically done using accurate depth maps from specialized sensors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b64">65]</ref> or a potentially costly meshing step <ref type="bibr" target="#b7">[8]</ref>. Furthermore, when a point is seen in different images simultaneously, the 2D features must be merged in a meaningful way.</p><p>In the mesh texturation literature, multi-view aggregation is typically addressed by selecting images for each triangle based on their viewing conditions, e.g. distance, viewing angle, or occlusion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b67">68]</ref>. Hybrid 2D/3D methods for large-scale point cloud analysis usually rely on heuristics to select a fixed number of images per point and pool their features uniformly without considering viewing conditions. Multi-view aggregation has also been extensively studied for shape recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70]</ref>, albeit in a controlled and synthetic setting not entirely applicable to the analysis of large scenes.</p><p>In this paper, we propose to learn to merge features from multiple images with a dedicated attention-based scheme. For each 3D point, the information from relevant images is aggregated based on the point's viewing condition. Thanks to our GPU-based implementation, we can efficiently compute a point-pixel mapping without mesh or true depth maps, and without sacrificing precision. Our model can handle large-scale scenes with an arbitrary number of images per point taken at any position (with camera pose information), which corresponds to a standard industrial operational setting [33, <ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b66">67]</ref>. Using only standard 2D and 3D backbone networks, we set a new state-of-the-art for the S3DIS and KITTI-360 datasets. Our method improves on both standard and hybrid 2D/3D approaches without requiring point cloud colorization, mesh reconstruction, or depth sensors. In this paper, we present a novel and modular multi-view aggregation method for semantizing hybrid 2D/3D data based on the viewing conditions of 3D points in images. Our approach combines the following advantages:</p><p>? We set a new state-of-the-art for S3DIS 6-fold <ref type="bibr">(74.7</ref> mIoU), and KITTI-360 Test (58.3 mIoU) without using points' colorization.</p><p>? Our point-pixel mapping operates directly on 3D point clouds and images without requiring depth maps, meshing, colorization, or virtual view generation.</p><p>? Our efficient GPU-based implementation handles arbitrary numbers of 2D views and large 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Attention-Based Modality Fusion. Methods using attention mechanisms to learn multi-modal representation have attracted a lot of attention, in particular for combining textual and visual information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> as well as videos <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref>. Closer to our setting, Lu et al. <ref type="bibr" target="#b51">[52]</ref> use an attention scheme to select the most relevant parts of an image for visual question answering. Li et al. <ref type="bibr" target="#b48">[49]</ref> define a two-branch attentionbased modality fusion network merging 2D semantic and 3D occupancy for scene completion. Such work confirms the relevance of using attention for learning multi-modal representations.</p><p>2D/3D Scene Analysis with Deep Learning. Over the last few years, deep networks specifically designed to handle the 3D modality have reached impressive degrees of performance and maturity, see the review of Guo et al. <ref type="bibr" target="#b27">[28]</ref>. Recent work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> propose to use a dedicated 3D network for processing point clouds, while a 2D convolutional network extracts radiometric features which are projected to the point cloud. These methods require the true depth of each pixel to compute the point-pixel mapping, which makes them less applicable in a real-world setting. SnapNet <ref type="bibr" target="#b7">[8]</ref>, as well as more recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44]</ref> generate virtual views processed by a 2D network and whose predictions are then projected back to the point cloud. These approaches, while performing well, require a costly mesh reconstruction preprocessing step to generate meaningful images. Some approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref> fuse RGB and range images, which requires dedicated sensors and can not handle multiple views with occlusions. Existing hybrid 2D/3D methods rely on a fixed number of images per point chosen with heuristics such as the maximization of unseen points <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. Then, the different views are merged using pooling operations (max <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b62">63]</ref> or sum-pool <ref type="bibr" target="#b39">[40]</ref>) or based on the 2D features' content <ref type="bibr" target="#b35">[36]</ref>. To the best of our knowledge, no method has yet been proposed to leverage the viewing conditions for multi-view aggregation for the semantic segmentation of large scenes.</p><p>View Selection. The problem of selecting and merging the best images for a 3D scene has been extensively studied for surface reconstruction and texturing. Images are typically chosen according to the viewing angle with the surface normal <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48]</ref>, proximity and resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, geometric and visibility priors <ref type="bibr" target="#b59">[60]</ref>, as well as crispness <ref type="bibr" target="#b24">[25]</ref>, and consistency with respect to occlusions <ref type="bibr" target="#b67">[68]</ref>. While most of these criteria do not directly apply to point clouds, they illustrate the importance of camera pose information for selecting relevant images. Related to our setting is the Next Best View selection problem <ref type="bibr" target="#b60">[61]</ref>, which consists in planning the camera position giving the most information about an object of interest <ref type="bibr" target="#b16">[17]</ref>. This criterion takes different meanings according to the setting, such as the number of unseen voxels <ref type="bibr" target="#b65">[66]</ref>, diversity <ref type="bibr" target="#b53">[54]</ref>, information-theoretic measures of uncertainty <ref type="bibr" target="#b38">[39]</ref>, or can be directly learned end-to-end <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b71">72]</ref>. Our setting differs in that the images have already been acquired, and the task is to choose which one contains the most relevant information for each point. We draw inspiration from the end-to-end approaches demonstrating that a neural network can assess the quality of information contained in an image from pose information.</p><p>The problem of view selection is also addressed in the literature on shape recognition <ref type="bibr" target="#b62">[63]</ref>. Features from different images can be merged based on their similarity <ref type="bibr" target="#b68">[69]</ref>, discriminativity <ref type="bibr" target="#b23">[24]</ref>, or using patch matching schemes <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b74">75]</ref> or graph-neural networks <ref type="bibr" target="#b69">[70]</ref>. Some methods use 3D features <ref type="bibr" target="#b73">[74]</ref> or camera position <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref> to select the best views, but no technique yet makes explicit use of the viewing configuration. Furthermore, these methods operate on synthetic views of artificial shapes, which differs from our goal of analyzing large scenes with images in arbitrary poses.</p><p>Closer to our problem, Armeni et al. <ref type="bibr" target="#b4">[5]</ref> aggregate views using handcrafted heuristics. Bozic et al. <ref type="bibr" target="#b8">[9]</ref> use a distanceaware attentive view aggregation for 3D reconstruction, but disregard other viewing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Let P be a set of 3D points and I a collection of coregistered images, all acquired from the same scene. We characterize points by their position in space, and images by their pixels' RGB values along with intrinsic and extrinsic camera parameters. Our goal is to exploit the correspondence between points and image pixels to perform 3D point cloud semantic segmentation with features learned from both modalities. Our method starts by computing an occlusionaware mapping between 3D points and pixels, then uses viewing conditions through an attention scheme to aggregate relevant image features for each 3D point. This approach can be easily integrated into a standard 3D network architecture, allowing us to learn from both point clouds and images simultaneously in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point-Image Mapping</head><p>We start by efficiently computing a mapping between the images of I and the points of P . We say that a point-image pair (p, i) ? P ? I is compatible if p is visible in i, i.e. p is in the frustum of i and not occluded. For such a pair, we define the re-projection pix(p, i) as the pixel of i in which p is visible. Note that as points are zero-dimensional objects (zero-volume), pix(p, i) is a single pixel. We denote by v(p) the views of p, i.e. the set of images in which p is visible.</p><p>Point-Pixel Mapping Construction. We operate in a general in the wild multi-view setting in which the optical axes of the cameras and the 3D sensor are not necessarily aligned. Consequently, computing the point-image mapping requires a visibility model to detect occlusions. This can be done by computing a full mesh reconstruction from the point clouds or by using a depth map obtained by a camera-aligned depth sensor or other means. In contrast, we propose an efficient implementation of the straightforward Z-buffering method <ref type="bibr" target="#b61">[62]</ref> to compute the mapping directly from images and point clouds. For each image i ? I, we replace all 3D points in the frustum of i under a pre-determined distance by a square plane section facing towards i and whose size depends on their distance to the sensor and the resolution of the point cloud. We can compute the projection mask-or splat-of each square onto i using the camera parameters  <ref type="figure" target="#fig_8">Figure 2</ref>. Mapping Computation. We estimate pixel depth for all (a) images using the co-registered (b) point cloud. We compute (c) Z-buffers with an efficient GPU-accelerated implementation, resulting in depth maps comparable to the (d) true distance given by camera-aligned depth sensors. We use our estimated depth maps to compute point-image mappings. Better seen on a monitor. of i. We iteratively accumulate all splats in a depth map called Z-buffer by keeping track of the closest point-camera distance for each pixel. Simultaneously, we store corresponding point indices in an index map, along with other relevant point attributes. Once all splats have been accumulated, visible points are the ones whose indices appear in the index map. For each visible point p, we set pix(p, i) as the pixel of i in which p itself is projected. Our GPU-accelerated implementation can process the entire S3DIS dataset <ref type="bibr" target="#b5">[6]</ref> subsampled at 5cm (12 million points and 1413 high-resolution equirectangular images) within 65 seconds. See <ref type="figure" target="#fig_8">Figure 2</ref> for an illustration, and the Appendix for more details on the mapping computation, alternative visibility models, and our memory-efficient implementation for large-scale mappings.</p><p>Viewing Conditions. To each compatible point-image pair (p, i), we associate a D-dimensional vector o (p,i) describing the conditions under which the point p is seen in i. In practice, we define this vector as a set of D = 8 handcrafted features qualifying the observation conditions of (p, i): (i) normalized depth, (ii-iv) local geometric descriptors (linearity, planarity, scattering) (v) viewing angle w.r.t. the estimated normal, (vi) row of the pixel, (vii) local density, (viii) occlusion rate. See the Appendix for more details on the computation and impact of these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Multi-View Aggregation</head><p>We denote by {f 2D i } i?I a set of 2D feature maps of width C associated to the images I, typically obtained with a convolutional neural network (CNN). Our goal is to transfer these features to the 3D points by exploiting the correspondence between points and images. However, not all viewing images contain equally relevant information for a given 3D point. We propose an attention-based approach to weigh and aggregate features from the viewing images for each point p.</p><p>View Features. The mapping pix(p, i) described in Section 3.1 allows us to associate image features to each compatible point-image pair (p, i):</p><formula xml:id="formula_0">f 2D (p,i) = ? 0 f 2D i [pix(p, i)] ,<label>(1)</label></formula><formula xml:id="formula_1">with ? 0 : R C ? R C a Multi-Layer Perceptron (MLP).</formula><p>Learned image features can contain information of different natures: contextual, textural, class-specific, and so on. To reflect this consideration, we split the channels off 2D (p,i) into K contiguous blocks of ?C/K? channels:</p><formula xml:id="formula_2">f 2D (p,i) = f 2D (p,i),1 , ? ? ? ,f 2D (p,i),K .<label>(2)</label></formula><p>with [ ? ] the channel-wise concatenation operator. Each block of channels represents a subset of the image information contained inf 2D .</p><p>View Quality. The conditions under which a point is seen in an image can be more or less conducive to certain types of information, see <ref type="figure" target="#fig_1">Figure 3</ref>. For example, an image viewing a point from a distance may give important contextual cues, while an image taken close and at a straight angle may give detailed textural information. In contrast, an image in which a point is seen from a slanted angle or under high distortion may not contain relevant information and may need to be discarded. To model these complex dependencies, we propose to predict for each compatible point-image pair (p, i) a set of K quality scores x k (p,i) ? R from its viewing conditions o (p,i) defined in Section 3.1. The quality x k (p,i) represents the relevance for point p of the information contained in the feature block k of image i .</p><p>For each point p, we consider the set v(p) of images in which it is visible. We propose to learn to predict the view quality x k (p,i) for each feature block k by considering all images i ? v(p) simultaneously. Indeed, the relevance of an image can depend on the context of the other views. For example, while a given image may provide less-thanperfect viewing conditions of a given 3D point, it may be the only available image with global information of the point's context. We use a deep set architecture <ref type="bibr" target="#b75">[76]</ref> to map the set of viewing conditions {o (p,i) } i?v(p) to a vector of size K:</p><formula xml:id="formula_3">z (p,i) = ? 1 (o (p,i) ) (3) x (p,i) = ? 3 z (p,i) , ? 2 max{z (p,i) } i?v(p) ,<label>(4)</label></formula><p>with  View Attention Scores. We can now compute K attention scores a k (p,i) in [0, 1] corresponding to the relative relevance for point p of the kth feature block of image i. The attentions are obtained by applying a softmax function to the quality scores x k (p,i) across the images in v(p). To account for the possibly varying number of views per point, we scale the softmax according to the number of images seeing the point p :</p><formula xml:id="formula_4">? 1 : R D ? R M , ? 2 : R M ? R M ,</formula><formula xml:id="formula_5">a k (p,i) = softmax 1 |v(p)| x k (p,i) i?v(p)</formula><p>. <ref type="formula">(5)</ref> View Gating. A limitation of using a softmax in this context is that the attention scores? k (p,i) always sum to 1 over v(p) regardless of the overall quality of the image set. Because of occlusion or limited viewpoints, some 3D points may not be seen by any relevant image for a given feature block k (e.g. no close or far images). In this case, it may be beneficial to discard an information block from all images altogether and purely rely on geometry. This allows the 2D network to learn image features without accounting for potentially spreading corrupted information to points with dubious viewing conditions. To this end, we introduce a gating parameter g k p whose role is to block the transfer of the features block k if the overall quality of the image set v(p) is too low:</p><formula xml:id="formula_6">g k p = ReLU tanh ? k max i?v(p) x k (p,i) + ? k ,<label>(6)</label></formula><p>with ?, ? ? R K trainable parameters and ReLU the rectified linear activation <ref type="bibr" target="#b54">[55]</ref>. If all quality scores x k (p,i) are negative for a given point p and block k, the gating parameter g k p will be exactly zero and block possibly detrimental information due to sub-par viewing conditions.  <ref type="figure" target="#fig_14">Figure 4</ref>. Bimodal 2D/3D Architecture. Using our multi-view aggregation module, we combine a 2D convolutional encoder E 2D and a 3D network composed of an encoder E 3D , a decoder D 3D , and a classifier C 3D . We associate relevant 2D features to each 3D point according to their viewing conditions in each compatible image. We propose three different 2D/3D fusion strategies: early (our choice in the experiments), intermediate, and late fusion.</p><p>Attentive Image Feature Pooling. For each point p seen in one or more images, we merge the feature mapsf 2D (p,i) from each view (p, i). For each block k, we compute the sum of the view featuresf 2D (p,i),k weighted by their respective attention scores a k (p,i) and multiplied by the gating parameter g k p . The combined image feature P(f 2D , p) associated to point p is then defined as the channelwise concatenation of the resulting tensors for all blocks:</p><formula xml:id="formula_7">P(f 2D , p) = ? ? g k p i?v(p) a k (p,i)f 2D (p,i),k ? ? K k=1 .<label>(7)</label></formula><p>3.3. Bimodal Point-Image Network.</p><p>We can use the multi-view feature aggregation method described above to perform semantic segmentation of a point cloud and co-registered images by combining a network operating on 3D point clouds and 2D CNN.</p><p>Fusion Strategies. We use a 2D fully convolutional network to compute pixel-wise image feature maps f 2D . We also consider a 3D deep network following the classic U -Net architecture <ref type="bibr" target="#b58">[59]</ref> and composed of three parts: (i) an encoder E 3D mapping the point cloud into a set of 3D feature maps at different resolution (innermost map and skip connections); (ii) a decoder D 3D converting these maps into a 3D feature map at the highest resolution (iii) a classifier C 3D associating to each point a vector for class scores of size N , the number of target classes. <ref type="figure">Figure 5</ref>. Dynamic Batching. We can improve the quantity of information contained in each training batch by cropping images around the sampled point clouds. We represent a set of 10 images with different crop size fitting in a budget of pixels corresponding to 4 full-size images.</p><p>As shown in <ref type="figure" target="#fig_14">Figure 4</ref>, we investigate three classic fusion schemes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>, connecting the image features at different points of the 3D network: (i) directly with the raw 3D features before E 3D (early fusion), (ii) in the skip connections (intermediate fusion) (iii) between the decoder D 3D and the classifier C 3D (late fusion:). See the Appendix for the details and equations for these fusion schemes.</p><p>Dynamic-Size Image-Batching The number of images v(p) in which a point p is visible can vary significantly. Furthermore, when dealing with large-scale scenes, only a subset P sample of the 3D scene is typically processed at once (e.g. spherical sampling). For this reason, the part of an image i for which points of P sample are visible can sometimes be only a small fraction of the entire image. This will typically occur with equirectangular images or when P sample is far away from i. We use the adaptive batching scheme depicted in <ref type="figure">Figure 5</ref> to stabilize memory usage across batches and avoid needless computations on excessively large images. The first step is to crop each image using the smallest window across a fixed set of sizes (e.g. 64 ? 64, 128 ? 64, etc.) such that the crop contains the bounding box of all seen points of P with a given margin. Observing that the memory consumption of a fully convolutional encoder is linear w.r.t. the number of input pixels, we allocate to each point cloud in the batch a budget of pixels. Images are then chosen randomly by iteratively selecting images with a probability proportional to their number of pixels and to the number of newly seen points in the cloud, until the pixel budget is spent. Finally, the images are organized into different batches according to their sizes, allowing for their simultaneous processing. Note that at inference time, we can take batches as large as the GPU memory allows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We use sparse encoding for mappings in order to only store compatible point-image pairs. This proves necessary  for the large scale, in-the-wild setting with varying number of images seeing each point. Our code is available at https: //github.com/drprojects/DeepViewAgg, the exact network and training configurations are given in the Appendix, and all the metrics of all runs can be accessed at https://wandb.ai/damien_robert/ DeepViewAgg-benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We propose several experiments on public large-scale semantic segmentation benchmarks to demonstrate the benefits of our deep multi-view aggregation module (DeepViewAgg). Our approach yields significantly better results than our 3D backbone directly operating on colorized point clouds. We set a new state-of-the-art for the highly contested S3DIS benchmark using only standard 2D and 3D architectures combined with our proposed module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>S3DIS <ref type="bibr" target="#b5">[6]</ref>. This indoor dataset of office buildings contains over 278 million semantically annotated 3D points across 6 building areas-or folds. A companion dataset can be downloaded at https://github.com/alexsax/2D-3D-Semantics, and contains 1413 equirectangular images. To represent our large-scale, in-the-wild setting, we merge each fold into a large point cloud and discard all room-related information. We apply minor registration adjustments detailed in the Appendix.</p><p>ScanNet <ref type="bibr" target="#b19">[20]</ref>. This indoor dataset contains over 1501 scenes obtained from 2.5 million RGB-D images with pose information. To account for the high redundancy between images, we select one in every 50 image. This dataset deviates slightly from our intended setting as 2D and 3D are derived from the same sensors.</p><p>KITTI-360 <ref type="bibr" target="#b49">[50]</ref>. This large outdoor dataset contains over 100k laser scans and 320k images captured with a multisensor mobile platform. We use one image every five from the left perspective camera. We report the classwise performance on the official withheld test set.</p><p>General Setting. All datasets provide colorized point clouds obtained with dataset-specific preprocessings. To handle the large size of scans, we define batches using a sampling strategy for S3DIS (2m-radius spheres) and KITTI-360 (6m-radius vertical cylinders), while we process ScanNet room-by-room, see <ref type="figure" target="#fig_5">Figure 6</ref>. We down-sample the point clouds for processing (S3DIS: 2cm, ScanNet: 3cm, KITTI-360: 5cm) and interpolate our prediction to full resolution for evaluation. To mitigate the memory impact of the 2D encoder, we also down-sample S3DIS images to 1024 ? 512 but keep the full resolution for ScanNet (320 ? 240) and KITTI-360 (1408 ? 376).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Evaluation</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare the performance of our approach and other learning methods on S3DIS, ScanNet Validation, and KITTI-360 Test using the classwise mean Intersectionover-Union (mIoU) as metric. Our method (DeepViewAgg) uses images in the 2D encoder and raw uncolored point clouds in the 3D encoder. All other approaches, including our backbone (3D Backbone), use the colorized point clouds provided by the datasets.</p><p>DeepViewAgg sets a new state-of-the-art for S3DIS for all 6 folds and the second-highest performance for the 5th fold. In particular, we outperform the VMVF network <ref type="bibr" target="#b43">[44]</ref>, showing that our multi-view aggregation model can overtake methods relying on costly virtual view generation using only available images. Furthermore, VMVF uses true depth maps, colorized point clouds, normals, and room-wise normalized information. In contrast, our method only uses raw XYZ data in the 3D encoder and estimates the mappings. Our approach also overtakes the recent PointTransformer <ref type="bibr" target="#b22">[23]</ref> (PointTrans.) by 1.2 mIoU points, even though this method outperforms our 3D backbone by 4 points on colorized points. Our model also improves the performance of our 3D backbone on the KITTI-360 test set by 4.4 points, illustrating the importance of images for both indoor and outdoor datasets alike.</p><p>While giving reasonable results, our method does not perform as well on the validation set of ScanNet comparatively. We outperform the 2D/3D fusion method of BPNet <ref type="bibr" target="#b35">[36]</ref> when restricted to 3D annotations, illustrating the importance of view selection. We argue that the limited variety in the camera points of view of ScanNet RGB-D scans, as well as their small field-of-view and blurriness reduce the quality of the information provided by images. This is reinforced by the impressive performance of VMVF, which synthesizes its own images with controlled points of view and resolution. See <ref type="figure" target="#fig_7">Figure 7</ref> for qualitative illustrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>We conduct further analyses on Fold 5 and Fold 2 of S3DIS (subsampled at 5cm for processing) and the validation set of KITTI-360 in <ref type="table" target="#tab_1">Table 2</ref>. We added Fold 2 along the commonly used Fold 5, as it benefited most from our method, and hence is more conducive to evaluating the impact of our design choices. <ref type="table" target="#tab_1">Table 2</ref>, combining a 3D deep network operating on raw 3D features and a 2D network with our method (Best Configuration) improves the performance by over 6 to 15 points compared to the same 3D backbone operating on colorized point clouds alone (XYZRGB). To illustrate that point colorization is not a trivial task, we train our 3D backbone with point clouds colorized by averaging for each point the color of all pixels in which it is visible (XYZ Average-RGB). Compared to the "official" colored point clouds, we observe a drop of 1 point for Fold 5 and 1.3 point for KITTI-360, but a gain of almost 5 points for Fold 2. This shows how different point cloud colorization schemes can yield vastly different results. Not using any radiometric information and purely relying on 3D points without color (XYZ) decreases the score of XYZRGB by a further 3 to 4 points on S3DIS. For KITTI-360, XYZ outperforms XYZ Average-RGB, suggesting that poor colorization can even be detrimental. We also evaluate a scheme in which the 3D network is entirely removed, and 3D points are classified solely based on features coming from a 2D encoder-decoder and our view aggregation module, without any 3D convolution (Pure RGB). This method outperforms even (XYZRGB) for S3DIS, illustrating the relevance of images for point cloud segmentation. On KITTI-360, as many 3D points are not seen by the cameras used, this approach perform worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality Combinations. As observed in</head><p>Training our best 2D+3D model with images downsampled by a factor of 2 (Lower Image Resolution) brings a large performance drop. In contrast, using 2cm of 3D resolution instead of 5cm (Higher 3D Resolution) has little impact for S3DIS. We conclude that when the images already contain fine-grained information, the impact of the resolution of the 3D voxel grid decreases.  360, but significantly worse for Fold 2 for which the gain of using images is more pronounced. Using only one feature group (Only One Group, K = 1 in Equation 2) results in a drop of 4.8 points for Fold 2, highlighting that our method can learn to treat different types of radiometric information specifically. Removing the gating mechanism (No Gating, see Equation 6) decreases the IoU by 3 points for Fold 2, and 1.1 on KITTI-360. Not using dynamic batches forces us to limit ourselves to 4 full-size images per 3D sphere/cylinder, which results in performance drops of 2 to 7 points. Pretraining the 2D network on related open-access datasets (No 2D Pre-Training) accounts for up to 7 mIoU points. Not only do images contain rich radiometric information, but they also allow us to leverage the ubiquitous availability of annotated 2D datasets.</p><p>Using featurewise max-pooling to merge the views results in a drop of 1 to 1.5 points for S3DIS and 3 points for KITTI-360. This illustrates that as long as we employ proper mapping, batching, and pre-training strategies, even simple pooling operations can perform very well. However, the addition of our model appears necessary to improve the precision even further and reach state-of-the-art results.</p><p>Switching our 3D backbone to a lighter version of MinkowskiNet with decreased widths, we observe no significant impact on the prediction quality. This suggests that we could use our approach successfully with smaller models. See the Appendix for further analysis of our design choices and of the influence of viewing conditions.</p><p>Limitations. While our method does not require sensors with aligned optical axes, true depth maps, or a meshing step, we still need camera poses. In some "in the wild" settings, they may not be available or require a pose estimation and registration step which may be costly and error-prone. Our mapping computation also relies on the assumption that the 2D and 3D modalities are acquired simultaneously.</p><p>Our multi-view aggregation method operates purely on viewing conditions and does not take the geometric and radiometric features into account in the computation of attention scores. We implemented a self-attention-based approach using such features, which resulted in a significant increase in memory usage without tangible benefits: the viewing conditions appear to be the most critical factor when selecting and aggregating images features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a deep learning-based multi-view aggregation model for the semantic segmentation of large 3D scenes. Our approach uses the viewing condition of 3D points in images to select and merge the most relevant 2D features with 3D information. Combined with standard image and point cloud encoders, our method improves the state-of-theart for two different datasets. Our full pipeline can run on a point cloud and a set of co-registered images at arbitrary positions without requiring colorization, meshing, or true depth maps. These promising results illustrate the relevancy of using dedicated architectures for extracting information from images even for 3D scene analysis. <ref type="figure">Figure SM-1</ref>. Visualizations. We propose interactive visualizations of hybrid 2D/3D data along with predictions of our model. We also provide the code necessary to create more such visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-2. Efficient Point-Pixel Mapping</head><p>In the Z-buffering step, we only consider points at a maximum distance R = 8m for indoor scenes and R = 20m for outdoor settings. We replace the points in image i by cubes oriented towards i and with a size given by the following formula involving dist(p, i) the distance between point p and image i, k = 1 a swell factor ruling how much closer cubes are expanded and c the resolution of the voxel grid, or a typical inter-point distance (2-8cm in our experiments): size of cubes(dist(p, i)) = c(1 + ke ? dist(p,i)/R ) .</p><p>(SM-8)</p><p>This heuristic increases the size of cubes that are close to the image to ensure that they do hide the cubes behind them. Note that this heuristic operates on the size of the 3D cubes before camera projection and not on their projected pixel masks, which are computed based on camera intrinsic parameters. See Algorithm SM-1 for the pseudo-code of the mapping computation. Storing point-pixel mappings for large-scale scenes with many images can be challenging. To minimize the memory impact of such a procedure, we use the Compressed Sparse Row (CSR) format. This allows us to represent the mappings compactly and treat large scenes at once. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-3. Projection Information</head><p>We use 8 handcrafted features to qualify the viewing conditions of a point p seen in image i.</p><p>? Normalized depth. An image seeing a point at a distance may contain relevant contextual cues but poor textural information. We compute the distance dist(p, i) between point p and image i and divide by the maximum viewing distance R = 8m for indoor scenes and R = 20m for outdoor scenes. 12</p><p>? Local geometric descriptors. The geometry of a point cloud can impact the quality of its views in images. Indeed, while planar surfaces may be better captured by a camera, a highly irregular surface may present many occlusions or grazing rays. We compute geometric descriptors (linearity, planarity, scattering) based on the eigenvalues of the covariance matrix between a point and its 50 neighbors <ref type="bibr" target="#b21">[22]</ref>.</p><p>? Viewing angle. An image seeing a surface from a right angle may better capture its surroundings than if the view angle is slanted with respect to the surface. We compute the absolute value of the cosine between the viewing angle and the normal estimated from the covariance matrix calculated at the previous step.</p><p>? Pixel row. To account for potential camera distortion near the top and bottom of the image (e.g. for equirectangular images), we report the row of pixels and divide by the image height (number of rows). Note that we could derive a similar feature for cameras with radial distortion, such as fisheye cameras.</p><p>? Local density. Density can impact occlusion and be an indicator of the local precision of the 3D sensor. We compute the area of the smallest disk containing the 50th neighbor and normalize it by the square of the voxel grid resolution.</p><p>? Occlusion rate. Occlusion may significantly impact the quality of the projected image features. We compute the ratio of the 50 nearest neighbors of p also seen in i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-4. Fusion schemes</head><p>We denote by {f 2D i } i?I a set of 2D feature maps of width C associated with the images I, typically obtained with a convolutional neural network. f 3D designates the raw feature of the point cloud given by the sensor: position, but also intensity/reflectance if available (not used in this paper). We denote by P(f 2D , P ) the projection of the learned image features f 2D onto the point cloud P by our multi-view aggregation technique. The early and late fusion schemes can be written as follows:</p><formula xml:id="formula_8">y early = C 3D ? D 3D ? E 3D f 3D , P(f 2D , P ) (SM-9) y late = C 3D D 3D ? E 3D f 3D , P(f 2D , P ) . (SM-10)</formula><p>For the intermediate fusion scheme, the 2D and 3D features are merged directly in the 3D encoder. Our 3D backbone follows a classic U-Net architecture, and its encoder is organized in L levels {E 3D l } L l=1 processing maps of increasingly coarse resolution. Each level l &gt; 1 is composed of of a downsampling module down l , typically strided convolutions (down 1 = Id), and a convolutional module conv l , typically a sequence of ResNet blocks. The 2D encoder is also composed of L levels {E 2D l } L l=1 corresponding to the different image resolutions. We propose to match the 2D and 3D levels at full resolution (1024 ? 512 and 2cm for S3DIS), and all subsequent levels after the same number of 2D/3D downsampling. At each level l = 1 . . . L, we concatenate the downsampled higher resolution 3D map f 3D l?1 with the map f 2D l obtained from the images at the matched resolution:</p><formula xml:id="formula_9">f 3D l = conv down f 3D l?1 , P(f 2D l , P ) , (SM-11)</formula><p>with f 3D 0 the raw 3D features. The decoder and classifiers follow the same organization than the 3D backbone. We consider P sample a portion of a point cloud to add to the 3D part of a batch. In order to build the image batch we iteratively select images according to the following procedure. We first select the image set I sample seeing at least one point of P sample . For equirectangular images, we rotate the images to place the mappings at the center. We then crop each image i along the tightest bounding box containing all seen point of P sample with a minimum margin of m along a fixed set of image size along: crops = {64 ? 64, 128 ? 64, 128 ? 128, 256?128, 256?256, 512?256, 512?512, 1024?512}. We then associate with each cropped image a score defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-5. Dynamic-Size Image-Batching</head><formula xml:id="formula_10">score(i, P sample , I sample , batch)) = (SM-12) area(i) max(area(i) | i ? I sample ) + (SM-13)</formula><p>? unseen(i, P sample , batch)) max(unseen(i, P sample , batch) | i ? I sample )</p><p>, (SM-14)</p><p>with area(i) the area of the cropped image i in pixels, batch the current image batch, unseen(i, P sample , batch) the number of points of P sample seen in image i but not in any image of the current batch, and ? = 2 a parameter controlling the trade-off between maximum area and maximum coverage. The current image batch is initialized as an empty set, but the scores must be updated as it is filled. The images are chosen randomly with a probability proportional to their score. We chose in all experiments a margin m = 8 pixels and a budget corresponding to 4 full resolution mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-6. Implementation Details</head><p>Our method is developed in Pytorch and is implemented within the open-source framework Torch-Points3d <ref type="bibr" target="#b12">[13]</ref>.</p><p>For our backbone 3D network, we use Torch-Point3D's <ref type="bibr" target="#b12">[13]</ref> Res16UNet34 implementation of MinkowskiNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>This UNet-like architecture comprises 5 encoding layers and 5 decoding layers. Encoding layers are composed of a strided convolution of kernel_size=[3, 2, 2, 2, 2] and stride=[1, 2, 2, 2, 2] followed by N=[0, 2, 3, 4, 6] ResNet blocks <ref type="bibr" target="#b31">[32]</ref> of channel size [128, <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>, respectively.</p><p>The decoding layers are built in the same manner, with a strided transposed convolution of kernel_size=[2, 2, 2, 2, 3] and stride=[2, 2, 2, 2, 1] as their first operation, followed by a concatenation with the corresponding skipped-features from the encoder and N=1 ResNet block of channel size <ref type="bibr">[128,</ref><ref type="bibr">128,</ref><ref type="bibr">96,</ref><ref type="bibr">96,</ref><ref type="bibr">96]</ref>, respectively. A fully connected linear layer followed by a softmax converts the last features into class scores. Unless specified otherwise, ReLU activation and batch normalization <ref type="bibr" target="#b37">[38]</ref> are used across the architecture.</p><p>For our 2D encoders, we use the encoder part of pre-trained ResNet18 networks <ref type="bibr" target="#b30">[31]</ref>. For indoor scenes, we use the modified ResNet18 from <ref type="bibr" target="#b18">[19]</ref> pre-trained on ADE20K <ref type="bibr" target="#b77">[78]</ref>, which has 5 layers of output channel sizes [128, <ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref> and resolution scale= <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8]</ref>. The relatively high resolution of the output feature map allows us to map the 2D features of size C = 512 from the last layer directly to the point cloud without upsampling. For outdoor scenes, we use the ResNet18 from <ref type="bibr" target="#b46">[47]</ref> pre-trained on Cityscapes <ref type="bibr" target="#b17">[18]</ref>, which has 5 layers of output channel sizes [128, <ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512</ref>] and resolution scale= <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. We use pyramid feature pooling <ref type="bibr" target="#b76">[77]</ref> on layers 1, 2, 3, and 4, which results in a feature vector of size C = 960 passed to the mapped points.</p><p>For our DeepViewAgg module, the extracted image features are converted into view features of size C with the MLP ? 0 of size C ? C ? C. For the computation of quality scores, we use the following MLPs: For indoor scenes, we use the lowest resolution map of a network <ref type="bibr" target="#b18">[19]</ref> pretrained on ADE20K <ref type="bibr" target="#b77">[78]</ref>. For the outdoor scenes, we use pyramid feature pooling <ref type="bibr" target="#b76">[77]</ref> with a network <ref type="bibr" target="#b46">[47]</ref> pretrained on Cityscapes <ref type="bibr" target="#b17">[18]</ref>. We use early fusion in all experiments unless specified otherwise.</p><p>All models are trained with SGD with an initial learning rate of 0.1 for 200 epochs with decay steps of 0.3 at epoch 80, 120, 160 for indoor datasets, and 20 epochs with decay at epoch 10, 16 for the outdoor dataset. The pre-trained 2D networks use a learning rate 100 times smaller than the rest of the model. We use random rotation and jittering for point clouds, random horizontal flip and color jittering for images, and featurewise jittering for descriptors of the viewing conditions.</p><p>For more details on the implementation, we refer the reader to our provided code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-7. Supplementary Ablation</head><p>We propose further ablations whose results in <ref type="table" target="#tab_4">Table SM</ref></p><formula xml:id="formula_11">-1.</formula><p>To assess the quality of our visibility model, we propose to compute the point-pixel mappings using the depth maps provided with S3DIS instead of Z-buffering. When running our method with such mappings (Mapping from Depth), we observe lower performances. This can be explained by the fact that depth-based mapping computation is sensitive to minor discrepancies between the depth map and the real point positions. Such a phenomenon can be observed on S3DIS depth images where the surfaces are viewed from a slanted angle, resulting in fewer point-image mappings being recovered. See <ref type="figure" target="#fig_8">Figure SM-2</ref> for an illustration of this phenomenon. In conclusion, not only can our approach bypass the need for specialized sensors or costly mesh reconstruction altogether, but our direct point-pixel mapping may yield better results than the provided mappings obtained with more involved methods.   <ref type="bibr">(b)</ref> by searching points within a small margin of the target depth. We note that slight depth discrepancies near slanted surfaces prevents mapping from being recovered. Better seen on a monitor. tive resolutions yields better results and that matching the encoder levels of 2D and 3D networks may not be straightforward. To ensure that our proposed module captures all radiometric information contained in colorized point clouds, we trained our chosen architecture to run on colorized point clouds and images (XYZRGB + DeepViewAgg). The resulting performance confirms that colorizing 3D points does not bring additional information not already captured by images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-8. Influence of Maximum Depth</head><p>The maximum point-image depth is chosen as the distance beyond which adjacent 3D points appear in the same image pixel: 8m for S3DIS sampled at 5cm with images of width 1024 and 20m for KITTI-360. As illustrated in Table SM-2, reducing this parameter too much leads to a drop in performance both on S3DIS Fold 5 and KITTI-360, while slight modifications do not significantly affect the results. Since the number of point-image mappings grows quadratically with this parameter, one may consider smaller values to decrease the memory usage or computing time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-9. Influence of Number of Images</head><p>In contrast to existing methods (e.g. MVPNet <ref type="bibr" target="#b39">[40]</ref>, VMVF <ref type="bibr" target="#b43">[44]</ref>, BPNet <ref type="bibr" target="#b35">[36]</ref>), our set-based, sparse implementation of point-image mappings allows us to have a varying number of views per 3D point. In <ref type="table" target="#tab_4">Table SM-3</ref> we investigate the performance drop w.r.t. our best model when limiting the number of images per point cloud to a fixed number of images and not using dynamic batching. Under these conditions, our performance decreases by 6.8 pts on S3DIS Fold 5 and 3.5 pts on KITTI-360 when using only 3 images, i.e. the configuration of BPNet. For comparison, 3D points of S3DIS are seen in 5.0 images on average (STD 3.3), and 2.5 for KITTI-360 (STD 2.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-10. Influence of Viewing Conditions</head><p>We propose to highlight the role viewing conditions descriptor. In Table SM-4, we estimate the usage by our model of each feature as the drop in mIoU on S3DIS Fold 5 &amp; KITTI-360 when they are replaced by their dataset average (e.g. all points appear at the same distance). We also measure the feature sensitivity by averaging the squared partial derivative <ref type="bibr">[26, 3.3</ref>.1] of the view compatibility score x defined in (4) w.r.t. each view descriptor. We observe that our model makes use of all observation features, and that the compatibility scores are most sensitive to small differences in scattering for S3DIS Fold 5, and depth for KITTI-360 Val.</p><p>To visualize the influence of viewing conditions, we represent in <ref type="figure">Figure SM</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-11. S3DIS Adjustment</head><p>We adjusted some room and image positions in S3DIS <ref type="bibr" target="#b5">[6]</ref> and https://github.com/alexsax/2D-3D-Semantics to recover mappings between points and equirectangular images. More specifically, we rotate Area_2/hallway_11 and Area_5/hallway_6 by 180?around the Z-axis, and we shift and rotate all images in Area_5b by the same manually-found corrective offset and angle. These fixes are all available in our repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-12. Detailed Results</head><p>We report in Table SM-5 the classwise performance across all datasets. We see a clear improvement for indoor datasets for classes such as windows, boards, and pictures. These are expected results because these classes are hard to parse in 3D but easily identified in 2D. Besides, we can see that S3DIS's classes such as beams, columns, chairs, and tables also benefit from the contextual information provided by images. For the KITTI-360 dataset, the multimodal model outperforms the 3D-only baseline for all classes. We can see the benefit of image features on small objects or underrepresented classes in 3D, such as traffic signs, persons, trucks, motorcycles, and bicycles.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>and ? 3 :</head><label>3</label><figDesc>R 2M ? R K three MLPs, M the size of the set embedding, and max the channelwise maximum operator for a set of vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Multi-View Information. A 3D point is seen in several images with different insights. Here, the green image contains contextual information, while the pink image captures the local texture. The orange image sees the point at a slanted angle and may contain no additional relevant information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Datasets. Illustration of the sampling procedure for all considered datasets with point clouds alongside some of the available images. The 3D components of batches are constituted of spheres for (a) S3DIS, rooms for (b) ScanNet, and cylinders for (c) KITTI-360.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Design Choices. Using late fusion (Late Fusion) instead of early fusion gives comparable results on Fold 5 and KITTI-(a) Colorized Point Cloud (b) Ground Truth (c) 3D Backbone Predictions (d) Our Model Predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative illustration. Scenes from our considered datasets (top: S3DIS, middle: ScanNet, bottom: KITTI-360) with (a) colorized point clouds, (b) ground truth point annotations, (c) prediction of the backbone network operating on the colorized point cloud, and (d) our method operating on raw uncolored point clouds and images. Our approach is able to use images to resolve cases in which the geometry is ambiguous or unusual, such as a large amphitheater with tiered rows of seats (top row). Color legend given in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm SM- 2</head><label>2</label><figDesc>Dynamic-Size Image-Batching Input: P sample point cloud, I image set Parameters: B budget of pixels, m border margin I sample ? {i | i ? I and ?p ? P sample s.t i ? v(p)} scores ? array of 0 of size I sample for i ? I sample do i ? tightest crop(i) to contain P sample with margin m scores[i] ? score(i, P sample , I sample , ?) end for batch ? [] while B &gt; 0 and length I sample &gt; 0 do pick i ? I sample randomly w.r.t. scores batch ? [batch, i] I sample ? I sample \ {i} B ? B ? area(i) for i ? I sample do scores[i] ? score(i, P sample , I sample , batch) end for end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>? 1 : 8 ? M ? M , ? 2 : M ? M ? M , and ? 3 : 2M ? M ? K with M = 32 and K = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure SM- 2 .</head><label>2</label><figDesc>Depth-Based Mapping Computation. Based on an input depth map (a), we compute the point-image mappings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>-3 quality score heatmaps when varying pairs of features for a given view point from S3DIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure SM- 3 .</head><label>3</label><figDesc>Influence of Viewing Descriptors. Given a pointimage pair (top left), we compute the quality scores when varying two of its viewing conditions from their initial values ?. For simplicity, we omit the influence of other images. We observe feature blocks specializing in retrieving information from views at a given depth range and containing planar objects (bottom left) or blocking straight yet occluded (top right) or sparse and occluded (bottom right) views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure SM- 4 .</head><label>4</label><figDesc>Color Legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Evaluation. Mean Intersection-over-Union of different state-of-the-art methods on S3DIS's Fold 5 and 6fold, ScanNet Val, and KITTI-360 Test. All methods except the last line are trained on colorized point clouds. State-of-the-art, second highest. 1 with 3D supervision only.</figDesc><table><row><cell>Model</cell><cell cols="2">S3DIS Fold 5 6-Fold</cell><cell>ScanNet Val</cell><cell>KITTI 360 Test</cell></row><row><cell cols="4">Methods operating on colorized point clouds</cell><cell></cell></row><row><cell>PointNet++ [58]</cell><cell>-</cell><cell cols="3">56.7 [13] 67.6 [14] 35.7 [50]</cell></row><row><cell>SPG+SSP [45, 46]</cell><cell>61.7</cell><cell>68.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MinkowskiNet [16] 65.4</cell><cell cols="3">65.9 [13] 72.4 [56] -</cell></row><row><cell>KPConv [64]</cell><cell>67.1</cell><cell>70.6</cell><cell cols="2">69.3 [56] -</cell></row><row><cell>RandLANet [35]</cell><cell>-</cell><cell>70.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PointTrans. [23]</cell><cell>70.4</cell><cell>73.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Our 3D Backbone</cell><cell>64.7</cell><cell>69.5</cell><cell>69.0</cell><cell>53.9</cell></row><row><cell cols="4">Methods operating on point clouds and images</cell><cell></cell></row><row><cell>MVPNet [40]</cell><cell>62.4</cell><cell>-</cell><cell>68.3</cell><cell>-</cell></row><row><cell>VMVF [44]</cell><cell>65.4</cell><cell>-</cell><cell>76.4</cell><cell>-</cell></row><row><cell>BPNet [36]</cell><cell>-</cell><cell>-</cell><cell>69.7 1</cell><cell>-</cell></row><row><cell>3D Backbone+ DeepViewAgg (ours)</cell><cell>67.2</cell><cell>74.7</cell><cell>71.0</cell><cell>58.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation Study. Mean IoU comparison of different modalities and design choices on Fold 2 and Fold 5 of S3DIS down-sampled at 5cm for processing and KITTI-360 Val.</figDesc><table><row><cell>Model</cell><cell cols="2">S3DIS Fold 2 Fold 5</cell><cell>KITTI 360 Val</cell></row><row><cell>Best Configuration</cell><cell>63.2</cell><cell>67.5</cell><cell>57.8</cell></row><row><cell cols="3">Modality Combinations</cell><cell></cell></row><row><cell>XYZRGB</cell><cell>-15.9</cell><cell>-6.0</cell><cell>-3.6</cell></row><row><cell>XYZ Average-RGB</cell><cell>-10.8</cell><cell>-7.0</cell><cell>-4.9</cell></row><row><cell>XYZ</cell><cell>-19.5</cell><cell>-9.5</cell><cell>-4.1</cell></row><row><cell>Pure RGB</cell><cell>-5.3</cell><cell>-5.4</cell><cell>-14.5</cell></row><row><cell>Lower Image Resolution</cell><cell>-5.9</cell><cell>-0.8</cell><cell>-0.7</cell></row><row><cell>Higher 3D Resolution</cell><cell>-1.0</cell><cell>-0.3</cell><cell>-</cell></row><row><cell cols="2">Design Choices</cell><cell></cell><cell></cell></row><row><cell>Late Fusion</cell><cell>-9.1</cell><cell>-1.0</cell><cell>-1.3</cell></row><row><cell>Only One Group</cell><cell>-4.8</cell><cell>-0.8</cell><cell>-0.4</cell></row><row><cell>No Gating</cell><cell>-3.0</cell><cell>-0.4</cell><cell>-1.1</cell></row><row><cell>No Dynamic Batch</cell><cell>-6.9</cell><cell>-1.9</cell><cell>-4.5</cell></row><row><cell>No Pre-training</cell><cell>-7.2</cell><cell>-6.7</cell><cell>-3.7</cell></row><row><cell>MaxPool</cell><cell>-0.8</cell><cell>-1.5</cell><cell>-2.9</cell></row><row><cell>Smaller 3D backbone</cell><cell>-0.5</cell><cell>-0.7</cell><cell>+0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm SM-1 Z-buffering-Based Point-Pixel MappingInput: I image set, P point cloud for i ? I do zBuffer ? maxFloat array of size i indexMap ? NaN array of size i P</figDesc><table><row><cell>for (u, v) ? mask do</cell></row><row><cell>if dist(p, i) &lt; zBuffer[u, v] then</cell></row><row><cell>zBuffer[u, v] ? dist(p, i)</cell></row><row><cell>indexMap[u, v] ? p</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>for p ? P ? do</cell></row><row><cell>if p appears in indexMap then</cell></row><row><cell>pix(p, i) ? pixel at the projection of p on i</cell></row><row><cell>else</cell></row><row><cell>pix(p, i) not defined, p not seen in i</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row></table><note>? ? points of P in frustum of i and closer than R for p ? P ? do s ? size of cubes(dist(p, i)) mask ? pixel mask covered by the projection of a cube of size s at p onto the image i</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table SM - 1 .</head><label>SM1</label><figDesc>Supplementary Ablation Study. Mean IoU comparison of different design choices on Fold 2 and Fold 5 of S3DIS down-sampled at 5cm for processing.To compare our fusion schemes, we evaluate a model with the intermediate fusion scheme described in SM-11 (Intermediate). We observe that, for our module, intermediate fusion does not perform as well as early and late fusion. This could indicate that fusing modalities at their highest respec-</figDesc><table><row><cell>Model</cell><cell cols="2">Fold 2 Fold 5</cell></row><row><cell>Best Configuration</cell><cell>63.1</cell><cell>67.5</cell></row><row><cell cols="2">Design Choices</cell><cell></cell></row><row><cell>Mapping from Depth</cell><cell>-5.4</cell><cell>-1.9</cell></row><row><cell>Intermediate</cell><cell>-7.6</cell><cell>-3.2</cell></row><row><cell cols="2">XYZRGB + DeepViewAgg -0.5</cell><cell>-0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table SM -</head><label>SM</label><figDesc>2. Effect of maximum DepthWe report the drop in mIoU when removing mappings beyond a threshold distance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table SM -</head><label>SM</label><figDesc>3. Impact of the Number of Images. We report the drop in mIoU when limiting the number of images per point cloud.</figDesc><table><row><cell># images</cell><cell>8</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell></row><row><cell cols="7">S3DIS Fold 5 0.5 1.2 1.7 2.1 3.5 6.8</cell></row><row><cell>KITTI-360</cell><cell cols="6">0.5 0.1 0.5 1.3 1.9 3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table SM -</head><label>SM</label><figDesc><ref type="bibr" target="#b3">4</ref>. Usage and Sensitivity of Viewing Conditions. Feature usage is reported as a drop in mIoU, and the sensitivity is given as the proportion of squared partial derivative of the compatibility across all features.</figDesc><table><row><cell>view feature</cell><cell cols="4">usage (mIoU drop) S3DIS KITTI-360 S3DIS KITTI-360 sensitivity (in %)</cell></row><row><cell>depth</cell><cell>1.1</cell><cell>1.7</cell><cell>12.6</cell><cell>46.0</cell></row><row><cell>linearity</cell><cell>1.0</cell><cell>0.8</cell><cell>11.9</cell><cell>0.7</cell></row><row><cell>planarity</cell><cell>1.0</cell><cell>1.4</cell><cell>15.8</cell><cell>1.9</cell></row><row><cell>scattering</cell><cell>0.7</cell><cell>1.0</cell><cell>52.7</cell><cell>0.7</cell></row><row><cell>viewing angle</cell><cell>1.3</cell><cell>1.2</cell><cell>2.8</cell><cell>7.4</cell></row><row><cell>pixel row</cell><cell>1.1</cell><cell>0.8</cell><cell>1.6</cell><cell>33.2</cell></row><row><cell>local density</cell><cell>1.2</cell><cell>1.3</cell><cell>0.6</cell><cell>1.8</cell></row><row><cell>occlusion</cell><cell>0.7</cell><cell>0.9</cell><cell>2.0</cell><cell>8.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table SM -</head><label>SM</label><figDesc>5. Classwise Performance. Classwise mIoU across all datasets for our 3D backbone network with and without learned multi-view aggregation. DeepViewAgg 57.8 93.5 77.5 89.3 53.5 47.1 55.6 18.0 44.5 91.8 71.8 40.2 87.8 30.8 39.6 26.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">S3DIS FOLD 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Avg</cell><cell cols="3">ceiling floor</cell><cell>wall</cell><cell cols="11">beam column window door chair table bookcase</cell><cell>sofa</cell><cell></cell><cell cols="2">board clutter</cell><cell></cell></row><row><cell>3D Backbone</cell><cell>64.7</cell><cell>88.6</cell><cell cols="2">97.5</cell><cell>82.1</cell><cell>0.0</cell><cell></cell><cell>16.6</cell><cell cols="2">53.8</cell><cell>66.2</cell><cell cols="2">89.1</cell><cell>78.2</cell><cell></cell><cell>73.4</cell><cell>66.0</cell><cell></cell><cell>69.6</cell><cell>59.2</cell><cell></cell></row><row><cell cols="2">+ DeepViewAgg 67.2</cell><cell>87.2</cell><cell cols="2">97.3</cell><cell>84.3</cell><cell>0.0</cell><cell></cell><cell>23.4</cell><cell cols="2">67.6</cell><cell>72.6</cell><cell cols="2">87.8</cell><cell>81.0</cell><cell></cell><cell>76.4</cell><cell>54.9</cell><cell></cell><cell>82.4</cell><cell>58.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">S3DIS 6-FOLD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D Backbone</cell><cell>69.5</cell><cell>91.2</cell><cell cols="2">90.6</cell><cell>83.0</cell><cell cols="2">59.8</cell><cell>52.3</cell><cell cols="2">63.2</cell><cell>75.7</cell><cell cols="2">63.2</cell><cell>64.0</cell><cell></cell><cell>69.0</cell><cell>72.1</cell><cell></cell><cell>60.1</cell><cell>59.2</cell><cell></cell></row><row><cell cols="2">+ DeepViewAgg 74.7</cell><cell>90.0</cell><cell cols="2">96.1</cell><cell>85.1</cell><cell cols="2">66.9</cell><cell>56.3</cell><cell cols="2">71.9</cell><cell>78.9</cell><cell cols="2">79.7</cell><cell>73.9</cell><cell></cell><cell>69.4</cell><cell>61.1</cell><cell></cell><cell>75.0</cell><cell>65.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ScanNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Avg</cell><cell>wall</cell><cell>floor</cell><cell>cabinet</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>door</cell><cell>window</cell><cell>bookshelf</cell><cell>picture</cell><cell>counter</cell><cell>desk</cell><cell>curtain</cell><cell>refrigerator</cell><cell>shower cur.</cell><cell>toilet</cell><cell>sink</cell><cell>bathtub</cell><cell>other</cell></row><row><cell>3D Backbone</cell><cell cols="2">69.0 82.7</cell><cell>94.5</cell><cell>62.2</cell><cell>77.9</cell><cell>88.9</cell><cell>80.7</cell><cell>70.3</cell><cell>61.4</cell><cell>60.2</cell><cell>80.0</cell><cell>28.4</cell><cell>60.3</cell><cell>60.8</cell><cell>70.4</cell><cell>46.2</cell><cell>67.4</cell><cell>89.9</cell><cell>62.6</cell><cell>85.3</cell><cell>51.0</cell></row><row><cell cols="2">+ DeepViewAgg 71.0</cell><cell>84.3</cell><cell>94.4</cell><cell>57.8</cell><cell>78.9</cell><cell>90.1</cell><cell>78.0</cell><cell>71.9</cell><cell>63.4</cell><cell>66.9</cell><cell>77.8</cell><cell>39.9</cell><cell>62.2</cell><cell>55.6</cell><cell>71.9</cell><cell>57.4</cell><cell>71.4</cell><cell>89.5</cell><cell>66.2</cell><cell>82.8</cell><cell>56.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">KITTI-360 Val</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Avg</cell><cell>road</cell><cell></cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell></cell><cell>fence</cell><cell>pole</cell><cell>traffic lig.</cell><cell>traffic sig.</cell><cell>vegetation</cell><cell></cell><cell>terrain</cell><cell>person</cell><cell>car</cell><cell>truck</cell><cell></cell><cell>motorcycle</cell><cell>bicycle</cell><cell></cell></row><row><cell>3D Backbone</cell><cell cols="9">54.2 90.6 74.4 84.5 45.3 42.9 52.7</cell><cell>0.5</cell><cell cols="6">38.6 87.6 70.3 26.9 87.3</cell><cell cols="2">66.0</cell><cell cols="2">28.2 17.2</cell><cell></cell></row><row><cell cols="11">+ S3DIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ceiling</cell><cell></cell><cell>floor</cell><cell></cell><cell></cell><cell>wall</cell><cell></cell><cell cols="2">beam</cell><cell></cell><cell></cell><cell cols="2">column</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">window</cell><cell></cell><cell>door</cell><cell></cell><cell></cell><cell>chair</cell><cell></cell><cell cols="2">table</cell><cell></cell><cell></cell><cell cols="2">bookcase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>sofa</cell><cell></cell><cell></cell><cell cols="2">board</cell><cell></cell><cell>clutter</cell><cell></cell><cell cols="3">unlabeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ScanNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>wall</cell><cell></cell><cell></cell><cell></cell><cell cols="2">floor</cell><cell></cell><cell cols="2">cabinet</cell><cell></cell><cell>bed</cell><cell></cell><cell></cell><cell cols="2">chair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>sofa</cell><cell></cell><cell></cell><cell></cell><cell cols="2">table</cell><cell></cell><cell cols="2">door</cell><cell></cell><cell cols="2">window</cell><cell></cell><cell cols="2">bookshelf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">picture</cell><cell></cell><cell></cell><cell cols="2">counter</cell><cell></cell><cell cols="2">desk</cell><cell></cell><cell cols="2">curtain</cell><cell></cell><cell cols="3">refrigerator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">shower curtain</cell><cell></cell><cell cols="2">toilet</cell><cell></cell><cell>sink</cell><cell></cell><cell></cell><cell cols="2">bathtub</cell><cell></cell><cell cols="3">otherfurniture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ignored</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">KITTI-360</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">road</cell><cell></cell><cell cols="2">sidewalk</cell><cell></cell><cell></cell><cell cols="2">building</cell><cell></cell><cell cols="2">wall</cell><cell></cell><cell></cell><cell cols="2">fence</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">pole</cell><cell></cell><cell cols="3">traffic light</cell><cell></cell><cell cols="2">traffic sign</cell><cell></cell><cell cols="3">vegetation</cell><cell></cell><cell cols="2">terrain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">person</cell><cell></cell><cell>car</cell><cell></cell><cell></cell><cell></cell><cell>truck</cell><cell></cell><cell></cell><cell cols="3">motorcycle</cell><cell></cell><cell cols="2">bicycle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ignored</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was funded by ENGIE Lab CRIGEN and carried on in the LASTIG research unit of Universit? Paris-Est. The authors wish to thank AI4GEO for sharing their computing resources. AI4GEO is a project funded by the French future investment program led by the Secretariat General for Investment and operated by public investment bank Bpifrance. We thank Philippe Calvez, Dmitriy Slutskiy, Marcos Gomes-Borges, Gisela Lechuga, Romain Loiseau, Vivien Sainte Fare Garnot and Ewelina Rupnik for inspiring discussions and valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-1. Interactive Visualization and Code</head><p>We release our code at https://github.com/ drprojects / DeepViewAggand our run metrics at https : / / wandb . ai / damien _ robert / DeepViewAgg -benchmark.</p><p>The provided code allows for reproduction of our experiments and inference using pretrained models.</p><p>Our repository also contains interactive visualizations as HTML files, showing different images and model predictions for spheres sampled in S3DIS, as shown in <ref type="figure">Figure SM-1</ref>. This tool makes it easier to see the additional insights brought by images than the visuals included in the paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>2021-11-05. 1</idno>
		<ptr target="https://geospatial.trimble.com/products-and-solutions/trimble-tx8" />
		<title level="m">Trimble TX8 3D Laser Scanner</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faro Focus Laser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scanners</surname></persName>
		</author>
		<idno>2021-11-05. 1</idno>
		<ptr target="https://www.faro.com/products/construction-bim/farofocus" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<idno>2021-11-05. 1</idno>
		<ptr target="https://leica-geosystems.com/products/laser-scanners/scanners/leica-rtc360" />
		<title level="m">Leica RTC360 3D Laser Scanner</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Seamless image-based texture atlases using multi-band blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>All?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Keriven</surname></persName>
		</author>
		<idno>ICPR. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D scene graph: A structure for unified semantics, 3D space, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiway cut for stereo and motion with slanted surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Snapnet: 3D point cloud semantic labeling with 2D deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformerfusion: Monocular RGB scene reconstruction using transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljaz</forename><surname>Bozic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Palafox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unstructured lumigraph rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multimodal attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03976</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rgb-d datasets using microsoft kinect or similar sensors: a survey. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Torch-points3D: A modular multi-task frameworkfor reproducible deep learning on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chaulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofiane</forename><surname>Horache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://github.com/daveredrum/Pointnet2.ScanNet" />
		<title level="m">Pointnet2.scannet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified point-based framework for 3D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yueh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">4D spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The determination of next best views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl</forename><surname>Connolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CSAILVision. semantic-segmentation-pytorch. github. com / CSAILVision / semantic -segmentationpytorch</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3DMV: Joint 3D-multiview prediction for 3D semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality based scale selection in 3D LiDAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Demantk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Vallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Laserscanning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Vasileios Belagiannis, and Klaus Dietmayer. Point transformer. ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hugues Hoppe, and Daniel Cohen-Or. Seamless montage for texturing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Ofek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Review and comparison of methods to study the contribution of variables in artificial neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muriel</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sovan</forename><surname>Lek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Modelling</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal affective analysis using hierarchical attention strategy with word-level alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep learning for 3D point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Laser scanning and digital outcrop geology in the petroleum industry: A review. Marine and Petroleum Geology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hodgetts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bidirectional projection network for cross dimension scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An information gain formulation for active volumetric 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Isler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Sabzevari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3D scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluating the quality of tls point cloud colorization. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arttu</forename><surname>Julin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Kurkela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho-Pekka</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Maksimainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antero</forename><surname>Kukko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Kaartinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><forename type="middle">T</forename><surname>Vaaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Hyypp?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Hyypp?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">RotationNet: joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fuseseg: LiDAR point cloud segmentation fusing multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sfsegnets</surname></persName>
		</author>
		<idno>2021. 14</idno>
		<ptr target="https://github.com/lxtGH/SFSegNets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Seamless mosaicing of image-based texture maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Ivanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention-based multi-modal fusion network for semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13410</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal keyless attention fusion for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Supervised learning of the next-best-view for 3D object reconstruction. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Vasquez-Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hind</forename><surname>Taud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Reta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic selection of optimal views in multi-view object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzin</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadegh</forename><surname>Abbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mix3D: Out-of-context data augmentation for 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3D point cloud model color adjustment by combining terrestrial laser scanner and close range photogrammetry datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Fregonese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiana</forename><surname>Achille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Heritage</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIICAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">View planning for automated three-dimensional object reconstruction and inspection. Computing Surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rivest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast curve and surface generation for interactive shape design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Strasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Industry</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Accurate 3D measurement using a structured light system. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Valkenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcivor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">View/state planning for three-dimensional object reconstruction under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Irving</forename><surname>Vasquez-Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Murrieta-Cid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Interactive dense point clouds in a game engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho-Pekka</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvie</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Turppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingli</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arttu</forename><surname>Julin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Hyypp?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Hyypp?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Let there be color! large-scale texturing of 3D reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Waechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Moehrle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dominant set clustering and pooling for multi-view 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A multispectral canopy LiDAR demonstrator project. Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Iain H Woodhouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">J</forename><surname>Morsdorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Malthus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patenaude</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning relationships for multiview 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">PVRNet: Point-view relation neural network for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-view harmonized bilinear network for 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Depth-based visibility</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
