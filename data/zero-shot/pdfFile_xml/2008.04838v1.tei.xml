<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransNet V2: An effective deep network architecture for fast shot transition detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Sou?ek</surname></persName>
							<email>tomas.soucek1@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering Faculty of Mathematics and Physics</orgName>
								<orgName type="laboratory">SIRET Research Group</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
							<email>lokoc@ksi.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering Faculty of Mathematics and Physics</orgName>
								<orgName type="laboratory">SIRET Research Group</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransNet V2: An effective deep network architecture for fast shot transition detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although automatic shot transition detection approaches are already investigated for more than two decades, an effective universal human-level model was not proposed yet. Even for common shot transitions like hard cuts or simple gradual changes, the potential diversity of analyzed video contents may still lead to both false hits and false dismissals. Recently, deep learning-based approaches significantly improved the accuracy of shot transition detection using 3D convolutional architectures and artificially created training data. Nevertheless, one hundred percent accuracy is still an unreachable ideal. In this paper, we share the current version of our deep network TransNet V2 that reaches state-of-the-art performance on respected benchmarks. A trained instance of the model is provided so it can be instantly utilized by the community for a highly efficient analysis of large video archives. Furthermore, the network architecture, as well as our experience with the training process, are detailed, including simple code snippets for convenient usage of the proposed model and visualization of results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Automatic identification of shot transitions in a video is a classical task of video analysis, where various traditional (even simple) algorithms provided results deemed as sufficient approximations at some popular benchmarks <ref type="bibr" target="#b10">[11]</ref>. However, the benchmarks do not cover all "open-world" aspects of video content, and so the problem cannot be considered as solved yet. For example, fast abrupt changes of the visual contents caused by camera/environment/entity can still confuse even state-of-the-art shot transition detection models and may lead to many false hits. Besides false hits, trained models can sometimes miss a difficult (e.g. long) transition, even if such type was present in the utilized train set. In addition, novel "wild" transition types supported in common video editing tools and often used in television content may find shot transition detection models "unprepared" for a given type. Hence, it is necessary to search for more advanced approaches that could push the accuracy of shot transition detectors to new levels.</p><p>Currently, end-to-end deep learning approaches have become a mainstream research direction for video analysis tasks. For transition detection, temporal context is essential, and so a detection approach requires either an aggregation of extracted features from individual frames <ref type="bibr" target="#b6">[7]</ref> or utilization of 3D convolutions that jointly process spatial and temporal information. The latter approach, popularized by Tran et al. <ref type="bibr" target="#b13">[14]</ref> for video classification, was also considered for shot boundary detection networks proposed by Hassanien et al. <ref type="bibr" target="#b5">[6]</ref> and Gygli <ref type="bibr" target="#b4">[5]</ref>. Hassanien et al. predict a likelihood of sharp or gradual transition in a 16 frame sequence by the C3D network <ref type="bibr" target="#b13">[14]</ref>. The predictions are, however, not used directly, and an SVM classifier is trained to give a labeling estimate. Further, some false positives are suppressed by color histogram differencing. Gygli, on the other hand, utilizes only predictions from a 3D convolutional network without any post-processing. However the network is much smaller and outperformed by Hassanien et al. Our previously proposed shot transition detection 3D convolutional network TransNet <ref type="bibr" target="#b7">[8]</ref> combines the end-to-end no-post-processing approach of Gygli with performance comparable to Hassanien et al. on RAI dataset <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this paper, we present a new, improved version of TransNet architecture. The new model, labeled as "TransNet V2", is provided as an open-source project with an easy-to-use trained instance and utilized training/evaluation codes (all available at https: //github.com/soCzech/TransNetV2). The current version provides promising detection accuracy and enables efficient processing of larger datasets 1 . Given three different benchmark datasets (Clip-Shots <ref type="bibr" target="#b12">[13]</ref>, BBC <ref type="bibr" target="#b1">[2]</ref>, RAI <ref type="bibr" target="#b2">[3]</ref>), we also demonstrate that the new TransNet version represents a state-of-the-art approach. We emphasize that all compared related models were re-evaluated (given available info) on the benchmarks, and the results were processed with the same evaluation script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRANSNET V2</head><p>In this section, we present our enhanced model TransNet V2 for shot detection, summarize key changes made to the architecture, and detail the training process. We also re-evaluate selected related work architectures and present the results at the end of the section. More details on the architecture and comprehensive evaluations are included in the forthcoming thesis of T. Sou?ek <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><formula xml:id="formula_0">2.1.1 Overview.</formula><p>The proposed TransNet V2 builds on basic original TransNet concepts <ref type="bibr" target="#b7">[8]</ref>, where a resized input sequence of frames is initially processed with Dilated DCNN cells. Specifically, the previously released TransNet version comprises six DDCNN cells where every cell consists of four 3 ? 3 ? 3 convolution operations, each with F filters and different dilation rates 1, 2, 4, 8 for the temporal dimension. Hence, a larger receptive field of 97 frames is reached by the final sixth TransNet's DDCNN cell while using still an acceptable number of learnable parameters. In the new version, DDCNN cells also incorporate batch normalization that stabilizes gradients and adds noise during training. Every second cell contains a skip connection followed by spatial average pooling that reduces spatial dimension by two, as illustrated, with additional improvements, in the overall schema of the shared TransNet V2 network instance in <ref type="figure" target="#fig_0">Figure 1</ref>. Due to the lack of space, only the key additional concept changes to the architecture are further detailed in the following paragraphs.</p><p>2.1.2 Convolution Kernel Factorization. Xie et al. <ref type="bibr" target="#b14">[15]</ref> show that it might be beneficial to disentangle 3D k ? k ? k convolutions into a 2D k ?k spatial convolution followed by a 1D temporal convolution with kernel size k. Such disentanglement of the 3D convolutional kernel forces separate learning of image feature extraction and temporal comparison of the inferred features. Furthermore, if a low-enough number of spatial convolution filters is used, factorized convolutions reduce the number of learnable parameters, which may prevent over-fitting to artificially generated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Frame Similarities as Features.</head><p>Handcrafted and/or learnable features provide another option to design a shot transition detector relying on similarity scores between consecutive frames. Therefore the new version of TransNet also incorporates RGB color histograms (8 3 = 512 bins) as well as learned features computed by spatially averaging activations of each average pooling and projecting them by a single dense layer. The features are processed by a similarity evaluation network node (denoted with red color in <ref type="figure" target="#fig_0">Figure 1</ref>), where the cosine similarity matrix is evaluated for collected features of processed frames. Each frame is then represented by similarity to its 50 preceding and following frames 2 . The similarity vector is further transformed with a dense layer and concatenated to other inferred features from other parts of the network. Let us note that using similarity vectors was already proposed by a traditional shot transition detection approach [4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Multiple Classification</head><p>Heads. TransNet V2 relies on two prediction heads. One head is trained to predict only a single middle frame of a transition, no matter its length. The second head predicts all transition frames. However, the only purpose of the second head is to update network weights during training to improve the network's "understanding" of what constitutes a transition and how long the transition is. More details about the prediction process are presented in Section 2.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Setup and Experience</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Train datasets.</head><p>To train TransNet V2 classification heads, it is essential to acquire a large annotated train dataset comprising both hard cuts and gradual transitions. We follow the trend of synthetic training transitions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> that can be rendered on the fly for pairs of randomly selected shots from a large reference shot collection. Specifically, the TRECVID IACC.3 <ref type="bibr" target="#b0">[1]</ref> is used as it provides the master shot reference with several hundred thousand shots. We also experimented with a larger dataset of real transitions extracted from the ClipShots dataset <ref type="bibr" target="#b12">[13]</ref> with 4039 videos, where 128636 hard cuts and 38120 gradual transitions are manually annotated. However, according to our evaluations presented in <ref type="table" target="#tab_2">Table 1</ref>, synthetically rendered transitions significantly boost performance, and so the training process uses just 15% of real ClipShots transitions <ref type="bibr" target="#b1">2</ref> Unavailable similarity values are replaced with zeros.    compared to 85% of synthetic transitions (35% hard cuts, 50% dissolves) generated from both IACC.3 and ClipShots. In addition, a significant proportion of gradual dissolve transitions in the train set seems to be important as well. Following our experience with TransNet, all videos are resized to the lower resolution 48x27.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Train sequence rendering.</head><p>For real transitions, a sequence of 100 frames containing an annotated transition is randomly selected by the training script. For the synthetic train set, we extract 300 frame segments from each reference dataset scene. The frames are extracted from the start, center, and end part of the scene while omitting some if the scene is not long enough. During training, two random segments are selected and randomly cropped to the length of 100 frames. The two segments are joined by a random transition type at a random position. Specifically, hard cuts and dissolves spanning over 2 to 30 frames are generated. Sequences without any transition are not employed for training as we assume that hard negatives are contained in already used input sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Data augmentation.</head><p>We apply standard image augmentation to all frames in a shot. In order to prevent an introduction of fake shot boundaries into an input frame sequence, every image in the sequence is augmented in the same way. Firstly, shot frames are flipped left to right with probability 0.5 and top to bottom with probability 0.1. Further, standard TensorFlow image operations adjusting saturation, contrast, brightness, and hue are applied. With probability 0.05 we also apply Equalize, Posterize and Color operations from Python image library PIL 3 . In the case of automatic transition generation, a color transfer technique is utilized on 10% of input sequences before shot joining. The color transfer simulates a similar appearance of shots in one video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Technical details.</head><p>We train both classification heads using standard cross-entropy loss averaged over batch. The positive class in the first single-frame head is weighted by a factor of 5. The second all-frame head's contribution to the loss is discounted by 0.1. L2 regularization is added to the loss weighted by 0.0001. We optimize the loss function by SGD with momentum set to 0.9 and a fixed learning rate of 0.01. We train the network for 50 epochs, each with 750 batches of size 16 (in total 600,000 transitions). The best performing model on our ClipShots validation set (a subset of the official train set) is selected. Together with validation, the training takes approximately 17 hours on a single Tesla V100 16GB GPU. TensorFlow deep learning library was used for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison To Related Work</head><p>To relate the effectiveness of TransNet models to a baseline, we identified two state-of-the-art models, mostly according to reported F1 scores on the popular RAI dataset. Specifically, DeepSBD by Hassanien et al. <ref type="bibr" target="#b5">[6]</ref> reporting F1 score 93.4% and DSM by Tang et al. <ref type="bibr" target="#b12">[13]</ref> reporting F1 score 93.5%. However, for two additional ClipShots and BBC datasets, the F1 scores are not reported by Hassanien et al., while the evaluation code of Tang et al. <ref type="bibr" target="#b3">4</ref> is inconsistent with our evaluation method 5 following Baraldi et al. <ref type="bibr" target="#b2">[3]</ref>. Therefore, we decided to re-evaluate both methods from available shared repositories and report results with a unified scoring methodology.</p><p>We emphasize that not all information was available and so the presented results do not have to correspond to the most optimal setting of related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Re-evaluation Protocol for Related Models.</head><p>Although the description of DeepSBD includes SVM and post-processing steps, the publicly available code 6 contains just the network structure. Hence, we utilize only its softmax predictions. Since the model distinguishes between cut and gradual transition types, we sum  their predicted confidence scores into a single "transition class". To generate per-frame predictions, we assign the predicted confidence score to the middle 8 frames of the 16 frame input sequence (the input video sequence is always shifted by 8 frames). The final prediction is then thresholded, and the F1 score is computed for all test sets. Multiple thresholds were investigated, and the value of 0.9 was chosen as it achieved the best overall results on the test sets. Therefore, the resulting model's performance can be overestimated.</p><p>On the other hand, it raises the bar that could be potentially surpassed by a competitor. Please also note this evaluation approach even slightly surpasses the reported performance of DeepSBD on the RAI dataset (93.4% vs. 93.9%). Hence, we are more confident that our re-evaluation may objectively correspond to the work of Hassanien et al. Similar difficulties appeared also for the DSM model <ref type="bibr" target="#b12">[13]</ref>, where the implementation of the multi-step process for shot boundary detection was not available as well. The authors pointed us to their ResNet-18 baseline, being the only publicly available code 4 . Regarding evaluations, the same approach, as in the case of DeepSBM, was taken. The best performing threshold of 0.8 was again determined on the test sets.</p><p>TransNet V2 follows the same evaluation procedure as the original TransNet. The confidence of a transition appearance is predicted for all N = 100 input frames; however, only the middle 50 predictions are considered for testing due to limited temporal context for the remaining predictions. While the original TransNet uses threshold 0.1, TransNet V2 uses a fixed threshold of 0.5. Only the single-frame head confidence scores are utilized for transition prediction even though information from the second head could potentially improve the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Shot detection and results.</head><p>Given confidence scores predicted by a network for every video frame and a threshold ? , all the longest consecutive frame sequences with confidence scores greater than ? are declared as transitions. A shot is formed by all frames between two detected transitions. In <ref type="table" target="#tab_4">Table 2</ref>, TransNet V2 is compared to selected related models on the official ClipShots test set <ref type="bibr" target="#b12">[13]</ref>, BBC Planet Earth documentary series <ref type="bibr" target="#b1">[2]</ref>, and RAI dataset <ref type="bibr" target="#b2">[3]</ref>. Our evaluations show that TransNet V2 is preferable for both ClipShots and BBC Planet Earth datasets and comparable to other models on the RAI dataset. All compared methods have lower performance on ClipShots, which is the largest collection, and so manual validation of the ground truth might be a challenge. According to our observations, some video parts remain unannotated, and some frames are incorrectly labeled as transition frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of scenes (indexed from zero, both limits inclusive):</head><p>(0, 6) (7, 16) (17, 81) (82, 137) (138, 159) (160, 186) (187, 199) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Simple Usage Interface</head><p>We provide both the full training and evaluation code available at https://github.com/soCzech/TransNetV2. We also provide the trained TransNet V2 weights and especially simple API in inference directory of the repository. Detecting shots in a video is as follows:</p><p>from transnetv2 import TransNetV2 model = TransNetV2 ( " / path / to / weights_dir / " ) video_frames , single_frame_p , all_frame_p = \ model . predict_video ( " / path / to / video . mp4 " )</p><p>We show the visualization of the model's predictions together with its list of scenes in <ref type="figure" target="#fig_1">Figure 2</ref>, both achieved by the following code: list_of_scenes = model . predictions_to_scenes ( predictions = single_frame_p ) pil_image = model . visualize_predictions ( frames = video_frames , predictions =( single_frame_p , all_frame_p ) ) Note the visualization shows predictions from both heads with green and blue colors, although only "green" predictions from the single-frame head are used for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCLUSION</head><p>This paper presents TransNet V2 -a deep network for detection of common shot transitions, which represents an important initial step of video analysis processes. The network architecture and training experience are discussed in connection with performance evaluations comparing the network to other recent deep learningbased approaches. Simple code examples illustrate an easy usage of a selected pre-trained instance of the network. We believe that the presented software component can be easily integrated into video pre-processing pipelines of various multimedia search/analytics frameworks that require information about shots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>TransNet V2 Architecture (left), DDCNN V2 cell (right top), and learnable frame similarities computation (right bottom) with visualization of Pad + Gather operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualized predictions from both classification heads with a corresponding list of scenes. The original video authored by Blender Foundation licensed under CC-BY. Sequences with no transitions shortened due to limited space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Effects of real and synthetic transitions. Mean F1 scores and standard deviations computed from 3 best epochs of 3 independent runs as measured on validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>TransNet V2 compared to related works (F1 scores 5 ).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, we employ TransNet to analyze the V3C1 collection<ref type="bibr" target="#b9">[10]</ref>, which is currently used at the Video Browser Showdown<ref type="bibr" target="#b8">[9]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pillow.readthedocs.io, re-implemented in TensorFlow at https://github.com/ tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py.<ref type="bibr" target="#b3">4</ref> Available at https://github.com/Tangshitao/ClipShots_basline.<ref type="bibr" target="#b4">5</ref> We use the same evaluation metric as Baraldi et al.<ref type="bibr" target="#b2">[3]</ref> and the original TransNet<ref type="bibr" target="#b7">[8]</ref>. However, due to minor errors in ground truth of some test sets, we also count correctly any detection that misses ground truth by at most two frames. With correct ground truth its effect compared to the original metric is minimal.<ref type="bibr" target="#b5">6</ref> Available at https://github.com/melgharib/DSBD.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This paper has been supported by Czech Science Foundation (GA?R) project 19-22071Y and GA UK project number 1310920.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating Ad-hoc and Instance Video Search, Events Detection, Video Captioning and Hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID 2017</title>
		<meeting>TRECVID 2017<address><addrLine>NIST, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Deep Siamese Network for Scene Detection in Broadcast Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia<address><addrLine>Brisbane, Australia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1199" to="1202" />
		</imprint>
	</monogr>
	<note>MM ???15)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shot and Scene Detection via Hierarchical Clustering for Re-using Broadcast Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<editor>George Azzopardi and Nicolai Petkov</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous detection of abrupt cuts and dissolves in videos using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Chasanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aristidis Likas, and Nikolaos Galatsanos</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08214</idno>
		<ptr target="http://arxiv.org/abs/1705.08214" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hefeeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03281</idno>
		<ptr target="http://arxiv.org/abs/1705.03281" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Framework for Effective Known-Item Search in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Koval?ik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Sou?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslav</forename><surname>Moravec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?emysl</forename><surname>?ech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1777" to="1785" />
		</imprint>
	</monogr>
	<note>MM ???19)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive Video Retrieval in the Age of Deep Learning -Detailed Evaluation of VBS 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lokoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Soucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leibetseder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">V3C -A Research Video Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><forename type="middle">A</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video shot boundary detection: Seven years of TRECVid activity. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special issue on Image and Video Retrieval Evaluation</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning based approaches for shot transition detection and known-item search in video</title>
		<idno>Tom?? Sou?ek. 2020</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast Video Shot Transition Localization with Deep Structured Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04234</idno>
		<ptr target="http://arxiv.org/abs/1808.04234" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features With 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
