<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMFNet: Interpretable Multimodal Fusion for Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Qu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Sany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Image X Institute</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMFNet: Interpretable Multimodal Fusion for Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing state-of-the-art point descriptor relies on structure information only, which omit the texture information. However, texture information is crucial for our humans to distinguish a scene part. Moreover, the current learning-based point descriptors are all black boxes which are unclear how the original points contribute to the final descriptor. In this paper, we propose a new multimodal fusion method to generate a point cloud registration descriptor by considering both structure and texture information. Specifically, a novel attention-fusion module is designed to extract the weighted texture information for the descriptor extraction. In addition, we propose an interpretable module to explain the original points in contributing to the final descriptor. We use the descriptor element as the loss to backpropagate to the target layer and consider the gradient as the significance of this point to the final descriptor. This paper moves one step further to explainable deep learning in the registration task. Comprehensive experiments on 3DMatch, 3DLoMatch and KITTI demonstrate that the multimodal fusion descriptor achieves state-of-theart accuracy and improve the descriptor's distinctiveness. We also demonstrate that our interpretable module in explaining the registration descriptor extraction. * Corresponding author</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual explanations</head><p>Interpretable learning has endured several developments in 2D image fields. There are mainly two categories in the area of convolution neural networks. 1) those interpret how the intermediate layers represent in the real world, and 2) those try to map back the output in the input space to visualize which parts contribute to the output.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration is a technique that aims to estimate the transformation matrix (rotation and translation) between two point clouds. This technique played a critical role in many applications, including robotics and augmented reality <ref type="bibr" target="#b13">[14]</ref>. Among the existing registration methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, descriptor-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> are an important category and achieve the state-of-the-art accuracy in the large-scale real-world datasets (e.g., 3DMatch <ref type="bibr" target="#b25">[26]</ref>). The distinctiveness of the 3D point descriptor dominates the performance of these descriptor-based registration methods.</p><p>Most of the current 3D descriptors utilize the structure information to describe the points <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. However, repeatable and ambiguous structures widely exist in point clouds, such as floor, wall, and ceiling are all planes (see <ref type="figure" target="#fig_0">Figure 1</ref> as an example). These repeatable and ambiguous structure information will largely impact the descriptor distinctiveness. Consequently, the correspondences estimated by comparing the structure-only point descriptors contain significant outliers. The existing published literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> has demonstrated this phenomenon that the feature match recall drops a lot when the inlier threshold increases to 0.2. Moreover, the current descriptors' neural networks are all black boxes. We never know how the structure information contributes to the final descriptor. Without knowing the internal mechanism of the descriptor extraction process, it is difficult to understand the reason for its failures or success from new testing datasets. This paper aims to improve the distinctiveness and unfold the black box for 3D point descriptor learning.</p><p>To improve the distinctiveness of point descriptors, we propose a new multimodal fusion method to learn the 3D point descriptor by fusing the structure information of the point cloud and the texture information of its correspond-ing image. Our motivation lies in our humans usually considering both texture and structure when we watch a scene and discriminate two parts-for example, red wall(I p ) and yellow floor(I q ) (see <ref type="figure" target="#fig_0">Figure 1</ref> as an example). Moreover, the current vision system in the intelligent agents (e.g., autonomous cars and home robotics) usually contains both point cloud sensors and image sensors. Data acquisition of both point clouds and images becomes widely affordable.</p><p>Specifically, our multimodal fusion method is an encoder and decoder architecture based on FCGF <ref type="bibr" target="#b4">[5]</ref>. Inspired by the transformer <ref type="bibr" target="#b21">[22]</ref>, a new cross attention module is developed to extract the weighted texture information for each point after the encoder module. Then, we concatenate the texture and structure information and feed them into the decoder module for the final descriptor learning.</p><p>Moreover, we move one step further to unfold the black box for the 3D descriptor learning. We designed an interpretable module, descriptor activation mapping (DAM), which interprets how the neighbour points are involved in the descriptor extraction. With the interpretable module, the descriptor internal generation process is presented to us. Our interpretable module is inspired by Grad-CAM <ref type="bibr" target="#b20">[21]</ref> but different to Grad-CAM with several critical improvements and theoretical analysis specifically for the registration task. The reasons for these improvements lie in two aspects: <ref type="bibr" target="#b0">(1)</ref> the Grad-CAM can be applied to ordinary 3D CNN but fails to sparse tensor convolution. (2) the Grad-CAM requires a class label (e.g., dog or cat) to calculate the categoryspecified gradient, but the registration task does not contain such class labels.</p><p>Specifically, our DAM introduces a novel method to calculate a heat map that consists of the significance of the input points contributing to the output in the target layer. We use the descriptor's channel value as the loss to backpropagate the gradient into the target layer, which does not require class labels. To interpret the multimodal fusion descriptor, we considers the last layer as the target layer and constructs a heat map based on the gradient addition of all the descriptor's channels.</p><p>The main contributions of this paper could be summarized as ? A novel multimodal fusion method is proposed to learn 3D point descriptors with texture and structure information. Our method will improve the descriptor's distinctiveness.</p><p>? An interpretable module is proposed to unfold the black box of the 3D point descriptor neural network. This module will interpret how the neighbour points in contributing the final descriptor.</p><p>? Comprehensive experiments demonstrate that the proposed multimodal fusion descriptor achieves the state-of-the-art performance on both indoor and outdoor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Our work builds on prior work in several domains: 3D descriptor and visual explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D descriptor</head><p>Before deep learning was prevalent in 3D computer vision, many handcrafted descriptors were proposed to utilize the structure information (e.g., edge, face, normal) to describe the points, such as FPFH <ref type="bibr" target="#b19">[20]</ref> and ESF <ref type="bibr" target="#b22">[23]</ref>. Several pieces of literature consider the texture and structure information into separate descriptors and combine them into an optimization process to solve the registration <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. For example, ColorICP <ref type="bibr" target="#b18">[19]</ref> improved the ICP by adding a 3D color objective. Recently, <ref type="bibr" target="#b24">[25]</ref> designed a hybrid descriptor by concatenating the spatial coordinates and colour moment vector.</p><p>After deep learning is introduced into the point cloud registration task, many learning descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref> designed neural networks to learn the descriptor by utilizing the neighbour structure information of one point cloud. The recent PREDATOR <ref type="bibr" target="#b11">[12]</ref> designs a transformer module for learning the point descriptor by considering the neighbour structure information of paired point clouds. Because of information fusion of paired point clouds, PREDATOR improves the descriptor's discriminative. SpinNet <ref type="bibr" target="#b0">[1]</ref> projected the point clouds into a spherical space and used spherical convolution operations to extract the structural information as the point descriptor. MS-SVConv <ref type="bibr" target="#b10">[11]</ref> designed a multi-scale framework to learn the descriptor by exploring the multi-scale structure information in describing the points.</p><p>However, these descriptors are still facing a challenge in distinctively representing the 3D points. Although handcraft features utilize colour information, the strategies are straightforward: they use it directly or concatenate it with spatial coordinates. The recent state-of-the-art descriptor has not considered the image texture information. However, the texture information is crucial in improving the descriptor distinctiveness. This paper aims to improve the distinctiveness of descriptors by integrating both texture and structure information. One example of the first category is the deep generator network (DQN) <ref type="bibr" target="#b17">[18]</ref>. DQN generates synthetic images for each neuron and reveals the features learned by each neuron in an interpretable way. <ref type="bibr" target="#b5">[6]</ref> proposed a method to build a saliency map related to the output and explain the relationship between inputs and outputs that the model learned. The complete review is beyond the scope of this paper. Please refer the survey <ref type="bibr" target="#b1">[2]</ref> for more information.</p><p>The proposed interpretable module belong to the second category. One wide-known example of the second category is Grad-CAM <ref type="bibr" target="#b20">[21]</ref>, which flows the gradient back into the convolutional layers to produce a localization map highlighting the important regions in the input for predicting the output. <ref type="bibr" target="#b23">[24]</ref> extends Grad-CAM to 3D-CNN in solving the Alzheimer's disease classification. <ref type="bibr" target="#b8">[9]</ref> extends the Grad-CAM to recognize difficult-to-manufacture drilled holes in a complex CAD geometry.</p><p>However, the existing interpretable methods are all focused on classification tasks. None of them focuses on registration tasks that are also an important branch of computer vision. Moreover, the Grad-CAM works on ordinary CNN but faces difficulty in directly applying to sparse tensor convolution. The reason is that the sparse tensor framework has no direct feature map gradient, which is critical for Grad-CAM <ref type="bibr" target="#b4">[5]</ref>. Nevertheless, sparse tensor convolution has been widely used in the point cloud registration problem. In this paper, we aim to build an interpretable method for the registration problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm: IMFNet</head><p>The overall architecture of the proposed interpretable multimodal fusion network (IMFNet) is surprisingly simple and depicted in <ref type="figure" target="#fig_9">Figure 9</ref>. It follows the standard UNet architecture with four main components: the encoder module including point encoder and image encoder, attentionfusion module, decoder module and interpretable descriptor activation mapping (DAM) module.</p><p>IMFNet is implemented in Minkowski Engine <ref type="bibr" target="#b3">[4]</ref> and PyTorch that provides the sparse tensor convolution and common CNN backbone architecture implementation with just a few hundred lines. The new attention-fusion module can be implemented within 50 lines. We hope that the simplicity of our method will attract more researchers to utilize the multimodal information and develop interpretable learning algorithms on the registration problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>The point Encoder follows the FCGF <ref type="bibr" target="#b4">[5]</ref> to use four sparse tensor convolution layers. The input is P ? R M1?3 and the output is F pe ? R M4?C4 . The image decoder is a pre-trained ResNet34. The input is I ? R W ?H?3 and the output is the feature of the second stage F ie ? R Mi?Ci . In our algorithm, we consider the whole pixels as one dimension M i = H/8 * W/8, which is similar to point dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention-Fusion</head><p>The goal of attention-fusion module is to extract the texture information for each point to increase the descriptors' distinctiveness. Inspired by the Transformer <ref type="bibr" target="#b21">[22]</ref>, our attention-fusion module follows the cross-attention style. The input is F pe ? R M4?C4 with rich structural information and F ie ? R Mi?Ci with texture information. M 4 is the number of abstract points of the point encoder, and M i is the number of abstract pixels of the image encoder. The output of attention-fusion is F f e ? R M4?C4 , which fuses weighted texture information and structure information for each abstract point.</p><p>Specifically, the F pe and F ie firstly go through a onelayer MLP. Then, the output of F pe is considered as the query array Q ? R M4?Ct , the output of F ie is regarded as key array K ? R Mi?Ct and value array V ? R Mi?Ct . C t is the MLP output dimension. As shown in <ref type="figure" target="#fig_9">Figure 9</ref>, the W ? R M4?Mi = sof tmax( QK T ? Ct ) is the weight matrix that represents the weight of each pixel' texture information that could contribute to describing each point. Then, the F I ? R M4?C4 is the point texture feature that is calculated with one-layer MLP. Mathematically, the point texture feature F I could be calculated,</p><formula xml:id="formula_0">F I = M LP (W * V )<label>(1)</label></formula><p>Finally, we conduct an element-wise addition between the point texture feature F I and the spatial structure feature F pe to fuse multimodal information. Mathematically, the fused encoder feature (F f e ) is calculated as</p><formula xml:id="formula_1">F ij f e = F ij pe + F I ij , ?i ? [1, M 4 ], ?j ? [1, C 4 ]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>Following FCGF <ref type="bibr" target="#b4">[5]</ref>, we use four sparse tensor transpose convolution to decode the point descriptors. The key difference is that the input of the decoder module is the fused feature of texture and structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Descriptor activation mapping (DAM)</head><p>We propose a descriptor activation mapping (DAM) to visually interpret how the neighbour points in contributing the above final descriptor extraction. The main idea of the proposed DAM is to utilize the descriptor's channel value as the loss to backpropagate into the target layer. The motivation is to use the gradient to investigate the significance of input points in generating the descriptor's channel value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1</head><p>The feature map gradient is linearly related to the kernel gradient.</p><p>The lemma proof is attached in the supplement material.</p><p>In the Grad-CAM <ref type="bibr" target="#b20">[21]</ref>, the class activation mapping is calculated by the addition of feature map gradient at channel dimension. Our method is based on the sparse tensor framework, where only kernel gradient is available. It is not easy to directly calculate the feature map gradient. According to Lemma 1, the feature map gradient has a linear relation with the kernel gradient. In the proposed DAM, we introduce a method to calculate the activation mapping for the 3D point descriptor with only kernel gradient available. <ref type="figure" target="#fig_0">Figure 10</ref> shows the calculation process. Firstly, the point descriptor F ? R M ?C is calculated by running a forward step of the descriptor network. Secondly, we consider the descriptor's each dimension as a loss, and the loss is backpropagated from the descriptor to the last layer (target layer). After the backpropagation, we can obtain kernel gradient of i th descriptor element G ? R S?Cin?C in the sparse tensor framework as</p><formula xml:id="formula_2">G = ?d i ?? ? (3)</formula><p>where ?di ?? is the gradient of i th descriptor element d i related to kernel parameter ?, which can be obtained from sparse tensors' automatic back-propagation operation. S represents the size of the convolution kernel, C in represents the size of the input channel, C represents the size of the output channel, ? represents a marker function, 1 if d i &gt; 0, and -1 otherwise.</p><p>Thirdly, the channel weight of i th descriptor element x ? R 1?C is calculated by adding up the kernel gradient G along the convolution kernel dimension S and input channel dimension C in .</p><formula xml:id="formula_3">x = Cin j=1 S i=1 G ijk , ?k ? [1, C],<label>(4)</label></formula><p>The channel weight x describes the significance of each channel on the output feature map of the target layer. Fourthly, the feature significance map of the target layer is obtained by multiplying the weights with the feature map. The feature significance map represents the significance of each feature channel on the output feature map of the target layer contributing to the final descriptor value. Then, the descriptor activation mapping from i th descriptor element is calculated by summing up the feature significance map along channel dimension. Mathematically,</p><formula xml:id="formula_4">dam i = 1 C C i=1 (F * x) i<label>(5)</label></formula><p>The dam i ? R M ?1 describes the contribution of input points to the value of i th descriptor element.  Finally, the descriptor activation map is calculated by summing dam i from all the C output descriptor elements.</p><formula xml:id="formula_5">DAM ? R M ?1 = Relu( C i=1 dam i )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed algorithm is trained using the same loss and parameters of FCGF <ref type="bibr" target="#b4">[5]</ref>. Then, following FCGF <ref type="bibr" target="#b4">[5]</ref> and SpinNet <ref type="bibr" target="#b0">[1]</ref>, we evaluate our IMFNet on the indoor 3DMatch <ref type="bibr" target="#b25">[26]</ref> and outdoor KITTI <ref type="bibr" target="#b7">[8]</ref> datasets. We also evaluate on 3DLoMatch <ref type="bibr" target="#b11">[12]</ref> that contains low overlap pairs between 10% ? 30%. Regarding the 3DMatch and 3DLo-Match, we manually inspect and select the images for each point cloud to construct a dataset of paired images and point clouds named 3DImageMatch. Our experiments are conducted on this dataset. The dataset construction and training details are attached in the supplement material.</p><p>Ground truth. Given pair of fragments P and Q, following FCGF <ref type="bibr" target="#b4">[5]</ref> and SpinNet <ref type="bibr" target="#b0">[1]</ref>, we randomly select 5000 anchor points from the overlapping region of P, and then apply the ground truth transformation T = {R,t} to determine the corresponding point in Q fragment. The descriptor is evaluated using these ground-truth correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on 3DMatch</head><p>The 3DMatch <ref type="bibr" target="#b25">[26]</ref> is a well-known indoor registration dataset of 62 scenes captured by RGBD sensor. Following the experimental setting of FCGF and SpinNet, we train the network and evaluate the descriptor's performance. <ref type="table">Table 1</ref> shows the feature match recall (FMR) comparison with the current state-of-the-art methods. The results show that the proposed IMFNet obtains state-of-the-art accuracy. Notably, our IMFNet achieves 91.6% (?1.7%) in inlier threshold ? 2 = 0.2, which shows that multimodal fusion can reduce the outliers and improve the descriptors' distinctiveness. <ref type="figure" target="#fig_5">Figure 5</ref> visually demonstrates the better performance than the recent state-of-the-art descriptors. Evaluation on different error thresholds. Following the experimental setting of FCGF <ref type="bibr" target="#b4">[5]</ref>, <ref type="figure" target="#fig_0">Figure 11</ref> shows the accuracy comparison on different error thresholds (? 1 and ? 2 ). As shown in <ref type="figure" target="#fig_0">Figure 11</ref>, the proposed IMFNet consistently outperforms all other methods across different error thresholds. This experiment demonstrates that multimodal fusion improves the descriptor's distinctiveness and achieves state-of-the-art accuracy among different accuracy requirements. Looking at <ref type="figure" target="#fig_0">Figure 11-(b)</ref>, it is worth noting that FMR scores of our IMFNet are significantly higher than those of other methods when a high matching rate (e.g., ? 2 = 20%) is required.  Evaluation on a different number of sampled points. We further evaluated the performance of IMFNet with a different number of sampled points on 3DMatch. <ref type="table">Table 2</ref> illustrates that the proposed IMFNet achieves state-of-the-art accuracy across the different number of sampled points. Particularly, our IMFNet achieves &gt; 97% feature match recall (FMR) across the different number of sampled points and even 3.2% accuracy improvement on 250 sampling points. This result demonstrates that the proposed method achieves high robustness to the number of sampled points.</p><p>Computation efficiency comparison. We compared the single descriptors' extraction speed with the recent FCGF <ref type="bibr" target="#b4">[5]</ref>, PREDATOR <ref type="bibr" target="#b11">[12]</ref>, SpinNet <ref type="bibr" target="#b0">[1]</ref> and MS-SVConv <ref type="bibr" target="#b10">[11]</ref> on the 3DMatch testing dataset. <ref type="table">Table 3</ref> shows that our method obtains comparable efficiency to FCGF and is faster than the recent PREDATOR, SpinNet and MS-SVConv. Specifically, the proposed method obtains &gt; 440 times more speed improvement than SpinNet. This is because the spherical projection is very slow, while this step is indispensable for spherical convolution. Compared to the PREDATOR, the proposed IMFNet achieves &gt; 5 times speed improvements. The reason is that its transformer module contains both self and cross attention and requires a pair of point clouds for each point descriptor extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All (s) Time (s) FCGF <ref type="bibr" target="#b4">[5]</ref> 25.06 0.06 PREDATOR <ref type="bibr" target="#b11">[12]</ref> 762.58 0.47 SpinNet <ref type="bibr" target="#b0">[1]</ref> 17155.5 39.62 MS-SVConv <ref type="bibr" target="#b10">[11]</ref> 41.23 0.10 Ours 39.98 0.09 <ref type="table">Table 3</ref>. Running speed comparison on 3DMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on 3DLoMatch.</head><p>We also compare the performance on 3DLoMatch <ref type="bibr" target="#b11">[12]</ref> with the recent descriptors FCGF <ref type="bibr" target="#b4">[5]</ref>, PREDATOR <ref type="bibr" target="#b11">[12]</ref>, SpinNet <ref type="bibr" target="#b0">[1]</ref> and MS-SVConv <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="table">Table 4</ref>, the proposed IMFNet also achieves the state-of-the-art feature match recall at the registration dataset with low-overlap point clouds. Particularly, our method achieves 80.6% (? 2%) at the ? 2 = 0.05 and 49.8% (?4%) accuracy at the ? 2 = 0.2. Although we achieve best accuracy, this experiment also shows that the low-overlap problem is still a challenge for correspondence-based registration methods.  <ref type="table">Table 4</ref>. Feature match recall (FMR) on 3DLoMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on KITTI</head><p>KITTI <ref type="bibr" target="#b7">[8]</ref> is well-known outdoor dataset captured by 3D LiDAR sensor. The first 11 sequences (0-10) of KITTI's odometry dataset is always used for point cloud registration evaluation because they have point cloud sequences and pose information. Following FCGF <ref type="bibr" target="#b4">[5]</ref>, we use the first 6 sequences for training(0-5), 2 sequences for verification(6,7), and 3 sequences for testing <ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref>. Following FCGF <ref type="bibr" target="#b4">[5]</ref>, the same evaluation metrics are utilized to evaluate the relative translational error (RTE), relative rotation error (RRE), and success rate. We compare the performance of our IMFNet with the recent published descriptors FCGF <ref type="bibr" target="#b4">[5]</ref>, D3Feat <ref type="bibr" target="#b2">[3]</ref>, PREDATOR <ref type="bibr" target="#b11">[12]</ref> and SpinNet <ref type="bibr" target="#b0">[1]</ref>.</p><p>The KITTI dataset provides both point clouds and images of the scene. However, the point clouds are 360 ? while the images only have the front view of their corresponding point clouds. The proposed IMFNet is trained and tested using the point cloud and its corresponding image. We conducted the experiments on KITTI data to demonstrate that  the proposed IMFNet can improve the distinctiveness when only partial texture information is available. The lower RTE and RRE in <ref type="table">Table 5</ref> shows that the proposed IMFNet can improve the distinctiveness of point descriptors when partial texture information is available. Also, the better success rate than FCGF indicates that adding partial texture information can improve the registration performance. <ref type="figure" target="#fig_6">Figure 6</ref> visually compare the registration results on the KITTI dataset. From 3DMatch to KITTI We also performed a crossdataset evaluation on the KITTI dataset to test the generalization capability of the proposed IMFNet. We trained all the methods on 3DMatch and tested them on KITTI with the same experimental setting (e.g., RANSAC iterates 50K times). As shown in <ref type="table" target="#tab_5">Table 6</ref>, our IMFNet obtains better performance than the compared methods. We also increased the RANSAC iteration time from 50K to 400K, and the proposed algorithm achieves 99.46% registration recall. This experiment shows that multimodal fusion is a promising way to achieve a high generalization ability in cross domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>During the descriptor extraction, the critical contribution of the proposed IMFNet is the attention-fusion module. This section reported several ablation studies we have already done on this module. The ablation study is performed on the 3DMatch dataset, and the ? 1 = 0.1(m) is set for all  the ablation studies.</p><p>With/without the attention-fusion module. Firstly, we removed the attention-Fusion module of the proposed IMFNet and extracted 3D descriptors using only structural information. After we remove the attention-fusion module, the architecture is the FCGF <ref type="bibr" target="#b4">[5]</ref>. <ref type="table">Table 7</ref> shows the feature match recall (FMR) comparison. This ablation study demonstrates that the fusion of texture information will significantly improve the feature match accuracy with a large margin. Notably, the attention-fusion module improved 24.2% on feature match recall when the inlier threshold was set to 0.2. This ablation study demonstrates the importance of texture information in improving the descriptors' distinctiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Fusion</head><formula xml:id="formula_6">? 2 (0.05) ? 2 (0.2) with (w)</formula><p>98.6 91.6 without (wo) 95.2 67.4 <ref type="table">Table 7</ref>. Ablation study of w/wo attention-fusion module.</p><p>Single/multiple attention-fusion modules. In the proposed IMFNet algorithm, we only use one attention-fusion module between encoder and decoder. We also added the attention-fusion module behind each upsampling layer of the decoder as additional information (three in total). This architecture could integrate hierarchical fusion between texture and structure information. The experimental results of <ref type="table">Table 8</ref> show that there is no significant performance increase when we consider three attention-fusion modules. However, both the GPU memory consumption and computation time largely increase. The reason is that the features obtained by the large receptive field in the last encoder layer contain enough structural information. Texture and structure fusion on these features is the most appropriate choice.  <ref type="table">Table 8</ref>. Ablation study of attention-fusion modules.</p><p>Attention-fusion module design. Our attention-fusion contains one cross-attention (CA) layer. Following the concept of Transformer architecture <ref type="bibr" target="#b21">[22]</ref>, we also considered adding a certain number of self-attention (SA) layers after the cross-attention layer. <ref type="table">Table 9</ref> shows that the model with one-layer self-attention achieves the best accuracy. The reason is that the attention-fusion module aims to conduct feature fusion rather than feature enhancement, and multiple self-attention layers may confuse the matching relationship between structure information and texture information.  <ref type="table">Table 9</ref>. Ablation study of different self-attention (SA) and crossattention (CA) layers for the attention-fusion module design.</p><p>Different setting of query(Q), key(K) and value(V). We also evaluated the different settings of Q and K for our attention-fusion module. As shown in <ref type="table">Table 10</ref>, the model that takes point cloud features as query K and value V and image features as query Q achieves relatively poor feature match recall (FMR). The reason is that the output of attention-fusion will keep enhanced point feature if we consider point cloud features as K and V . Therefore, the attention-fusion module performs a feature enhancement instead of texture and structure fusion. Moreover, the final output features only retain structural information without texture information fusion.  <ref type="table">Table 10</ref>. Ablation study of different QKV options. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Interpretable results and analysis</head><p>We conduct experiments to demonstrate the effectiveness of the proposed interpretable module. Three points P , Q1 and Q2 are selected based on descriptor search. P and Q1 is matched pair, and P and Q2 are non-matched pair. P is from the one point cloud PC1, and Q1 and Q2 are from the matched point cloud PC2. The last decoder layer is selected as the target layer. Then, we run the proposed interpretable method to interpret the proposed descriptor IMFNet, and the most related FCGF [5] on these points. <ref type="figure" target="#fig_7">Figure 7</ref> shows that descriptors of the FCGF and IMFNet have similar heat maps on the matched points (P and Q1). In contrast, the heat maps of non-matched points (P and Q2) show a significant difference. These results demonstrate that the proposed interpretable module can robustly generate a heat map for descriptors at different point clouds.</p><p>Moreover, looking deep into the heat maps, our IMFNet selects relatively consistent regions to describe the selected matched points and get the similar descriptors. However, the heat maps of FCGF have variants in the matched points. Our better accuracy implies that finding the consistent important regions for the matched points is an important indicator for the descriptor's distinctiveness. More interpretable results can be found in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a simple and effective 3D descriptor by fusing structure and texture information. The experiments demonstrate that our descriptor achieves stateof-the-art accuracy and high efficiency on indoor, outdoor and low-overlap datasets. For the first time, we develop a method to move a step further in unfolding the black-box for the 3D registration tasks. The proposed interpretable module can be used to interpret neighbour points in contributing the descriptor and analyze the descriptor ability.</p><p>In this supplementary material, we provide additional ablation study on Kitti (Sec. 1), a detailed 3DImageMatch fabrication process (Sec. 2), data preprocessing (Sec. 3), and model training details (Sec. 4). We further provide our detail IMFNet network framework(Sec. 5) and prove the Lemma 1 (Sec. 6). Finally, we show some registration visualization(Sec. 7) and interpretable visualizations(Sec. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Ablation study on KITTI</head><p>To evaluate the generalization ability of the attentionfusion module, we also conduct ablation studies on the KITTI dataset by using trained model of 3DMatch.</p><p>With/without the attention-fusion module.  <ref type="table" target="#tab_9">Table 11</ref>. Ablation study of w/wo attention-fusion module on KITTI dataset.</p><p>Single/multiple attention-fusion modules. In table 12, the single attention-fusion module achieves the better accuracy. The reason is that the points obtained by the large receptive field in the last encoder layer contain enough structural information. Texture and structure fusion on these points is the most appropriate choice.  <ref type="table">Table 13</ref>. Ablation study of different self-attention (SA) and crossattention (CA) layers for the attention-fusion module design on KITTI dataset.</p><p>Attention-fusion module design. <ref type="table">Table 13</ref> show that the one-layer attention-fusion design achieves best success rate, which demonstrates it best generalization ability in this design. As discussed in section 4.4 of the main manuscript, increasing the number of layers of attention-fusion may confuse the matching relationship between structure information and texture information.</p><p>Different setting of query(Q), key(K) and value(V). <ref type="table" target="#tab_12">Table 14</ref> shows that setting the point cloud as query achieves the best accuracy. In addition, the generalization ability of the setting point cloud as key and value matrix achieves only slightly higher than that of the model without attention-fusion module. The reason is that the attentionfusion module performs a feature enhancement when we setting as key and value, instead of texture and structure fusion. Therefore, the final output descriptor only retains structural information without texture information fusion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">3DImageMatch</head><p>We consider that if we want to extract descriptors by fusing the structural information and texture information of points, we need to have a dataset of point cloud and image pairs depicting the same scene. However, there is no such dataset that contains paired point cloud and image depicting the same scene. In this paper, we construct a dataset based on 3DMatch, named as 3DImageMatch, that consists of paired point cloud and image describing the same scene. <ref type="figure" target="#fig_8">Figure 8</ref> shows several examples.</p><p>Tranining set. In the 3DMatch training dataset, each point cloud is generated by fusing 50 depth images. To get the corresponding image of a point cloud, we need to select an image from the 50 RGB images corresponding to the 50 depth images. Since these 50 images contain slight movement, we manually select the image that has the most similar content with the image projected by the point cloud according to the Z axis. Formally,</p><formula xml:id="formula_7">C = ? ? f x 0 c x 0 f y c y 0 0 1 ? ? u = f x X Z + C x , v = f y Y Z + C y<label>(7)</label></formula><p>where C is the intrinsic matrix of camera, (X, Y, Z) are the coordinates of a 3D point, (u, v) are the coordinates of the projection point in pixels, (cx, cy) is a principal point that is usually at the image center, f x and f y are the focal lengths of the camera in the X and Y directions, respectively. Test Set. In 3DMatch's test set, we find that the point clouds are generated from depth images as the same as those in trainning set excepting the following three scenes: 7 ? scenes ? eedkitchen sun3d ? home at ? home at scan1 2013 jan 1 Sun3d ? home md ? home md scan9 2012 sep 30</p><p>In the above three scenarios, each point cloud is generated by fusing the first 50 depth images every 100 depth images (skip the next 50 depth images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Details of model training</head><p>Since our IMFNet framework is based on FCGF, most parameters of IMFNet can refer to FCGF. We trained 200 epochs on both 3DMatch and KITTI. All of our models trained are based on batch size = 2. The ResNet34 model in IMFNet is pre-trained and the image input size is (120, 160) on 3DMatch and (160, 160) on KITTI. In attention-fusioon, we set the size of the C t to be half of the dimension of the input point cloud ,i.e. <ref type="bibr">M4</ref> 2 , and in the cross-attention calculation, we set only one head. We will release our code for these details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Detailed network framework</head><p>The details of our IMFNet network framework are shown in <ref type="figure" target="#fig_9">Figure 9</ref>. In this framework, the MEConv, MEBN, MEResBlock, MEReLU and MEcat refer to FCGF.   <ref type="figure" target="#fig_0">figure 10</ref>, which shows that our attention-fusion module is simple and can be implemented easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Proof of Lemma 1</head><p>Define the input feature map of a target layer as A ? R M ?Cin , the output feature map as Z ? R M ?Cout , the Kernal as K ? R Cin?Cout , and the kernal size as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b0">1]</ref>. We perform an element-wise addition for the above n 3 output feature maps if kernel size is [n, n, n]. In forward prop-agation:</p><formula xml:id="formula_8">A ? K = Z Z ij = Cin n=1</formula><p>A in K nj , ?i ? M, ?j ? C out <ref type="bibr" target="#b7">(8)</ref> where C in represents the input dimension of the target layer, C out represents the output dimension of the target layer, and M represents the number of elements. Let is the loss that we want to propagate backwards. Then, Kernel gradient was calculated by backward propagation as:</p><formula xml:id="formula_9">? ?K ij = ? ?Z ?Z ?K ij<label>(9)</label></formula><p>due to the:</p><formula xml:id="formula_10">Z ij = Cin n=1 A in K nj , ?i ? M, ?j ? C out ?Z nj ?K ij = A nj , ?n ? M<label>(10)</label></formula><p>so:</p><formula xml:id="formula_11">? ?K ij = ? ?Z 1j ?Z 1j ?K ij + ... + ? ?Z M j ?Z M j ?K ij ? ?K ij = M n=1 ? ?Z nj A nj , ?j ? C out<label>(11)</label></formula><p>The above equation 11 shows that the kernel gradient ? ?Kij is only related to the same j column (channel dimension) of output feature map gradient ? ?Znj . Because A nj is scalar, equation 11 means that kernel gradient is linearly related to the output feature map gradient in the same channel. Therefore, the Lemma 1 has been proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Interpretable visualization</head><p>In this section, we show more interpretable results for both FCGF (figure 11) and our IMFNet ( <ref type="figure" target="#fig_0">figure 12</ref>). Inside each heat map, the black point region is generated by using KNN to find the nearest 10 neighbor points around the target point. Both <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref> illustrate that heat maps of matched points are similar while different in the nonmatched points. Compared with the heat maps of FCGF descriptor, our IMFNet shows more consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">3DMatch/3DLoMatch visualization</head><p>In this section, we generate more visualization examples for our IMFNet to show its registration ability. <ref type="figure" target="#fig_0">Figure  13</ref> shows the visual examples on 3Dmatch, and <ref type="figure" target="#fig_0">Figure 14</ref> shows the visual examples on 3DLoMatch.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A visual example to show that descriptors of nonmatched points p and q are similar if considering the structure only (top). However, descriptors are discriminative when considering structure and texture information (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The network architecture of the proposed IMFNet. The input is a point cloud and an image, and the output is a point descriptor. Inside the attention-fusion module, W is the weight matrix, F I is the point texture feature. Then, the point structure feature (Fpe) and point texture feature (F I) are concatenated as an input to the decoder module to get the output descriptor. Descriptor activation mapping (DAM) interprets how the neighbour points in contributing the final descriptor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Diagram of the DAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Inlier Distance Threshold (b) Inlier Ratio Threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of IMFNet with other state-of-the-art method on 3DMatch under different inlier distance threshold ?1(a) and different inlier ratio threshold ?2(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison on 3DMatch dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visual comparison on KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Interpretable results to show the point significance in generating the final descriptor. The heat maps interpret the descriptors of black points inside the red circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Some visual examples of the 3DImageMatch. The first row shows the point clouds. The second row shows projection results from point clouds to images. The third column shows the images that corresponding to the point clouds in the first row. It can be seen that the selected images are almost identical with the projected results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Detailed network framework of the proposed IMFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>The details of attention-fusion module. ? stands for matrix multiplication. ? means the element multiplication, and ? means the element addition. Here, MEConv, MEBN, MEResBlock, MEReLU, MEcat means MinkowskiConvolution, MinkowskiBatchNorm, BasicBlockIN, relu activation function in Minkowski engine, concatenation in Minkowski engine. For the stage of ImageEncoder, see ResNet34 [?]. The details of the attention-fusion module is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Interpretable results of FCGF. Points p and q are matched, and p and q' are non-matched.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Interpretable results of our IMFNet. Points p and q are matched, and p and q' are non-matched.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Some visualization results of IMFNet on 3DMatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Some visualization results of IMFNet on 3DLoMatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The performance from 3DMatch to KITTI.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="6">shows that registration recall is improved &gt; 13% if adding</cell></row><row><cell cols="6">the attention-fusion module. This demonstrates that the</cell></row><row><cell cols="6">texture information can largely improve the generalization</cell></row><row><cell>ability.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attention Fusion</cell><cell>Avg</cell><cell>RTE(cm) std</cell><cell>Avg</cell><cell>RRE(?) std</cell><cell>Success(%)</cell></row><row><cell>With (w)</cell><cell>21.9</cell><cell>3.10</cell><cell>1.94</cell><cell>0.30</cell><cell>85.59</cell></row><row><cell>Without (wo)</cell><cell>24.5</cell><cell>3.79</cell><cell>2.19</cell><cell>0.33</cell><cell>72.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>Ablation study of different QKV options on KITTI dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Data preprocessing 3DMatch. To demonstrate the value of texture information, our experiments are conducted on 3DImageMatch by following the same processing of 3DMatch. We used 54 scenarios as training sets, 6 scenarios as validation sets, and 8 scenarios as test sets. We use TSDF volumetric fusion to synthesize 50 depth images into a point cloud, and apply certain downsampling on all point clouds.KITTI. Following the setting of FCGF, we used the first 10 scenarios for model evaluation and training, among which 0-5 sequences were used for training, 6 and 7 sequences were used for verification, and 8-10 sequences were used for testing.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spinnet: Learning a general surface descriptor for 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>D?az-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Del</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gil-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="82" to="115" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D3feat: Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8958" to="8966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6970" to="6979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soumik Sarkar, and Adarsh Krishnamurthy. Learning localized features in 3d cad models for manufacturability analysis of drilled holes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sambit</forename><surname>Ghadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Balu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5545" to="5554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">3d point cloud registration with multi-scale architecture and self-supervised fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofiane</forename><surname>Horache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14533</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predator: Registration of 3d point clouds with low overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Featuremetric registration: A fast semi-supervised approach for robust point cloud registration without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11366" to="11374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02690</idno>
		<title level="m">Jian Zhang, and Rana Abbas. A comprehensive survey on point cloud registration</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end learning local multi-view descriptors for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Color-aware surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Quan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3387" to="3395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Colored point cloud registration revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE international conference on robotics and automation</title>
		<imprint>
			<biblScope unit="page" from="3212" to="3217" />
			<date type="published" when="2005" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of shape functions for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Wohlkinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on robotics and biomimetics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2987" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual explanations from deep 3d convolutional neural networks for alzheimer&apos;s disease classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">1571</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color point cloud registration based on supervoxel correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weile</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyi</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7362" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
