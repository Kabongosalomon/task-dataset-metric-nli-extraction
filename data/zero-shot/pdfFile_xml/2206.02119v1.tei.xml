<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multimodal Corpus for Emotion Recognition in Sarcasm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupama</forename><surname>Ray</surname></persName>
							<email>anupamar@in.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">IBM Research India</orgName>
								<orgName type="institution" key="instit2">IIT Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Mishra</surname></persName>
							<email>shubham101mishra@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">IBM Research India</orgName>
								<orgName type="institution" key="instit2">IIT Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorva</forename><surname>Nunna</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">IBM Research India</orgName>
								<orgName type="institution" key="instit2">IIT Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">IBM Research India</orgName>
								<orgName type="institution" key="instit2">IIT Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multimodal Corpus for Emotion Recognition in Sarcasm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Emotion understanding</term>
					<term>sarcasm</term>
					<term>multimodal</term>
					<term>valence-arousal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While sentiment and emotion analysis have been studied extensively, the relationship between sarcasm and emotion has largely remained unexplored. A sarcastic expression may have a variety of underlying emotions. For example, "I love being ignored" belies sadness, while "my mobile is fabulous with a battery backup of only 15 minutes!" expresses frustration. Detecting the emotion behind a sarcastic expression is non-trivial yet an important task. We undertake the task of detecting the emotion in a sarcastic statement, which to the best of our knowledge, is hitherto unexplored. We start with the recently released multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. We identify and correct 343 incorrect emotion labels (out of 690). We double the size of the dataset, label it with emotions along with valence and arousal which are important indicators of emotional intensity. Finally, we label each sarcastic utterance with one of the four sarcasm types-Propositional, Embedded, Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection research. Exhaustive experimentation with multimodal (text, audio, and video) fusion models establishes a benchmark for exact emotion recognition in sarcasm and outperforms the state-of-art sarcasm detection. We release the dataset enriched with various annotations and the code for research purposes: https://github.com/apoorva-nunna/MUStARD_Plus_Plus</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Emotion understanding leads to a deeper insight into the intent of the speaker and is key to generating the right response in conversational systems. Detecting emotions and sarcasm is crucial for all services involving human interactions, such as chatbots, e-commerce, e-tourism, and several other businesses. To be able to understand the user's intent, we started with the research problem on understanding the emotions that lead to the usage of sarcasm in a conversation. Sarcasm is a very sophisticated linguistic articulation where the surface meaning often stands in contrast to the underlying deeper meaning. While this incongruity is the key element of sarcasm, the intent could be to appear humorous, ridicule someone, or to express contempt. Thus sarcasm is considered a very nuanced or intelligent language construct that poses several challenges to emotion recognition; for example, perceived emotion could be completely flipped due to the presence of sarcasm. Sarcasm often relies on verbal and non-verbal cues (pitch, tone, emphasis in speech, and body language in video). Even for humans, understanding the underlying emotion is challenging without the audio/video or the context of the conversation. However, researchers have worked on sarcasm detection on text modality with textual datasets (such as tweets <ref type="bibr" target="#b20">(Oprea and Magdy, 2020)</ref>, Reddit short texts <ref type="bibr" target="#b13">(Khodak et al., 2017)</ref>, dialogue <ref type="bibr" target="#b21">(Oraby et al., 2016)</ref> etc) for a decade. Recently we have seen multimodal datasets in the space of sarcasm detection, for example, image data from Twitter <ref type="bibr" target="#b2">(Cai et al., 2019)</ref>, code-mixed sarcasm and humor detection dataset <ref type="bibr" target="#b0">(Bedi et al., 2021)</ref>. <ref type="bibr" target="#b33">(Castro et al., 2019)</ref> released a video dataset for sarcasm detection called MUStARD with 345 sarcastic videos and 345 non-sarcastic videos. <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> annotated MUStARD data with 9 emotion labels and sentiment (all sarcastic utterances having negative sentiment) and used emotion and sentiment to improve sarcasm detection. We started with this emotion-labeled variant of MUS-tARD provided by <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> to build a multiclass emotion recognizer on sarcastic utterances and observed several labeling errors while performing error analysis. During the annotation effort, we doubled the dataset by adding new utterances from similar sitcom genre series as in MUStARD while maintaining 50% sarcastic and 50% non-sarcastic videos. The affective dimensions of valence and arousal are commonly studied in the psychological and cognitive exploration of emotion <ref type="bibr" target="#b19">(Mohammad, 2021)</ref> and help in better understanding of emotion category and intensity. Thus the entire dataset is annotated with arousal and valence along with the perceived emotion of the speaker. While valence indicates the extent to which the emotion is positive or negative, arousal measures the intensity of the emotion associated <ref type="bibr" target="#b5">(Cowie and Cornelius, 2003)</ref>. Finally, we also add sarcasm type as metadata which would help advance sarcasm detection research as well as give an understanding of what kind of information/modality is required to improve sarcasm detection. The four types of sarcasm are: Propositional, Embedded, Like-Prefixed and Illocutionary <ref type="bibr" target="#b3">(Camp, 2012)</ref>. Propositional sarcasm needs context information to be able to detect whether it's sarcasm or not. For example: "your plan sounds fantastic!" may seem non-sarcastic if the context information is not present <ref type="bibr">(Zvolenszky, 2012)</ref>. Embedded sarcasm has an embedded incongruity within the utterance; thus, the text itself is sufficient to detect sarcasm. For example: "It's so much fun working at 2 am at night". Like-prefixed sarcasm as the name suggests uses a like-phrase to show the incongruity of the argu-ment being said, for example, "Like you care" <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref>. Illocutionary sarcasm is a type of sarcasm that bears the sarcasm in the non-textual cues, and the text is often the opposite of the attitude captured in the audio or video modality. <ref type="bibr">(Zvolenszky, 2012)</ref> give an example of rolling eyes while saying "Yeah right" being a sarcastic sentence; although the text is sincere prosodic features in audio and eye movement in the video clearly show the sarcasm. The main contributions of this paper are:</p><p>? An extended data resource which we call MUS-tARD++ where we have doubled the existing MUStARD dataset and added labels for emotion, valence, arousal, and sarcasm-type information.</p><p>? Identify and correct labeling issues in emotion labels on MUStARD data presented in <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref>.</p><p>? Exhaustive experimentation to benchmark multimodal fusion models for emotion detection in sarcasm.</p><p>Figure 1 is a sample in MUStARD++ with the labels and metadata information added to each video utterance. The text in the red bubble is the transcription of the sarcastic utterance, and the text in the yellow bubbles is the contextual sentences transcribed from the contextual video frames. The sarcasm is clearly evident just from the text modality (Embedded sarcasm). This utterance is also an illustration of cases where the explicit emotion and implicit emotion of the speaker are different. This is common in sarcastic utterances where the speaker makes a sarcastic comment with either no expression or vocal changes but means quite the opposite. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While there exist several studies on sentiment and emotion analysis, the relationship between emotion and sar-casm has been largely unaddressed. Most of the existing research has focused on the detection of sarcasm <ref type="bibr" target="#b10">(Joshi et al., 2016)</ref>, <ref type="bibr" target="#b12">(Joshi et al., 2018)</ref>. Research studying the impact of sarcasm on sentiment analysis <ref type="bibr" target="#b17">(Maynard and Greenwood, 2014)</ref> showed that sarcasm often has a negative sentiment, but the associated emotion(s) is important to frame the response and follow-up communication. <ref type="bibr" target="#b29">(Schifanella et al., 2016)</ref> extended sarcasm detection to multimodal data (images and text) from social media and observed that visual features did boost the performance over the textual models. Along similar lines, <ref type="bibr" target="#b28">(Sangwan et al., 2020)</ref> reported the improvement of the sarcasm detection task by using image data in addition to text. The dataset is curated from Instagram and the authors consider the image, text caption, and the transcript embedded within the image as multiple modalities. <ref type="bibr" target="#b2">(Cai et al., 2019)</ref> used text features, image features and image attributes as three modalities and proposed a multimodal hierarchical fusion model for sarcasm detection on tweets.</p><p>MUStARD <ref type="bibr" target="#b33">(Castro et al., 2019)</ref> is a subset of Multimodal Emotion Lines Dataset (MELD) <ref type="bibr" target="#b24">(Poria et al., 2018)</ref> and MELD is a multimodal extension of textual dataset EmotionLines <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>. MELD contains about 13,000 utterances from the TV series Friends, labeled with one of the seven emotions (anger, disgust, sadness, joy, neutral, surprise, and fear) and sentiment. EmotionLines <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> and EmoryNLP <ref type="bibr" target="#b32">(Zahiri and Choi, 2017)</ref> are textual datasets with conversational data, the former containing data from the TV show Friends and private Facebook messenger dialogues, while the latter was also curated from the series Friends. Iemocap <ref type="bibr" target="#b1">(Busso et al., 2008</ref>) is a wellknown multimodal, dyadic dataset with 151 recorded videos annotated with categorical emotion labels, as well as dimensional labels such as valence, activation, and dominance. However, none of them have sarcastic utterances. <ref type="bibr" target="#b0">(Bedi et al., 2021)</ref> released a Hindi-English code-mixed dataset for the problem of Multimodal Sarcasm Detection and Humour Classification in a conversational dialog. They also propose an attention-based architecture named MSH-COMICS for enhanced utterance classification. Along with categorical classification of basic emotions, seminal works <ref type="bibr" target="#b27">(Russell, 1980;</ref><ref type="bibr" target="#b23">Plutchick, 1980)</ref> also propose dimensional models of emotion (Ex: Valence, Arousal, Dominance), which could help in capturing the complicated nature of human emotions better. <ref type="bibr" target="#b31">(Zafeiriou et al., 2017</ref>) created a database of 298 videos (non-enacted, in-the-wild) and captured facial affect in their subjects in terms of valence arousal annotations ranging between -1 to +1. Similar work was undertaken in <ref type="bibr" target="#b25">(Preo?iuc-Pietro et al., 2016)</ref> where valence and arousal were annotated on a nine-point scale on Facebook data. They also release bag-of-words regression models trained on their data which outperform popular sentiment analysis lexicons in valence-arousal prediction tasks. <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> annotated the MUStARD dataset with emotions and sentiment and showed that emotion and sentiment labels could help sarcasm detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Label Changes in MUStARD</head><p>Since our study mainly focuses on understanding the speaker's emotion leading to the use of sarcasm, we used their basic emotion annotation. After performing extensive experiments with all combinations of Video, Text and Audio and several state-of-art models, we observed that most of the errors arise from the model predicting negative emotions when the true label is either Neutral or Happy. On detailed qualitative analysis, we observed that the labels for those sarcastic datapoints seemed intuitively incorrect. We built several models using each modality separately and also in combinations using different types of feature extractors and classifiers on the dataset. We flagged cases where majority of the models agreed with each other but disagreed with ground truth labels to obtain instances that needed re-annotation. We also grouped the error categories and the flagged cases to few distinct categories and observed that most of the errors are in sarcastic utterances. Our analysis flagged 399 cases of disagreement with <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> out of the 690 video utterances. We initiated an unbiased manual labeling effort by a team of annotators on the entire dataset without giving them the labels from <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref>. The reannotation effort led to identifying 88 labeling issues in non-sarcastic and 255 labeling issues in sarcastic sentences.</p><p>A major chunk of errors (90 out of 345 sarcastic sentences) is in utterances previously labeled as Neutral.</p><p>Literature shows that people resort to sarcasm in their utterances when they have negative intent or negative sentiment <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref>. In sarcasm, the explicit emotion can be positive, but the implied emotion/sentiment must have opposite polarity; hence it seemed unlikely for neutral or happy to appear in implicit emotion. <ref type="table" target="#tab_0">Table  1</ref> shows an example of a label error wherein the utterance is marked as neutral for both explicit and implicit emotion. The sarcastic utterance (in gray) is expressed out of Disgust and cannot be Neutral. Also, the audio and video clearly indicate that the speaker was overexcited to place the order before anyone else could speak. This particular utterance is an example of Propositional sarcasm since we need the prior conversations to understand the sarcasm but the textual sentences are enough and doesn't need additional modalities for sarcasm detection. Additional modalities are however crucial for understanding the emotions for such cases. Our annotators felt that the cases labeled as neutral originally might have been difficult to annotate and thus were marked as neutral under majority voting. <ref type="table" target="#tab_1">Table 2</ref> shows the number of label changes that were done on original MUStARD dataset. As seen in <ref type="table" target="#tab_1">Table  2</ref>, most labeling errors are in sarcastic utterances due to the challenges sarcasm adds. There were 62 sarcastic utterances which were labeled with happy as implicit Speaker Utterance <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> New Explicit Implicit Explicit Implicit</p><p>The backwash into this glass is every pathogen that calls your mouth home, sweet home. Not to mention the visitors who arrive on the dancing tongue of your subtropical girlfriend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neu Neu Exi Dis</head><p>Hey! That's my sister and my country you're talking about. Leonard may have defiled one, but I won't have you talking smack about the other.</p><p>You guys ready to order?</p><p>Yes, I'd like a seven-day course of penicillin, some, uh, syrup of ipecac-to induce vomiting-and a mint.   <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> annotations and OUR is proposed annotations.)*Ridicule in introduced in new annotations emotion and 114 sarcastic sentences had happy as explicit emotion. While happy can be an explicit emotion, our annotators suggested that the correct intent for such sarcastic utterances should be ridicule or mockery as these shows belong to the genre of situational comedy (sit-com), wherein characters use sarcasm to ridicule their friends while demonstrating happiness explicitly. Thus we introduced a new label Ridicule and allowed annotators to label as per these labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset</head><p>Towards understanding emotions in sarcasm, we had two main challenges: difficulty in getting multimodal sarcastic data, and challenges of annotating the perceived emotion of a speaker for every sarcastic utterance. While 10,000 non-sarcastic videos could be gathered in MELD <ref type="bibr" target="#b24">(Poria et al., 2018)</ref>, only 345 out of them were sarcastic <ref type="bibr" target="#b33">(Castro et al., 2019)</ref> which stands proof to the difficulty in finding multimodal sarcasm data. In this work, we doubled the size of this dataset, by carefully adding sarcastic videos from similar genre, annotate it with sarcasm presence or absence, as well as emotions, arousal and valence. We also point out that to improve sarcasm detection, it is important to understand the type of sarcasm present and thus annotate each video with the sarcasm types -Propositional, Illocutionary, Like-Prefixed and Embedded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data collection</head><p>In <ref type="bibr" target="#b33">(Castro et al., 2019)</ref>, authors collected videos from sit-com TV shows: Friends, Big Bang Theory (seasons 1-8), The Golden Girls, and Burnistoun (also referred to as Sarcasmaholics Anonymous). We collected all videos from The Big Bang Theory season 9-12, out of which we could only get 216 sarcastic video utterances. We considered another series of similar genre called The Silicon Valley 1 which has 6 seasons of 53 episodes. Out of the 53 episodes, we could only find 41 video utterances that are sarcastic. Although all such videos have humor, not all are sarcastic, thus needing careful observation while selecting and manual annotation. We added equal number of non-sarcastic videos with context to create balanced sarcasm detection dataset. While in non-sarcastic sentences a speaker might have only one emotion, sarcasm due to its incongruous nature, exhibits an extrinsic and an intrinsic emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Annotation Protocol and Observations</head><p>We employ seven annotators proficient in English.</p><p>While one annotator has a Ph.D. in Humanities, two of them were linguists; others were engineering graduates. They were selected from a pool of annotators due to their prior working experience in the field of Sentiment and Emotion Analysis and understanding of emotion and sarcasm. We have four male and three female annotators and all annotators were in the age group of 18-30. They were provided the detailed instructions on the annotation protocol before beginning the annotation process with examples of each type of sarcasm. In the first round of manual annotation, we gave our annotators the original MUStARD dataset for emotion annotation and asked them to put their labels for extrinsic and intrinsic emotion of the speaker without access to the emotion labels provided by <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref>. Instead of annotating only those videos where we observed the incorrect labels, we decided to annotate all videos of existing dataset as well as our newly collected video utterances. Annotators had access to full videos for annotation but were instructed to start with only text transcription of the utterance, then proceed with text transcriptions of contextual frames and finally watch the utterance and context video. Following this annotation protocol is especially important to be able to correctly classify the sarcasm type. sarcastic videos in this proposed dataset. <ref type="figure" target="#fig_1">Figure 2</ref> shows a breakdown of emotion distribution per type of sarcasm present.</p><p>Below are some examples of labeling issues: utterances with different extrinsic and intrinsic emotions and the emotion label changes introduced for more clarity. We also provide insights that help understand the challenges in emotion recognition in sarcasm. These were previously annotated as Neutral explicit emotions, which were changed to Ridicule  We thought you were just here for the company. He is a sarcasmoholic Stewart.  <ref type="figure" target="#fig_0">Figure-1</ref>) and named this category Ridicule. According to Plutchik's wheel of emotion <ref type="bibr" target="#b23">(Plutchick, 1980)</ref> contempt is a higher-order emotion composed of the two basic emotions -anger and disgust. Since in this genre of situational comedy, the most likely emotion is frustration and disgust, and not anger, we cannot directly call all such instances contempt. Also, the intent of the speaker of the sarcastic utterance is predominantly to mock or ridicule; thus, we called this category Ridicule. Also, Ridicule is not a basic emotion present in the gold standard emotion scales <ref type="bibr" target="#b23">(Plutchick, 1980;</ref><ref type="bibr" target="#b7">Ekman, 1999)</ref>, and we call it just a category in our labeling scheme and not a basic emotion.  During re-annotation, 345 of the sarcastic videos from MUStARD are annotated as sarcastic, implying that the annotators have good understanding of sarcasm. Also, out of 264 new videos, 8 videos are moved to nonsarcastic because at least one annotator annotated it as non-sarcastic, making the total number of sarcastic videos in MUStARD++ 601. We increased the number of non-sarcastic videos to 601 in the extended dataset. The final label was chosen via majority voting. The overall inter-annotator agreement was calculated with a Kappa score of 0.595, which is comparable with the Kappa score of 0.5877 of original MUStARD annotations. <ref type="table" target="#tab_7">Table 5</ref> shows the explicit and implicit emotion distribution of the extended dataset. Additionally, annotators were instructed to rate each utterance with a valence and an arousal rating ranging from 1 to 9, with 1 being extremely low, 9 being extremely high and 5 being neutral. Pair-wise Quadratic Cohen Kappa was used to evaluate the inter-annotator agreement, and the average agreement was found to be 0.638 for valence and 0.689 for arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Although our primary goal is to detect emotions in sarcasm, we also benchmark the proposed extended dataset  We analyze the impact of context and speaker information in each of the models. In the speaker dependent setup we passed the speaker information as a one-hot vector along with the utterance. When no such speaker information is passed, that method is being referred to as speaker-independent method. This was done as we observed that there are specific characters in each series who pass most of the sarcastic comments. Thus we wanted to see if the models benefit from the speaker information or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Preprocessing and Feature Extraction</head><p>Owing to the presence of multiple modalities, the features from text, audio and video were separately extracted and fused appropriately to act as input into our model. We discuss our feature extraction methods in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Modality</head><p>In order to extract features from transcript (context and utterance), we tried using different transformer models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>,BART , RoBERTa  and T5 <ref type="bibr" target="#b26">(Raffel et al., 2020)</ref>. BART performed slightly better in all experiments (only text, as well as in combination with audio and video) over BERT, RoBERTa and T5 models, thus we continued with BART-large representations for text. BART provides a feature vector representation x t ? R dt for every instance x. We encode the text using BART Large model with d t = 1024 and use the mean of the last four transformer layer representations to get a unique embedding representation for both utterance and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Modality</head><p>We extract low-level features from the audio data stream for each utterance in the dataset to take advantage of information from the audio modality. We sampled the audio signal at 22.5KHz. Since the audio has background noise and canned laughter, we used vocal-separation method to process it 2 . We extracted three low-level features: Mel Frequency Cepstral Coefficients (MFCC), Mel spectrogram (using Librosa library(McFee et al., 2022)), and prosodic features using OpenSMILE 3 .</p><p>We split the audio signal into equal length segments to maintain consistent feature representation in all instances. Since the audio signal length varies for different utterances, this segmentation helps in keeping vector size constant across dataset. For each segment we extract MFCC, Mel spectrogram and prosodic features of size d m , d s , d p respectively. Then we take the average across segments to get the final feature vector. Here d m = 128 , d s = 128 , d p = 35 , so our audio feature vector is of size d a = 291 . We had also experimented with self-supervised speech encoders <ref type="bibr" target="#b22">(Pascual et al., 2019)</ref>. However due to the very small number of sarcastic utterances, such models are unable to learn and thus we decided to stick with our low-level audio features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Modality</head><p>In order to extract visual features from the videos, we have used a pool5 layer of pre-trained ResNet-152 <ref type="bibr" target="#b8">(He et al., 2016)</ref> image classification model. To improve the video representation and reduce noise, we extracted the key frames to be passed to ResNet-152, instead of feeding in information from all of the frames. Key frame extraction is widely used in the vision community and is defined as the frames that form the most appropriate summary of a given video <ref type="bibr" target="#b9">(Jadon and Jasim, 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Setup</head><p>We perform multi-class emotion classification experiments using the features extracted. Since we have three modalities, context and speaker information, we perform several ablation studies to understand the impact of presence or absence of each of these aspects. For the multi-modal fusion, we use collaborative gating architecture introduced in , with the only difference that not all the input embeddings need to be pretrained. First we calculate projection ? (i) (V ) where i ? {t, tc, a, ac, v , vc} and t, a, v , c are text, audio, video and corresponding context. The collaborative gating module implements two tasks: first, we find the attention vector prediction for three main input projections referred to as projection embeddings (i.e for our utterances in three modalities)</p><formula xml:id="formula_0">T = {T t (V ), T a (V ), T v (V )}. T (i) (V ) = h ? ( j =i g ? (? (i) (V ), ? (j ) (V )))<label>(1)</label></formula><p>where functions h ? and g ? are used to model the pairwise relationship between projection ? (i) and ? (j ) . Also i ? {t, a, v } and j ? {t, tc, a, ac, v , vc}.</p><p>Then we perform expert response modulation using the attention vector prediction calculated. For response 4 https://katna.readthedocs.io/en/ latest/ modulation of each modality projection we perform-</p><formula xml:id="formula_1">? (i) (V ) = ? (i) (V ) ? ?(T (i) (V ))<label>(2)</label></formula><p>where ? is an element-wise sigmoid activation and ? is the element-wise multiplication (Hadamard product). All the modulated projections are concatenated and passed to fully connected linear layers (ReLU) followed by a softmax layer to predict target class probability distribution. Cross entropy loss is used for all classification experiments.</p><p>For completeness in bench-marking our data, we also perform Majority sampling (assigns the emotion class with majority examples as all samples), Random Sampling (predictions are sampled equally throughout the test set using this baseline). We also perform one-vs-rest experiments for each emotion which contain a sigmoid layer instead of a softmax layer as classification head.</p><p>The results of these baselines are reported in the supplementary material section 8. In addition to emotion classification, similar set up was used to study the performance of sarcasm detection on our dataset which was treated as a binary classification problem. Furthermore, we build a regression model for valence-arousal prediction based on the same architecture explained above, with the only difference of ReLU layer replacing the classification head of classification models and the loss function being the smooth <ref type="formula" target="#formula_0">L1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Analysis</head><p>This section discusses the benchmarking experiments done with the proposed dataset for sarcasm detection, emotion recognition and arousal-valence prediction. <ref type="table" target="#tab_13">Table 7</ref> shows results of our best model for sarcasm detection on both MUStARD and MUStARD++. Our results outperform state-of-art models significantly on MUStARD which demonstrates the superiority of the collaborative gating-based multimodal fusion, and the best modality features selected were BART for text, low-level audio descriptors, and ResNET video features). We performed attention over modalities and intra-modality attention which helped us understand the importance of features. Also in <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> emotion and sentiment labels are used in the sarcasm detection task, but we are able to outperform them without using emotion or sentiment label. <ref type="table">Table 8</ref> shows the best results per modality and modality combination in the speaker independent and speaker dependent setup. The experiments with context and without context clearly show the importance of contextual dialogues to be able to detect sarcasm in an utterance. Although results with the speaker-dependent setting is marginally better in all modality combinations, we believe a speakerindependent setting is better since speaker information   <ref type="table">Table 8</ref>: Sarcasm detection results for MUStARD++, Weighted Average might bias the system towards sarcasm. We intend to use this sarcasm detection module as the first module of our system followed by emotion recognition on the sarcastic sentences and valence and arousal prediction to understand the degree of emotions identified, Thus did not use emotion, sentiment, or valence-arousal for sarcasm detection task. However, we plan to use sarcasm detection output and valence-arousal predictions to see if we can improve emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results of Sarcasm Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Emotion Recognition Results</head><p>Of the various models and features that we used, BART for text, MFCC, spectrogram and prosodic for audio and features learnt from keyframes using ResNET for video worked best for this dataset. Due to the small size of this dataset, we pretrained models on IEMOCAP <ref type="bibr" target="#b1">(Busso et al., 2008)</ref> and MELD <ref type="bibr" target="#b24">(Poria et al., 2018)</ref> and then tried zero-shot experiments on MUStARD and MUS-tARD++. Since IEMOCAP and MELD do not have any sarcastic utterances, the models saw a significant drop in F-score when tested on sarcastic data. We also extracted learnt audio features using state-of-art self-supervised PASE network <ref type="bibr" target="#b22">(Pascual et al., 2019)</ref> but models built on PASE features require significantly large sarcastic data although pretrained on different utterances. <ref type="table" target="#tab_15">Table 9</ref> and <ref type="table" target="#tab_0">Table 10</ref> show the detailed emotion classification results on sarcastic utterances for both implicit and explicit emotion in speaker-dependent and speakerindependent setting. Ablation studies across all modalities show that audio and video when used with text perform better. This is intuitive because we consider the variation in speech signals in audio and visual features from video while ignoring the actual spoken content. We observe that for emotion classification, contextual information plays a key role. Although we observe similar numbers in speaker-dependent and speaker-independent setting, it is better to have a speaker-independent setting than limit the overall method by passing speaker's information.</p><p>Post hyper-parameter search, best parameters in a 5-fold cross validation is selected across 28 different experimental configurations: 7 modality combinations (rows of <ref type="table" target="#tab_15">Table 9</ref>) each run with 4 settings (columns of Table 9). <ref type="table" target="#tab_0">Table 11</ref> shows results of valence and arousal predictions across different modalities Here also we observed that in a speaker-independent setting the model performs better, but as the length of the speaker vector increases with more people, the effect of speaker information confuses the models. We also observe that contextual information doesn't affect valence arousal prediction as that of the actual utterance, which is intuitive.</p><p>In order to prove the performance improvement due to correcting labels, we ran a Wilcoxon signed rank test <ref type="bibr" target="#b30">(Wilcoxon, 1945)</ref> on old versus new labeled data. We made 10 runs with the best trained multimodal model on old as well as new labels on the sarcastic MUStARD data for proper comparison. The mean F1 score of emotion recognition on the sarcastic set with old labels is 20.64?1.15 while with new labels on same set the mean is 39.5 ? 1.00 which is statistically significant with a p-value of 0.002. We observed reduction in label confusion in the confusion matrix. Thus these re-labeling    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>This paper presents a multimodal sarcasm dataset that can be used by researchers in the area of sarcasm detection and emotion recognition. We start with the version of MUStARD data provided by <ref type="bibr" target="#b34">(Chauhan et al., 2020)</ref> and correct several emotion labels, while appending the sarcasm type and arousal-valence labels as additional metadata that is useful for both the research avenues. We doubled the number of sarcastic videos, finding which is very challenging, thus making it a beneficial contribution to the research community. We also added equal number of non-sarcastic videos with their context along with similar metadata annotations. To the best of our knowledge this is the first work on emotion recognition in sarcasm and towards that we present a curated dataset which is benchmarked using several pretrained feature extractors and multimodal fusion techniques in different setups. Sarcasm type information enables a multimodal system to choose the right modality combination for a given utterance, thereby optimizing performance of the sarcasm detector and the emotion recognizer for different utterances. In this work, we have used arousal only to understand the degree/intensity of an emotion, for example, annoyance to anger to rage. In future we want to use arousal and valence to investigate its effect on emotion classification. While we currently explore the use of sarcasm label in emotion recognition, an interesting research direction would be using emotion labels to improve sarcasm detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Ethical Considerations in Data Curation</head><p>This paper does not claim to find the exact intended emotion of the speaker that led to a sarcastic sentence. Rather we try to predict the perceived emotion. Our annotators annotate on the recorded video, thus observing the perceived emotion, arousal and valence. This is very important for conversational systems where the bot needs to understand the emotion, valence and intensity to be able to respond better. Also, this is in accordance to the suggestions in the ethics sheet for automated emotion recognition <ref type="bibr" target="#b19">(Mohammad, 2021)</ref> where authors explain that given the complexity of human emotion, it is very difficult to predict the exact emotional state of the speaker. The authors hereby acknowledge that there could be a possibility of bias in the final emotion label assigned since the label is chosen based on majority voting. In order to minimize the effect of bias, we collect videos from a diverse set of sources and ask seven annotators of different age, gender, and educational background to label their perceived emotions. We have considered the guidelines <ref type="bibr" target="#b19">(Mohammad, 2021)</ref> for responsible development and use of Automated Emo-tion Recognition systems (AER) and adhered to them in our research statement, data collection, annotation protocol, and during the benchmarking experiments.  While arousal values in sarcastic utterances have majority ratings as 7-8, in non-sarcastic utterances the distribution is more diverse in comparison with sarcastic utterances and they have majority ratings fall into the range of 5-7. This shows the implied negativity and intensity that sarcasm usually tends to portray. In each of the 5 TV shows which form the source of our dataset, different characters contribute to the sarcastic instances with different emotions. To understand the number of speakers in each show and their contribution to various implicit emotions in the sarcastic subset of our data, <ref type="figure">Figure 3</ref> provides the distribution of implicit emotions in the utterances of each character from the shows. For sarcastic utterances, the dataset contains sarcasm type metadata which can help provide deeper insights into the task of sarcasm detection. To also understand how these sarcasm types co-occur with different implicit emotions, <ref type="table" target="#tab_0">Table 13</ref> presents the distribution of sarcastic utterances belonging to different implicit emotion classes fall under different sarcasm types.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>As part of the bench-marking exercise, as mentioned in Section 6, we perform Majority sampling (assigns the emotion class with majority examples as all samples), Random Sampling (predictions are sampled equally throughout the test set using this baseline). We also perform one-vs-rest experiments for each emotion which contain a sigmoid layer instead of a softmax layer as classification head. The results of these baselines are reported in the table 14. In order to study the advantage of our model, we presented a comparison of sarcasm detection results on original MUStARD dataset using our proposed model against the numbers reported by the authors of <ref type="bibr" target="#b33">(Castro et al., 2019;</ref><ref type="bibr" target="#b34">Chauhan et al., 2020)</ref> in <ref type="table" target="#tab_13">Table 7</ref>. The detailed results of this experiment   <ref type="table" target="#tab_0">Table 15</ref>. <ref type="table" target="#tab_0">Table 16</ref> and <ref type="table" target="#tab_0">Table 17</ref> represent the detailed weighted average results of all modality combinations for ONEvs-REST classification experiments in implicit and explicit emotions respectively. Though the main focus of the paper is detection of emotion in sarcasm, in order to benchmark the entire data, we perform implicit and explicit emotion classification experiments in a multiclass setting. The results of these experiments across different modalities, context and speaker settings are presented in <ref type="table" target="#tab_0">Table 18 and Table 19</ref> for implicit and explicit emotions respectively.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example to show that different explicit and implicit emotions in sarcasm. Explicit = Surprise, and Implicit = Ridicule; Sarcasm Type: Embedded; Valence = 4; Arousal = 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of Emotion over Sarcasm Type in Proposed Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="8">: Example of labeling error. Sarcastic utterance</cell></row><row><cell cols="8">(in gray) and the 3 preceding utterances are context.</cell></row><row><cell cols="8">Sarcasm type: Propositional {Neu-Neutral, Exi -Excitement, Dis</cell></row><row><cell>-Disgust }</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Non-Sarcastic</cell><cell></cell><cell></cell><cell cols="2">Sarcastic</cell><cell></cell></row><row><cell cols="2">Explicit</cell><cell cols="2">Implicit</cell><cell cols="2">Explicit</cell><cell cols="2">Implicit</cell></row><row><cell cols="8">OLD OUR OLD OUR OLD OUR OLD OUR</cell></row><row><cell>Anger 28</cell><cell>28</cell><cell>35</cell><cell>28</cell><cell>26</cell><cell>3</cell><cell>62</cell><cell>68</cell></row><row><cell>Excitement 15</cell><cell>32</cell><cell>15</cell><cell>31</cell><cell>15</cell><cell>41</cell><cell>2</cell><cell>0</cell></row><row><cell>Fear 6</cell><cell>8</cell><cell>10</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>0</cell></row><row><cell>Sad 62</cell><cell>61</cell><cell>68</cell><cell>62</cell><cell>56</cell><cell>47</cell><cell>53</cell><cell>18</cell></row><row><cell>Surprise 20</cell><cell>19</cell><cell>18</cell><cell>18</cell><cell>15</cell><cell>21</cell><cell>11</cell><cell>0</cell></row><row><cell>Frustrated 13</cell><cell>27</cell><cell>17</cell><cell>28</cell><cell>10</cell><cell>3</cell><cell>40</cell><cell>88</cell></row><row><cell>Happy 92</cell><cell>82</cell><cell>81</cell><cell>80</cell><cell cols="2">114 85</cell><cell>62</cell><cell>0</cell></row><row><cell cols="2">Neutral 115 75</cell><cell cols="2">108 75</cell><cell cols="2">113 144</cell><cell>90</cell><cell>0</cell></row><row><cell>Disgust 6</cell><cell>13</cell><cell>7</cell><cell>14</cell><cell>4</cell><cell>0</cell><cell>32</cell><cell>81</cell></row><row><cell>Ridicule* -</cell><cell>0</cell><cell>-</cell><cell>0</cell><cell>-</cell><cell>0</cell><cell>-</cell><cell>89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Emotion distribution comparison in MUStARD between earlier and updated labels.(OLD is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For example, text to be observed in isolation for Embedded and Like-Prefixed sarcasm, transcript of utterance and context should be observed in isolation for Propositional sarcasm, and the whole video to be considered for Illocutionary sarcasm. Out of the 601 sarcastic videos, we have 333 Propositional ( 55.4%), 178 Illocutionary 29.6%, 87 Embedded 14.4% and 3 Like-prefixed 0.4%</figDesc><table /><note>1 https://en.wikipedia.org/wiki/ Silicon_Valley_(TV_series)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Some examples of ridicule</figDesc><table><row><cell>Speaker</cell><cell>Utterance</cell><cell cols="2">(Chauhan et al., 2020) Explicit Implicit</cell><cell>New Explicit Implicit</cell></row><row><cell></cell><cell>My name's Scott and I am a sarcasmaholic.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Sur</cell><cell>Sur</cell><cell>Sur</cell><cell>Rid</cell></row><row><cell></cell><cell>Nooo.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Example of Incorrect Labeling: Text transcription might suggest Surprise as the emotion, however video makes it apparent that the intent is to ridicule.</figDesc><table><row><cell>Sarcasm Type: Illocutionary{Sur-Surprise, Rid-Ridicule }</cell></row><row><cell>Initially we kept the emotion labels the same as</cell></row><row><cell>(Chauhan et al., 2020) (i.e. Anger, Excitement, Fear,</cell></row><row><cell>Sad, Surprise, Frustrated, Happy, Neutral, Disgust), but</cell></row><row><cell>after annotating 25 to 30% videos, annotators suggested</cell></row><row><cell>for a label which is in between frustration and disgust,</cell></row><row><cell>and close to mockery. We looked at some examples</cell></row><row><cell>(one of them is mentioned in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Emotion distribution in Sarcastic Utterances of MUStARD++. {An-Anger, Ex-Excitement, Fe-Fear, Sa-Sad, Sp-Surprise, Fs-Frustrated, Hp-Happy Neu-Neutral, Dis-Disgust, Ri-Ridicule}</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics by utterance and context for sarcasm detection and valence-arousal prediction. We perform various experiments examining different modalities independently and in various combinations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Sarcasm detection results (weighted average) comparison with SOTA on MUStARD and MUStARD++. Proposed MUStARD is our best model on MUStARD and %?MUStARD is the improvement we observe with our model and corrected labels of MUStARD. Proposed MUStARD++ is results of our best model for sarcasm detection on extended dataset MUStARD++ presented in this paper.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Speaker Independent</cell><cell></cell><cell></cell><cell cols="4">Speaker Dependent</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>T</cell><cell cols="6">67.9 67.7 67.7 69.3 69.2 69.2</cell><cell cols="4">69.4 69.3 69.3 70.2</cell><cell>70</cell><cell>70</cell></row><row><cell>A</cell><cell cols="6">63.9 63.5 63.6 64.3 64.1 64.1</cell><cell cols="6">65.3 65.2 65.2 65.0 64.9 64.9</cell></row><row><cell>V</cell><cell cols="6">59.5 59.4 59.4 60.3 60.0 60.0</cell><cell cols="6">61.8 61.7 61.7 61.6 61.4 61.5</cell></row><row><cell>T+A</cell><cell cols="6">68.8 68.6 68.7 70.2 70.2 70.2</cell><cell cols="6">69.8 69.5 69.5 69.2 69.1 69.1</cell></row><row><cell>A+V</cell><cell cols="6">65.7 65.4 65.5 67.5 67.3 67.4</cell><cell cols="6">64.9 64.5 64.5 64.2 64.0 64.0</cell></row><row><cell>V+T</cell><cell cols="6">68.2 68.1 68.1 67.9 67.6 67.6</cell><cell cols="6">69.1 69.0 69.0 69.4 69.1 69.1</cell></row><row><cell cols="7">T+A+V 69.5 69.4 69.4 69.6 69.5 69.6</cell><cell cols="6">69.6 69.3 69.3 70.6 70.3 70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>9?0.9 30.3?0.8 30.1?0.8 30.2?0.9 30.8?0.9 30.5?0.9 A 26.7?1.1 27.1?1.4 26.8?1.1 24.9?1.0 26.3?1.4 25.5?1.2 24.3?0.8 24.7?0.6 24.5?0.7 26.7?1.2 26.9?1.2 26.8?1.2 V 28.8?0.9 29.4?1.3 29?1.1 28.5?1.2 29.2?1.4 28.8?1.3 30.3?1.4 31.4?1.2 30.6?1.4 28.7?0.8 30.08?1.1 29.1?1.0 T+A 31.5?1.7 31.6?1.8 31.6?1.7 32.1?0.7 32.04?0.6 32.03?0.6 29.1?1.6 29.2?1.5 29.1?1.5 31.2?2 31.8?1.8 31.4?1.8 A+V 25.9?1.9 26.3?2 26.1?1.9 28.2?1.1 28.3?1.2 28.2?1.1 29.7?0.6 30.6?0.9 30.1?0.7 25.2?1.0 25.2?0.9 25.2?0.9 V+T 31.9?0.8 32.5?0.7 32.2?0.7 32.7?1.1 33.3?1.0 33.0?1.1 31.1?0.8 31.2?0.7 31.1?0.7 31.8?0. 31.9?0.5 31.8?0.6 9?0.3 30.5?0.4 30.7?0.3 31.6?1.5 31.3?1.3 31.5?1.4</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Speaker Independent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Speaker Dependent</cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Context</cell><cell></cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell>w/o Context</cell><cell></cell><cell></cell><cell>w Context</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="6">T 29.T+A+V 33?0.9 33.6?1 33.3?0.9 32.3?0.7 32.7?0.6 32.5?0.6 31.2?1 31.6?0.1 31.4?1 28.9?1.3 29.0?1.4 28.9?1.3 30.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Mean, std-dev of 5 runs for Implicit Emotion Classification (Multiclass) on MUStARD++</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Speaker Independent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Speaker Dependent</cell><cell></cell></row><row><cell></cell><cell></cell><cell>w/o Context</cell><cell></cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell>w/o Context</cell><cell></cell><cell></cell><cell>w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>T</cell><cell cols="6">38.5?0.8 39.2?0.9 38.8?0.8 38.2?1.2 38.8?1.3 38.5?1.3</cell><cell cols="2">38.7?0.5 39.3?0.5</cell><cell>39?0.5</cell><cell cols="3">38.9?0.4 39.7?0.6 39.3?0.5</cell></row><row><cell>A</cell><cell>26.4?0.9</cell><cell>28?1.4</cell><cell>27.1?1</cell><cell cols="3">27.1?1.1 28.1?1.3 27.6?1.2</cell><cell cols="6">28.1?1.1 29.3?1.0 28.6?1.1 28.4?0.7 31.6?1.3 29.6?0.8</cell></row><row><cell>V</cell><cell cols="6">25.1?0.6 25.9?0.6 25.5?0.6 24.4?0.7 24.9?0.9 24.6?0.8</cell><cell cols="5">25.7?1.4 36.1?0.8 27.7?0.8 27.0?0.8 29.6?1.6</cell><cell>28.0?1</cell></row><row><cell>T+A</cell><cell cols="6">38.9?1.1 39.5?1.3 39.2?1.2 39.2?0.6 39.5?0.6 39.3?0.6</cell><cell cols="6">39.1?0.8 39.7?0.7 39.4?0.7 39.1?0.5 39.5?0.7 39.3?0.6</cell></row><row><cell>A+V</cell><cell cols="6">26.4?1.4 26.5?1.5 26.4?1.4 26.2?1.22 26.3?1.6 26.2?1.5</cell><cell>27.6?1</cell><cell cols="3">28.2?1.2 27.9?1.1 27.8?0.5</cell><cell>28?0.4</cell><cell>27.9?0.4</cell></row><row><cell>V+T</cell><cell cols="6">38.6?0.7 39.2?0.8 38.8?0.8 40.5?0.7 41.2?0.7 40.8?0.7</cell><cell>39.8?0.1</cell><cell>40?0.2</cell><cell cols="3">39.8?0.2 39.9?0.4 40.3?0.6</cell><cell>40?0.5</cell></row><row><cell cols="7">T+A+V 37.8?0.1 38.3?0.8 38.0?0.9 39.5?0.8 39.6?0.9 39.5?0.9</cell><cell>40?0.6</cell><cell cols="5">39.8?0.5 39.9?0.9 39.7?1.3 39.4?1.2 39.5?1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Mean, std-dev of 5 runs for Explicit Emotion Classification (Multiclass) on MUStARD++</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Valence Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Arousal Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Speaker Independent</cell><cell></cell><cell cols="3">Speaker Dependent</cell><cell></cell><cell></cell><cell cols="2">Speaker Independent</cell><cell></cell><cell cols="3">Speaker Dependent</cell><cell></cell></row><row><cell></cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell></row><row><cell></cell><cell cols="8">MAE RMSE MAE RMSE MAE RMSE MAE RMSE</cell><cell cols="8">MAE RMSE MAE RMSE MAE RMSE MAE RMSE</cell></row><row><cell>T</cell><cell>0.91</cell><cell>0.74</cell><cell>0.89</cell><cell>0.73</cell><cell>0.98</cell><cell>0.78</cell><cell>0.95</cell><cell>0.75</cell><cell>1.42</cell><cell>1.15</cell><cell>1.45</cell><cell>1.16</cell><cell>1.42</cell><cell>1.10</cell><cell>1.42</cell><cell>1.12</cell></row><row><cell>A</cell><cell>0.82</cell><cell>0.65</cell><cell>0.91</cell><cell>0.73</cell><cell>0.97</cell><cell>0.77</cell><cell>0.95</cell><cell>0.76</cell><cell>1.24</cell><cell>1.00</cell><cell>1.34</cell><cell>1.07</cell><cell>1.45</cell><cell>1.15</cell><cell>1.43</cell><cell>1.11</cell></row><row><cell>V</cell><cell>0.85</cell><cell>0.68</cell><cell>0.84</cell><cell>0.68</cell><cell>0.94</cell><cell>0.76</cell><cell>1.02</cell><cell>0.81</cell><cell>1.22</cell><cell>0.96</cell><cell>1.16</cell><cell>0.93</cell><cell>1.45</cell><cell>1.14</cell><cell>1.47</cell><cell>1.16</cell></row><row><cell>T+A</cell><cell>0.83</cell><cell>0.67</cell><cell>0.86</cell><cell>0.69</cell><cell>0.93</cell><cell>0.74</cell><cell>0.91</cell><cell>0.73</cell><cell>1.18</cell><cell>0.96</cell><cell>1.33</cell><cell>1.08</cell><cell>1.41</cell><cell>1.11</cell><cell>1.59</cell><cell>1.23</cell></row><row><cell>A+V</cell><cell>0.83</cell><cell>0.66</cell><cell>0.82</cell><cell>0.66</cell><cell>0.92</cell><cell>0.79</cell><cell>0.88</cell><cell>0.71</cell><cell>1.19</cell><cell>0.96</cell><cell>1.21</cell><cell>0.98</cell><cell>1.37</cell><cell>1.11</cell><cell>1.43</cell><cell>1.16</cell></row><row><cell>V+T</cell><cell>0.82</cell><cell>0.66</cell><cell>0.86</cell><cell>0.71</cell><cell>0.90</cell><cell>0.72</cell><cell>0.93</cell><cell>0.74</cell><cell>1.22</cell><cell>0.97</cell><cell>1.34</cell><cell>1.09</cell><cell>1.50</cell><cell>1.12</cell><cell>1.56</cell><cell>1.24</cell></row><row><cell>T+A+V</cell><cell>0.78</cell><cell>0.62</cell><cell>0.78</cell><cell>0.62</cell><cell>0.85</cell><cell>0.70</cell><cell>0.94</cell><cell>0.75</cell><cell>1.20</cell><cell>0.95</cell><cell>1.20</cell><cell>0.95</cell><cell>1.23</cell><cell>0.98</cell><cell>1.45</cell><cell>1.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: Valence-Arousal Prediction on MUStARD++ (Mean of 5 runs)</cell></row><row><cell>efforts added trust to the extended MUStARD++ dataset</cell></row><row><cell>annotated using same label set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12</head><label>12</label><figDesc>shows the distribution of implicit and explicit emotions in our dataset including sarcastic and nonsarcastic utterances.</figDesc><table><row><cell></cell><cell cols="6">An Ex Fe Sa Sp Fs</cell><cell cols="3">Hp Neu Dis</cell><cell>Ri</cell></row><row><cell cols="6">Explicit-NS 49 47 22 82 28</cell><cell cols="3">44 109 190</cell><cell>30</cell><cell>0</cell></row><row><cell>Explicit-S</cell><cell>4</cell><cell>68</cell><cell>1</cell><cell cols="2">67 73</cell><cell>4</cell><cell cols="2">135 248</cell><cell>1</cell><cell>0</cell></row><row><cell cols="6">Implicit-NS 50 50 25 89 30</cell><cell cols="3">46 111 167</cell><cell>33</cell><cell>0</cell></row><row><cell>Implicit-S</cell><cell>87</cell><cell>0</cell><cell>0</cell><cell>24</cell><cell>0</cell><cell>130</cell><cell>0</cell><cell>0</cell><cell cols="2">134 226</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Emotion distribution in MUStARD++. {NS-</figDesc><table><row><cell>Non sarcastic, S-Sarcastic, An-Anger, Ex-Excitement, Fe-</cell></row><row><cell>Fear, Sa-Sad, Sp-Surprise, Fs-Frustrated, Hp-Happy Neu-</cell></row><row><cell>Neutral, Dis-Disgust, Ri-Ridicule}</cell></row><row><cell>From the valence-arousal ratings of the dataset, it was</cell></row><row><cell>observed that all sarcastic utterances received lower va-</cell></row><row><cell>lence values, inclined towards the unpleasant end of the</cell></row><row><cell>spectrum. Non-sarcastic utterances, however, have a</cell></row><row><cell>more diverse set of valence values with ratings from</cell></row><row><cell>both pleasant and unpleasant halves of the spectrum.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="6">: Implicit Emotion Distribution per Sarcasm</cell></row><row><cell cols="6">Type. {An-Anger, Sa-Sad, Fs-Frustrated, Dis-Disgust, Ri-</cell></row><row><cell cols="6">Ridicule} and {PRO-Propositional, ILL-Illocutionary, EMB-</cell></row><row><cell cols="2">Embedded, LIK-Likeprefixed}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Implicit</cell><cell></cell><cell></cell><cell>Explicit</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="3">Majority 14.1 37.6 20.6</cell><cell cols="3">17.0 41.3 24.1</cell></row><row><cell cols="3">Random (Uniform) 25.3 20.3 21.4</cell><cell cols="3">27.9 11.8 15.5</cell></row><row><cell cols="3">Random (Prior) 25.9 25.9 25.9</cell><cell cols="3">26.1 26.1 26.1</cell></row><row><cell cols="3">ONEvsREST 35.6 37.6 36.5</cell><cell cols="3">41.2 43.6 42.0</cell></row><row><cell cols="3">MultiClass 34.4 36.1 34.9</cell><cell cols="3">41.7 44.0 42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14 :</head><label>14</label><figDesc>Benchmarking emotion classification results comparison across different methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 15 :</head><label>15</label><figDesc>Sarcasm detection results for MUStARD, Weighted Average of sarcasm detection by using our proposed model on MUStARD across different modality, speaker, context combinations is given in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>35.4 34.6 35.2 36.8 35.9 33.6 34.4 33.8 34.2 35.1 34.6 A 29.4 30.3 29.7 29.5 30.4 29.9 30.7 31.1 30.6 29.4 30.6 29.7 V 31.7 34.6 32.9 32.7 31.3 31.8 32.2 33.6 32.5 31.1 32.8 31.8</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Speaker Independent</cell><cell></cell><cell></cell><cell cols="4">Speaker Dependent</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="7">T 34.1 T+A 34.7 37.9 35.8 35.3 38.9 35.5</cell><cell cols="6">33.9 34.4 34.0 33.4 34.8 33.9</cell></row><row><cell>A+V</cell><cell cols="6">29.8 31.6 30.5 29.3 33.4 30.6</cell><cell cols="6">31.8 33.4 32.5 30.2 32.6 30.6</cell></row><row><cell>V+T</cell><cell cols="6">34.0 36.1 34.9 35.7 37.6 36.5</cell><cell cols="6">34.5 35.4 34.8 33.9 36.3 34.5</cell></row><row><cell cols="7">T+A+V 32.5 37.3 32.9 31.9 34.6 32.6</cell><cell cols="6">32.5 32.6 32.4 32.7 34.4 32.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 16 :</head><label>16</label><figDesc>Implicit Emotion Classification in Sarcastic Utterance (ONEvsREST) Weighted Average</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Speaker Independent</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Speaker Dependent</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell cols="2">w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>T</cell><cell cols="6">39.8 42.8 41.0 40.2 43.6 41.5</cell><cell cols="6">37.6 41.6 38.5 39.4 41.6 40.3</cell></row><row><cell>A</cell><cell cols="6">28.3 31.8 29.3 27.7 30.8 28 8</cell><cell cols="6">29.8 33.4 31.3 30.5 35.9 31.6</cell></row><row><cell>V</cell><cell cols="6">26.6 30.6 28.2 28.3 30.1 29.0</cell><cell cols="6">29.3 33.6 30.9 29.0 33.4 30.6</cell></row><row><cell>T+A</cell><cell cols="6">41.1 43.9 42.2 39.7 41.9 40.5</cell><cell cols="6">38.1 40.6 39.1 37.5 39.4 38.3</cell></row><row><cell>A+V</cell><cell cols="6">29.6 31.9 30.4 27.7 30.0 28.6</cell><cell cols="6">30.1 34.3 31.2 29.8 31.9 30.7</cell></row><row><cell>V+T</cell><cell cols="6">41.2 43.6 42.1 39.9 41.9 40.7</cell><cell cols="6">38.8 40.4 39.5 36.6 38.4 37.4</cell></row><row><cell cols="7">T+A+V 38.6 41.6 39.7 37.8 39.6 38.5</cell><cell cols="6">36.3 38.8 37.3 34.9 37.6 36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 :</head><label>17</label><figDesc>Explicit Emotion Classification in Sarcastic Utterance (ONEvsREST) Weighted Average</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Speaker Independent</cell><cell></cell><cell></cell><cell cols="4">Speaker Dependent</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>T</cell><cell cols="6">22.6 23.6 23.0 23.3 24.3 23.6</cell><cell cols="6">25.7 25.9 25.1 25.4 26.3 25.3</cell></row><row><cell>A</cell><cell cols="6">21.1 22.3 21.5 20.6 21.2 20.7</cell><cell cols="6">20.7 22.0 21.2 21.1 22.0 21.5</cell></row><row><cell>V</cell><cell cols="6">18.5 19.4 18.9 20.2 21.4 20.6</cell><cell cols="5">19.9 21.4 20.5 22.6 23.6</cell><cell>23</cell></row><row><cell>T+A</cell><cell cols="6">25.5 26.9 26.0 23.6 24.5 24.0</cell><cell cols="6">25.6 27.1 26.1 24.6 26.0 25.3</cell></row><row><cell>A+V</cell><cell cols="6">19.6 20.9 20.1 21.3 23.0 21.8</cell><cell cols="6">20.6 21.9 21.2 22.0 23.6 22.6</cell></row><row><cell>V+T</cell><cell cols="6">24.1 24.9 24.4 24.0 25.3 24.6</cell><cell cols="6">23.0 24.4 23.5 22.6 23.9 23.2</cell></row><row><cell cols="7">T+A+V 23.4 24.7 23.8 25.7 26.9 26.1</cell><cell cols="6">25.6 26.9 25.9 24.3 25.5 24.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 18 :</head><label>18</label><figDesc>Implicit Emotion Classification in MUStARD++ (Multiclass, Weighted Average)</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Speaker Independent</cell><cell></cell><cell></cell><cell cols="4">Speaker Dependent</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell><cell></cell><cell></cell><cell cols="2">w/o Context</cell><cell></cell><cell>w Context</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>T</cell><cell cols="6">34.8 37.5 35.8 35.2 39.5 36.5</cell><cell cols="6">35.5 36.7 35.7 35.2 37.3 35.8</cell></row><row><cell>A</cell><cell cols="6">26.2 30.0 27.6 26.9 30.0 27.5</cell><cell cols="6">24.4 28.6 26.1 25.7 29.4 27.1</cell></row><row><cell>V</cell><cell cols="6">23.9 27.0 25.3 24.1 27.9 25.6</cell><cell cols="6">24.3 28.4 26.1 25.3 30.8 27.5</cell></row><row><cell>T+A</cell><cell cols="6">34.2 37.9 35.5 35.6 38.1 36.5</cell><cell cols="6">34.2 37.8 35.7 34.7 37.0 35.6</cell></row><row><cell>A+V</cell><cell cols="6">25.8 29.2 27.0 24.6 28.1 25.9</cell><cell cols="6">25.6 29.7 27.2 26.1 31.1 28.0</cell></row><row><cell>V+T</cell><cell cols="6">34.2 37.2 35.4 34.8 37.9 36.1</cell><cell cols="6">33.3 36.8 34.7 35.5 38.4 36.5</cell></row><row><cell cols="7">T+A+V 34.7 36.4 35.4 35.1 38.0 36.4</cell><cell cols="6">34.7 37.1 35.7 32.4 35.4 35.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 19 :</head><label>19</label><figDesc></figDesc><table /><note>Explicit Emotion Classification in MUStARD++ (Multiclass, Weighted Average)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://librosa.org/doc/main/auto_ examples/plot_vocal_separation.html 3 https://audeering.github.io/ opensmile/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material is intended to provide additional information with regards to characteristics of our data, additional analysis and detailed results of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Data Statistics and Analysis</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-modal sarcasm detection and humor classification in code-mixed conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno>abs/2105.09984</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-modal sarcasm detection in Twitter with hierarchical fusion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2506" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sarcasm, pretense, and the semantics/pragmatics distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">No?s</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="587" to="634" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multiparty conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ku</surname></persName>
		</author>
		<idno>abs/1802.08379</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing the emotional states that are expressed in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Cornelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Basic emotions. Handbook of cognition and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Video summarization using keyframe extraction and video skimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jadon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jasim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04792</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatic sarcasm detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
		</author>
		<idno>abs/1602.03426</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic sarcasm detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Investigations in Computational Sarcasm</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A large self-annotated corpus for sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vodrahalli</surname></persName>
		</author>
		<idno>abs/1704.05579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BART: denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>translation, and comprehension. CoRR, abs/1910.13461</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2014 Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Metsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thom?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zalkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seyfarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Here??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Friesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vollrath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thassilo</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
	<note>librosa/librosa: 0.9.1</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ethics sheet for automatic emotion recognition and sentiment analysis. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<idno>abs/2109.08256</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">iSarcasm: A dataset of intended sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Creating and characterizing a diverse corpus of sarcasm in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Emotion: a psychoevolutionary synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plutchick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Happer &amp; Row</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno>abs/1810.02508</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modelling valence and arousal in facebook posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis</title>
		<meeting>the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">I didn&apos;t mean what i wrote! exploring multimodality for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting sarcasm in multimodal social platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schifanella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Aff-wild: Valence and arousal &apos;in-the-wild&apos; challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion detection on TV show transcripts with sequence-based convolutional neural networks. CoRR, abs/1708.04299. Zvolenszky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resource References</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Z.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P?rez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ver?nica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhanush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Sentiment and Emotion Analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
