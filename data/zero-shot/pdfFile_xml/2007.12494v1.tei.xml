<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Monocular 3D Face Reconstruction by Occlusion-Aware Multi-view Geometry Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
							<email>jshang@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Everest Innovation Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Monocular 3D Face Reconstruction by Occlusion-Aware Multi-view Geometry Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Face Reconstruction, Multi-view geometry consistency</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000?0001?7161?9765] , Tianwei Shen 1[0000?0002?3290?2258] , Shiwei li 1[0000?0003?0712?0059] , Lei Zhou 1[0000?0003?4988?5084] , Mingmin Zhen 1[0000?0002?8180?1023] , Tian Fang 2[0000?0002?5871?3455] , and Long Quan 1[00000001?8148?1771]</p><p>Abstract. Recent learning-based approaches, in which models are trained by single-view images have shown promising results for monocular 3D face reconstruction, but they suffer from the ill-posed face pose and depth ambiguity issue. In contrast to previous works that only enforce 2D feature constraints, we propose a self-supervised training architecture by leveraging the multi-view geometry consistency, which provides reliable constraints on face pose and depth estimation. We first propose an occlusion-aware view synthesis method to apply multi-view geometry consistency to self-supervised learning. Then we design three novel loss functions for multi-view consistency, including the pixel consistency loss, the depth consistency loss, and the facial landmark-based epipolar loss. Our method is accurate and robust, especially under large variations of expressions, poses, and illumination conditions. Comprehensive experiments on the face alignment and 3D face reconstruction benchmarks have demonstrated superiority over state-of-the-art methods. Our code and model are released in https://github.com/jiaxiangshang/MGCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D face reconstruction is extensively studied in the computer vision community. Traditional optimization-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> formulate the 3D Morphable Model (3DMM) <ref type="bibr" target="#b6">[7]</ref> parameters into a cost minimization problem, which is usually solved by expensive iterative nonlinear optimization. The supervised CNN-based methods <ref type="bibr">[16-18, 24, 32, 33, 40, 50, 54, 60]</ref> require abundant 3D face scans and corresponding RGB images, which are limited in amount and expensive to acquire. Methods that focus on face detail reconstruction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref> need even high-quality 3D faces scans. To address the insufficiency of scanned 3D face datasets, some unsupervised or self-supervised methods are proposed <ref type="bibr">[15, 22, 41, 51-53, 55, 56, 62, 68]</ref>, which employ the 2D facial landmark loss between inferred 2D landmarks projected from 3DMM and the ground truth 2D landmarks from images, as well as the render loss between the rendered images from 3DMM and original images. One critical drawback of existing unsupervised methods is that both landmark loss and render loss are measured in projected 2D image space and do not penalize incorrect face pose and depth value of 3DMM, resulting in the ambiguity issue of the 3DMM in the face pose and depth estimation.</p><p>To address this issue, we resort to the multi-view geometry consistency. Multi-view images not only contain 2D landmarks and pixel features but also they form the multi-view geometry constraints. Such training data is publicly available and efficient to acquire (e.g.,videos). Fortunately, a series of multi-view 3D reconstruction techniques named view synthesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> help to formulate self-supervised learning architecture based on multi-view geometry. View synthesis is a classic task that estimates proxy 3D geometry and establishes pixel correspondences among multi-view input images. Then they generate N ? 1 synthetic target view images by compositing image patches from the other N ? 1 input view images. View synthesis is commonly used in Monocular Depth Estimation (MDE) task <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67]</ref>. However, MDE only predicts depth map and relative poses between views without inferring camera intrinsics. The geometry of MDE is incomplete as MDE loses the relationship from 3D to 2D, and they can not reconstruct a full model in scene. MDE also suffers from erroneous penalization due to self-occlusion.</p><p>Inspired by multi-view geometry consistency, we propose a self-supervised Multi-view Geometry Consistency based 3D Face Reconstruction framework (MGCNet). The workflow of MGCNet is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. I t is always considered to be the target of multi-view data. To simplify the following formulation, we denote all N ? 1 views adjacent to the target view as the source views. To build up the multi-view consistency in the training process via view synthesis, we first design a covisible map that stores the mask of covisible pixels for each target-source view pair to solve self-occlusion, as the large and extreme face pose cases is common in the real world, and the self-occlusion always happens in such profile face pose cases. Secondly, we feed the 3DMM coefficients and face poses to the differentiable rendering module <ref type="bibr" target="#b21">[22]</ref>, producing the rendered image, depth map, and covisible map for each view. Thirdly, pixel consistency loss and depth consistency loss are formulated by input images and rendered depth maps in covisible regions, which ensures the consistency of 3DMM parameters in the multi-view training process. Finally, we introduce the facial epipolar loss, which formulates the epipolar error of 2D facial landmarks via the relative pose of two views, as facial landmarks is robust to illumination changes, scale ambiguity, and calibration errors. With these multi-view supervised losses, we are able to achieve accurate 3D face reconstruction and face alignment result on multiple datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69]</ref>. We conduct ablation experiments to validate the effectiveness of covisible map and multi-view supervised losses.</p><p>To summarize, this paper makes the following main contributions: -We propose an end-to-end self-supervised architecture MGCNet for face alignment and monocular 3D face reconstruction tasks. To our best knowledge, we are the first to leverage multi-view geometry consistency to mitigate the ambiguity from monocular face pose estimation and depth reconstruction in the training process. -We build a differentiable covisible map for general view synthesis, which can mitigate the self-occlusion crux of view synthesis. Based on view synthesis, three differentiable multi-view geometry consistency loss functions are proposed as pixel consistency loss, depth consistency loss, and facial epipolar loss. -Our MGCNet result on the face alignment benchmark <ref type="bibr" target="#b68">[69]</ref> shows that we achieve more than a 12% improvement over other state-of-the-art methods, especially in large and extreme face pose cases. Comparison on the challenging 3D Face Reconstruction datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66]</ref> shows that MGCNet outperforms the other methods with the largest margin of 17%.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-view Method</head><p>Recent CNN methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref> train the CNN network supervised by 3D face scan ground truth and achieve impressive results. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> generate synthetic rendered face images with real 3D scans. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref> propose their deep neural networks trained using fitted 3D shapes by traditional methods as substitute labels. Lack of realistic training data is still a great hindrance.</p><p>Recently, some self-supervised or weak-supervised methods are proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68]</ref> to solve the lack of high-quality 3D face scans with robust testing result. Tewari et al. <ref type="bibr" target="#b52">[53]</ref> propose an differentiable rendering process to build unsupervised face autoencoder based on pixel loss. Genova et al. <ref type="bibr" target="#b21">[22]</ref> train a regression network mainly focus on identity loss that compares the features of the predicted face and the input photograph. Nevertheless, face pose and depth ambiguity originated from only monocular images still a limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-view or Video Based Method</head><p>There are established toolchains of 3D reconstruction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58]</ref>, aimming at recovering 3D geometry from multi-view images. One related operation is view synthesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b48">49]</ref>, and the goal is to synthesize the appearance of the scene from novel camera viewpoints.</p><p>Several unsupervised approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref> are proposed recently to address the 3D face reconstruction from multiple images or videos. Deng et al. <ref type="bibr" target="#b14">[15]</ref> perform multi-image face reconstruction from different images by shape aggregation. Sanyal et al. <ref type="bibr" target="#b45">[46]</ref> take multiple images of the same and different person, then enforce shape consistency between the same subjects and shape inconsistency  between the different subjects. Wu et al. <ref type="bibr" target="#b58">[59]</ref> design an impressive multi-view framework (MVFNet), which is view-consistent by design, and photometric consistency is used to generate consistent texture across views. However, MVFNet is not able to generate results via a single input since it relies on multi-view aggregation during inference. Our MGCNet explicitly exploit multi-view consistency (both geometric and photometric) to constrain the network to produce view-consistent face geometry from a single input, which provides better supervision than 2D information only. Therefore, MGCNet improves the performance of face alignment and 3D face reconstruction as Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The inference process of MGCNet takes a single image as input, while in the training process the input is N -view images (e.g. I t?1 , I t , I t+1 for N = 3 ) of the same face and the corresponding ground-truth 2D landmarks q gt t?1 , q gt t , q gt t+1 . Then, the MGCNet estimates the 3DMM coefficients and face poses, whose notations are introduced in Section 3.2 in detail. I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Face model 3D Morphable Model (3DMM) proposed by <ref type="bibr" target="#b6">[7]</ref> is the face prior model of the MGCNet. Specifically, the 3DMM encodes both face shape and texture as</p><formula xml:id="formula_0">S = S(?, ?) = S mean + A id ? + B exp ? T = T (?) = T mean + T id ?,<label>(1)</label></formula><p>where S mean and T mean denote the mean shape and the mean albedo respectively. A id , B exp and T id are the PCA bases of identity, expression and texture. ?, ? ? R 80 and ? ? R 64 are corresponding coefficient vectors to be estimated follow <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. We use S mean , T mean , A id and T id provided by the Basel Face Model (BFM) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>, and B exp from FaceWarehouse <ref type="bibr" target="#b8">[9]</ref>. We exclude the ear and neck region as <ref type="bibr" target="#b14">[15]</ref>, and our final face model contains ? 36K vertices. Camera model The pinhole camera model is employed to define the 3D-2D projection. We assume the camera is calibrated. The face pose P is represented by an euler angle rotation R ? SO(3) and translation t ? R 3 . The relative poses P rel t?s ? SE(3) from the target view to N ? 1 source views are defined as</p><formula xml:id="formula_1">P rel t?s = R ?1 t R s R ?1 t (t s ? t t ) 0 1 .</formula><p>Illumination model To acquire realistic rendered face images, we model the scene illumination by Spherical Harmonics (SH) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> as SH(N re , T |?) = T *</p><formula xml:id="formula_2">B 2 b=1 ? b H b ,</formula><p>where N re is the normal of the face mesh, ? ? R 27 is the coefficient. The H b : R 3 ? R are SH basis functions and the B 2 = 9 (B = 3 bands) parameterizes the colored illumination in red, green and blue channels.</p><p>Finally, we concatenate all the parameters together into a (?, ?, ?, R, t, ?) 257-dimensional vector. All 257 parameters encode the 3DMM coefficients and the face pose, which are abbreviated as coefficient and pose in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">2D Feature Loss</head><p>Inspired by recent related works, we leverage preliminary 2D feature loss functions in our framework. Render loss The render loss aims to minimize the difference between the input face image and the rendered image as L render = 1</p><formula xml:id="formula_3">M M i=1 w i skin ||I i ? I i re ||,</formula><p>where I re is the rendered image, I is the input image, and M is the number of all 2D pixels in the projected 3D face region. w i skin is the skin confidence of i th pixel as in <ref type="bibr" target="#b14">[15]</ref>. Render loss mainly contributes to the albedo of 3DMM. Landmark loss To improve the accuracy of face alignment, we employ the 2D landmark loss which defines the distance between predicted landmarks and ground truth landmarks as</p><formula xml:id="formula_4">L lm = N i=1 c i lm (q i gt ? q i ) 2 ,</formula><p>where N is the number of landmarks, and q the projection of the 3D landmarks picked from our face model. It is noted that the landmarks have different levels of importance, denoted as the confidence c lm for each landmark. We set the confidence to 10 only for the nose and inner mouth landmarks, and to 1 else wise. Identity loss The fidelity of the reconstructed face is an important criterion. We use the identity loss as in <ref type="bibr" target="#b21">[22]</ref>, which is the cosine distance between deep features of the input images and rendered images as L id = ?1??2 |?1||?2| , where ? means the element-wise multiplication. ? 1 and ? 2 are deep features of input images and rendered images. Regularization loss To prevent the face shape and texture parameters from diverging, regularization loss of 3DMM is used as</p><formula xml:id="formula_5">L reg = w id N? i=1 ? 2 +w exp N ? i=1 ? 2 + w tex N? i=1 ? 2 where w id , w exp , w shape are trade-off parameters for 3DMM coef- ficients regularization (1.0, 0.8, 3e?3 by default). N ? , N ? , N ? are the length of 3DMM parameters ?, ?, ?. Final 2D feature loss The combined 2D feature loss function L 2D is defined as L 2D = w render L render +w lm L lm +w id L id +w reg L reg ,</formula><p>where the trade-off parameters for 2D feature losses are set empirically w render = 1.9, w lm = 1e?3, w id = 0.2, w reg = 1e?4. We regard the baseline approach in the later experiement as the model trained by only 2D feature losses. In the followings, we present key ingredients of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Occlusion-Aware View Synthesis</head><p>The key of our idea is to enforce multi-view geometry consistency, so as to achieve the self-supervised training. This could be done via view synthesis, which establishes dense pixel correspondences across multi-view input images. However, the view synthesis for face reconstruction is very easily affected by self-occlusion, as large and extreme face pose cases are common in read world applications. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, assuming a pixel p t is visible in the left cheek as shown in <ref type="figure" target="#fig_3">Figure 2</ref>(a), the correspondence pixel p s could not be found in <ref type="figure" target="#fig_3">Figure 2</ref>(b) due to nose occlusion. Self-occlusion leads to redundant pixel consistency loss and depth consistency loss. Furthermore the related gradient of self-occlusion pixels will be highly affected by the salient redundant error as red part in <ref type="figure">Figure 3</ref> (Pixel Consistency Loss subfigure), which makes the training more difficult. For more practical and useful navigation in real scenarios, self-occlusion is worth to solve. We introduce the covisible maps C s to account for the self-occlusion. Covisible map is a binary mask indicating the pixels which are visible in both source and target views. During the rendering process of the MGCNet, rasterization builds the correspondence between vertices of a triangle and image pixels (V 1,2,3 ? U x , U y ), as shown in <ref type="figure" target="#fig_3">Figure 2</ref>(c). The common vertices visible in two views (i.e., vertices that contribute to pixel rendering) are called covisible points. Then we define all triangles adjacent to covisible points as covisible triangles. Finally, we project covisible triangles of the 3D face from the target view to image space, as shown in <ref type="figure" target="#fig_3">Figure 2(d)</ref>, where the white region is covisible region. The improvement brings from covisible maps is elaborated in <ref type="figure">Figure 3</ref>, pixels are not covisible in the left of the nose in target view (red in <ref type="figure">Figure 3</ref>), which result in redundant error. The quantitative improvements are discussed in Section 4.5.</p><p>To generate the synthetic target RGB images from source RGB images, we first formulate the pixel correspondences between view pairs (I s , I t ). Given a pair correspondence pixel coordinate p t , p s in I t , I s , the pixel value p s is computed by bilinear-sampling <ref type="bibr" target="#b28">[29]</ref>, and the pixel coordinate p s is defined as</p><formula xml:id="formula_6">ps ? Ks[P rel t?s ]Dt(pt)K ?1 t pt,<label>(2)</label></formula><p>where ? represents the equality in the homogeneous coordinates, K s and K t are the intrinsics for the input image pairs, D t is the rendered depth map of the target view, and D t (p t ) is the depth for this particular pixel p t in D t . <ref type="figure">Fig. 3</ref>. The view synthesis results with and without covisible map. Without covisiable map, the pixel consistency loss is highly affected by self-occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pixel Consistency Loss</head><p>We generate the synthesized target images by view synthesis, then we minimize the pixel error between the target view and the synthesized target views from the source views as</p><formula xml:id="formula_7">L pixel = 1 |Cs| |Cs| i=1 C i s * I s t (i) ? It(i) ,<label>(3)</label></formula><p>where I s t represents the synthesized target views from the source views. I t (i) is the i ? th pixel value . Concretely, the first term I s t is the bilinear-sampling operation, which computes the corresponding pixel coordinates using the relative pose P rel t?s and target depth map D re t . C s is covisible map and |C s | denotes the total number of covisible pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Dense Depth Consistency Loss</head><p>Compared to RGB images, depth maps are less adversely affected by the gradient locality issue <ref type="bibr" target="#b4">[5]</ref>. Thus, we propose a dense depth consistency loss function which contributes to solving depth ambiguity more explicitly, enforcing the multi-view consistency upon depth maps. Similarly, we synthesize the target depth maps D s t from the source views via bilinear interpolation, and compute the consistency against the target depth map D t .</p><p>One critical issue is that the face region is cropped in the face detection datapreprocessing stage, making the depth value up to scale. To tackle this issue, we compute a ratio of two depth maps S depth and rectify the scale of depth maps. Therefore, we define the dense depth consistency loss as</p><formula xml:id="formula_8">S depth = |Cs| i=1 Dt(i)Cs(i) |Cs| i=1 D s t (i)Cs(i) L depth = 1 |Cs| |Cs| i=1 S depth ? D s t (i) ? Dt(i) ,<label>(4)</label></formula><p>where C s (i), D t (i) are the i ? th covisible and depth value. S depth is the depth scale ratio. Our experiment shows that the multi-view geometry supervisory signals significantly improve the accuracy of the 3D face shape. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Facial epipolar Loss</head><p>We use facial landmarks to build the epipolar consistency, as our epipolar loss is based on sparse ground-truth 2D facial landmarks, which is less likely to be affected by radiometric or illumination changes compared to pixel consistency or depth consistency losses, The epipolar loss in the symmetric epipolar distance <ref type="bibr" target="#b24">[25]</ref> form between source and target 2D landmark q t?s = {p ? p } is defined as</p><formula xml:id="formula_9">Lepi(q|R, t) = ?(p,p )?q p T Ep (Ep) 2 (1) + (Ep) 2 (2)<label>(5)</label></formula><p>where E being the essential matrix computed by E = [t] ? R, [?] ? is the matrix representation of the cross product with t. We simply omit the subindices for conciseness (q for q t?s , R for R t?s , t for t t?s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Combined Loss</head><p>The final loss function L for our MGCNet is the combination of 2D feature loss and multi-view geometry loss. Training the network by only 2D feature losses leads to face pose and depth ambiguity, which is reflected in geometry inconsistency as the large epipolar error shown in <ref type="figure" target="#fig_4">Figure 4</ref>(a). Our MGCNet trained with pixel consistency loss, dense depth consistency, and facial epipolar loss shows remarkable improvement in <ref type="figure" target="#fig_4">Figure 4(b)</ref>, which outstands our novel multi-view geometry consistency based self-supervised training pipeline. Finally, the combined loss function is defined as</p><formula xml:id="formula_10">L =w2D * L2D + w mul * [w pixel * L pixel + w depth * L depth + wepi * Lepi],<label>(6)</label></formula><p>where w 2D = 1.0 and w mul = 1.0 balance the weights between the 2D feature loss for each view and the multi-view geometry consistency loss. The trade-off parameters to take into account are w pixel = 0.15, w depth = 1e?4, w epi = 1e?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We evaluate the performance of our MGCNet on the face alignment and 3D face reconstruction tasks which compared with the most recent state-of-the-art methods <ref type="bibr">[6, 8, 15, 18, 22, 46, 51-53, 57, 68, 69]</ref> on diverse test datasets including AFLW20003D <ref type="bibr" target="#b68">[69]</ref>, MICC Florence <ref type="bibr" target="#b2">[3]</ref>, Binghamton University 3D Facial Expression (BU-3DFE) <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref>, and FRGC v2.0 <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Data 300W-LP <ref type="bibr" target="#b68">[69]</ref> has multi-view face images with fitted 3DMM model, the model is widely used as ground truth in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>, such multi-view images provide better supervision than only 2D features. Multi-PIE <ref type="bibr" target="#b22">[23]</ref> are introduced to provide multi-view face images that help solve face pose and depth ambiguity. As multi-view face datasets are always captured indoor, and thus cannot provide diversified illumination and background for training, CelebA <ref type="bibr" target="#b33">[34]</ref> and LS3D <ref type="bibr" target="#b7">[8]</ref> are used as part of training data, which only contribute to 2D feature losses. Detail data process can be found in the suppl. material. Network We use the ResNet50 <ref type="bibr" target="#b25">[26]</ref> network as the backbone of our MGCNet, we only convert the last fully-connected layer to 257 neurons to match the dimension of 3DMM coefficients. The pre-trained model from ImageNet <ref type="bibr" target="#b44">[45]</ref> is used as an initialization. We only use N = 3 views in practice, as N = 5 views lead to a large pose gap between the first view and the last view. We implement our approach by Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The training process is based on Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with a batch size of 5. The learning rate is set to 1e?4, and there are 400K total iterations for the whole training process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Result</head><p>Result in different situations Our MGCNet allows for high-quality reconstruction of facial geometry, reflectance and incident illumination as <ref type="figure" target="#fig_5">Figure 5</ref>, under full range of lighting, pose, and expressions situations. Geometry We evaluate the qualitative results of our MGCNet on AFLW20003D <ref type="bibr" target="#b68">[69]</ref>. First, we compare our MGCNet with 3DDFA <ref type="bibr" target="#b68">[69]</ref>, RingNet <ref type="bibr" target="#b45">[46]</ref>, PRN <ref type="bibr" target="#b17">[18]</ref>, and Deng et al. <ref type="bibr" target="#b14">[15]</ref> on front view samples, as Row 1 and Row 2 in <ref type="figure">Figure 6</ref>. Our predicted 3DMM coefficients produce more accurate results than the most methods, and we get comparable results with Deng et al. <ref type="bibr" target="#b14">[15]</ref>. For these large and extreme pose cases as Row 3-6 in <ref type="figure">Figure 6</ref>, our MGCNet has better face alignment and face geometry than other methods. We have more vivid emotion in Row 4 of <ref type="figure">Figure 6</ref>, and the mouths of our result in Row 3,5 have obviously better shape than 3DDFA <ref type="bibr" target="#b68">[69]</ref>, RingNet <ref type="bibr" target="#b45">[46]</ref>, PRN <ref type="bibr" target="#b17">[18]</ref>, and Deng et al. <ref type="bibr" target="#b14">[15]</ref>. Besides, the face alignment results from Row 3 to Row 6 support that we achieve better face pose estimation, especially in large and extreme pose cases. Texture, illumination shadings We also visualize our result under geometry, texture, illumination shadings, and notice that our approach performs better than Tewari18 et al. <ref type="bibr" target="#b51">[52]</ref> and Tewari19 et al. <ref type="bibr" target="#b50">[51]</ref>, where the overlay result is very similar to the input image as <ref type="figure" target="#fig_6">Figure 7</ref>(a). Further result and analysis about the result can be found in the suppl. material.</p><p>MGCNet does not focus on the appearance of 3DMM as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref>, which is only constrained by render loss. However, our multi-view geometry supervision can help render loss maximize the potential during training by accurate face alignment and depth value estimation. This makes MGCNet able to handle 3DMM texture and illumination robustly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">2D Face Alignment</head><p>The quantitative comparison of 6Dof pose is not conducted due to different camera intrinsic assumptions of different methods. Therefore, to validate that our MGCNet can mitigate the ambiguity of monocular face pose estimation, we evaluate our method on AFLW2000-3D, and compare our result with Zhu  <ref type="bibr" target="#b17">[18]</ref> (PRN). Normalized mean error (NME) is used as the evaluation metric, and the bounding box size of ground truth landmarks is deemed as the normalization factor. As shown in <ref type="table">Table 1</ref>(a) Column 5, our result outperforms the best method with a large margin of 12% improvement. Qualitative results can be found in the suppl. material. Learning face pose from 2D features of monocular images leads to face pose ambiguity, the results of large and extreme face pose test samples suffer from this heavily. As the supervision of large and extreme face pose case is even less, which is not enough for training monocular face pose regressor. Our MGCNet provides further robust and dense supervision by multi-view geometry for face alignment in both frontal and profile face pose situations. The comparison in <ref type="table">Table 1</ref>(a) corroborates our point that the compared methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref> obviously degrades when the yaw angles increase from <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b59">60)</ref> to <ref type="bibr" target="#b59">(60,</ref><ref type="bibr">90)</ref> in Column 4 of Table 1(a). We also conduct an ablation study that our MGCNet outperforms the baseline, especially on large and extreme pose case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">3D Face Reconstruction</head><p>MICC Florence with Video MICC Florence provides videos of each subject in cooperative, indoor and outdoor scenarios. For a fair comparison with Genova et al. <ref type="bibr" target="#b21">[22]</ref>, Trans et al. <ref type="bibr" target="#b56">[57]</ref> and Deng et al. <ref type="bibr" target="#b14">[15]</ref>, we calculate error with the average shape for each video in different scenarios. Following <ref type="bibr" target="#b21">[22]</ref>, we crop the ground truth mesh to 95mm around the nose tip and run iterative closest point (ICP) algorithm for rigid alignment. The results of <ref type="bibr" target="#b56">[57]</ref> only contain part of the forehead region. For a fair comparison, we process the ground-truth meshes similarly. We use the point-to-plane root mean squared error(RMSE) as the evaluation metric. We compare with the methods of Zhu et al. <ref type="bibr" target="#b68">[69]</ref> (3DDFA), Sanyal et al. <ref type="bibr" target="#b45">[46]</ref> (RingNet), Feng et al. <ref type="bibr" target="#b17">[18]</ref> (PRN), Genova et al. <ref type="bibr" target="#b21">[22]</ref>, Trans et al. <ref type="bibr" target="#b56">[57]</ref> and Deng et al. <ref type="bibr" target="#b14">[15]</ref>. Table 1(b) shows that our method outperforms state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b68">69]</ref> on all three scenarios. MICC Florence with Rendered Images Several current methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b59">60]</ref> also generate rendered images as test input. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b59">60]</ref>, we render face images of each subject with 20 poses: a pitch of -15, 20 and 25 degrees, yaw angles of -80, -60, 0, 60 and 80 degrees, and 5 random poses. We use the point-to-plane RMSE as the evaluation metric, and we process the ground truth mesh as above. <ref type="figure" target="#fig_6">Figure 7(b)</ref> shows that our method achieves a significant improvement of 17% higher than the state-of-the-art methods.</p><p>The plot also shows that our MGCNet performs obvious improvement on the extreme pose setting x ? axis[?80, 80] in <ref type="figure" target="#fig_6">Figure 7</ref>(b). As we mitigate both pose and depth ambiguity by multi-view geometry consistency in the training process. Extreme pose sample benefits from this more significantly, since the extreme pose input images have even less 2D features. Profile face case contains more pronounced depth info (eg. bridge of the nose), where large error happen. FRGC v2.0 Dataset FRGC v2.0 is a large-scale benchmark includes 4007 scans. We random pick 1335 scans as test samples, then we crop the ground truth mesh to 95mm around the nose tip. We first use 3D landmark as correspondence to align the predict and ground truth result, then ICP algorithm is used as fine alignment. Finally, point-to-point mean average error (MAE) is used as the evaluation metric. We compare with the methods of Galteri et al. <ref type="bibr" target="#b20">[21]</ref> (D3R), 3DDFA <ref type="bibr" target="#b68">[69]</ref>, RingNet <ref type="bibr" target="#b45">[46]</ref>, PRN <ref type="bibr" target="#b17">[18]</ref>, and Deng et al. <ref type="bibr" target="#b14">[15]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BU-3DFE Dataset</head><p>We evaluate our method on the BU-3DFE dataset following <ref type="bibr" target="#b50">[51]</ref>. Following <ref type="bibr" target="#b50">[51]</ref>, a pre-computed dense correspondence map is used to calculate a similarity transformation from predict mesh to the original groundtruth 3D mesh, and help to calculate the point-to-point RMSE. From <ref type="table" target="#tab_1">Table  2</ref>(b), the reconstruction error of our method is lower than the current state-ofart methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>. Our MGCNet achieves better performance by using the multi-view geometry consistency loss functions in the training phase. Qualitative results can be found in the suppl. material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study</head><p>To validate the efficiency of our multi-view geometry consistency loss functions. We conduct ablation studies for each component on the MICC Florence dataset <ref type="bibr" target="#b2">[3]</ref>, as shown in <ref type="table" target="#tab_3">Table 3</ref>. The ablation study mainly focuses on the proposed multi-view geometry consistency loss functions. Firstly, we deem the baseline method as the model trained with only 2D feature losses, as in Row 1. Secondly, we add our pixel consistency loss, dense depth consistency loss, and epipolar loss to the baseline in Row 2, Row 3 and Row 4. It shows that these losses help produce lower reconstruction errors than the baseline, even when they are used separately. Thirdly, comparing from Row 5 to Row 7, we combine multiple multi-view geometry loss functions and achieve state-of-the-art results, which demonstrates the effectiveness of the proposed self-supervised learning pipeline. Finally, comparing from Row 6 to Row 7, we prove that our novel covisible map to solve self-occlusion in view synthesis algorithm can help training a more accurate model. The qualitative ablation study is in the suppl. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a self-supervised pipeline MGCNet for monocular 3D Face reconstruction and demonstrated the advantages of exploiting multi-view geometry consistency to provide more reliable constraint on face pose and depth estimation. We emphasize on the occlusion-aware view synthesis and multi-view losses to make the result more robust and reliable. Our MGCNet profoundly reveals the capability of multi-view geometry consistency self-supervised learning in capturing both high-level cues and feature correspondences with geometry reasoning. The results compared to other methods indicate that our MGCNet can achieve the outstanding result without costly labeled data. Our further investigations will focus on multi-view or video-based 3D face reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>This supplementary document provides detailed evaluation results that are supplementary to the main paper. We propose a self-supervised Multi-view Geometry Consistency based 3D Face Reconstruction framework (MGCNet), which helps mitigate the monocular face pose and depth ambiguity. Firstly, we propose the detailed data pre-process pipeline in Section 2, then we introduce the quantitative evaluation datasets in Section 3. Secondly, we introduce the morphable model and highlight that our MGCNet is a general framework in Section 4. Thirdly, we evaluate the quantitative result by render error between the input image and rendered image in Section 5.1 and we show the qualitative ablation study in Section 5.2. Furthermore, we show further comparison with Tewari19 <ref type="bibr" target="#b50">[51]</ref> under geometry, texture and lighting in Section 5.3, then we conduct further comparison with some methods on the in the wild images in Section 5.4. Finally, we demonstrate the qualitative comparisons against other methods on MICC Florence dataset <ref type="bibr" target="#b2">[3]</ref> in Section 5.5 and we also demonstrate some results from AFLW20003D <ref type="bibr" target="#b68">[69]</ref> in Section 5.6, which further certify our MGCNet performs accurate result on face alignment task. Then we show the qualitative result on BU-3DFE dataset <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref> in Section 5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocess</head><p>The images are automatically annotated by the 2D landmark detection method in <ref type="bibr" target="#b7">[8]</ref> and the face detection method in <ref type="bibr" target="#b64">[65]</ref>. We filter the face pose, face attribution, low-resolution images, and blurred images and obtain ?390K face images from the above four datasets as our training set. The images are scaled to a resolution of 224 ? 224.</p><p>The multi-view images of the training dataset are captured with a consistent lighting condition across views. Theoretically, the photometric consistency will be violated if the lighting across views is dramatically different. Our MGCNet shares the same property with multi-view stereo methods that require overlap across views, this also the reason that we only use N = 3 views in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Quantitative Evaluation Dataset</head><p>AFLW20003D is constructed to evaluate face alignment on challenging in the wild images. This database contains the first 2000 images from AFLW <ref type="bibr" target="#b30">[31]</ref> with landmarks annotations. We use this database to evaluate the performance of our method on face alignment tasks <ref type="bibr" target="#b68">[69]</ref>. MICC Florence is a 3D face dataset that contains 53 faces with their ground truth High-resolution 3D scans of human faces are acquired from a structuredlight scanning system from each subject with several video sequences of varying resolution, conditions and zoom level <ref type="bibr" target="#b2">[3]</ref>. FRGC v2.0 includes 4007 scans of 466 individuals acquired with the frontal view from the shoulder level, with very tiny pose variations. About 60% of the faces have neutral expression, while the others show spontaneous expressions of disgust, happiness, sadness, and surprise <ref type="bibr" target="#b36">[37]</ref>. Scans are given as matrices of 3D points of size <ref type="bibr">[480,</ref><ref type="bibr">640]</ref>, with a binary mask indicating the valid points of the face (about 40 K on average). BU-3DFE BU-3DFE database includes 100 subjects with 2500 facial expression models. The database presently contains 100 subjects (56% female, 44% male), ranging age from 18 years to 70 years old, with a variety of ethnic/racial ancestries, including White, Black, East-Asian, Middle-east Asian, Indian, and Hispanic Latino. Each subject performed seven expressions in front of the 3D face scanner. With the exception of the neutral expression, each of the six prototypic expressions (happiness, disgust, fear, angry, surprise and sadness) includes four levels of intensity <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D Morphable Model</head><p>Blanz and Vetter <ref type="bibr" target="#b6">[7]</ref> introduce the 3D morphable model (3DMM). 3DMM benefits the 3D face reconstruction by constraining the solution space, thereby simplifying the problem. In this paper, our goal is to estimate 3DMM parameters from a single photograph.</p><p>We conduct our experiments with 3DMM model since it is still a general method that widely used by single-image based latest methods (as in works of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>), and we have proved the superiority of our method over theirs under a fair comparison, as shown qualitatively in the main paper as well as our quantitative result.</p><p>As we have clarified in the main paper, our method is focused on improving single-view reconstruction quality via multi-view consistency, our proposed framework is general and is not limited to any specific face model. We believe other face models with better representation ability <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> can easily plug into our proposed MGCNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further Evaluation Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CelebA Dataset</head><p>We evaluate the photometric error of our approach by heat maps on the CelebA dataset <ref type="bibr" target="#b33">[34]</ref> as shown in <ref type="figure" target="#fig_1">Figure 1</ref> that these images are only used for testing and visualization. We achieve low pixel error, which benefits from using multi-view geometry consistency. This also demonstrates better reconstruction capabilities of our MGCNet to in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Alation Study</head><p>To evaluate the effects of multi-view geometry consistency losses on the quality of the reconstructed meshes. We conduct ablation studies on the MICC Florence 3D Face dataset <ref type="bibr" target="#b2">[3]</ref>, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. We calculate the point-to-plane root mean squared error, and normalize the error to a heatmap. This heatmap indicate that the major improvements regions are jaw, nose and cheekbones region in frontal case, and eye contour, nose, cheekbones regions for the large-pose case. Face geometry (especially in large pose cases), as well as better 3D pose estimation results, are the major improvements bring by our method, thanks to our multi-view geometry constraints that explicitly regularizes the geometry across different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Baseline Ours Input Baseline Ours <ref type="figure" target="#fig_3">Fig. 2</ref>. Quantitative evaluation of point-to-plane root mean squared error as the error map format on the MICC Florence 3D Face dataset <ref type="bibr" target="#b2">[3]</ref>. The error map range is [0, 8.29].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Texture, illumination shadings</head><p>We compare our MGCNet with Tewari19 <ref type="bibr" target="#b50">[51]</ref> as <ref type="figure">Figure 3</ref>. The result of Tewari19 <ref type="bibr" target="#b50">[51]</ref> results of geometry are visually more detail since they use a face representation more complicated than 3DMM used by our method. However, it is hard to say Tewari19 <ref type="bibr" target="#b50">[51]</ref> has better geometry results since our method does have a better quantitative result shown in the main paper. The texture model used in Tewari19 <ref type="bibr" target="#b50">[51]</ref> is also different from 3DMM. Despite that, better geometry generated by our method leads to better texture via the render loss used, which can also support the validity of our MGCNet. As our method is focused on improving the reconstruction quality via multi-view consistency. Our MGCNet is a general system that is not limited to any specific face model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">In the wild data</head><p>Secondly, we visualize our result under geometry overlay situation compared with Richardson et al. <ref type="bibr" target="#b40">[41]</ref>, Sela et al. <ref type="bibr" target="#b49">[50]</ref>, Tewari17 et al. <ref type="bibr" target="#b52">[53]</ref>, Tewari19 et al. <ref type="bibr" target="#b50">[51]</ref> and RingNet et al. <ref type="bibr" target="#b45">[46]</ref>, and we notice that our approach performs better than methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. We also show some detail intermediate result about <ref type="figure" target="#fig_5">Figure 5</ref> in the main paper in <ref type="figure" target="#fig_6">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MICC Florence Dataset</head><p>Firstly, we compare our MGCNet with Zhu et al. <ref type="bibr" target="#b68">[69]</ref> (3DDFA), Sanyal et al. <ref type="bibr" target="#b45">[46]</ref> (RingNet), Feng et al. <ref type="bibr" target="#b17">[18]</ref> (PRN), and Deng et al. <ref type="bibr" target="#b14">[15]</ref>. For each sample in MICC Florence, we pick both front face images and large face pose images as test data. We show the geometry overlay of the reconstruction result, which we achieve more accurate results than the most methods, and we get better results than Deng et al. <ref type="bibr" target="#b14">[15]</ref> in the large pose case as <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">AFLW20003D Dataset</head><p>AFLW20003D is constructed by <ref type="bibr" target="#b68">[69]</ref> to evaluate face alignment. Since the images are captured in the wild and show large variations in pose and appearance, which is a challenging 3D face alignment dataset. We use this database to evaluate the performance of our method on face alignment tasks.</p><p>As a supplementary to the quantitative evaluation in the main paper, we first demonstrate some results even better than the ground truth from AFLW20003D <ref type="bibr" target="#b68">[69]</ref> in <ref type="figure">Figure 6</ref>. Besides, we also show our result that performs accurate face alignment results, where red lines are predicted landmarks by our method, white lines are ground truth from <ref type="bibr" target="#b68">[69]</ref>.</p><p>Furthermore, we visual our MGCNet result from multiple viewpoints in <ref type="figure" target="#fig_8">Figure 8</ref> on AFLW20003D <ref type="bibr" target="#b68">[69]</ref>, which shows that we get vivid reconstruction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">BU-3DFE Dataset</head><p>We present more qualitative reconstruction results of our MGCNet on BU-3DFE dataset <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref>. In <ref type="figure">Figure 9</ref>, we show six samples with various expressions, the reconstructed 3D face showed with face pose, texture, geometry and illumination. This quantitative evaluation of our geometry reconstruction on the BU-3DFE dataset <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref> shows that our MGCNet can even handle different expression situation. For the prediction of albedo and lighting, while the texture quality would be benefited from better geometry implicitly via the render loss, the ambiguity in illumination and face albedo is an intrinsic issue due to the problems nature, and our SH lighting is RGB-channel, limit the SH lighting to one channel will help. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our result</head><p>Better than GT <ref type="figure">Fig. 6</ref>. Examples from AFLW20003D dataset <ref type="bibr" target="#b68">[69]</ref> show that our predictions are more accurate than ground truth in some cases. <ref type="figure" target="#fig_6">Fig. 7</ref>. Face reconstruction results under texture, geometry and illumination of our method on AFLW20003D <ref type="bibr" target="#b68">[69]</ref> and CelebA <ref type="bibr" target="#b33">[34]</ref>. Input Overlay Texture Geometry Illumination <ref type="figure">Fig. 9</ref>. Our accurate result in face pose, texture, geometry and illumination on BU-3DFE dataset <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The training flow of our MGCNet architecture, which is annotated in Section 3.1. The 2D feature loss part is our baseline in Section 3.3. Our novel multi-view geometry consistency loss functions are highlighted as * in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>target images and the depth maps from the source views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>(a) and (b) are the target view and source view pair; (c) is the covisible points and triangles; (d) is the covisible map; and (e) is the synthetic target view</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Eipipolar error from target to source views, (a) is the epipolar visualization from baseline network trained by 2D feature loss; and (b) is epipolar visualization of the MGCNet. The red, green and blue point means left ear bound, lower jaw and nose tip landmarks. The epipolar error of baseline is significantly larger than MGCNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>A few results on a full range of lighting, pose, including large expressions. Each image pair is input image (left) and reconstruction result overlay (right). Further detail result (shape, albedo, and lighting) can be found in the suppl. material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>(a)Comparison to Tewari18 et al.<ref type="bibr" target="#b51">[52]</ref> and Tewari19 et al.<ref type="bibr" target="#b50">[51]</ref>. Our MGC-Net trained by multi-view consistency loss outperforms Tewari's results in face pose, illumination and geometry. Further result can be found in the suppl. material. (b) Comparison with 3DDFA<ref type="bibr" target="#b68">[69]</ref>, RingNet<ref type="bibr" target="#b45">[46]</ref>, PRN<ref type="bibr" target="#b17">[18]</ref>, and Deng et al.<ref type="bibr" target="#b14">[15]</ref> on MICC Florence rendered images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>Quantitative evaluation of photometric error on the CelebA [34] dataset. The error map range is [0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Face reconstruction results of our method on AFLW20003D<ref type="bibr" target="#b68">[69]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.64 2.23 ? 0.49 2.22 ? 0.56 Sanyal et al. [46] 2.33 ? 0.43 2.19 ? 0.43 2.07 ? 0.45 Feng et al. [18] 2.30 ? 0.54 2.02 ? 0.50 2.10 ? 0.60 Tran et al. [57] 2.00 ? 0.55 2.05 ? 0.51 1.95 ? 0.51 Genova et al. [22] 1.87 ? 0.61 1.86 ? 0.60 1.87 ? 0.57 Deng et al. [15] 1.83 ? 0.59 1.78 ? 0.53 1.78 ? 0.59 Ours 1.73 ? 0.48 1.78 ? 0.47 1.75 ? 0.47 et al. [69] (3DDFA), Bulat and Tzimiropoulos [8] (3D-FAN), Bhagavatula et al. [6] (3DSTN), Zhou et al. [68] (CMD), and Feng et al.</figDesc><table><row><cell>Input</cell><cell cols="2">3DDFA</cell><cell></cell><cell>PRN</cell><cell>Deng</cell><cell>RingNet</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Front</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Large Pose</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell>Method</cell><cell cols="4">0 to 30 30 to 60 60 to 90 Mean</cell><cell>Method</cell><cell>Cooperative</cell><cell>Indoor</cell><cell>Outdoor</cell></row><row><cell>3DDFA [69]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell><cell>Zhu et al. [69]</cell><cell>2.69</cell></row><row><cell>3D-FAN [8]</cell><cell>3.61</cell><cell>4.34</cell><cell>6.87</cell><cell>4.94</cell><cell></cell><cell></cell></row><row><cell>3DSTN [6]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell><cell></cell><cell></cell></row><row><cell>CMD [68]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.98</cell><cell></cell><cell></cell></row><row><cell>PRN [18]</cell><cell>2.75</cell><cell>3.55</cell><cell>5.11</cell><cell>3.62</cell><cell></cell><cell></cell></row><row><cell>Ours+BL</cell><cell>2.75</cell><cell>3.28</cell><cell>4.31</cell><cell>3.45</cell><cell></cell><cell></cell></row><row><cell cols="2">Ours+MGCNet 2.72</cell><cell>3.12</cell><cell>3.76</cell><cell>3.20</cell><cell></cell><cell></cell></row></table><note>Fig. 6. Comparisons with 3DDFA [69], RingNet [46], PRN [18], and Deng et al. [15] on ALFW20003D. Table 1. (a)Performance comparison on AFLW2000-3D (68 landmarks). The normal- ized mean error (NME) for 2D landmarks with different yaw angles is reported. The first best result is highlighted in bold. (b) Average and standard deviation root mean squared error (RMSE) with mm in three environments of MICC Florence.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>(a) shows that our method outperforms these state-of-the-art methods. Our MGCNet performs higher fidelity and accurate result on both frontal and profile face pose view.</figDesc><table><row><cell>Input</cell><cell>Overlay</cell><cell>Texture</cell><cell>Geometry</cell><cell>Illumination</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.4 2.6</cell><cell></cell><cell>Ours: 1.69 PRN: 2.33 Deng: 2.03 3DDFA: 2.04 RingNet: 2.04</cell><cell></cell></row><row><cell>Tewari19</cell><cell></cell><cell></cell><cell></cell><cell>Mean RSME</cell><cell>2.0 2.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tewari18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.8</cell><cell>80</cell><cell>60</cell><cell>40</cell><cell>20 Yaw rotation in degree 0 20</cell><cell>40</cell><cell>60</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>(a) Comparison with D3R<ref type="bibr" target="#b20">[21]</ref>, 3DDFA<ref type="bibr" target="#b68">[69]</ref>, RingNet<ref type="bibr" target="#b45">[46]</ref>, PRN<ref type="bibr" target="#b17">[18]</ref>, and Deng et al.<ref type="bibr" target="#b14">[15]</ref> with MAE of mm on FRGC v2.0 dataset. (b) Mean and standard deviation point-to-point RMSE with mm on the BU-3DFE dataset<ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66]</ref> compared with Tewari17 et al.<ref type="bibr" target="#b52">[53]</ref>, Tewari18 et al.<ref type="bibr" target="#b51">[52]</ref>, Tewari19 et al.<ref type="bibr" target="#b50">[51]</ref>, Deng et al.<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell cols="2">Method [21] [18] [46]</cell><cell>[69] [15] Ours</cell><cell cols="5">Method [53] [52] Fine [52] Coarse [51] [15] Ours</cell></row><row><cell cols="3">MAE 3.63 2.33 2.22 2.21 2.18 1.93</cell><cell cols="2">Mean 3.22</cell><cell>1.83</cell><cell>1.81</cell><cell>1.79 1.63 1.55</cell></row><row><cell>Time</cell><cell cols="2">-9.8ms 2.7ms 75.7ms 20ms 20ms</cell><cell>Std</cell><cell>0.77</cell><cell>0.39</cell><cell>0.47</cell><cell>0.45 0.33 0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of different training loss configurations. The ablation study performance of MGCNet is evaluated on MICC Florence 3D Face dataset<ref type="bibr" target="#b2">[3]</ref> by RMSE.</figDesc><table><row><cell></cell><cell>Loss Configuration</cell><cell></cell><cell></cell><cell cols="3">MICC Florence video</cell></row><row><cell cols="7">2D feature Pixel Consistency Depth Consistency Epipolar Covisible map Cooperative Indoor Outdoor</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.83</cell><cell>1.82</cell><cell>1.81</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>1.80</cell><cell>1.80</cell><cell>1.80</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>1.77</cell><cell>1.79</cell><cell>1.80</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1.79</cell><cell>1.81</cell><cell>1.77</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>1.76</cell><cell>1.81</cell><cell>1.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>1.80</cell><cell>1.81</cell><cell>1.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.73</cell><cell>1.78</cell><cell>1.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Quantitative evaluation compare with Tewari19 [51]. Comparison with Richardson et al. [41], Sela et al. [50], Tewari17 et al. [53], Tewari19 et al. [51] and RingNet et al.<ref type="bibr" target="#b45">[46]</ref>. Our MGCNet trained by multi-view consistency loss outperforms these state-of-the-art methods in face reconstruction geometry</figDesc><table><row><cell>Tewari19 Ours Tewari19 Ours Tewari19 Ours Tewari19 Ours Fig. 3. Input Input Richardson Fig. 5. Input Overlay</cell><cell>Overlay Sela Geometry</cell><cell>Texture Tewari18 Tewari19 Geometry Input Overlay</cell><cell>Illumination RingNet Ours Geometry</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Inverse rendering of faces with a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1080" to="1093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2072572.2072597</idno>
		<ptr target="http://doi.acm.org/10.1145/2072572.2072597" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding. p. 7980. J-HGBU 11</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding. p. 7980. J-HGBU 11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fitting a 3d morphable model to edges: A comparison between hard and soft correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="377" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical model-based motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hingorani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="237" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Siggraph</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time facial animation with image-based dynamic avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photo-realistic facial details synthesis from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9429" to="9439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">View interpolation for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 20th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling and rendering architecture from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view 3d face reconstruction with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end 3d face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5908" to="5917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-based rendering using imagebased priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards internet-scale multiview stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1434" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep 3d morphable model refinement via progressive growing of conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1294" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangling features in 3d face shapes for joint face reconstruction and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5216" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and egomotion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A signal-processing framework for inverse rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1259" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unconstrained 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2606" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive 3d face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4197" to="4206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7763" to="7772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">View morphing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fml: face model learning from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10812" to="10822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3935" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards high-fidelity nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5163" to="5172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Visualsfm: A visual structure from motion system</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mvfnet: Multi-view 3d face morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mmface: A multimetric regression network for unconstrained face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7663" to="7672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th international conference on automatic face and gesture recognition (FGR06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Self-supervised adaptation of highfidelity face models for monocular performance tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Df2net: A dense-fine-finer network for detailed 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scaleinvariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dense 3d face decoding over 2500fps: Joint texture &amp; shape convolutional mesh decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1097" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
	<note>Input 3DDFA PRN Deng RingNet Ours Fig. 4. Comparison with Zhu et al. [69] (3DDFA). RingNet. and Deng et al. [15</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
