<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Early-Learning Correction for Segmentation from Noisy Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiu</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NYU Courant Institute of Mathematical Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Early-Learning Correction for Segmentation from Noisy Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately annotated data. We observe a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medicalimaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental problem in computer vision. The goal is to assign a label to each pixel in an image, indicating its semantic category. Deep learning models based on convolutional neural networks (CNNs) achieve state-of-the-art performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b46">51,</ref><ref type="bibr" target="#b60">65]</ref>. These models are typically trained in a supervised fashion, which requires pixel-level annotations. Unfortunately, gathering pixel-level annotations is very costly, and may require significant domain expertise in some applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr">48]</ref>. <ref type="bibr">*</ref> The first two authors contribute equally, order decided by coin flipping. <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/Kangningthu/ ADELE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Baseline Baseline+ADELE <ref type="figure">Figure 1</ref>. Visualization of the segmentation results of the baseline method SEAM <ref type="bibr" target="#b47">[52]</ref> and the baseline combined with the proposed ADaptive Early-Learning corrEction (ADELE). Our proposed ADELE improves segmentation quality. More examples can be found in Appendix A.1.</p><p>Furthermore, annotation noise is inevitable in some applications. For example, in medical imaging, segmentation annotation may suffer from inter-reader annotation variations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b58">63]</ref>. Learning to perform semantic segmentation from noisy annotations is thus an important topic in practice. Prior works on learning from noisy labels focus on classification tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b52">57]</ref>. There are comparatively fewer works on segmentation, where existing works focus on designing noise-robust network architecture <ref type="bibr" target="#b45">[50]</ref> or incorporating domain specific prior knowledge <ref type="bibr" target="#b42">[42]</ref>. We instead focus on improving the performance in a more general perspective by studying the learning dynamics. We observe that the networks tend to first fit the clean annotations during an "early-learning" phase, before eventually memorizing the false annotations, thus jeopardizing generalization performance. This phenomenon has been reported in the context of classification <ref type="bibr" target="#b32">[33]</ref>. However, this phenomenon in semantic segmentation differs significantly from its counterpart in classification in the following ways:</p><p>? The noise in segmentation labels is often spatially dependent. Therefore, it is beneficial to leverage spatial information during training. <ref type="figure">Figure 2</ref>. A prevailing pipeline for training WSSS. We aim to improve the segmentation model from noisy annotations.</p><p>? In semantic segmentation, early learning and memorization do not occur simultaneously for all semantic categories due to pixel-wise imbalanced labels. Previous methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref> in noisy label classification often assume class balanced data and thus either detecting or handling wrong labels for different classes at the same time.</p><p>? The annotation noise in semantic segmentation can be ubiquitous (all examples have some errors) while the stateof-the-art methods in classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">67]</ref> assume that some samples are completely clean.</p><p>Inspired by these observations, we propose a new method, ADELE (ADaptive Early-Learning corrEction), that is designed for segmentation from noisy annotations. Our method detects the beginning of the memorization phase by monitoring the Intersection over Union (IoU) curve for each category during training. This allows it to adaptively correct the noisy annotations in order to exploit early-learning for individual classes. We also incorporate a regularization term to promote spatial consistency, which further improves the robustness of segmentation networks to annotation noise.</p><p>To verify the effectiveness of our method, we consider a setting where noisy annotations are synthesized and controllable. We also consider a practical setting -Weakly-Supervised Semantic Segmentation (WSSS), which aims to perform segmentation based on weak supervision signals, such as image-level labels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">54]</ref>, bounding box <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">44]</ref>, or scribbles <ref type="bibr" target="#b29">[30]</ref>. We focus on a popular pipeline in WSSS. This pipeline consists of two steps (See <ref type="figure">Figure 2</ref>). First, a classification model is used to generate pixel-level annotations. This is often achieved by applying variations of Class Activation Maps (CAM) <ref type="bibr" target="#b61">[66]</ref> combined with post-processing techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. Second, these pixel-level annotations are used to train a segmentation model (such as deeplabv1 <ref type="bibr" target="#b7">[8]</ref>). Generated by a classification model, the pixel-wise annotations supplied to the segmentation model are inevitably noisy, thus the second step is indeed a noisy segmentation problem. We therefore apply ADELE to the second step. In summary, our main contributions are:</p><p>? We analyze the behavior of segmentation networks when trained with noisy pixel-level annotations. We show that the training dynamics can be separated into an earlylearning and a memorization stage in segmentation with annotation noise. Crucially, we discover that these dynamics differ across each semantic category.</p><p>? We propose a novel approach (ADELE) to perform semantic segmentation with noisy pixel-level annotations, which exploits early learning by adaptively correcting the annotations using the model output.</p><p>? We evaluate ADELE on the thoracic organ segmentation task where annotations are corrupted to resemble human errors. ADELE is able to avoid memorization, outperforming standard baselines. We also perform extensive experiments to study ADELE on various types and levels of noises.</p><p>? ADELE achieves the state of the art on PASCAL VOC 2012 for WSSS. We show that ADELE can be combined with several different existing methods for extracting pixellevel annotations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">52]</ref> in WSSS, consistently improving the segmentation performance by a substantial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Early learning and memorization in segmentation from noisy annotations</head><p>In a typical classification setting with label noise, a subset of the images are incorrectly labeled. It has been observed in prior works that deep neural networks tend to first fit the training data with clean labels during an early-learning phase, before eventually memorizing the examples with incorrect labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. Here, we show that this phenomenon also occurs in segmentation when the available pixel-wise annotations are noisy (i.e. some of the pixels are incorrect). We consider two different problems. First, segmentation in medical imaging, where annotation noise is mainly due to human error. Second, the annotation noise in weakly-supervised semantic segmentation due to the bias of classification models, as they mostly focus on discriminative regions, and the post-processing errors may result in systematic over or under segmentation.</p><p>Given noisy annotations for which we know the ground truth, we can quantify the early-learning and memorization phenomena by analyzing the model output on the pixels that are incorrectly labeled:</p><p>? early learning IoU el : We quantify early learning using the overlap (measured in terms of the Intersection over Union (IoU) metric) between the outputs and the corresponding ground truth label on the pixels that are incorrectly labeled, denoted by IoU el . Epoch Aorta <ref type="figure">Figure 3</ref>. We visualize the effect of early learning (IoU el , green curves) and memorization (IoUm, red curves) on incorrectly annotated pixels with (solid lines) and without (dashed lines) ADELE for each foreground category of a medical dataset SegThor <ref type="bibr" target="#b26">[27]</ref>. The model is a UNet trained with noisy annotations that mimic human errors. IoU el is the IOU between the model output and the ground truth computed over the incorrectly-labeled pixels. IoUm is the IOU between the model output and the incorrect annotations. For all classes, IoUm increases substantially as training proceeds because the model gradually memorizes the incorrect annotations. This occurs at different speeds for different categories. In contrast, IoU el first increases during an early-learning stage where the model learns to correctly segment the incorrectly-labeled pixels, but eventually decreases as memorization occurs. Like memorization, early-learning also happens at varying speeds for the different semantic categories. See <ref type="figure">Figure 10</ref> in Appendix for the plot on PASCAL VOC.</p><p>? memorization IoU m : We quantify memorization using the overlap (measured in IoU) between the CNN outputs and the incorrect labels, denoted by IoU m . <ref type="figure">Figure 3</ref> demonstrates the phenomena of early-learning and memorization on a randomly corrupted CT-scan segmentation dataset (SegTHOR <ref type="bibr" target="#b26">[27]</ref>). We analyze the learning curve on the incorrectly-annotated pixels during the training process. The plots show the IoU m (dashed red line) and IoU el (dashed green line) at different training epochs. For all classes, the IoU between the output and the incorrect labels (IoU m ) increases substantially as training proceeds because the model gradually memorizes the incorrect annotations. This memorization process occurs at varying speeds for different semantic categories (compare heart and Aorts with Traches or Esophagus in the SegThor dataset). The IoU between the output and the correct labels (IoU el ) follows a completely different trajectory: it first increases during an early-learning stage where the model learns to correctly segment the incorrectly-labeled pixels, but eventually decreases as memorization occurs (for the WSSS dataset, we observe a very similar phenomenon shown in <ref type="figure">Figure 11</ref> in the Appendix). Like memorization, early-learning also happens at varying speeds for the different semantic categories. <ref type="figure">Figure 4</ref> illustrates the effect of early learning and memorization on the model output. In the medical-imaging application, the noisy annotations (third column) are synthesized to resemble human annotation errors which either miss or encompass the ground truth regions (compare to second column). Right after early learning, these regions are identified by the segmentation model (fourth column), but after memorization the model overfits to the incorrect annotations and forgets how to segment these regions correctly (fifth column).</p><p>Similar effects are observed in WSSS, in which the noisy annotations generated by the classification model are missing some object regions, perhaps because they are not particularly discriminative (e.g. the body of the dog, cat and people in the first, second, and fourth row respectively, or the upper half of the bus in the third row). The segmentation model first identify these regions but eventually overfits to the incorrect annotations. Our goal in this work is to modify the training of segmentation models on noisy annotations in order to prevent memorization. This is achieved by combining two strategies described in the next two sections. <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref> shows that the resulting method substantially mitigates memorization (solid red lines) and promotes continued learning beyond the early-learning stage (solid green lines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adaptive label correction based on earlylearning</head><p>The early-learning phenomenon described in the previous section suggests a strategy to enhance segmentation models: correcting the annotations using the model output. Similar ideas have inspired works in classification with noisy labels <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b55">60]</ref>. However, different from the classification task where the noise is mainly sample-wise, the annotation noise is ubiquitous across examples and distributed in a pixel-wise manner. There is a key consideration for this approach to succeed: the annotations cannot be corrected too soon, because this degrades their quality. Determining when to correct the pixel-level annotations using the model output is challenging for two reasons:</p><p>? Correcting all classes at the same time can be suboptimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground truth Noisy annotations Model output after early learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model output after memorization</head><p>Corrected annotations in ADELE <ref type="figure">Figure 4</ref>. Visual examples illustrating the early-learning and memorization phenomena. For several images in a medical dataset Segthor <ref type="bibr" target="#b26">[27]</ref> (top tow rows) and the WSSS dataset VOC 2012 <ref type="bibr" target="#b12">[13]</ref> (bottom four rows), we show the ground-truth annotations (second column), noisy annotations (third column) obtained by a synthetic corruption process for the medical data and by the classification-based SEAM [52] model for WSSS, the output of a model segmentation model trained on the noisy annotations after early learning (fourth column), and the output of the same model after memorization (fifth column). The model for the medical dataset is a UNet. The WSSS model is a standard DeepLab-v1 network trained with the SEAM annotations. As suggested by the graphs in <ref type="figure">Figure 3</ref> after early learning the model corrects some of the annotation errors, but these appear again after memorization. ADELE is able to correct the labels leveraging the early learning output, thereby avoiding memorization (sixth column). We set the background color to light gray for ease of visualization.</p><p>? During training, we do not have access to the performance of the model on ground-truth annotations (otherwise we would just use them to train the model in the first place!).</p><p>To overcome these challenges we propose to update the annotations corresponding to different categories at different times by detecting when early learning has occurred and memorization is about to begin using the training performance of the model.</p><p>In our experiments, we observe that the segmentation performance on the training set (measured by the IoU between the model output and the noisy annotations) improves rapidly during early learning, and then much more slowly during memorization (see the rightmost graph in <ref type="figure" target="#fig_0">Figure 5</ref>). We propose to use this deceleration to decide when to update the noisy annotations. To estimate the deceleration we first fit the following exponential parametric model to the training IoU using least squares:</p><formula xml:id="formula_0">f (t) = a 1 ? e ?b?t c ,<label>(1)</label></formula><p>where t represents training time and 0 &lt; a ? 1, b ? 0, and c ? 0 are fitting parameters. Then we compute the derivative f (t) of the parametric model with respect to t at t = 1 and at the current iteration. <ref type="bibr" target="#b1">2</ref> For each semantic category, the annotations are corrected when the relative change in derivative is above a certain threshold r, i.e. when</p><formula xml:id="formula_1">|f (1) ? f (t)| |f (1)| &gt; r,<label>(2)</label></formula><p>which we set to 0.9, and at every subsequent epoch. We only correct annotations for which the model output has confidence above a certain threshold ? , which we set to 0.8.</p><p>A detailed description about the label correction is attached in the Appendix B. As shown in <ref type="table">Table 2</ref>, adaptive label correction based on early learning improves segmentation models in the medical-imaging applications and WSSS, both on its own and in combination with multiscale-consistency regularization. <ref type="figure">Figure 4</ref> shows some examples of annotation corrections (rightmost column). <ref type="bibr" target="#b1">2</ref> The derivative is given by f (t) = abce ?bt c t c?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multiscale consistency</head><p>As we previously mentioned, model outputs after earlylearning are used to correct noisy annotations. Therefore, the quality of model outputs is crucial for the effectiveness of the proposed method. Following a common procedure that has shown to result in more accurate segmentation from the outputs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b53">58]</ref>, we average model outputs corresponding to multiple rescaled copies of inputs to form the final segmentation, and use them to correct labels. Furthermore, we incorporate a regularization that imposes consistency of the outputs across multi-scales and is able to make averaged outputs more accurate (See the right graph of <ref type="figure">Figure 6</ref>). This idea is inspired by consistency regularizations, a popular concept in the semi-supervised learning literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr">47</ref>] that encourages the model to produce predictions that are robust to arbitrary semanticpreserving spatial perturbations. In segmentation with noisy annotation, we introduce the consistency loss to provide an extra supervision signal to the network, preventing the network from only training on the noisy segmentation annotations, and overfitting to them. This regularization effect is also observed in the literature of classification with label noise <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. Since our method uses the network predictions to correct labels, it is crucial to avoid overfitting to the noisy segmentation.</p><p>To be more specific, let s be the number of scaling  <ref type="figure">Figure 6</ref>. Left: In the proposed multiscale-consistency regularization, rescaled copies of the same input (here upscaled ?1.5 and downscaled ?0.7) are fed into the segmentation model. The outputs (p1, p2 andp3) are rescaled to have the same dimensionality (p1, p2 and p3). Regularization promotes consistency between these rescaled outputs and their elementwise average q. Right: Multi-scale consistency regularization leads to more accurate corrected annotations (results on SegThor, results for VOC 2012 can be found in <ref type="figure" target="#fig_7">Figure 12</ref>). operations. In our experiments we set s = 3 (downscaling ?0.7, no scaling, and upscaling ?1.5). We denote by p k (x), 1 ? k ? s, the model predictions for an input x rescaled according to these operations (see <ref type="figure">Figure 6</ref>). We propose to use a regularization term L Multiscale to promote consistency between p k (x), 1 ? k ? s, and the average</p><formula xml:id="formula_2">q(x) = 1 s s k=1 p k (x): L Multiscale (x) = ? 1 s s k=1 KL (p k (x) q(x)) ,<label>(3)</label></formula><p>where KL denotes the Kullback-Leibler divergence. The term is only applied to the input x where the maximum entry of q(x) is above a threshold ? (equal to 0.8 for all experiments). The regularization is weighted by a parameter ? (set to one in all experiments) and then combined with a cross-entropy loss based on the available annotations. As shown in Tables 2, with multiscale consistency regularization, adaptive label correction further improves segmentation performance in both medical-imaging applications and the WSSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>Classification from noisy labels. Early learning and memorization were first discovered in image classification from noisy labels <ref type="bibr" target="#b32">[33]</ref>. Several methods exploit early learning to improve classification models by correcting the labels or adding regularization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b52">57,</ref><ref type="bibr" target="#b55">60]</ref>. Here we show that segmentation from noisy labels also exhibits early learning and memorization. However, these dynamics are different for different semantic categories. ADELE exploits this to perform correction in a class-adaptive fashion. Segmentation from noisy annotations. Segmentation from noisy annotations is an important problem, especially in the medical domain <ref type="bibr" target="#b4">[5]</ref>. Some recent works address this problem by explicitly taking into account systematic human labeling errors <ref type="bibr" target="#b58">[63]</ref>, and by modifying the segmentation loss to increase robustness <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b45">50]</ref>. <ref type="bibr" target="#b34">[35]</ref> propose to discover noisy gradient by collecting information from two networks connected with mutual attention. <ref type="bibr" target="#b33">[34]</ref> shows that the network learns high-level spatial structures for fluorescence microscopy images. These structures are then leveraged as supervision signals to alleviate influence from wrong annotations. These methods mainly focus on improving the robustness by exploiting some setting-specific information (e.g. network architecture, dataset, requiring some samples with completely clean annotation). In contrast, we propose to study the learning dynamics of noisy segmentation and propose ADELE, which performs label correction by exploiting early learning.</p><p>Weakly supervised semantic segmentation (WSSS). Recent methods for WSSS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b56">61]</ref> are mostly based on the approach introduced by Ref. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">54]</ref>, where a classification model is first used to produce pixel-level annotations <ref type="bibr" target="#b61">[66]</ref>, which are then used to train a segmentation model. These techniques mostly focus on improving the initial pixel-level annotations, by modifying the classification model itself <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b48">53,</ref><ref type="bibr" target="#b50">55]</ref>, or by post-processing these annotations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">49]</ref>. However, the resulting annotations are still noisy <ref type="bibr" target="#b57">[62]</ref> (see <ref type="figure">Figure 4</ref>). Our goal is to improve the segmentation model by adaptively accounting for this noise. Similar approach to our method has been observed in object detection where network outputs are dynamically used for training <ref type="bibr" target="#b20">[21]</ref>. In semantic segmentation, the work that is most similar to our label-correction strategy is <ref type="bibr" target="#b17">[18]</ref>, which is inspired by traditional seeded region-growing techniques <ref type="bibr" target="#b0">[1]</ref>. This method estimates the foreground using an additional model <ref type="bibr" target="#b18">[19]</ref>, and initializes the foreground segmentation es-timate with classification-based annotations. This estimate is used to train a segmentation model, which is then used to iteratively update the estimate. ADELE seeks to correct the initial annotations, as opposed to growing them, and does not need to identify the foreground estimate or an initial subset of highly-accurate annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Segmentation on Medical Images with Annotation Noise</head><p>Segmentation from noisy annotations is a fundamental challenge in the medical domain, where available annotations are often hampered by human error <ref type="bibr" target="#b58">[63]</ref>. Here, we evaluate ADELE on a segmentation task where the goal is to identify organs from computed tomography images. Settings. The dataset consists of 3D CT scans from the SegTHOR dataset <ref type="bibr" target="#b26">[27]</ref>. Each pixel is assigned to the esophagus, heart, trachea, aorta, or background. We treat each 2D slice of the 3D scan as an example, resizing to 256 ? 256 pixels. We randomly split the slices into a training set of 3638 slices, a validation set of 570 slices, and a test set of 580 slices. Each patient only appears in one of these subsets. We generate annotation noise by applying random degrees of dilation and erosion to the ground-truth segmentation labels, mimicking common human errors <ref type="bibr" target="#b58">[63]</ref> (see <ref type="figure">Figure 4</ref>). In the main experiment, the noisy annotation is with a mIoU of 0.6 w.r.t the ground truth annotation. We further control the degree of dilation and erosion to simulate noisy annotation sets with different noise levels for testing the model robustness. We corrupt all annotations in the training set, but not in the validation and test sets. Our evaluation metric is Mean Intersection over Union (mIoU). Results. For a fair comparison, we choose a UNet trained with multi-scale inputs as our baseline. We report the mIoU of the baseline and ADELE on the test set of SegTHOR dataset in <ref type="table">Table 1</ref>. ADELE outperforms the baseline method at all three evaluation epochs. Moreover, correcting labels at the same time for all classes will have a detrimental effect on the performance. Impacts of noise levels. <ref type="figure">Figure 7</ref> provides empirical evi- Baseline ADELE <ref type="figure">Figure 7</ref>. The performance comparison of the baseline and ADELE on the test set of SegTHOR <ref type="bibr" target="#b26">[27]</ref>. The model is trained on noisy annotations with various levels of corruption (measured in mIoU with the clean ground truth annotations). ADELE is able to improve the model performance across a wide range of corruption levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>dence that ADELE is robust to a wide range of noises. The mIoU of noisy annotations (x-axis) indicates the correctness of the noisy annotations. Thus the smaller the mIoU shows the higher noise levels. The improvements achieved by ADELE are substantial when the noise levels are moderate.</p><p>Ablation study for each part of ADELE. We perform an ablation study to understand how different parts of ADELE contribute to the final performance. From <ref type="table">Table 2</ref>, we observe that the model trained with multiple rescaled versions of the input (illustrated in left graph of <ref type="figure">Figure 6</ref>) performs better than the model trained only with the original scale of the input. The proposed spatial consistency regularization further improves the performance. Most importantly, combining any of these methods with label correction would substantially improve the performance. ADELE, which combines label correction with the proposed regularization, achieves the best performance. We also include ablation studies for the hyperparameters r, ? and ? in Appendix C. Additional segmentation results are provided in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Noisy Annotations in Weakly-supervised Semantic Segmentation</head><p>We adopt a prevailing pipeline for training WSSS (described in detail in Section 1), in which some pixel-wise annotations are generated using image level labels to supervise a segmentation network. These pixel-wise annotations are noisy. Therefore, we apply ADELE to this WSSS pipeline.</p><p>We evaluate ADELE on a standard WSSS dataset -PAS-CAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref>, which has 21 annotation classes (including background), and contains 1464, 1449 and 1456 images in the training, validation (val) and test sets respectively. Following <ref type="bibr" target="#b41">[41,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b54">59,</ref><ref type="bibr" target="#b56">61,</ref><ref type="bibr" target="#b57">62]</ref>, we use an augmented training set with 10582 images with annotations from <ref type="bibr" target="#b15">[16]</ref>. Baseline Models. To demonstrate the broad applicability of our approach, we apply ADELE using pixel-level anno-  <ref type="table">Table 2</ref>. Ablation study for ADELE on SegTHOR <ref type="bibr" target="#b26">[27]</ref> and PASCAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref>. We report the mIoU achieved at the last epoch on the validation set for both dataset. Class-adaptive label correction mechanism achieves the best performance when combined with multi-scale consistency regularization.</p><p>Previous methods ADELE + DSRG <ref type="bibr" target="#b17">[18]</ref> ICD <ref type="bibr" target="#b13">[14]</ref> SCE <ref type="bibr" target="#b6">[7]</ref> AffinityNet <ref type="bibr" target="#b2">[3]</ref> SSDD <ref type="bibr" target="#b41">[41]</ref> SEAM <ref type="bibr" target="#b47">[52]</ref> CONTA <ref type="bibr" target="#b57">[62]</ref> AffinityNet <ref type="bibr" target="#b2">[3]</ref> SEAM <ref type="bibr" target="#b47">[52]</ref> ICD <ref type="bibr" target="#b13">[14]</ref>  <ref type="table">Table 3</ref>. Comparison with state-of-the-art methods on the Pascal VOC 2012 dataset using mIoU (%). The best and the best previous method performance under each set are highlighted in red and blue respectively. The version of CONTA <ref type="bibr" target="#b57">[62]</ref> reported here is deployed combined with SEAM <ref type="bibr" target="#b47">[52]</ref>. The results clearly show that ADELE outperforms other approaches.</p><p>tations generated by three popular WSSS models: Affini-tyNet <ref type="bibr" target="#b2">[3]</ref>, SEAM <ref type="bibr" target="#b47">[52]</ref> and ICD <ref type="bibr" target="#b13">[14]</ref>, which do not rely on external datasets or external saliency maps. The annotations are produced by a classification model combined with the post-processing specified in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">52]</ref>. We provide details on the training procedure in Section B in the Appendix. We use the same inference pipeline as SEAM <ref type="bibr" target="#b47">[52]</ref>, which includes multi-scale inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b59">64]</ref> and CRF <ref type="bibr" target="#b24">[25]</ref>.</p><p>Comparison with the state-of-the-art. <ref type="table">Table 3</ref> compares the performance of the proposed method ADELE to stateof-the-art WSSS methods on PASCAL VOC 2012. ADELE improves the performance of AffinityNet <ref type="bibr" target="#b2">[3]</ref>, SEAM <ref type="bibr" target="#b47">[52]</ref> and ICD <ref type="bibr" target="#b13">[14]</ref> substantially on the validation and test sets. Moreover, ADELE combined with SEAM <ref type="bibr" target="#b47">[52]</ref> and ICD <ref type="bibr" target="#b13">[14]</ref> achieves state-of-the-art performance on both sets. Although it uses only image-level labels, ADELE outperforms state-of-the-art methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b54">59,</ref><ref type="bibr" target="#b59">64]</ref> that rely on external saliency models <ref type="bibr" target="#b18">[19]</ref>. To show that our method is complementary with other more advanced WSSS methods, we have conducted an experiment with a recent WSSS method NSROM <ref type="bibr" target="#b54">[59]</ref>, which uses external saliency models. ADELE+NSROM achieves mIoU of 71.6 and 72.0 on the validation and test set respectively, which is the SoTA for WSSS with ResNet segmentation backbone (see Appendix A.2). <ref type="figure" target="#fig_4">Figure 8</ref> compares the performance of SEAM and the performance of ADELE combined with SEAM on the validation set separately for each semantic category. ADELE improves performance for most categories, with the exception of a few categories where the baseline model does not perform well (e.g. chair, bike). On <ref type="figure">Figure 1</ref> and 9, we show some qualitative segmentation results from the validation set. <ref type="figure">Figure 1</ref> shows examples where ADELE successfully improves the SEAM segmentation. <ref type="figure">Figure 9</ref> shows examples where it does not. In both the output of SEAM has highly structured segmentation errors: the prediction encompasses   <ref type="bibr" target="#b47">[52]</ref> and SEAM combined with the proposed method ADELE on the validation set of PASCAL VOC 2012. We separate the categories based on IoUs for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth SEAM SEAM+ADELE <ref type="figure">Figure 9</ref>. Visualization of the segmentation results of both methods for several examples. ADELE fails to improves segmentation for the bicycle and chair due to highly structured segmentation errors. We set the background color to gray for ease of visualization.</p><p>the bike but completely fails to capture its inner structure, and the chair is missclassified as a sofa. This supports the conclusion that ADELE provides less improvement when the baseline method performs poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>The success of ADELE seems to rely to some extent on the quality of the initial annotations. When these annotations are of poor quality, ADELE may only produce a marginal improvement or even have negative impact (see <ref type="figure" target="#fig_4">Figure 8 and  1</ref>). An related limitation is that when the annotation noise is highly structured, early-learning may not occur, because there may not be sufficient information in the noisy annotations to correct the errors. In that case label correction based on early-learning will be unsuccessful. Illustrative examples are provided in the fifth and sixth rows of <ref type="figure">Figure 1)</ref>, where the initial annotations completely encompass the bicycle, and completely missclassify the chair as a sofa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we introduce a novel method to improve the robustness of segmentation models trained on noisy annotations. Inspired from the early-learning phenomenon, we proposed ADELE to boost the performance on the segmentation of thoracic organ, where noise is incorporated to resemble human annotation errors. Moreover, standard segmentation networks, equipped with ADELE, achieve the state-of-theart results for WSSS on PASCAL VOC 2012. We hope that this work will trigger interest in the design of new forms of segmentation methods that provide robustness to annotation noise, as this is a crucial challenge in applications such as medicine. We also hope that the work will motivate further study of the early-learning and memorization phenomena in settings beyond classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix for "Adaptive Early-Learning Correction for Segmentation from Noisy Annotations"</head><p>The appendix is organized as follows.</p><p>? In Appendix A, we include additional figures illustrating the early-learning and memorization phenomena in PASCAL VOC 2012 and SegTHOR. We also include additional results for these two datasets.</p><p>? In Appendix B, we describe the implementation details of our proposed method ADELE, including a description of the hyperparameters, experiment settings, and other technical details.</p><p>? In Appendix C, we report ablation studies on the influence of different components and hyperparameters of ADELE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>License of the assets Licence for the codes</head><p>We reproduce the code for AffinityNet <ref type="bibr" target="#b2">[3]</ref>, SEAM <ref type="bibr" target="#b47">[52]</ref>, ICD <ref type="bibr" target="#b13">[14]</ref>, all of which are under MIT License according to https://opensource.org/licenses/MIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Licence for the dataset</head><p>As the PASCAL VOC <ref type="bibr" target="#b12">[13]</ref> includes images obtained from the "flickr" website, we respect the corresponding terms of use for "flickr" according to https://www.flickr.com/help/terms.</p><p>For the SegThor <ref type="bibr" target="#b26">[27]</ref> dataset, we follow the data use agreement according to https://pagesperso.litislab. fr/cpetitjean/wp-content/uploads/sites/19/2021/03/DataUseAgreement_CHBDatabase.pdf. 0 tvmonitor <ref type="figure">Figure 10</ref>. We visualize the effect of early learning (IoU el , green curves) and memorization (IoUm, red curves) on segmentation models trained with (solid lines) and without (dashed lines) ADELE for each category of the WSSS dataset VOC 2012 <ref type="bibr" target="#b12">[13]</ref> The WSSS model is a standard DeepLab-v1 network trained with annotations obtained from SEAM <ref type="bibr" target="#b47">[52]</ref>. IoU el is the IOU between the model output and the ground truth computed over the incorrectly-labeled pixels. IoUm is the IOU between the model output and the incorrect annotations. For all classes, IoUm increases substantially as training proceeds because the model gradually memorizes the incorrect annotations. This again occurs at different speeds for different categories. In contrast, IoU el first increases during an early-learning stage where the model learns to correctly segment the incorrectly-labeled pixels, but eventually decreases as memorization occurs (the phenomenon is more evident when we zoom in, as shown in <ref type="figure">Figure 11</ref> in the Appendix). Like memorization, early-learning also happens at varying speeds for the different semantic categories. <ref type="figure">Figure 11</ref>. Zoomed-in illustration of early learning for the different semantic categories on PASCAL VOC 2012. IoU el first increases during an early-learning stage where the model learns to correctly segment the incorrectly-labeled pixels, but eventually decreases as memorization occurs. Early learning happens at varying speeds for the different semantic categories. The experimental setting is the same as    IOU el (early-learning) <ref type="figure">Figure 13</ref>. Illustration of the proposed curve fitting method to decide when to begin label correction in ADELE (Results on Pascal VOC). On the left, we plot the IoU between the model predictions and the initial noisy annotations for the same model used in <ref type="figure">Figures 11 and 4</ref> and the corresponding fit with the parametric model in Equation 1. The label correction beginning iteration is based on the relative slope change of the fitted curve. The center image shows the label correction times for different semantic categories, showing that they are quite different. On the right graph, the green line shows the IoU el for a given category. The IoU el equals the IoU between the model output and the ground truth computed over the incorrectly-labeled pixels, and therefore quantifies early-learning. The label correction begins close to the end of the early-learning phase, as desired. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experimental Results</head><p>In this section, we include additional examples (A.1) and results (A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Additional Examples</head><p>WSSS dataset (PASCAL VOC 2012)</p><p>? Early-learning. (Supplementary material for <ref type="figure">Figure 3</ref> and 5 in Section 2) <ref type="figure">Figure 10</ref> demonstrates that early-learning and memorization on a segmentation CNN trained with noisy annotations generated by a classification model for the standard WSSS dataset (PASCAL VOC2012). In <ref type="figure">Figure 11</ref> we show the zoomed-in version of the early-learning IoU el curves. Early-learning happens for most of the classes at different speeds. <ref type="figure">Figure 13</ref> show the curve fitting illustration for PASCAL VOC. <ref type="figure">Figure 14</ref> shows that the label correction begins close to the end of the early-learning phase for most of the semantic categories for PASCAL VOC.</p><p>? Multiscale consistency. <ref type="figure" target="#fig_7">Figure 12</ref> shows that multi-scaleconsistency regularization leads to more accurate corrected annotations for PASCAL VOC. ? Highly structured annotation noise. (Supplementary material for <ref type="figure">Figure 1</ref>) <ref type="figure" target="#fig_4">Figure 18</ref> shows training examples with highly structured noise, which may prevent early learning from happening, and therefore from being used for label correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More visual examples for SegTHOR</head><p>? Superior performance. (Supplementary material for Section 4). We show some visual examples that indicate ADELE improves segmentation performance on the validation set of SegTHOR dataset in <ref type="figure">Figure 19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Results</head><p>ADELE outperforms methods using external information. (Supplementary material for Section 5) ADELE using initial labels generated from SEAM <ref type="bibr" target="#b47">[52]</ref> and ICD <ref type="bibr" target="#b13">[14]</ref> achieves state-of-the-art performance without using external saliency models. This performance is on par with or even slightly better than methods that rely on external saliency models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b54">59,</ref><ref type="bibr" target="#b59">64]</ref>.</p><p>To show that our method is complementary with other more advanced WSSS methods, we have conducted an experiment with a recent WSSS method NSROM <ref type="bibr" target="#b54">[59]</ref>. ADELE+NSROM achieves mIoU of 71.6 and 72.0 on the validation and test set respectively, which is the SoTA for WSSS with ResNet segmentation backbone.</p><p>ADELE improves performance across different noise levels. We provided the full result of <ref type="figure">Figure 7</ref> in <ref type="table">Table 6</ref> for SegTHOR dataset, which shows that ADELE can improve the model performance across different noise level.  <ref type="table">Table 6</ref>. The mIoU (%) comparison of the baseline and ADELE on the test set of SegTHOR <ref type="bibr" target="#b26">[27]</ref>. We report the test mIoU of the model that achieves best mIoU on the validation set. It can be seen that ADELE stably improve the model performance across different noise levels.</p><p>B. Implementation details B.1. Hyperparameters <ref type="table">Table 7</ref> provides a complete list of hyperparameter values for our experiments. We use almost identical hyperparameters of ADELE on PASCAL VOC 2012 and SegTHOR. ? (consistency strength) controls the strength of the consistency regularization added to the cross entropy loss. ? (consistency confidence threshold) is the threshold that determines when multiscale consistency regularization is applied (when the maximum prediction probability for any category in any pixel of the average q is above ?). r (curve fitting threshold) is the threshold that controls label correction for each semantic category (see Equation equation 2). ? (label correction confidence threshold) is the threshold that determines which pixels are corrected by the model prediction. Only the pixels with confidence (maximum prediction probability for any category) above ? are corrected. Note that we report ablation studies for most of these hyperparameters in Section C.</p><p>Dataset ? ? r ? PASCAL VOC 2012 1 0.8 0.9 0.8 SegThor 1 0.8 0.9 0.7 <ref type="table">Table 7</ref>. Complete list of ADELE hyperparameters for PASCAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref> and SegTHOR <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Medical segmentation with simulated noise: SegTHOR</head><p>Here we provide implementation details for our experiments on SegTHOR (Supplementary material for Section 4 in the main paper).</p><p>Details of Noise Synthesis. We apply dilation and erosion on the ground-truth segmentation masks to simulate "overannotation" and "under-annotation" labels respectively. In particular, "over-annotation" labels tend to assign background pixels surrounding a target organ to the class of this organ. On the contrary, "under-annotation" labels tend to assign a foreground pixel on the edge of a target organ to the background class. These two noise patterns have been previously utilized to simulate  <ref type="figure">Figure 20</ref>. Ablation study for r on SegTHOR. We fixed the other hyperparameters to the default settings in <ref type="table">Table 7</ref>. We report the test mIoU (%) at the best validation epoch on the Left, the full result is shown on the Right.</p><p>iteration number itr: lr itr = lr init 1 ? itr max_itr ? with ? = 0.9. We train our segmentation network for 20000 iterations (max_itr = 20000) with a batch size of 10. The input images are randomly scaled and then randomly cropped to 448 ? 448.</p><p>Inference during testing. During testing, we use the same inference pipeline as SEAM <ref type="bibr" target="#b47">[52]</ref>, which includes multi-scale inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b59">64]</ref> and CRF <ref type="bibr" target="#b24">[25]</ref>.</p><p>Generating model predictions to conduct label correction. Updating the model using the model predictions after processing each batch would be difficult for the PASCAL VOC 2012 dataset. The reason is that random data augmentations (e.g. rescaling, cropping, random changing contrast in the image, etc.) are often applied <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b59">64]</ref> and these augmentations would need to be inverted in order to use the output for label correction (otherwise the predictions would be inconsistent across training epochs). In fact, some augmentations are not invertible at all, e.g. cropping cuts the object out thus is not invertible, rescaling might result in loss of information thus is not invertible as well. In order to avoid this issue on the PASCAL VOC dataset, we evaluate the model at the end of each training epoch on inputs without any random augmentation, and then use the model outputs to perform label correction (the model output are processed with arg max to produce hard labels for label correction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional ablation studies C.1. Medical segmentation with simulated noise: SegTHOR</head><p>Here we report additional ablation studies for different components in the proposed label correction method on SegTHOR dataset (Supplementary material for Section 4).</p><p>Different options for label correction As described in Section 2.2, ADELE uses the model outputs to correct labels. In this section, we compare two options for computing these outputs.</p><p>? Iteration: the outputs are computed after each training iteration.</p><p>? Epoch: the outputs are computed at the end of each training epoch. <ref type="table">Table 8</ref> shows that on SegTHOR Iteration performs slightly better on the best test mIoU than Epoch, and outperforms it substantially on the mIoU at the last epoch, suggesting that Iteration is more effective in preventing memorization. Both two options are improving results with respect to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Best  <ref type="figure">Figure 21</ref>. Ablation study for ? on SegTHOR. We fixed the other hyperparameters to the default settings in <ref type="table">Table 7</ref>. We report the test mIoU (%) at the best validation epoch on the Left, the full result is shown on the Right. The result shows that ADELE is not very sensitive to the value of ? .  <ref type="figure">Figure 22</ref>. Ablation study for r and ? on PASCAL VOC. We fixed the other hyperparameters to the default settings in <ref type="table">Table 7</ref>. We report the validation mIoU (%) at the last training epoch. r and ? for label correction. We report the ablation for r and ? for ADELE on SegTHOR dataset in <ref type="figure">Figure 20</ref> and 21 respectively. For the r value, we observe similar results as in the PASCAL VOC dataset. If the r value is too small (e.g. 0, 0.3) and label correction is conducted too early, the network has not been trained well. This degrades the label correction quality, which hurts the generalization of the model. If the r value is too large (e.g. 0.99) then the network barely conduct label correction for any class before stopping training. Results are again very robust to the choice of ? value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Weakly Supervised Semantic Segmentation: PASCAL VOC</head><p>r and ? for label correction. We report the ablation result for r and ? of ADELE on PASCAL VOC dataset in <ref type="figure">Figure 22</ref>. Small r values encourage the model to conduct label correction earlier, larger r values delay correction. If the r value is too small (e.g. 0.3) and label correction is conducted too early, the network has not been trained well. This degrades the label correction quality, which hurts the generalization of the model. If the r value is too large (e.g. 0.99), then the method barely conducts label correction for any class before the end of training, which results in a performance similar to the case without label correction. We observe that our model is very robust to the choice of ? .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the proposed curve fitting method to decide when to begin label correction in ADELE (Results on SegThor). First column: On the top, we plot the IoU between the model predictions and the initial noisy annotations for the same model used in Figures 3 and 4 and the corresponding fit with the parametric model in Equation 1. The label correction beginning iteration is based on the relative slope change of the fitted curve. The bottom image shows the label correction times for different semantic categories, showing that they are quite different. Second and third columns: the green lines show the IoU el for different categories Esophagus, Heart, Trachea and Aorta. The IoU el equals the IoU between the model output and the ground truth computed over the incorrectly-labeled pixels, and therefore quantifies early-learning. The label correction begins close to the end of the early-learning phase, as desired. More result in section A.1 in Appendix shows that this also occurs for VOC 2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Category-wise comparison of the IoU (%) of SEAM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Multi-scaleconsistency regularization leads to more accurate corrected annotations (result on Pascal VOC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .Figure 15 .Figure 16 .Figure 17 .Figure 18 .Figure 19 .</head><label>141516171819</label><figDesc>Illustration of the proposed curve fitting method to decide when to begin label correction in ADELE. The blue line shows the parametric model in Equation 1 fit to the IoU between the model predictions and the initial noisy annotations for the same WSSS model used in Figures 3 and 4. The green line shows the IoU el for a given category. The IoU el equals the IOU between the model output and the ground truth computed over the incorrectly-labeled pixels, and therefore quantifies early-learning. The label correction begins close to the end of the early-learning phase for most of the categories with a few exceptions (boat and motorbike). Additional segmentation results of SEAM and SEAM+ADELE for several examples on the validation set of PASCAL VOC 2012. We set the background color to gray for ease of visualization. Supplementary for Figure 1. Additional segmentation results of SEAM and SEAM+ADELE for several examples on the validation set of PASCAL VOC 2012. We set the background color to gray for ease of visualization. Supplementary for Figure 1. Additional segmentation results of SEAM and SEAM+ADELE for several examples on the validation set of PASCAL VOC 2012. We set the background color to gray for ease of visualization. Supplementary materials for Figure 1. Illustration of a possible limitation of the proposed label-correction approach on PASCAL VOC 2012. The initial annotations coarsely segment the bike and misclassify the chair as a sofa consistently in several training examples. This highly structured annotation noise could potentially prevent early learning from happening, and therefore from being exploited for label correction. We set the background color to light gray for ease of visualization. Visualization of the segmentation results of baseline and baseline+ADELE for several examples on the validation set of SegTHOR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>?</head><label></label><figDesc>Superior performance. (Supplementary material for Figure 1 in Section 5) We provide more visualization examples on the validation set of PASCAL VOC in Figure 15, 16 and 17 comparing ADELE to the baseline method SEAM. ADELE reduces false positives for some examples (e.g. boat, person, sofa, bottle, tv etc.) and produces a more complete segmentation of other examples (e.g. cat, dog, bird, bus, bottle, tv, horse etc.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We report the mean and standard deviation after training the model with five realizations of the noisy annotations.</figDesc><table><row><cell></cell><cell></cell><cell>ADELE w/o class adaptive</cell><cell>ADELE</cell></row><row><cell>Best val</cell><cell>62.6?2.3</cell><cell>40.7?2.5</cell><cell>71.1?0.7</cell></row><row><cell>Max test</cell><cell>63.3?2.0</cell><cell>40.7?2.4</cell><cell>71.2?0.6</cell></row><row><cell>Last Epoch</cell><cell>59.1?1.3</cell><cell>40.5?2.3</cell><cell>70.8?0.7</cell></row></table><note>Table 1. The mIoU (%) comparison of the baseline and ADELE with or without class-adaptively correcting labels, on the test set of SegTHOR [27]. We report the test mIoU of the model that performs best on the validation set (Best Val), the test mIoU at the last epoch (Last Epoch), and the highest test performance during training (Max Test).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>loss for weakly supervised semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.[45] Guolei Sun, Wenguan Wang, Jifeng Dai, and Luc Van Gool.Mining cross-image semantics for weakly supervised semantic segmentation. In Proceedings of the European Conference on Computer Vision. Springer, 2020.</figDesc><table><row><cell>[46] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-</cell></row><row><cell>oharu Aizawa. Joint optimization framework for learning</cell></row><row><cell>with noisy labels. Proceedings of the IEEE Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition, 2018.</cell></row><row><cell>[47] Antti Tarvainen and Harri Valpola. Mean teachers are better</cell></row><row><cell>role models: Weight-averaged consistency targets improve</cell></row><row><cell>semi-supervised deep learning results. In Advances in Neural</cell></row><row><cell>Information Processing Systems, 2017.</cell></row><row><cell>[48] Michael Treml, Jos? Arjona-Medina, Thomas Unterthiner,</cell></row><row><cell>Rupesh Durgesh, Felix Friedmann, Peter Schuberth, Andreas</cell></row><row><cell>Mayr, Martin Heusel, Markus Hofmarcher, Michael Widrich,</cell></row><row><cell>et al. Speeding up semantic segmentation for autonomous</cell></row><row><cell>driving. In MLITS, NIPS Workshop, 2016.</cell></row><row><cell>[49] Paul Vernaza and Manmohan Chandraker. Learning random-</cell></row><row><cell>walk label propagation for weakly-supervised semantic seg-</cell></row></table><note>mentation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Method bkg aero bike bird boat bottle bus car cat chair cow table dog horse mbk person plant sheep sofa train tv mIoU SEAM 88.8 68.5 33.3 85.7 40.4 67.3 78.9 76.3 81.9 29.1 75.5 48.1 79.9 73.8 71.4 75.2 48.9 79.8 40.9 58.2 53.0 64.5 SEAM + Ours 91.1 77.6 33.0 88.9 67.1 71.7 88.8 82.5 89.0 26.6 83.8 44.6 84.4 77.8 74.8 78.5 43.8 84.8 44.6 56.1 65.3 69.3 Category-wise comparison of the IoU (%) of SEAM<ref type="bibr" target="#b47">[52]</ref> and SEAM combined with the proposed method ADELE on the validation set of PASCAL VOC 2012.</figDesc><table><row><cell>Iteration of ero-sion/dilation</cell><cell>Annotation mIoU</cell><cell>Multiscale input augmentation (baseline)</cell><cell>Multiscale label correction</cell><cell>Multiscale regularization consistency</cell><cell>ADELE</cell></row><row><cell>0</cell><cell>1.00</cell><cell>0.745</cell><cell>0.743</cell><cell>0.762</cell><cell>0.766</cell></row><row><cell>1</cell><cell>0.91</cell><cell>0.743</cell><cell>0.726</cell><cell>0.759</cell><cell>0.757</cell></row><row><cell>2</cell><cell>0.73</cell><cell>0.702</cell><cell>0.710</cell><cell>0.714</cell><cell>0.734</cell></row><row><cell>3</cell><cell>0.61</cell><cell>0.646</cell><cell>0.658</cell><cell>0.647</cell><cell>0.711</cell></row><row><cell>4</cell><cell>0.52</cell><cell>0.556</cell><cell>0.651</cell><cell>0.606</cell><cell>0.666</cell></row><row><cell>6</cell><cell>0.39</cell><cell>0.446</cell><cell>0.556</cell><cell>0.514</cell><cell>0.564</cell></row><row><cell>8</cell><cell>0.28</cell><cell>0.416</cell><cell>0.407</cell><cell>0.423</cell><cell>0.481</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Iteration ) 71.1 ? 0.7 70.8 ? 0.7 71.2 ? 0.6 ADELE (Epoch) 69.6 ? 0.8 64.1 ? 1.3 70.2 ? 0.5 Table 8. mIoU(%) of ADELE on SegTHOR when label correction is based on model output at the end of each iteration or each epoch. The former achieves better results.</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU (%)</cell><cell>70 60</cell><cell></cell><cell cols="4">? Best val Last epoch Max test 0 69.1 68.8 69.1 0.4 67.2 67.1 68.0 0.7 71.7 71.0 71.7 0.8 71.0 70.3 71.0</cell></row><row><cell></cell><cell>label correction confidence threshold ? ADELE (0.00 0.40 0.70 0.80</cell><cell>val 0.95</cell><cell>Last epoch 0.95</cell><cell>Max test 71.1</cell><cell>71.2</cell><cell>71.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments SL, KL, WZ, and YS were partially supported by NSF NRT grant HDR-1922658. SL was partially supported by NSF grant DMS 2009752 and Alzheimer's Association grant AARG-NTF-21-848627. KL was partially supported by NIH grant (R01LM013316). YS was partially supported by NIH grant (P41EB017183, R21CA225175). CFG acknowledges support from NSF OAC 2103936.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 8</ref> <p>in Section 5) <ref type="table">Table 5</ref> shows that ADELE performs substantially better than the baseline method SEAM on most of the semantic categories, as well as on average.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leanne</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Formulating spatially varying performance in the statistical fusion framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennett A Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1326" to="1336" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning with instance-dependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02347</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">high-dimensional perturbations. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A soft staple algorithm combined with anatomical knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Kats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayit</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Structured consistency loss for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04647</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segthor: Segmentation of thoracic organs at risk in ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zo?</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Dubray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Kuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weaklysupervised high-resolution segmentation of mammography images for breast cancer diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Chledowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof J</forename><surname>Geras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep neural networks learn meta-structures to segment fluorescence microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoru</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guole</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11594</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Shaobo Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4578" to="4585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1412.6596</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention-gated networks for improving ultrasound scan plane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05338</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lvc-net: Medical image segmentation with noisy label based on local visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Box-driven class-wise region masking and filling rate guided</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A noise-robust framework for automatic segmentation of covid-19 pneumonia lesions from ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiugen</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Non-salient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14581</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12547</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Disentangling human error from the ground truth in segmentation of medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mou-Cheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Barkhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel C</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15963</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Splitting vs. merging: Mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust curriculum learning: From clean label detection to noisy label self-correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Require: y i 1 ? i ? N // training noisy annotations Require: x i , 1 ? i ? N // training images Require: NN(x) // segmentation network Require: C = number of categories Require: t the training epoch Require: f c (t), 1 ? c ? C // curve function fitted on training IoU for each category c Require: ? = threshold, 0 &lt; ? &lt; 1 Require: r = threshold for when to correct annotation Require: S = set of rescaling transforms for each minibatch B do for b in B do for s in S do p bs = NN(S(x i )) // evaluate the network to obtain model&apos;s outputs corresponding to each scaled version of inputs b</title>
		<imprint/>
	</monogr>
	<note>Algorithm 1: Pseudocode for Proposed Annotation Correction. q bc ? ? ] = arg max q b [q bc ? ?</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">N // corrected annotations common errors that human annotators make during manual segmentation [63]. In our experiments, we randomly choose the type and degree of synthetic noise</title>
		<imprint/>
	</monogr>
	<note>that will be applied to each example</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">We train the UNet [38] using SGD optimizer. To optimize the hyper-parameters, we search for the learning rate in {0.1, 0.01, 0.001, 0.0001, 0.00001} and set to 0.01. ?, ?, r and ? are set according to Table 7. We trained our model for 100 epochs with a batch size of 5</title>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint/>
	</monogr>
	<note>For SegTHOR [27], we use one NVIDIA V100 GPU to train the model. Inference during testing. We conduct the single-scale evaluation using the input without augmentations</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">For the medical dataset, since we do not apply any augmentation to the input images, we directly use the outputs during training at every iteration to correct the labels (the model output are processed with arg max to produce hard labels for label correction)</title>
		<imprint/>
	</monogr>
	<note>Generating model prediction to conduct label correction. This is in contrast to PASCAL VOC where we compute the outputs at the end of each epoch, as explained in Section C.1</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Weakly Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno>PASCAL VOC 2012</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">We provide the implementation details of the model for PASCAL VOC 2012</title>
		<imprint/>
	</monogr>
	<note>Supplementary material for Section 5</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">We use the official code of AffinityNet [3], SEAM [52] and ICD [14] to generate initial pixel-level annotations that are used for training the segmentation network. The experimental settings to train the segmentation network follow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Training</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>For</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">we use two NVIDIA Quadro RTX 8000 GPUs to train the model</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>3, 41, 52, 61] in which DeepLabv1 [8] is adopted. The model uses ResNet38 [56] as a backbone, with the initial weight load from ImageNet [12] pretrained classfication model. Following the same settings as SEAM [52], we use a SGD optimizer with momentum 0.9 and weight decay 5e ?4 . The initial learning rate lr init is set to 0.001, and reduced following a polynomial function of the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
