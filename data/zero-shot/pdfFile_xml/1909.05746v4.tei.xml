<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sams-Net: A Sliced Attention-based Neural Network for Music Source Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingle</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data Science Research Center</orgName>
								<orgName type="institution">Duke Kunshan University</orgName>
								<address>
									<settlement>Kunshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tiangong University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tiangong University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Hou</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">AI Platform Department</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
							<email>ming.li369@dukekunshan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Science Research Center</orgName>
								<orgName type="institution">Duke Kunshan University</orgName>
								<address>
									<settlement>Kunshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sams-Net: A Sliced Attention-based Neural Network for Music Source Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Sliced Attention</term>
					<term>Music Source Separation</term>
					<term>Mu- sic Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Network (CNN) or Long short-term memory (LSTM) based models with the input of spectrogram or waveforms are commonly used for deep learning based audio source separation. In this paper, we propose a Sliced Attentionbased neural network (Sams-Net) in the spectrogram domain for the music source separation task. It enables spectral feature interactions with multi-head attention mechanism, achieves easier parallel computing and has a larger receptive field compared with LSTMs and CNNs respectively. Experimental results on the MUSDB18 dataset show that the proposed method, with fewer parameters, outperforms most of the state-of-the-art DNN-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The "cocktail party effect" was first proposed by Cherry <ref type="bibr" target="#b0">[1]</ref>: How does the human brain separate a conversation from the surrounding noise. Later, Bregman <ref type="bibr" target="#b1">[2]</ref> tried to study how the human brain analyzed the complex auditory signal, and proposed a framework for it. By the early 21st century, Roman <ref type="bibr" target="#b2">[3]</ref> attempted to simulate the brain's ability of source separation by means of algorithms, which served as the main framework of source separation right now. When it comes to music source separation, the first unsupervised method is <ref type="bibr" target="#b3">[4]</ref>. Recently, the supervised methods, especially the deep learning-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, have achieved promising performance for this task.</p><p>Music is produced by assembling sound from multiple individual instruments called stems. The goal of music source separation is to recover those individual stems from the mixed signal <ref type="bibr" target="#b6">[7]</ref>. In the SiSEC 2018 campaign <ref type="bibr" target="#b7">[8]</ref>, those individual stems were grouped into four categories: vocals, drums, bass and other. Given a song which is a mixture of these four sources, our goal is to separate it into four parts that correspond to the original sources. Unlike the speech separation task in which each single source is independent to each other, there are many similar music pieces recurring in the same source and among different sources in a song. This characteristic brings a great challenge for music source separation.</p><p>Most of the music source separation models can be categorized into two main categories, namely the spectrogram based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>, and the waveform based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. DNN based music source separation models are mainly based on three network architectures: Fully Connected Network (FCN) <ref type="bibr" target="#b12">[13]</ref>, Convolutional Neural Networks (CNN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> and Long Short-Term Memory (LSTM) <ref type="bibr" target="#b4">[5]</ref>. Recently, CNN and LSTM have been combined to achieve state-of-theart performance for music source separation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. However, both CNN and LSTM have certain limitations. For CNN, the problem lies in the requirement of a large receptive field for source separation task <ref type="bibr" target="#b14">[15]</ref>. Although deeper CNNs are able to obtain a larger receptive field, the increase of parameters makes training difficult. Another way to expand the receptive field is to apply pooling layers and aggregate the context, which, however, result in the loss of spectral detail. To solve this problem, CNN with a multi-scale structure is proposed <ref type="bibr" target="#b10">[11]</ref>. This multi-scale structure adopts downsampling (i.e., max-pooling layer) to get low resolution feature maps from which the upsampling layers are employed to recover the original resolution. For LSTM <ref type="bibr" target="#b8">[9]</ref>, the time-dependent property prevents the parallel computation and makes inference time-consuming. Furthermore, the longterm dependency issue is not well addressed in LSTMs according to the previous study in <ref type="bibr" target="#b15">[16]</ref>.</p><p>The attention mechanism <ref type="bibr" target="#b16">[17]</ref> is a recent advance in neural network modeling. It automatically learns feature interactions from data without any human domain knowledge. Recently, Vaswani et al. proposed a novel neural network structure -Transformer <ref type="bibr" target="#b17">[18]</ref>, which uses only the attention mechanism structure to obtain the state-of-the-art result in the English-French translation task. Using the attention mechanism, the Transformer is a structure that can automatically capture sequence distribution. It allows the network to learn which part of the input sequence is important, and which ones are not. Experimental results show that the Transformer is more suitable for processing sequences than LSTMs, because it can solve longterm dependency problems better than LSTMs <ref type="bibr" target="#b17">[18]</ref>. Also, since the attention mechanism has no time-dependent limitation for calculating, the Transformer can be computed in parallel easily. Furthermore, the Transformer has a larger receptive field compared with CNNs with the same number of layers. Shortly after that, a well-known framework called BERT <ref type="bibr" target="#b18">[19]</ref> was proposed to reduce the gap between the pre-training word embedding and the downstream specific Natural Language Processing (NLP) task.</p><p>Motivated by the successful application of attention mechanism, we try to study the capability of attention mechanism in the source separation task, and propose a neural network for music source separation named Sams-Net, which applies the proposed Sliced Attention mechanism to the spectrogram domain. Besides, although many time-domain models have achieved better signal-to-distortion ratio (SDR) <ref type="bibr" target="#b19">[20]</ref> than those of the spectrogram domain, modeling in the time domain does not produce good quality speech <ref type="bibr" target="#b20">[21]</ref>. So we are interested in building our model in the spectrogram domain like <ref type="bibr" target="#b8">[9]</ref>, which only feeds the magnitude spectrogram into the model. Experimental results show that our model has achieved a new state-ofthe-art result.</p><p>The rest of the paper is organized as follows. Section 2 introduces the problem formulation of the music source separation task. Section 3 illustrates the core modules of the proposed Sams-Net. Experimental setup and results are reported in Section 4, while conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Music Source Separation</head><p>Music source separation is to extract source signals from the musical mixture. Specifically, a stereo mixture x ? R 2? T can be expressed as a linear combination of c source signals s ? R 2? T :</p><formula xml:id="formula_0">x(t) = c i=1 si(t)<label>(1)</label></formula><p>To perform music source separation in spectrogram domain, the Short-time Fourier Transform (STFT) S ? R 2?T ?F for each source signal s is calculated to get the mixed spectrogram X ? R 2?T ?F during training:</p><formula xml:id="formula_1">X(t, f ) = c i=1 Si(t, f )<label>(2)</label></formula><p>where T and F are the dimension of the time and frequency axis respectively. The separation network then takes the magnitude spectrogram |X| as input to estimate a mask M ? R 2?T ?F for each source. The reconstruction of time domain source s is accomplished by calculating the inverse STFT (ISTFT) for the estimated spectrogram, which is:</p><formula xml:id="formula_2">si(t) = ISTFT(|X(t, f )| Mi(t, f )) ? e ?X(t,f )j ) (3)</formula><p>where ?X(t, f ) is the phase of mixed musical segment, and is element-wise multiplication.</p><p>To this end, the learning objective is to minimize the audio based squared l2-norm <ref type="bibr" target="#b10">[11]</ref> between the original and estimated sources:</p><formula xml:id="formula_3">L = arg min M c i=1 si(t) ? si(t) 2 2</formula><p>(4) <ref type="figure" target="#fig_1">Figure 1</ref> shows the flowchart of a typical music source separation system on the spectrogram domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture STFT Magnitude Spectrogram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase Spectrogram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sams-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-Frequency</head><p>Masking ISTFT Separated Sources  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Description</head><p>This section describes our proposed Sams-Net for music source separation, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. It includes two transform modules, a slice module and N attention modules. The first transform module is a CNN layer that takes the magnitude spectrogram as input and expands the channel of the feature map to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scaled Dot-Product Attention</head><p>The common attention methods include dot-product <ref type="bibr" target="#b17">[18]</ref>, concatenation <ref type="bibr" target="#b21">[22]</ref>, perceptron <ref type="bibr" target="#b22">[23]</ref>, etc. In this study, we apply scaled dot-product attention to each channel of a feature map. Specifically, for a given feature map in feature space of R C?T ?F , three CNN layers with kernel size of 1 ? 1 are employed separately to transform this feature map into queries Q, keys K, and values V ? R C?T ?F . Then, for each feature channel c, dot products of the t th time frame of query Qc,t ? R F and all keys Kc ? R T ?F are calculated as attention weights. Softmax is applied to the attention weights before aggregating the values Vc. In practice, we compute the attention function on a set of queries Qc simultaneously:</p><formula xml:id="formula_4">Attention(Qc, Kc, Vc) = softmax QcKc T ? C Vc<label>(5)</label></formula><p>where 1/ ? C is a regulating term to avoid large inner product as well as gradient vanishing. We denote the scaled dot-product attention for the whole feature map as Attention(Q, K, V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Head Attention</head><p>To further exploit the modeling ability of attention mechanism, we apply multi-head attention <ref type="bibr" target="#b17">[18]</ref> on top of the scaled dotproduct attention. In this study, the output feature map F of the transform block is firstly fed into three convolutional layers to produce queries Q h , keys K h , and values V h ? R C?T ?F for the h th head. Then, the scaled dot-product attention is applied for each of the H heads, and the outputs are concatenated along the channel axis:</p><formula xml:id="formula_5">MultiHead(Q, K, V) = Concat(head1, ? ? ? , headH ), head h = Attention(Conv Q h (F), Conv K h (F), Conv V h (F)) (6) where Conv Q h , Conv K h , Conv V h</formula><p>are CNN layers to produce queries, keys, and values for h th head.</p><p>Finally, an additional CNN layer with the kernel size 3 ? 3 is employed to recover the channel dimension of the multi-head attentive output from H ? C to C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sliced Attention</head><p>Due to the typically long duration of a song in the practical application, we apply slice operation to the magnitude spectrogram before attention operation.</p><p>Specifically, the magnitude spectrogram is sliced into I chunks without overlap, yielding T /I frames per chunks. Multi-head attention is applied to these I chunks separately. The resulting of I attention values are concatenated along the time axis as the final output SA: SA = Concat(slice1, slice2, ? ? ? , sliceI ) slicei = MultiHead(Qi, Ki, Vi)</p><p>where Qi, Ki and Vi are queries, keys and values for chunk i.</p><p>With the sliced operation, the scope of attention is narrowed down to the intra-chunk features. We define the slice operation with the multi-head scaled dot-product attention as sliced attention. <ref type="figure" target="#fig_3">Figure 3</ref> shows a flowchart of the sliced attention. The feature dimension of the output remains unchanged after sliced attention.</p><p>The reason we apply a slice operation lies in the inherent data pattern of songs. There are recurring elements <ref type="bibr" target="#b23">[24]</ref>, such as notes, pitch, timbre, and chords within a song. Also, musical style or pattern always changes. For example, the principal musical instrument can suddenly change from drums to bass, followed by a pure human voice. In this case, the sliced attention provides a mechanism for the network to focus a small piece of the song with the same musical style, without taking other parts which are irrelevant to the current one.</p><p>The slice operation is applied to a whole song at validation and test stages. During training, a short chunk is randomly sliced within a song at each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Depthwise Separable CNN</head><p>Considering the large size of our model, we choose lightweight depthwise separable CNN <ref type="bibr" target="#b24">[25]</ref> rather than conventional CNN. Specifically, it has two operations: depth-wise convolution and point-wise convolution. For depth-wise convolution, each channel of the feature map is applied to a convolutional layer with the kernel size of 3 ? 3 independently. Different from the conventional CNN, one convolutional kernel of the depth-wise convolutional layer is responsible for one channel. For point-wise convolution, the output of the depth-wise convolution operation is fed to a convolutional layer with the kernel size of 1 ? 1, which aims to further aggregate the information among different channels of the feature map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Layer Norm</head><p>In the proposed Sams-Net, layer normalization <ref type="bibr" target="#b25">[26]</ref> is performed after the sublayers, i.e., the sliced attention layers, and the depthwise separable CNN layers. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, residual connections are also applied between two normalization layers, i.e., shortcut connection is inserted between the inputs of the previous and current normalization layer. The computing formular is as follows:</p><formula xml:id="formula_7">x + Sublayer(LayerNorm(x))<label>(8)</label></formula><p>where Sublayer(x) is a function implemented by the sublayer itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We evaluated Sams-Net on MUSDB18 dataset <ref type="bibr" target="#b26">[27]</ref>, which is prepared for the SiSEC 2018 campaign <ref type="bibr" target="#b7">[8]</ref>. MUSDB18 has 100 and 50 songs in the training and test set, respectively. In this dataset, each song is a mixture with four sources, i.e., vocals, bass, drums, and other, all of which are recorded in stereo format with the sampling rate of 44.1 kHz. During the training stage, we randomly select 6 seconds from each training track as the training data. In the validation stage, we use the recommended tracklist provided by <ref type="bibr" target="#b26">[27]</ref> as the validation data. The STFT used in our system is computed based on a 93 ms Hamming window with 75% overlap between frames and a 4096-point discrete Fourier transform. Also, data augmentation <ref type="bibr" target="#b27">[28]</ref> is used during training. We apply random gains between 0.25 and 1.25 to all sources before mixing. Besides, the channels of the stereo mixtures are randomly swapped. For the evaluation on MUSDB18, we used the museval package <ref type="bibr" target="#b7">[8]</ref> and BSSEval v4 toolbox <ref type="bibr" target="#b19">[20]</ref> for a fair comparison with previously reported results. The average of the median signal-to-distortionratio (SDR) of each song in the test set is used as the evaluation metric.</p><p>We train the model based on PyTorch framework <ref type="bibr" target="#b32">[33]</ref> with 2 NVIDIA TITAN RTX GPU. The parameters of the network are updated using the Adam optimizer <ref type="bibr" target="#b33">[34]</ref> with an initial learning rate of 0.0001. We terminate the training process when the validation loss no longer descends after 140 epochs. Due to the limitation of the GPU memory, we can only train 3 Sliced Attention modules with 2 heads and 64 channels of the CNN feature maps.   <ref type="table" target="#tab_1">Table 2</ref> shows the SDR metrics with different numbers of slices used in Sams-Net. The observation is that models with the proposed slice operation (I = 2,4,8,12) outperform the model without slice operation (I =1). As the number of slices increases from 1 to 12, the four sources' SDR metrics also increase. However, performance degrades when the slice number increases above 12. The reason may be that each chunk's duration is too short to contain sufficient key features for attentive value learning with a large number of slices. For the MUSDB18 dataset, the optimal slice number is around 12, but this does not apply to other datasets, especially for those with unknown sources. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the performance of Sams-Net without the attention module decreases significantly, which means the effectiveness of the attention mechanism used here. Also, we compare our model against previously published and state-ofthe-art models for the MUSDB18 dataset (models trained with extra data are not listed here). The SDR results are either taken from the SiSEC 2018 <ref type="bibr" target="#b7">[8]</ref> evaluation scores <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref> or from the related papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. The Ideal Ratio Mask oracle (IRM oracle) <ref type="bibr" target="#b8">[9]</ref> at the top line computes the best possible mask using the ground truth sources. The proposed Sams-Net outperforms both waveform-based and spectrogram-based models on vocals and drums categories, and achieve the stateof-the-art result on the averaged score. Besides, compared with the best models of Demucs <ref type="bibr" target="#b11">[12]</ref> and Meta-TasNet <ref type="bibr" target="#b31">[32]</ref> on bass and other categories, the proposed network contains 99.4% and 91.8% fewer parameters with only 9.9% and 2.4% relative performance degradation respectively. We also provide some samples inferred by Sams-Net and other models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> online 1 for intuitive comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Discussions</head><p>The number of parameters in Sams-Net is about 3.7M, which is smaller than that of most other methods. Compared with the baseline Open-Unmix model <ref type="bibr" target="#b8">[9]</ref> containing about 8.9M parameters, our model achieves 5.4% improvement in terms of average SDR. We believe that the increase of parameters of Sams-Net can further enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a sliced attention-based neural network for music source separation, named Sams-Net. It improves the performance of music source separation by discriminating the importance of different feature interactions. We also propose a sliced attention mechanism to learn the importance of each feature interaction from every segment of the magnitude spectrogram. As shown by the experimental results, the proposed Sams-Net, with lesser parameters, achieves better performance than other methods in terms of SDR.</p><p>For future works, we intend to model the data in the timedomain to incorporate the phase information. Moreover, as the points of music style change are not uniformly distributed, we can't just simply slice each song equally. We hope to automatically identify the change point of musical style within a song, so that the sliced attention can be performed within a chunk with the same style. Classifier at frame-level may be a possible solution to find the pattern of musical styles for each frame, so that a change point can be determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The flow chart of our separation system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of Sams-Net. C, while the second one is a transpose CNN layer that aggregates the channel dimension of the feature map from C to 2 and generates the estimated masks. The slice module (section 3.3) is only applicable to the validation and test stage, not the training stage. The attention block consists of a multi-head scaled dot-product attention layer (section 3.1-3.3), a depth-wise separable CNN layer (section 3.4) and several layer normalization layers (section 3.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Model architecture of our Sliced Attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison SDR metric of our proposed method with other models on the test set of MUSDB18 dataset.</figDesc><table><row><cell>Model</cell><cell>Domain</cell><cell># Param</cell><cell>Vocals</cell><cell>Drums</cell><cell>Test SDR (dB) Bass</cell><cell>Other</cell><cell>Average</cell></row><row><cell>IRM oracle</cell><cell>N/A</cell><cell>N/A</cell><cell>9.43</cell><cell>8.45</cell><cell>7.12</cell><cell>7.85</cell><cell>8.21</cell></row><row><cell>DeepConvSep [29]</cell><cell>Spectrogram</cell><cell>0.32M</cell><cell>2.37</cell><cell>3.14</cell><cell>0.17</cell><cell>-2.13</cell><cell>0.89</cell></row><row><cell>WaveNet [30]</cell><cell>Waveform</cell><cell>3.30M</cell><cell>3.35</cell><cell>4.13</cell><cell>2.49</cell><cell>2.60</cell><cell>2.60</cell></row><row><cell>Wave-U-Net [11]</cell><cell>Waveform</cell><cell>10.20M</cell><cell>3.25</cell><cell>4.22</cell><cell>3.21</cell><cell>2.25</cell><cell>3.23</cell></row><row><cell>Spect U-Net [31]</cell><cell>Spectrogram</cell><cell>9.84M</cell><cell>5.74</cell><cell>4.66</cell><cell>3.67</cell><cell>3.40</cell><cell>4.37</cell></row><row><cell>Open-Unmix [9]</cell><cell>Spectrogram</cell><cell>8.90M</cell><cell>6.32</cell><cell>5.73</cell><cell>5.23</cell><cell>4.02</cell><cell>5.36</cell></row><row><cell>Demucs [12]</cell><cell>Waveform</cell><cell>648.00M</cell><cell>6.29</cell><cell>6.08</cell><cell>5.83</cell><cell>4.12</cell><cell>5.58</cell></row><row><cell>Meta-TasNet [32]</cell><cell>Waveform</cell><cell>45.50M</cell><cell>6.40</cell><cell>5.91</cell><cell>5.58</cell><cell>4.19</cell><cell>5.52</cell></row><row><cell>MMDenseLSTM [14]</cell><cell>Spectrogram</cell><cell>4.88M</cell><cell>6.60</cell><cell>6.41</cell><cell>5.16</cell><cell>4.15</cell><cell>5.58</cell></row><row><cell>Sams-Net (w/o Attention)</cell><cell>Spectrogram</cell><cell>3.64M</cell><cell>4.80</cell><cell>4.71</cell><cell>3.89</cell><cell>3.22</cell><cell>4.16</cell></row><row><cell>Sams-Net</cell><cell>Spectrogram</cell><cell>3.70M</cell><cell>6.61</cell><cell>6.63</cell><cell>5.25</cell><cell>4.09</cell><cell>5.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A comparison SDR metric of Sams-Net among several slice numbers (in dB), .</figDesc><table><row><cell cols="6">Slices Vocals Drums Bass Other Average</cell></row><row><cell>1</cell><cell>6.42</cell><cell>6.55</cell><cell>5.21</cell><cell>3.97</cell><cell>5.53</cell></row><row><cell>2</cell><cell>6.44</cell><cell>6.59</cell><cell>5.32</cell><cell>4.04</cell><cell>5.60</cell></row><row><cell>4</cell><cell>6.42</cell><cell>6.57</cell><cell>5.29</cell><cell>4.08</cell><cell>5.59</cell></row><row><cell>8</cell><cell>6.61</cell><cell>6.56</cell><cell>5.23</cell><cell>4.06</cell><cell>5.62</cell></row><row><cell>12</cell><cell>6.61</cell><cell>6.63</cell><cell>5.25</cell><cell>4.09</cell><cell>5.65</cell></row><row><cell>16</cell><cell>6.51</cell><cell>6.45</cell><cell>5.19</cell><cell>4.01</cell><cell>5.54</cell></row><row><cell>18</cell><cell>6.57</cell><cell>6.37</cell><cell>4.94</cell><cell>3.88</cell><cell>5.44</cell></row><row><cell>24</cell><cell>6.26</cell><cell>5.98</cell><cell>4.55</cell><cell>3.68</cell><cell>5.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://tinglok.netlify.com/files/samsnet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: The perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pitch-based monaural segregation of reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="458" to="469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Separation of singing voice from music accompaniment for monaural recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1475" to="1487" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Singing-voice separation from monaural recordings using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="477" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Musical source separation using time-frequency source priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LVA/ICA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Openunmix-a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ISMIR</title>
		<imprint>
			<biblScope unit="page" from="334" to="340" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Music Source Separation in the Waveform Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno>02379796v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LVA/ICA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4898" to="4906" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
	<note>A field guide to dynamical recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new framework for supervised speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="1136" to="1140" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6077" to="6086" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIDL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">State of the art report: Audio-based music structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="625" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Musdb18-a corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno>02190845v1</idno>
	</analytic>
	<monogr>
		<title level="j">HAL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="261" to="265" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monoaural audio source separation using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LVA/ICA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="258" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="4619" to="4623" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Denoising auto-encoder with recurrent skip connections and residual regression for music source separation,&quot; in ICMLA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="816" to="820" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
