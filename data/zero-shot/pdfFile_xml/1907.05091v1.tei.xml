<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
							<email>jiahui-z15@mails.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
							<email>zhao-h13@mails.</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
							<email>anbang.yao@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
							<email>yurong.chen@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
							<email>liao@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatial Group Convolution</term>
					<term>Sparse Convolutional Network</term>
					<term>Efficient Neural Network</term>
					<term>Semantic Scene Completion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Spatial Group Convolution (SGC) for accelerating the computation of 3D dense prediction tasks. SGC is orthogonal to group convolution, which works on spatial dimensions rather than feature channel dimension. It divides input voxels into different groups, then conducts 3D sparse convolution on these separated groups. As only valid voxels are considered when performing convolution, computation can be significantly reduced with a slight loss of accuracy. The proposed operations are validated on semantic scene completion task, which aims to predict a complete 3D volume with semantic labels from a single depth image. With SGC, we further present an efficient 3D sparse convolutional network, which harnesses a multiscale architecture and a coarse-to-fine prediction strategy. Evaluations are conducted on the SUNCG dataset, achieving state-of-the-art performance and fast speed. Code is available at https://github.com/zjhthu/SGC-Release.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D shape processing has attracted increasing attention recently, because large scale 3D datasets and deep learning based methods open new opportunities for understanding and synthesizing 3D data, such as segmentation and shape completion. These 3D dense prediction tasks are quite useful for many applications. For example, robots need semantic information to understand the world, while knowing complete scene geometry can help them to grasp objects <ref type="bibr" target="#b42">[43]</ref> and avoid obstacles. However, it is not a trivial task to adopt 3D Convolutional Neural Network (CNN) by just adding one dimension to 2D CNN. Dense 3D CNN methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30]</ref> face the problem of cubic growth of computational and memory requirements with the increase of voxel resolution. But meanwhile, we observe that 3D data has some attractive characteristics, which inspire us to build efficient 3D CNN blocks. Firstly, intrinsic sparsity in 3D data. Most of the voxels in a dense 3D grid are empty. Non-trivial voxels usually exist near the boundaries of objects. This property has been explored in several recent works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Secondly, redundancy in 3D voxels. Dense 3D voxels are usually redundant, discarding a large portion of voxels (e.g. 70%) randomly does not prevent humans from reasoning the overall semantic information, as shown in <ref type="figure">Fig. 1</ref>. Thirdly, different subsets of original dense voxels contain complementary information. It is hard to recognize objects with small size and complex geometry when giving only partial voxels. These properties motivate us to design computation-efficient 3D CNNs for dense prediction tasks. We adopt Sparse Convolutional Network (SCN) <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b3">4</ref> to exploit the intrinsic sparsity of 3D data, which encodes sparse 3D data with Hash Table and presents sparse convolution design. These designs can avoid unnecessary memory or computation cost on empty voxels. However, the computation is still intensive when the resolution is high or input is not so sparse. For example, the complexity of the baseline SCN used in this paper is about 80 GFLOPs while only outputting 1/64 sized predictions. Our work takes advantage of SCN and steps further by encouraging higher sparsity in feature maps. We propose SGC to exploit the redundancy of 3D voxels, which partitions features into different groups and makes voxels sparser. Then we conduct sparse convolution on each group. Because only valid voxels are considered in sparse convolution rather than all voxels in a regular grid, and only partial voxels exist in each group after partition, the computation of networks with SGC can be significantly reduced compared to previous SCN. Besides, in order to utilize the complementary information of different groups, results of different groups after certain SGC operations are gathered for further processing.</p><p>Network acceleration methods in 2D CNN such as weight pruning, quantization, and Group Convolution (GC) design <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> can also be used, but these methods have not been well explored in 3D CNNs for now. Different from these methods, SGC speeds up 3D CNNs from another perspective by encouraging sparsity in feature maps. Though recently there are works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> exploiting sparsity in feature maps, they are not suitable for dense prediction tasks because some voxels need to be predicted are deactivated in the network. Our method is orthogonal to Group Convolution, which is an operation widely used in recent CNN architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref>. SGC is defined on spatial dimensions while GC is defined on channel dimension. Besides, because voxels in different groups are similar, weights are shared between different groups in SGC, which is not the case in GC.</p><p>We validate our method on semantic scene completion as test case to show its effectiveness on 3D dense prediction tasks. This task not only aims to predict semantic labels, but also needs to output complete structure which is different from the input. We introduce a novel SCN architecture that is applicable to scenarios where output has a different structure with input. layer and Abstracting Module are designed to generate voxels which are absent in input and remove trivial voxels respectively. Multiscale encoder-decoder architecture and coarse-to-fine prediction strategy are used for final predictions. We evaluate our network on the SUNCG dataset <ref type="bibr" target="#b39">[40]</ref> and achieve state-of-theart results. Our SGC operation can reduce about 3/4 of the computation while losing only 0.7% and 1.2% in terms of Intersection over Union (IoU) for scene completion and semantic scene completion compared to networks without SGC. Our main contributions are as follows:</p><p>-We propose SGC by exploiting sparsity in features for 3D dense prediction tasks, which can significantly reduce computation with slight loss of accuracy. -We present a novel end-to-end sparse convolutional network design to generate unknown structures for 3D semantic scene completion. -We achieve state-of-the-art results on the SUNCG dataset, reaching an IoU of 84.5% for scene completion and 70.5% for semantic scene completion.</p><p>2 Related works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Deep Learning</head><p>The success of deep learning in 2D computer vision areas has inspired researchers to employ CNN in 3D tasks, such as object recognition <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>, shape completion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4]</ref>, and segmentation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1]</ref>. However, the cubic growth in data size impedes building wider and deeper networks because of memory and computation restrictions. Recently, several works attempt to solve this problem by utilizing the intrinsic sparsity of 3D data. FPNN <ref type="bibr" target="#b27">[28]</ref> used learned field probes to sample 3D data at a small set of positions, then fed features into fully connected layers. Graham et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> proposed <ref type="table">Hash Table based</ref> sparse convolutional networks and solved the "submanifold dilation" problem by forcing to keep the same sparsity level throughout the network. OctNet <ref type="bibr" target="#b36">[37]</ref> and O-CNN <ref type="bibr" target="#b43">[44]</ref> used Octree-based 3D CNN for 3D shape analysis. SBNet <ref type="bibr" target="#b34">[35]</ref> performed convolution on blockwise decomposition of the structured sparsity patterns. Apart from these methods based on volumetric representation, PointNet <ref type="bibr" target="#b30">[31]</ref> is a seminal work building deep neural networks directly on point clouds. PointNet++ <ref type="bibr" target="#b32">[33]</ref> and Kd-Networks <ref type="bibr" target="#b22">[23]</ref> further employed hierarchical architectures to capture local structures of point clouds.</p><p>Our main difference with these architectures is the introduction of SGC, which encourages higher sparsity in features and makes networks more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation-efficient Networks</head><p>Most previous computation-efficient networks focus on reducing model size to accelerate inference, such as pruning weight connections <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref> and quantizing weights <ref type="bibr" target="#b7">[8]</ref>. Another line of works uses GC to reduce the computation, such as MobileNet <ref type="bibr" target="#b19">[20]</ref> and ShuffleNets <ref type="bibr" target="#b49">[50]</ref>. GC separates features to different groups along channel dimension and performs convolution on each group parallelly. Besides, Graham <ref type="bibr" target="#b8">[9]</ref> used smaller filters on different lattices to decrease the computation.</p><p>However, there are seldom works designing computation-efficient networks by exploiting higher sparsity in feature maps for 3D dense prediction tasks. Vote3deep <ref type="bibr" target="#b5">[6]</ref> encouraged sparsity in feature maps using L 1 regularization. ILA-SCNN <ref type="bibr" target="#b12">[13]</ref> used adaptive rectified linear unit to control the sparsity of features. But these methods are not suitable for dense prediction tasks, because some desired voxels are deactivated in the network and cannot be recovered. Besides, Li et al. <ref type="bibr" target="#b25">[26]</ref> also exploited sparsity and reduced the computation of 2D segmentation task with cascaded networks, and only hard pixels are handled by deeper sub-models.</p><p>Different with these methods, we create groups along the spatial dimensions and make voxels in each group sparser. Computation of convolution can be largely reduced because only partial valid voxels are used in each computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">3D Semantic Segmentation and Shape Completion</head><p>3D semantic segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> and Shape Completion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> are both active areas in computer vision. 3D segmentation gives semantic labels to observed voxels, while shape completion completes missing voxels. SSCNet <ref type="bibr" target="#b39">[40]</ref> combined these two tasks together and showed that segmentation and completion can benefit from each other. In order to generate high resolution 3D structure, various methods had been explored, such as long short-term memorized <ref type="bibr" target="#b14">[15]</ref>, coarse-to-fine strategy <ref type="bibr" target="#b4">[5]</ref>, 3D generative adversarial network <ref type="bibr" target="#b46">[47]</ref>, and inverse discrete cosine transform <ref type="bibr" target="#b21">[22]</ref>. Recently, segmentation and completion are both benefited from these advanced 3D deep learning methods described in section 2.1.</p><p>Different methods have been presented in the 3D segmentation challenge <ref type="bibr" target="#b48">[49]</ref>, such as SCN, Pd-Network, densely connected PointNet, and Point CNN <ref type="bibr" target="#b26">[27]</ref>. For 3D completion tasks, advanced Octree-based CNN methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17]</ref> were also used for generating high resolution 3D outputs. Our network architecture shares some similarities with <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref>, while the main difference is that we focus on efficient model design in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly give a brief introduction to previous SCN architecture <ref type="bibr" target="#b10">[11]</ref>, and then introduce SGC for computation-efficient 3D dense prediction tasks. Thirdly, a novel sparse convolutional network architecture which can predict unknown structures will be presented for semantic scene completion. Finally, details about training and networks will be given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Convolutional Network</head><p>Previous dense 3D convolution is neither computational nor memory efficient because of the usage of dense 3D grid for representation. Another problem is that traditional "dense" convolution has the "dilation" problem <ref type="bibr" target="#b10">[11]</ref> which will destroy the sparsity of 3D feature maps. For example, after a 3?3?3 convolution, surrounding 26 voxels will be filled in. SCN addressed these problems by only storing non-empty voxels in 3D feature maps using Hash <ref type="table">Table.</ref> Only nonempty voxels are considered in sparse convolutional network. Besides, it forces to keep sparsity at the same level throughout the network when performing convolution, which means the activation pattern of next layer is the same as the previous layer. These designs can largely decrease computation and memory requirements, enabling the usage of deeper 3D CNNs.</p><p>However, there is still intensive computation in 3D sparse CNN as mentioned above. Thus reducing the computation of 3D sparse CNN is necessary for realtime applications. Another problem of previous SCN is that it cannot be directly used for scene completion task. Because completion needs to output a complete structure which is different from the input, while previous SCN can only output predictions with the same structure as input. We introduce a novel sparse convolutional network to predict unknown structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Group Convolution</head><p>This section introduce SGC which can significantly reduce the computation of 3D dense prediction tasks. Our design makes use of those three properties of 3D data described in section 1 (see <ref type="figure" target="#fig_1">Fig. 2</ref>). We partition voxels uniformly into different groups, then conduct 3D sparse convolution on each group. Weights are shared among different groups because these groups are similar. Features of different groups are gathered later in order to utilize the complementary information of different groups. In the implementation of SGC, we partition features along the spatial dimensions and then stack different groups along the batch dimension. For one sparse feature map whose size is B?D?H ?W ?C (batchsize?depth?height?width? channel), after the partition operation, it becomes (</p><formula xml:id="formula_0">G ? B) ? D ? H ? W ? C,</formula><p>where G is the group number. Note that because we use <ref type="table">Hash Table based rep</ref> </p><formula xml:id="formula_1">/G = N ? 1 G ?k 3 N ?k 3</formula><p>of original convolution when ignoring the bias computation, where N is the total number of valid voxels, and k is the filter size. SGC can readily replace plain 3D sparse convolution in existing CNNs.</p><p>Obviously, partition strategy plays an important role in SGC. Here we present two different partition strategies:</p><p>-Random partition method. Voxels of feature maps are partitioned into different groups randomly and uniformly. -Partition with a fixed pattern. Random partition expects convolutional filters to be invariant to all possible patterns of activation, which may be hard for CNN to learn. We propose to partition input voxels with a fixed pattern for all input voxels throughout training and testing. For example, we can partition voxels by the following formulation: where i is the group index, (x, y, z) is the position of the voxel, G is the total group number, mod is the modulus operation, (a, b, c) controls the distribution of different groups. This strategy can also partition voxels uniformly but in a fixed pattern manner. Different (a, b, c) and G give different patterns.</p><formula xml:id="formula_2">i = mod(ax + by + cz, G)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sparse Convolutional Networks for Semantic Scene Completion</head><p>This section will present a novel SCN architecture for semantic scene completion. Previous SCN <ref type="bibr" target="#b9">[10]</ref> keeps the sparsity unchanged to avoid "submanifold dilation" problem. The output of previous SCN has the same known structure as input. This design restricts its application in shape completion, RGB-D fusion and etc., which aim to predict unknown structures. In order to generate unknown voxels for semantic scene completion task, we have to break this restriction.</p><p>Here we use multiscale encoder-decoder architecuture <ref type="bibr" target="#b37">[38]</ref>. As shown in <ref type="figure">Fig.  3</ref>, encoder modules are constituted of sparse convolutions described in section 3.1. While in decoder modules, we implement a "dense" deconvolution layer to generate new voxels. More specifically, after a "dense" up-sampling deconvolution layer, each voxel in low resolution will generate 2 ? 2 ? 2 voxels in high resolution. The sparsity changes rather than keeping the same as the layers in encoder modules. New voxels can be generated in this process.</p><p>Applying this module repeatedly in each scale can generate all missing structures but it will soon destroy the sparsity of 3D feature maps just as the "submanifold dilation" problem. So we introduce Abstracting Module similar to <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref>, which abstracts a coarse structure and removes unnecessary voxels in low resolution. Details will be refined in high resolution. The Abstracting Module contains a 1 ? 1 ? 1 convolution layer and a softmax layer, these layers give a prediction in this scale and provide guiding information for abstracting. Only voxels with non-empty labels and their surrounding voxels are abstracted. Abstracting these surrounding empty voxels within a distance of k could provide fine details. k = 1 works well in our practice. We apply the Abstracting module in resolution higher than 32 because removing voxels in early stages may hurt the performance. Since our setting exploits resolution 64 for output, one Abastract module is enough.</p><p>Voxel-wise softmax loss is used in the two scales which give a prediction:</p><formula xml:id="formula_3">L i = 1 w j j w j L sm (p j , y j ),<label>(2)</label></formula><p>where i ? {0, 1} means resolution scale as shown in <ref type="figure">Fig. 3</ref>, L sm is softmax loss, y j is ground truth label of voxel j, p j is the predicted possibility, and w j ? 0, 1 is the weight of this voxel. The final loss is a summation of all losses as follows:</p><formula xml:id="formula_4">L = i ? i L i ,<label>(3)</label></formula><p>where ? i is the weight for each scale. We found ? i = 1 works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Dataset. We train and evaluate our network on the SUNCG dataset, which is a manually created large-scale synthetic scene dataset <ref type="bibr" target="#b39">[40]</ref>. It contains 139368 valid pairs of depth map and complete labels for training, and 470 pairs for testing. Depth maps are converted to volumes with a size of 240 ? 144 ? 240. The ground truth labels are 12-class volumes with 1/4 size of input volume. Network Details. The detailed network architecture is illustrated in <ref type="figure">Fig. 3</ref>. For volumetric data encoding, we use flipped Truncated Signed Distance Function (fTSDF), which can enhance performance because it eliminates strong gradients in empty space <ref type="bibr" target="#b39">[40]</ref>. The input size of our network is 256 3 , and we put the original fTSDF volume in the middle of input volume. The input volume is downsampled twice using Max-pooling layer. Then a U-Net architecture follows, which contains six resolution scales, from 64 3 to 2 3 . Features from encoding stages and decoding stages are summed, and zeros are filled at missing locations. The network uses pre-activation Resnet block in encoding and decoding modules <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and each block has two 3 ? 3 ? 3 convolutions. Down-sampling and upsampling are implemented by convolution layers with stride 2 and kernel size 2.</p><p>SGC is used in resolution scales not less than 32 <ref type="bibr" target="#b2">3</ref> , which account for most of the computation. Partition operation is performed again once the resolution scale changes, which can help information flow across each other group. The weight of each voxel is computed by randomly sampling empty and nonempty voxels at a ratio of 1 : 2 <ref type="bibr" target="#b39">[40]</ref>. All non-empty voxels are positive examples.</p><p>For negative examples, we mainly consider empty voxels around the surface as hard examples, which can be determined by the TSDF value of GT labels (|T SDF | &lt; 1). The ratio of hard negative and easy negative examples is 9:1. Training Policy. Networks are trained using stochastic gradient descent with a momentum of 0.9. The initial learning rate is 0.1, and L2 weight decay is 1e-4. We train our network for 10 epochs with a batch size of 4, and decay learning rate by a factor of exp(?0.5) in each epoch. In order to reduce training time, we randomly select 40000 samples in each epoch, and the total training time is about 5 days with a GTX TitanX GPU and two Intel E5-2650 CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we evaluate our network on the standard SUNCG test dataset. Both semantic scene completion results and scene completion results are given. Voxel-level IoU evaluation metric is used. Semantic scene completion results are evaluated on both observed and unobserved voxels, and completion results are evaluated on unobserved voxels. <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table">Table 2</ref> show the quantitative results of our network without or with SGC. <ref type="figure">Fig. 4</ref> shows the qualitative comparison with previous work. We also give results on real-word noisy NYU dataset <ref type="bibr" target="#b38">[39]</ref>. <ref type="table" target="#tab_3">Table 1</ref> shows the result of our baseline network without SGC (group number is 1). We outperform the previous SSCNet by a significant margin, having an improvement of 24.1% in semantic scene completion and 11.0% in scene completion, and achieving state-of-the-art results. Our network exceeds SSCNet in almost all classes, especially in small and hard categories such as chair, tvs and objects. We attribute this improvement to the novel SCN architecture that enables the usage of several advanced deep learning techniques such as deeper networks (15-layer vs 57-layer), multiscale network architecture (3 resolution scales vs 8 resolution scales), batch normalization layer <ref type="bibr" target="#b20">[21]</ref> and stacked Resnet style blocks. <ref type="figure">Fig. 4</ref> shows the visualization results of semantic scene completion from a single depth image. Obviously our baseline network produces visually better results compared to SSCNet, especially around the object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparision to SSCNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatial Group Convolution Evaluation</head><p>This section describes the results of networks with SGC (see <ref type="table">Table 2</ref>). We conduct experiments on 2,3,4,6 groups with different partition strategies. The effi-  <ref type="figure">Fig. 4</ref>. Qualitative results of our network and SSCNet. We achieve obviously much better results, such as predictions around object boundaries. ciency is evaluated with FLOPs, i.e., the number of floating-point multiplicationadds of the whole network. As shown in <ref type="table">Table 2</ref>, SGC can reduce about (G?1)/G of the whole computation. Experiments show that 3D sparse CNNs can be sparsity-invariant to some extent, because accuracy only drops about 0.5% when dividing voxels into two groups even randomly, while only about 50% voxels preserved in this case. Increasing group number will reduce more computation at the cost of a little drop of performance. Compared to random partition method, fixed pattern partition strategy can give better performance yet requires less computation. For example, 1.7% IoU enhancement for semantic completion can be achieved using fixed pattern partition method when dividing voxels into four groups. Overall, SGC can significantly reduce the computation while maintaining accuracy, achieving a drop of only 0.7% and 1.2% in terms of IoU for scene completion and semantic completion task while using only 27.8% computation. <ref type="table">Table 3</ref> shows the detailed semantic scene completion results of different categories using SGC. The accuracies of best and worst three categories compared to baseline network are marked in the table. It can be found that the IoUs of categories with small physical sizes such as chair, furniture, and objects drop more than categories with large size such as ceiling and floor. This may be caused by the fact that those small objects have fewer voxels. Dividing these voxels into different groups may lose important geometric information and makes it harder <ref type="table">Table 3</ref>. Influence of SGC on each category. The numbers in third to sixth row mean IoU (%) drop when using SGC. The best or worst three are underlined or bolded.  to distinguish these objects (see chair's leg in <ref type="figure">Fig. 1</ref>). While large objects have surplus voxels, sparser voxels can still keep a rough structure. So, a possible future work to increase the accuracy of semantic scene completion is to adaptively handle large easy objects and small hard objects, sampling small objects with high density while sampling large objects with relatively low density. We also tried sparsity invariant convolution <ref type="bibr" target="#b41">[42]</ref> in random partition method, which normalizes convolution by a factor of valid voxels number, but it does not work in our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on NYU dataset</head><p>NYU <ref type="bibr" target="#b38">[39]</ref> contains 1449 depth maps captured by Kinect. Following SSCNet <ref type="bibr" target="#b39">[40]</ref>, we use Guo et al.'s algorithm <ref type="bibr" target="#b11">[12]</ref> to generate ground truth annotations for semantic scene completion task. The object categories are mapped based on Handa et al. <ref type="bibr" target="#b15">[16]</ref>. We trained the network described above from scrath on NYU dataset. The base of exponential learning rate decay is 0.12 and we trained it for 40 epochs using the whole dataset. Other hyperparameters are same as experiments on SUNCG. <ref type="table" target="#tab_4">Table 4</ref> shows that our network achieves an improvement of 2.0% in semantic scene completion and 1.1% in scene completion compared to SSCNet. <ref type="table" target="#tab_5">Table 5</ref> gives detail results on NYU dataset. It shows that SGC operation is still effective on real data. The fixed pattern partition method gives comparable or even better results than baseline network, and it is consistently better than the random partition method. Note that there exists a gap between the improvements on SUNCG and NYU. We attribute this gap to the fact that misalignment and incomplete annotations are common in the generated labels <ref type="bibr" target="#b11">[12]</ref>. This may both mislead the training and evaluation procedures, and it may be unfavorable for our network considering the sparsity geometry representation.  The first row shows the statistics of the first convolution layer, and the second row shows that of the last convolution layer. Filters of SGC have "sharper" histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">What does Spatial Group Convolution learn?</head><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we visualize the histograms of learned weight values of networks without or with SGC using random partition method. It can be observed that filters of SGC have "sharper" histograms while normal SCN filters have relative "flat" histograms, which means the values of SGC filters are pretty close. The histograms become "sharper" with the increase of group number. This may be caused by that filters of SGC need to be invariant to different sparsity patterns, so the values of filters at different locations had better be close to adapt to different sparsity patterns. As for SGC with fixed pattern partition, we find it learned an irregular convolutional kernel. In <ref type="figure">Fig. 6a</ref>, we show a simple case in a 2D convolution which divides voxels into two groups. The valid convolutional kernel shape is always "X" because the sparsity pattern keeps the same when sliding the convolutional kernel. <ref type="figure">Fig. 6b</ref> shows the valid convolutional filter shapes used in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Information flow among different groups</head><p>The SGC operation partitions voxels into different groups. During convolution, different groups are independent and have no information flow across each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group2</head><p>Group3 Group4 Group6 (a) (b) <ref type="figure">Fig. 6</ref>. Illustration of SGC with fixed pattern partition. (a) shows that for a 3 ? 3 kernel, an "X" shape filter is learned when partitioning voxels into two groups. (b) shows the learned 3 ? 3 ? 3 filters in <ref type="table">Table 2</ref>. Filters are drawn by slice.</p><p>However, after SGC, the voxels are gathered and fed into down-sampling convolution or up-sampling deconvolution layers, in which information of different groups can communicate. Besides, we also explored more complicated methods to help information exchange among different groups. For example, Shuffled SGC, which is inspired by ShuffleNet <ref type="bibr" target="#b49">[50]</ref>. ShuffleNet uses channel shuffle to help information flow across feature channels, while we shuffle the features across spatial dimensions which is implemented by using different partition patterns in the two convolution layers of Resnet block. But no obvious improvement is observed in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The paper presents an efficient semantic scene completion network with Spatial Group Convolution. SGC partitions feature maps into different groups along the spatial dimensions and can significantly reduce the computation with slight loss of accuracy. Besides, we propose a novel end-to-end sparse convolutional network architecture for 3D semantic scene completion and set a new accurancy record on the SUNCG dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>indicates interns at Intel Labs China. indicates corresponding authors. arXiv:1907.05091v1 [cs.CV] 11 Jul 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of SGC. Feature maps are partitioned uniformly into different groups along the spatial dimensions (only two groups are shown here). 3D CNNs are conducted on different groups and give the final dense prediction for all voxels. Weights are shared between different groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>resentation, only non-empty voxels are stored. So this operation does not require extra memory. In each convolution computation, only part of original non-empty voxels in its receptive filed participate in the calculation, and the number of valid voxels in each group is about 1/G of the original non-empty voxels after partition. The final computation cost is thus about 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Group No. Method ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. Baseline 96.6 83.7 74.9 58.9 55.1 83.3 78.0 61.5 47.4 73.5 62.9 70.5 random 0.4 -0.6 -2.0 -2.9 -4.4 -3.0 -3.2 -4.1 -3.5 -4.4 -4.7 -2.9 4 pattern 0.2 -0.3 -0.8 0.7 -3.7 -1.7 -1.5 -3.1 1.1 -1.9 -2.1 -1.2 random 0.1 -0.7 -3.7 -2.8 -5.7 -2.9 -3.5 -5.6 -5.4 -6.8 -6.0 -3.9 6 pattern 0.2 -0.4 -2.3 -3.8 -5.5 -1.8 -3.7 -6.9 -4.0 -5.5 -5.8 -3.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Histograms of learned weight values of SCN and SGC with different groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of our network and SSCNet on the SUNCG dataset. Scene completion IoU is measured on unobserved voxels, and all non-empty classes are treated as one category. Semantic scene completion IoU is measured on both observed and unobserved voxels. Overall, our method outperforms SSCNet by a large margin. Better results of each category are bold. IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. SSCNet [40] 76.3 95.2 73.5 96.3 84.9 56.8 28.2 21.3 56.0 52.7 33.7 10.9 44.3 25.4 46.4</figDesc><table><row><cell cols="2">scene completion</cell><cell></cell><cell>semantic scene completion</cell><cell></cell></row><row><cell cols="5">Method prec. recall Our 92.6 90.4 84.5 96.6 83.7 74.9 59.0 55.1 83.3 78.0 61.5 47.4 73.5 62.9 70.5</cell></row><row><cell cols="5">Table 2. Quantitative IoU (%) results of networks using SGC with random partition</cell></row><row><cell cols="5">strategy or fixed pattern partition strategy. Both accuracy and FLOPs are given. For</cell></row><row><cell cols="5">fixed pattern partition method, flexible parameters (a,b,c) are also given and we select</cell></row><row><cell cols="4">the best results in our experiments. Best trade-off is bolded.</cell><cell></cell></row><row><cell>Group No.</cell><cell>Method</cell><cell cols="3">scene completion semantic scene completion FLOPs/G</cell></row><row><cell>1(Baseline)</cell><cell></cell><cell>84.5</cell><cell>70.5</cell><cell>79</cell></row><row><cell>2</cell><cell>Random Pattern(1,1,1)</cell><cell>83.9 84.0</cell><cell>69.9 69.6</cell><cell>42 39</cell></row><row><cell>3</cell><cell>Random Pattern(1,1,1)</cell><cell>82.6 84.1</cell><cell>67.6 69.5</cell><cell>29 27</cell></row><row><cell>4</cell><cell>Random Pattern(1,2,3)</cell><cell>83.1 83.8</cell><cell>67.6 69.3</cell><cell>23 22</cell></row><row><cell>6</cell><cell>Random Pattern(1,2,1)</cell><cell>82.3 82.6</cell><cell>66.6 66.9</cell><cell>17 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Scene IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. SSCNet 57.0 94.5 55.1 15.1 94.7 24.4 0 12.6 32.1 35 13 7.8 27.1 10.1 24.7 Ours 71.9 71.9 56.2 17.5 75.4 25.8 6.7 15.3 53.8 42.4 11.2 0 33.4 11.8 26.7</figDesc><table><row><cell cols="2">completion (IoU %) and semantic scene completion results (IoU %) on</cell></row><row><cell>NYU dataset.</cell><cell></cell></row><row><cell>scene completion</cell><cell>semantic scene completion</cell></row><row><cell>Method prec. recall</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results (IoU%) of networks with SGC using different partition strategies on NYU dataset. (SSC stands for semantic scene completion)</figDesc><table><row><cell>Group No.</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell><cell>4</cell><cell></cell></row><row><cell cols="8">Method baseline random pattern random pattern random pattern</cell></row><row><cell>SSC</cell><cell>26.4</cell><cell>24.1</cell><cell>26.5</cell><cell>23</cell><cell>26.7</cell><cell>22.6</cell><cell>25.9</cell></row><row><cell cols="2">Completion 55.7</cell><cell>53</cell><cell>54.8</cell><cell>52.2</cell><cell>56.2</cell><cell>52.6</cell><cell>55.1</cell></row><row><cell>Baseline</cell><cell></cell><cell>Group2</cell><cell></cell><cell cols="2">Group4</cell><cell></cell><cell>Group6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">or called Submaniflod Sparse Convolutional Network in<ref type="bibr" target="#b10">[11]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">3d point cloud classification and segmentation using 3d modified fisher vector representation for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08241</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.59.</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2012.59" />
		<title level="m">Sparse 3D convolutional neural networks pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predicting complete 3d models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10585</idno>
		<title level="m">Inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4077" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical surface prediction for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision</title>
		<meeting>the International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling cnns for high resolution volumetric reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vd Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd International Conference on Neural Information Processing Systems. pp. 598-605. NIPS&apos;89</title>
		<meeting>the 2Nd International Conference on Neural Information Processing Systems. pp. 598-605. NIPS&apos;89<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficultyaware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note type="report_type">PointCNN. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic parsing of large-scale 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D Graph Neural Networks for RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.556</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sbnet: Sparse blocks network for fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8711" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Octnetfusion: Learning depth fusion from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision</title>
		<meeting>the International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneidre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">IEEE International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Sparsity invariant cnns</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2442" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00411</idno>
		<title level="m">3d object dense reconstruction from a single depth view</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning hierarchical shape segmentation and labeling from online repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geetchandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Preetham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhugra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices. computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
