<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
							<email>damien.teney@idiap.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<email>stephen.gould@anu.edu.aucristian.rodriguezopazo@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&amp;L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-theart accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval. Our dataset, code and pre-trained models are available at https://cuberick-orion.github.io/CIRR/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We study the task of composed image retrieval, that is, finding an image from a large corpus that best matches a user query provided as an image-language pair. Unlike traditional content-based <ref type="bibr" target="#b38">[38]</ref> or text-based <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b42">42]</ref> image retrieval where a single modality is used to describe the target image, composed image retrieval involves both visual and textual modalities to specify the user's intent. For humans the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Image</head><p>Target Image #1 Target Image #2</p><p>Modification text for #1: "Be a same breed dog with his puppy running" Modification text for #2: "Two dogs of the same breed on the floor" <ref type="figure">Figure 1</ref>. Example of composed image retrieval from the proposed CIRR dataset. The input is composed of a reference image and a modifying text, to which the model must find a close match. A major challenge is the inherent ambiguity and underspecification of visual aspects to be preserved or modified. Our dataset includes open-domain images with rich contexts to facilitate the study of such challenge. through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details. The challenge is the inherent ambiguity in knowing what information is important (typically one object of interest in the scene) and what can be ignored (e.g., the background and other irrelevant objects). However, existing datasets for this task fall short of allowing us to adequately study this problem. Consider the example in <ref type="figure">Fig. 1</ref>. Real-life images usually contain rich object interactions on various scales. In each case, to readily identify the relevant aspects to keep or change and pay less attention elsewhere (e.g., the color of the dog's fur and background objects), a model must develop in-depth visual reasoning ability and infer implicit human agreements within both the visual and language contexts. However, existing datasets are constrained to domains such as fashion products <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> or synthetic objects <ref type="bibr" target="#b40">[40]</ref> with relatively simple image contents. We argue that the current datasets are insufficient for exploring the unique research opportunity mentioned above.</p><p>Motivated by this problem, we collect the Compose Im-age Retrieval on Real-life images (CIRR) dataset. It is based on the open-domain collection of real images from NLVR 2 <ref type="bibr" target="#b35">[35]</ref>, for which we collected rich, high-quality annotations that aim to tease out the important aspects of the reference image and textual description for a given query. Compared with existing datasets, CIRR places more emphasis on distinguishing between visually similar images, which provides a greater challenge, as well as a chance for studying fine-grained vision-and-language (V&amp;L) reasoning in composed image retrieval. Our dataset also allows for evaluation on fully labeled subsets, which addresses a shortcoming of existing datasets that are not fully labeled and therefore contain multiple false-negatives (as unlabeled images are considered negative).</p><p>Meanwhile, we propose Composed Image Retrieval using Pretrained LANguage Transformers (CIRPLANT), which extends current methods into open-domain images by leveraging the knowledge of large-scale V&amp;L pre-trained (VLP) model <ref type="bibr" target="#b25">[25]</ref>. Although the advantages of such pretrained models have been validated in many visiolinguistic tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28]</ref>, to the best of our knowledge, none have been applied to composed image retrieval. We conjecture one of the reasons being the existing domain-specific datasets cannot greatly benefit from the pre-training, which uses more complex, open-world images. Moreover, to adopt the VLP models for fine-tuning, most of the downstream tasks are formulated as classification tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">25]</ref>. For composed image retrieval, it requires taking as input both the reference and target images. However, this greatly raises the computational overhead for retrieval, as the model needs to exhaustively assess each input query paired with each candidate target before yielding the one with the highest prediction score. Instead, we propose to preserve the conventional metric learning pipeline, where the input queries are jointly embedded using the VLP model and later compared with features of candidate images through 2norm distance. Specifically, our design maintains the same objective of "language-conditioned image feature modification" as previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">40]</ref>, while manages to utilize the pre-trained V&amp;L knowledge in large-scale models. We demonstrate that our proposed model reaches state-of-theart on the existing fashion dataset while outperforming current methods on CIRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image retrieval. Existing work on image retrieval using deep learning can be categorized by the type of queries considered. Content-based Image Retrieval (CBIR) refers to the use of image-only queries for product search <ref type="bibr" target="#b26">[26]</ref>, face recognition <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b34">34]</ref>, etc. This setup leaves little room for iterative user feedback or refinement. Other possible modalities to form queries include attributes <ref type="bibr" target="#b12">[13]</ref>, natural language <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b42">42]</ref>, and sketches <ref type="bibr" target="#b31">[31]</ref>. These are motivated by a more natural user experience, but require more advanced retrieval mechanisms. Vo et al. <ref type="bibr" target="#b40">[40]</ref> propose composed image retrieval that combines visual and text modalities. Here the query consists of a reference image and short text describing desired differences with this image. Guo et al. <ref type="bibr" target="#b11">[12]</ref> demonstrate the potential of this setup for the narrow domain of fashion recommendation.</p><p>Our work focuses on composed image retrieval in an open-domain setting, i.e., not restricted to fashion products for example. We specifically address the case of distinguishing visually similar images, which requires more indepth, fine-grained reasoning ablility over both the visual and language modalities.</p><p>Compositional learning. The topic of compositional learning has been extensively studied in V&amp;L tasks including visual question answering (VQA) <ref type="bibr" target="#b2">[3]</ref>, image captioning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and video retrieval <ref type="bibr" target="#b41">[41]</ref>. The aim is to produce learned joint-embedding features that capture the salient information in both visual and text modalities along with their interactions. For composed image retrieval, Vo et al. <ref type="bibr" target="#b40">[40]</ref> first propose a residual-gating mechanism that aims to control variation of the input image features through text. Hosseinzadeh and Wang <ref type="bibr" target="#b16">[17]</ref> use region-based visual features from R-CNN models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">32]</ref> originally proposed for image captioning <ref type="bibr" target="#b0">[1]</ref> and VQA <ref type="bibr" target="#b37">[37]</ref>. Recently, Chen et al. <ref type="bibr" target="#b4">[5]</ref> use a transformer-based model <ref type="bibr" target="#b39">[39]</ref> and inject the text modality at varying depths of the image model. Dodds et al. <ref type="bibr" target="#b7">[8]</ref> introduce the concept of modality-agnostic tokens, which they obtain from "divided" spatial convolutional features and LSTM hidden states. In this work, we propose a method that leverages the rich knowledge in VLP models. Our method can modify the input image features based on natural language without the need of developing monolithic architecture on the specific task.</p><p>Vision-and-language pre-training. The success of pretrained BERT <ref type="bibr" target="#b6">[7]</ref> inspired numerous attempts on VLP models, including <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b36">36]</ref>. The aim is to develop Transformer-based <ref type="bibr" target="#b39">[39]</ref> models trained on large-scale image-text triplets to produces V&amp;L representations applicable to various tasks. The advantage is clear, instead of training monolithic models on task-specific datasets from zero, different V&amp;L tasks can start with the representations learned from (usually) a considerably larger image-text corpus, and fine-tune on specific tasks. Motivated by success in other V&amp;L tasks, we propose to adopt the VLP model on composed image retrieval. The key obstacle is to design the architecture to encourage a controlled modification of image features, which, differs greatly from the conventional use cases of such models.</p><p>Datasets for composed image retrieval. Most existing datasets suitable for composed image retrieval are repurposed from other tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">40]</ref>. Images are paired within classes and textual descriptions of their differences are generated automatically from existing labels. These datasets are relatively simple visually and only contain short descriptions with simple language. CSS <ref type="bibr" target="#b40">[40]</ref> uses the synthetic images of geometric 3D shapes from CLEVR <ref type="bibr" target="#b19">[20]</ref>, paired with descriptions generated according to differences in appearance of the objects. Fashion200k <ref type="bibr" target="#b12">[13]</ref> contains approx. 200k images tagged with attributes that can be used to compose text descriptions of differences between images. MIT-States <ref type="bibr" target="#b17">[18]</ref> contains images of entities in different states each labelled with one noun and one adjective. The adjectives can describe limited differences between images. More recent works introduced human-generated descriptions. Guo et al. <ref type="bibr" target="#b10">[11]</ref> present annotations for Shoes <ref type="bibr" target="#b3">[4]</ref>, a dataset of 10k footwear images. Fashion-IQ <ref type="bibr" target="#b11">[12]</ref> contains crowd-sourced descriptions of differences between images of fashion products. Dodds et al. <ref type="bibr" target="#b7">[8]</ref> introduce benchmarks for the Birds-to-Words <ref type="bibr" target="#b8">[9]</ref> and Spot-the-Diff <ref type="bibr" target="#b18">[19]</ref> datasets.</p><p>In this paper, we introduce a new dataset that addresses current deficiencies. Our dataset is open-domain and not restricted, e.g., to fashion products <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. We design a careful collection process to produce high-quality pairs from our diverse collection of images by only associating visually-and semantically-related images. We also address the issue of false-negative targets, that is, candidate target images that are valid for a certain input query, but not labeled as such. Previous datasets failed to resolve this issue due to the cost of exhaustively labeling images against every possible query, which is mitigated by our data collection strategy. Although not used in our current work, the dataset also contains a rich set of auxiliary annotations that clarify ambiguities not addressed in the textual query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Model</head><p>In this section, we first briefly introduce the vision-andlanguage pre-trained (VLP) models, then we discuss our adaptation of it for the task of composed image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vision-and-Language Pre-trained Models</head><p>Contemporary VLP models are inspired by BERT <ref type="bibr" target="#b6">[7]</ref>, which is constructed with multi-layer transformers <ref type="bibr" target="#b39">[39]</ref>. The model accepts variable-length sequential inputs i VLP , which consist of a concatenation among words in the text sequence(s) w = {w 1 , . . . , w T }, regional features from the image v = {v 1 , . . . , v K }, and other optional tokens. For instance, in OSCAR <ref type="bibr" target="#b25">[25]</ref>, an object label associated with each regional feature is appended to the end as l = {l 1 , . . . , l K }.</p><p>Within each transformer layer, a multi-head selfattention mechanism is designed to capture the dependencies among the sequential tokens. Layers are stacked hierarchically to attend to the output of the previous layer. Once pre-trained on a large corpus, the final output representations can be used for fine-tuning on arbitrary downstream tasks, where the usage varies depending on the task.</p><p>That said, downstream tasks share some common aspects. Mostly, a classification token [CLS] is inserted at the start of the input text sequence, which aggregates information from the modalities. The final [CLS] output is then used to make predictions, such as for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptation to Composed Image Retrieval</head><p>The task of composed image retrieval can be formally described as finding the target image in a large corpus of images I T ? D that best matches a query provided by a reference image-text pair q = I R , t . Our goal is to learn a text-image composition module, which maps a given I R , t into the same embedding space as, and close to, the corresponding I T . Intuitively speaking, this requires the composition module to modify I R conditioned on t.</p><p>In this work, we employ OSCAR <ref type="bibr" target="#b25">[25]</ref>, a recently proposed VLP model with state-of-the-art performance as the composition module to perform the mapping as follows.</p><p>Input sequence. We denote the input sequence of OS-CAR as i VLP = {w, v}, where we initialize OSCAR without the optional object label inputs l. We then follow Li et al. <ref type="bibr" target="#b25">[25]</ref> for processing text sequences, but introduce the following adaptations on image representations.</p><p>Rather than including a set of regional features, we pre-process images through an ImageNet pre-trained ResNet <ref type="bibr" target="#b13">[14]</ref> model and extract features from before the final FC-layer. We then process these features through a (newly) learned FC-layer and 2 -normalization to give a single image feature v = {v 1 } as the input to OSCAR. This same feature representation is used for the corpus of candidate target images I T ? D as shown in <ref type="figure">Fig. 2</ref>.</p><p>We choose this relatively simple design for two reasons. First, recent work (e.g., <ref type="bibr" target="#b15">[16]</ref>) has shown the compatibility between VLP models and non-regional features of images. Second, we hypothesize that using global image features is easier to achieve our goal of modifying I R conditioned on t so as to closely match I T .</p><p>Output token. As shown in <ref type="figure">Fig. 2</ref>, contrary to typical downstream tasks, we do not use the final representation of the [CLS] token as the text-image joint embedding. Instead, we extract the representation corresponding to the image feature token and treat it as the composed imagetext feature. This resembles the fine-tuning of REF <ref type="bibr" target="#b23">[23]</ref>, as well as VLN-BERT <ref type="bibr" target="#b15">[16]</ref>. In both cases, tokens other than [CLS] are used for prediction. For composed image retrieval, our design makes sense since the transformer model includes residual connections between input and output tokens. Intuitively, the reference image features are modified by aggregating the information from other word tokens to produce the target image features.  Metric learning. We use soft triplet-based loss with 2norm distance as in Vo et al. <ref type="bibr" target="#b40">[40]</ref> to bring the composed image-text feature closer to the feature of the target image (positive pair), while pulling apart the features of negative pairs. In essence, given the i-th positive pair ? i , ? + i and an arbitrary negative ? ? i,j among all negatives ? ? i , the loss is computed as:</p><formula xml:id="formula_0">L = log[1 + exp(?(? i , ? ? i,j ) ? ?(? i , ? + i ))],<label>(1)</label></formula><p>where ? is 2 -norm distance. In training, we randomly sample the negative for each pair and average the loss over all sampled triplets</p><formula xml:id="formula_1">? i , ? + i , ? ? i,j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The CIRR Dataset</head><p>Existing datasets for composed image retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">40]</ref> contain training and testing examples as triplets I R , q, I T where q = I R , t forms the query and I T is (an example of) the desired target from a large image corpus D. However, these existing datasets have two major shortcomings. First, they lack the sufficient visual complexity to facilitate the study of one of the major challenges in composed image retrieval, which is the subtle reasoning over what aspects are important and what shall be ignored. Second, since the candidate images cannot be extensively labeled for each I R , t pair, existing datasets contain many false-negatives. That is, images I ? D that are valid matches for the query but not labeled as the ground-truth target I T . Indeed, all images in D \{I R , I T } are considered as negatives. To circumvent this shortcoming, existing works choose to evaluate models with Recall@K and set K to larger values (e.g., 10, 50 <ref type="bibr" target="#b11">[12]</ref>), thus accounting for the presence of false-negatives. However, the issue persists during training. Moreover, by setting larger K values, these methods are essentially trading in their ability for learning detailed text-image modifications.</p><p>To mitigate these issues, we introduce the Compose Image Retrieval on Real-life images (CIRR) dataset, which includes over 36,000 annotated query-target pairs, q = I R , t , I T . Unlike existing datasets, we collect the modifying text to distinguish the target from a set of similar images (addressing the problem of false-negatives) and creating challenging examples that require careful consideration of visual and textual cues. Details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Collection</head><p>We first form image pairs then collect related annotations by crowd-sourcing. The pairs are drawn from subsets of images, as described below. This strategy plays a major role in mitigating the issue of false negatives (see Sec. 5). <ref type="figure">Fig. 3</ref> outlines our data collection procedure.</p><p>Image source. We use the popular NLVR 2 dataset for natural language visual reasoning <ref type="bibr" target="#b35">[35]</ref> as our source of images. We choose NLVR 2 for several reasons. First, it contains images of real-world entities with reasonable complexity in ImageNet-type <ref type="bibr" target="#b22">[22]</ref>. Second, the setup of our task requires image in pairs that are similar enough, and NLVR 2 is designed to have collections of similar images regarding 1,000 synsets (e.g., acorn, seawall). Also, Suhr et al. <ref type="bibr" target="#b35">[35]</ref> employs an additional step to manually remove non-interesting images, thus ensuring the content quality.</p><p>Image subset construction. The nature of our task requires collections of negative images with high visual similarity, as otherwise, it would be trivial to discriminate between the reference and target image. Thus, prior to forming reference-target image pairs, we construct multiple subsets of six images that are semantically and visually similar, denoted as S = {I 1 , . . . , I 6 }, shown in <ref type="figure">Fig. 3(a)</ref>.</p><p>Here, to construct a subset, we randomly pick one image from the large corpus I 1 ? D. We then sort the remaining images in D by their cosine similarity to I 1 using ResNet152 <ref type="bibr" target="#b13">[14]</ref> image feature vectors pre-trained on Ima-geNet <ref type="bibr" target="#b22">[22]</ref>. Denote by ? i the cosine similarity for image I i . We then pick five additional images to produce a similar yet diverse subset, as follows: First, we filter out images with ? i ? 0.94 to avoid near-identical images to I 1 . Then for the next top-20 ranked images, we greedily add each image in turn, skipping an image if its cosine similarity is within 0.002 of the last image added. If a subset of size six cannot be created, then the entire set is discarded.</p><p>Once constructed we further filter the collection subsets to avoid heavy overlap. We obtain in total 52,732 subsets from NLVR 2 , from which we randomly choose 4,351 for the construction of CIRR.</p><p>Image pairing. Within each constructed image subset S, we draw nine pairs of images, as shown in <ref type="figure">Fig. 3</ref>(b). We choose these pairs to have (1) consecutive modifications that will allow future training of a dialogue systems; and (2) multiple outcomes from the same reference image.</p><p>Annotations. We collect a modification sentence for each pair of reference-target images using Amazon Mechanical Turk (AMT). To ensure that no false-negatives exist within the same image subset from which we draw the pair, as illustrated in <ref type="figure">Fig. 3</ref>(c), we show AMT workers the remaining images from the subset and specifically ask them to write sentences that can only lead to the true target image.</p><p>AMT workers were instructed to avoid subjective descriptions, text mentions, plain side-by-side comparisons, or simple descriptions that only address the target images.</p><p>Following the collection of the modification sentences for each pair, we additionally collect some auxiliary annotations that more explicitly address the ambiguities associated with implicit human-agreements. While we believe that these auxiliary annotations will be useful for future work, we do not make use them in our current work 1 .</p><p>Data splits. Following convention, we randomly assign 80% of the data for training, 10% for validation and 10% for test. Detailed statistics are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on CIRR</head><p>We follow Suhr et al. <ref type="bibr" target="#b35">[35]</ref> and analyze coverage of various semantic concepts by keywords and sentence patterns (see <ref type="table">Table 1</ref>). Here, we show comparisons with Fashion-IQ <ref type="bibr" target="#b11">[12]</ref>, the most popular, comparable humanlabeled dataset. We observe a greater diversity and average length in the sentences in CIRR, indicating broad coverage and linguistic diversity. Over 40% of the annotations are compositional, which indicates an appreciable level of complexity of the sentences. Interestingly, our annotations should also encourage models to attend to both the reference and target images by implicitly (rows 1-4) or explicitly (rows 5-6) referring to the visual contents of both images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. To demonstrate the model's ability in untilizing pre-trained V&amp;L knowledge, as well as its generalizability to images of different domains, we evaluate our proposed model against baselines and state-of-the-art (SoTA) methods on two datasets, including (1) CIRR, our proposed dataset on open-domain composed image retrieval, and (2) Fashion-IQ <ref type="bibr" target="#b11">[12]</ref>, which contains images of fashion products among three subtypes (Dress, Shirt, Toptee) with human-generated annotations. We do not evaluate on other datasets discussed in Sec. 2, as they either contain synthetic image/annotation or are domain-wise similar to Fashion-IQ (e.g., Fashion200k <ref type="bibr" target="#b12">[13]</ref>).</p><p>Compared methods. For CIRR, we evaluate the following methods using publicly available implementations 2 :</p><p>? TIRG <ref type="bibr" target="#b40">[40]</ref> is an image-text composition model for composed image retrieval, which has proven to be effective on multiple datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">40]</ref>. The method uses a gating and residual design to encourage the learning of cross-modal features. Two setups for TIRG are available based on whether to inject text features at the last FC-layer (default), or the last convolution layer (LastConv). We test both setups. ? MAAF <ref type="bibr" target="#b7">[8]</ref> is specifically designed for composed image retrieval with state-of-the-art performance. By default, it treats the convolutional spatial image features and the learned text embeddings (randomly initialized with LSTM <ref type="bibr" target="#b14">[15]</ref>) as modality-agnostic tokens, which are passed to a Transformer <ref type="bibr" target="#b39">[39]</ref>. We evaluate three design choices that were originally reported with comparable results: (+BERT) pretrained contextaware word representations using BERT <ref type="bibr" target="#b6">[7]</ref>, (-IT) removing the output of text tokens in the last pooling layer, (-RP) substituting the final resolution-wise pooling with average pooling.  <ref type="table">Table 1</ref>. Analysis of semantic aspects covered by the annotations in CIRR and in Fashion-IQ <ref type="bibr" target="#b11">[12]</ref>. We also show average sentence length (nb. words). ? Numbers from <ref type="bibr" target="#b11">[12]</ref>. Image pair for each example is shown below with row number (left-right: reference-target).  For Fashion-IQ, we additionally include published results from the following methods:</p><p>? MRN <ref type="bibr" target="#b20">[21]</ref> uses stacked blocks of element-wise products with residual learning to embed V&amp;L jointly. ? FiLM <ref type="bibr" target="#b30">[30]</ref> modulates the image feature map conditioned on text features after the layers of CNN. ? Relationship <ref type="bibr" target="#b33">[33]</ref> learns the joint embeddings through relationship features constructed by concatenating the image and text features followed by FC-layers. ? VAL <ref type="bibr" target="#b4">[5]</ref> is specially designed for composed image retrieval, which adopts the Transformer to compose multi-level V&amp;L joint representations. For images with text descriptions as side information, an additional visual-semantic loss is applied to align visual features and the corresponding text features.</p><p>Metric. We follow previous work to report retrieval performance in Recall within top-K (Recall@K). For CIRR, we additionally report Recall subset , which is an extension to the standard (global) Recall, made possible by the unique design of our dataset. As discussed, our input queries q = I R , t and target images I T in our dataset are constructed such that both I R and I T are sampled from the same image set S (Sec. 4.1). We formulate Recall subset task by ranking images in S \{I R } according to model score. We define Recall subset @K as the proportion of (test) examples where the ground-truth target image I T is ranked within the top-K image in its subset.</p><p>Conceptually, Recall subset can be viewed as Recall while only considering images within the same subset as the pair. The benefits are twofold: First, Recall subset is not affected by false-negative samples, thanks to our careful design in data collection procedures. Second, with a selected batch of negative samples with high visual similarities, Recall subset can facilitate analysis on the reasoning ability of the methods for capturing fine-grained image-text modifications.</p><p>Implementation details. All experiments are conducted on a single NVIDIA RTX3090 with PyTorch. SoTA models use the default configurations proposed by their authors. See supp. mat. and our project website for more details on baseline training. For our proposed model, we use ResNet152 for image feature extraction. The model is optimized with AdamW <ref type="bibr" target="#b27">[27]</ref> with an initial learning rate of 10 ?5 . We set a linearly decreasing schedule without warmup. The batch size is set to 32 and the network is trained for 300 epochs. Other settings are kept as default by OSCAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>Baseline comparison on CIRR.  <ref type="table" target="#tab_3">Table 3</ref>. Retrieval performance on CIRR. Best (resp. second-best) numbers are in bold-black (resp. blue). ? See supplementary material on our collection details of human performance. We additionally report the average score over R@5 and RSubset@1, which better reveals the overall performance of models (discussed in Sec. 5.1). Note that R@5 accounts for possible false-negatives in the entire image corpus.</p><p>Since RSubset is not affected by such issues (Sec. 5), we consider RSubset@1 to better illustrate the fine-grained reasoning ability of methods.  <ref type="table">Table 4</ref>. Retrieval performance on Fashion-IQ, we follow <ref type="bibr" target="#b11">[12]</ref> to report average scores of R@10 and 50. Best numbers for SoTA models are in bold-black. Rows 1-4 reported by <ref type="bibr" target="#b11">[12]</ref>, rows 5-9 (shaded) reported by <ref type="bibr" target="#b4">[5]</ref>. Rows 9-10 are SoTA methods developed for composed image retrieval, where we report the originally published numbers of their best configurations. Note that we see multiple scores reported for TIRG on Fashion-IQ, here we only show the published results from the above two sources. Additional non peer-reviewed methods that involve ensembles of models or data augmentation are not included.</p><p>"Brown dog sits upright on the green grass" "Horse drawn carriage takes people around the city on the pavement" TIRG CIRPLANT CIRPLANT <ref type="figure">Figure 4</ref>. Qualitative results of image retrieval on CIRR, red/green boxes: reference/target images. Predictions are ranked from left to right. We show the ranked images within subsets, see Sec. 5 for details on metric. (Left) We compare the retrieval on the same query for TIRG and CIRPLANT. (Right) We demonstrate the implicit ambiguities within the dataset (in this case, the difficulty in selecting the most suitable candidate by preserving the breed of the dog across the images, which requires identifying subtle characteristics-e.g. pointy ears).</p><p>to the Image-only baseline, suggesting that its multi-modal composition layers often fail to extract information from the text. Instead, it relies primarily on visual content. We conjecture that CIRR focuses more on the fine-grained changes that are harder to capture and associate across modalities, therefore, requires stronger image-text composition layers. In addition, we note that MAAF (rows 10-13) does not generalize well to our dataset, even though it outperforms TIRG and other methods on existing ones <ref type="bibr" target="#b7">[8]</ref>. We believe the choice of forming image tokens by spatial feature maps does not generalize to our dataset where the modification concepts are more diverse and at multiple levels. Meanwhile, adding the contextual-aware BERT pretrained weights yields little effects, suggesting a plain initialization of word embeddings, though contains validated pre-trained language information, may not help the composition layers.</p><p>The Recall Subset results tell a similar story. Here the performance of all SoTA models is close to the theoretical random guess, indicating that current models fail to capture fine-grained modifications between similar images. Interestingly, we discover that the Text-only and Random-Image+Text baselines (rows 4,5) outperform SoTA models significantly. We believe this is because the modification sentences usually contain descriptions of visual content that is unique to the target image once limited to the smaller retrieval set (e.g., "add a leash to the dog" where only the target image contains the leash). However, as demonstrated by the low Recall performance, such descriptions are not detailed enough to single out the target image in the entire image corpus. This scenario further demonstrates Recall Subset reveals behaviors of models on different aspects, and can be used for more detailed analysis.</p><p>In short, the relatively low retrieval performance suggests that our dataset poses a challenge to existing methods developed and tested on narrow-domain datasets. <ref type="table" target="#tab_3">Table 3 (rows 14,15</ref>) compares our proposed model with SoTA methods on CIRR. We notice that on CIRR, CIRPLANT with no initialization (row 14) performs similarly as TIRG on Recall, while surpassing all other SoTA methods. This validates our design choice of using non-regional image features for composing image and text through the transformer architecture. Meanwhile, on Recall Subset our model, even without initialization, yields much higher scores than others, suggesting transformers are better in capturing more fine-grained visiolinguistic cues when composing image and text features. Comparing with SoTA methods that use LSTMs for generating a single language embedding of the entire sentence, we believe that the key difference lies within the fact that transformers accept word tokens as input, which can later be attended individually. Our model outperforms all other methods with OSCAR initialization (row 15) by a significant margin, demonstrating the benefit of VLP knowledge on open-domain images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of CIRPLANT on CIRR. Results in</head><p>Performance of CIRPLANT on Fashion-IQ. <ref type="table">Table 4</ref> compares the performance of our model with SoTA methods. We notice that our model with OSCAR initialization (row 14) outperforms most methods, including generic multimodal learning methods and TIRG. This strengthens the benefits of using transformer architecture that leverages VLP models. Additionally, we note that even on Fashion-IQ, our model still benefits greatly from OSCAR pre-trained initialization (rows <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14)</ref>. Given that the images in Fashion-IQ differ greatly from the data used for OSCAR pre-training <ref type="bibr" target="#b25">[25]</ref>, we believe this further demonstrates that the pre-trained model can transfer the learned V&amp;L knowledge and adapt to various contexts.</p><p>We note that two recent SoTA methods for composed image retrieval (VAL and MAAF, rows 9,10) perform better than our model. Despite the visible improvements brought by OSCAR initialization, we hypothesize that our model is still underperformed by the apparent domain shift in images, as the VLP model is pre-trained on generic ImageNettype data. Meanwhile, the low generalizability of MAAF on CIRR ( <ref type="table" target="#tab_3">Table 3</ref> rows 10-13) hints the possibility that current SoTA methods developed and tested on existing datasets may have been overly adapted to domain-specific images of low complexity. Hence, additional open-domain datasets, such as CIRR, can be beneficial in future research. <ref type="figure">Fig. 4</ref> (left) demonstrates the retrieval rankings within the image subset (see Sec. 5) on the same query for TIRG and CIRPLANT. Specifically, we show the effectiveness of pre-training in CIRPLANT when encountering visiolinguistic concepts (i.e., pavement) that occur less frequently in the training data. Additionally, CIRPLANT better captures fine-grained cues within language (e.g., takes people around, which implies must have people in the back of the carriage), thanks to the transformer architecture that accepts, and attends to individual word tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Results</head><p>We show one failure case of CIRPLANT on CIRR in <ref type="figure">Fig. 4 (right)</ref>. Note the implicit requirement of preserving same breed of dog across the reference and target image. This requires models to identify the fine-grained visiolinguistic cues (i.e., pointy ears in this sample) and retrieve the most suitable image, bringing more challenge to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work expands the task of composed image retrieval into more complex, open-domain images. We collect the CIRR dataset, which addresses shortcomings of existing datasets by placing more emphasis on distinguishing opendomain visually similar images. Our publicly available dataset is designed to facilitate future studies on subtle reasoning over visiolinguistic concepts, as well as iterative retrieval with dialogue. We also introduce CIRPLANT, a transformer-based model that leverages V&amp;L pre-training to compose image and text features. We validate CIR-PLANT on both CIRR and the existing fashion dataset, demonstrating the generalizability of our design and the effectiveness of V&amp;L pre-training. Collectively, we hope to inspire future work on composed image retrieval on a broader scope, yet fine-grained level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Existing methods on CIRR. As discussed in Sec. 5, we adopt the default configurations for state-of-the-art (SoTA) methods when testing on our proposed dataset CIRR.</p><p>Specifically, for TIRG <ref type="bibr" target="#b40">[40]</ref> and its corresponding baselines (incl. Random, Image/text-only, Random Image+Text and Concatenation), we use ResNet18 pretrained on Ima-geNet <ref type="bibr" target="#b22">[22]</ref> as the image encoder, and a randomly initialized LSTM as the text encoder. We note that the above methods do not benefit from more complex ResNet features (e.g., ResNet152) or word embedding initializations on CIRR. We train the models using soft-triplet based loss <ref type="bibr" target="#b40">[40]</ref>, as we discover that the batch-based classification loss introduces serious overfitting for TIRG on CIRR. For MAAF, following Dodds et al. <ref type="bibr" target="#b7">[8]</ref>, we use a pretrained ResNet50 along with an LSTM, while training the model with batch-based classification loss.</p><p>Note that the implementations of MAAF and TIRG share the same codebase. Hence, all methods above use a hidden size of 512, and models are optimized with vanilla stochastic gradient descent (SGD) as in Vo et al. <ref type="bibr" target="#b40">[40]</ref>.</p><p>CIRPLANT on Fashion-IQ. We do not perform hyperparameter tuning for our model on Fashion-IQ (i.e., the setup is kept the same as on CIRR, see Sec. 5 for details). Additionally, since the three subtypes in Fashion-IQ distinct greatly from each other, we sample each minibatch from a single subtype during training, as in Dodds et al. <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Metrics</head><p>See <ref type="table" target="#tab_6">Table 5</ref> on performance in mAP@K, where the comparisons are similar to Recall ( <ref type="table" target="#tab_3">Table 3</ref>). Note that since our task has only one true-positive for each query, Precision@K and Recall@K are the same (hence P@K not shown).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auxiliary Annotations in CIRR</head><p>As discussed in Sec. 4.1, following the collection of the modification sentences (main annotation), we additionally collect auxiliary annotations for each image pair. The auxiliary annotations are meant to provide explicit training signals that address the ambiguities caused by implicit humanagreements. Although we do not use such annotations in this work, we believe that they can benefit future work for clarifying and interpreting such ambiguities.</p><p>Collection. For each pair of reference-target image, we collect the answers to the following four questions from Amazon Mechanical Turk (AMT) workers, which tangibly address the implicit ambiguities mentioned above:</p><p>Q1 What characteristics of the objects are preserved across images? Q2 What objects were changed, but not relevant to the modifying sentence? Q3 Is there any change in camera angle/focus/viewpoint? Q4 Is there any change in the background/lighting across the images?</p><p>We provide the AMT workers with the reference-target image pair along with the collected modification sentence (main annotation). For each question, workers can choose to answer with a sentence or mark as not applicable (e.g., nothing worth mentioning or already covered by the main annotation). Statistics are shown in <ref type="table">Table 6</ref>. Collection interface is shown in <ref type="figure">Fig. 12 (bottom)</ref>, see examples in <ref type="figure">Fig. 11</ref>  <ref type="table">Table 6</ref>. Statistics of CIRR with auxiliary annotations. The visual contents and the (main) annotation determine whether a pair also has auxiliary annotations for Q1-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Collection Details on CIRR</head><p>We provide additional details about our data collection procedure (Sec. 4.1) including examples of each step (excl. the auxiliary annotations, which is discussed in Sec. C).</p><p>Image subsets. <ref type="figure">Fig. 5</ref> shows the procedure for constructing an image subset of six elements, noted as S = {I 1 , . . . , I 6 } in Sec. 4.1. We specifically demonstrate cases where images are removed. The process was designed to ensure that images in a given subset are visually similar to one another while exhibiting some appreciable differences.</p><p>Image pairs. As explained in Sec. 4.1, we draw nine pairs from each subset. <ref type="figure">Fig. 6</ref> demonstrates how we form consecutive modifications among pairs, which could facilitate the training and evaluation of dialogue systems in the future. <ref type="figure" target="#fig_3">Fig. 7</ref> shows that one reference image leads to multiple targets in each subset. This should allow the study of the impact of language modality in the future.</p><p>(a) Randomly pick an image as I 1 (leftmost), sort the remaining images in the large image corpus D by their cosine similarity to I 1 using ResNet features pre-trained on ImageNet, noted as ? i for I i . Images are ranked from left to right.  <ref type="figure">Figure 5</ref>. The procedure of forming an image subset as described in Sec. 4.1. We specifically show cases where images are removed or skipped. Note that after forming the subsets, we further filter them to avoid heavy overlaps.  <ref type="figure">Figure 6</ref>. Left: The six pairs we draw from a subset (in total we draw nine) that form a closed-loop dialogue. Each arrow represents a reference-to-target image pair with modification sentences. Right: An example of consecutive modification sentences that forms a dialogue.</p><formula xml:id="formula_2">I 1 ? i = 1.0 0.</formula><formula xml:id="formula_3">I 1 I 2 I 3 I 4 I 5 I 6 I 1 ? I 2 :</formula><p>Turn on the flat screen tv in the living room. I 1 ? I 3 : A hall with two bright sofas and a brown table between them. I 1 ? I 4 : Room with a large window, a bright armchair and a fireplace. I 1 ? I 5 : A room with a large window and windowsill, a high sofa and shelves above it, instead of a hall with one large sofa and four screens in front of it.  <ref type="figure">Figure 8</ref>. An example of connecting pairs from two subsets to form longer dialogue paths. Note that in this example, I6 ? I 1 .</p><p>We point out that the length of dialogue paths can vary for two reasons. First, we allow a slight overlap between the images of two subsets. Therefore, it is possible to form dialogue paths across subsets with variable lengths, as shown in <ref type="figure">Fig. 8</ref>. Second, AMT workers can mark pairs of poor quality and choose not to annotate them (see below). Such pairs will be removed from the dataset, thus rendering the dialogue incomplete. In total, 71.1% of the subsets have closed-loop dialogue paths (see <ref type="table">Table 6</ref> for detailed statistics).</p><p>Annotation collection on AMT. <ref type="table">Table 8</ref> demonstrates our guideline to AMT workers, specifying types of anno-tations to avoid. <ref type="figure">Fig. 12</ref> shows our collection interface. (top) For main annotations, we require AMT workers to write sentences that only lead to the true target image, thus removing falsenegatives in each subset. We also allow them to mark image pairs of poor quality for removal. (middle) For auxiliary annotations, we ask four detailed questions to clarify ambiguities within the given pair. (bottom) We evaluate human retrieval performance in Recall Subset using the test-split.</p><p>Quality control. We conduct a pre-selection process to manually whitelist workers with good annotation quality. Our pre-selection procedure plays a critical role in quality assurance, where we filter out over 60% of the submitted workers. Workers who have passed the selection process produce annotations with over 90% acceptance rate.</p><p>For annotations submitted by workers in the whitelist, we manually review ?30% of the annotations from each worker. The remaining are examined with an automated script to check for potential abuse of the use of checkboxes, irresponsible contents (e.g., very short sentences), and annotations that violate our guidelines.</p><p>Human performance. <ref type="table" target="#tab_3">Table 3 (row 7)</ref> lists the human retrieval performance of Recall Subset @1 (see Sec. 5 for details of the Recall Subset metric) on test-split. Here, we present the collection procedures of this score. <ref type="figure">Fig. 12 (bottom)</ref> shows the collection interface. Specifically, we ask AMT workers to choose the most probable target image for a given text-image query. We employ three different AMT workers for each pair in the test-split, our final score is calculated by averaging over all submitted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Analysis on CIRR</head><p>Image synsets. We analyze the image contents using the synset information in NLVR 2 <ref type="bibr" target="#b35">[35]</ref>. CIRR includes 124 out of the 1000 synsets in NLVR 2 . Each synset is associated with 136.6?73.1 (? ? ?) images. The five most common synsets are bookcase, bookshop, dobreman, timber wolf and pug. The five least common synsets are acorn, skunk, orange, ox, broccoli and padlock. Distributions of samples are shown in <ref type="figure" target="#fig_4">Fig. 9</ref>.</p><p>Note that we do not distinguish synsets of similar concepts (e.g., dobreman and French bulldog) when forming image pairs, instead, we choose by visual similarity. Additionally, we point out that for composed image retrieval, synset may not fully characterize an image, as the annotations focus on fine-grained visual comparisons.</p><p>Comparison to existing datasets. <ref type="table" target="#tab_10">Table 7</ref> compares CIRR with existing datasets used for composed image retrieval. We demonstrate that CIRR is comparable in size with existing datasets. Additionally, it provides rich auxiliary annotations for open-domain images.</p><p>False-negative analysis. <ref type="figure">Fig. 10</ref> demonstrates the presence of false-negatives in Fashion-IQ <ref type="bibr" target="#b11">[12]</ref>, as explained in Sec. 4. For comparison, our data collection procedures ensure that no false-negatives are present within each image subset, as discussed in Sec. D. Examples of CIRR are shown in <ref type="figure" target="#fig_3">Fig. 6, Fig. 7, and Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Examples of CIRR</head><p>We provide additional examples from the dataset in <ref type="figure">Fig. 11</ref>, where we demonstrate negative retrieval results from both TIRG and CIRPLANT. We point out that CIRR focuses more on the challenging task of distinguishing among visually similar images. Let us note that the auxiliary annotations provide explicit interpretations of errors, particularly regarding the implicit human-agreements in visual and language modalities. This suggests that the annotations can be used for fine-grained analysis or as training signals in future research on composed image retrieval. Auxiliary annotation clarifying ambiguities * Nb. pairs not pre-defined, pairs are generated on-the-fly. ? Approx. 100,000 images have low detection score, thus could be removed <ref type="bibr" target="#b12">[13]</ref>. Here, we show the available nb. images in total. ? Each pair has two sentences. ? Combining all three subtypes. Note that pairs and images overlap between subtypes. 6 Writing sentences that are not unique for the given image pair in the subset - <ref type="table">Table 8</ref>. Types of annotations we discourage workers from writing. Rows 4 and 5 might be admissible if the annotation contains implicit comparisons. (see <ref type="figure">Fig. 12 (top)</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Dataset File Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Is shiny and silver with shorter sleeves + fit and flare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Is less formal with different colored stripes + Does not have a collar.</p><p>(c) Is a solid black color, also shorter and tighter fitting + Is black and more skimpy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d)</head><p>Has more grey and longer sleeves + Is lighter. <ref type="figure">Figure 10</ref>. Examples of false-negatives in Fashion-IQ <ref type="bibr" target="#b11">[12]</ref>. First column shows the reference image. Each sample contains two modification sentences. For each query set (reference image + modification sentences), only one candidate image is labeled as the target. Thus, rendering the remaining valid predictions as false-negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Main -Goes from a black and white dog running to two dogs running. Q1 -[N/A] Nothing worth mentioning Q2 -Change to a brown-and-white dog and a black-and-white dog. Q3 -[N/A] Nothing worth mentioning Q4 -Make the grass a darer green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Main -Remove the concret to the right.   ["dev-147-2-img0", "dev-224-1-img1", "dev-410-2-img0", "dev-743-3-img0", "dev-846-2-img0", "dev-998-1-img0"]</p><p>14 reference rank Sequence identifier as in <ref type="figure">Fig. 6</ref> ? Range from 0 to 5, correspond to I 1 -I 6 . 0 15 target rank 1 * Used for cross-referencing image pairs between cap.sample.json and cap.ext.sample.json. ? See <ref type="figure">Fig. 12 (a)</ref> for the three possible labels. When constructing target soft, images labelled as [The same image] is added as 1.0, [No differences worth mentioning] is added as 0.5, [Images that are too different] is added as -1.0. ? See <ref type="figure">Fig. 12 (b)</ref> for the options we provide for AMT workers. ? Not public for test-split. Instead, see our project website for the test-split evaluation server. <ref type="table" target="#tab_9">Table 9</ref>. Data structure as in the data files. For details please refer to our project website.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 ."Figure 3 .</head><label>23</label><figDesc>(Left) Schematic of our model. Given a pair of reference image and text as input, we aim at learning a modified image feature of the reference image conditioned on the text, such that it matches the feature of the target image. To compare image features of reference and candidate target images, we extract ResNet features and use a shared FC-layer (with normalization) to project them into the same domain. (Right) Overview of the image-text composition module using vision-and-language pre-trained (VLP) multi-layer transformers. Dashed lines (not fully drawn) represent feature aggregation by attention, which learns a language-conditioned image feature modification. Similar angle photograph, with a larger field of view depicting decor of bedroom" Annotation is unique for the given pair in subset * Overview of the data collection process. (a) We demonstrate the construction of an image subset. (b) We illustrate how we choose and form 9 image pairs within one subset, where each arrow suggests the direction from a reference to a target image. (c) ? represents Human Tasks with AMT workers. * indicates the instruction that mitigates the issue of false-negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>6 I 1 ? I 2 :</head><label>612</label><figDesc>Turn on the flat screen tv in the living room. I 2 ? I 3 : Pull up the blinds to let in sunlight. I 3 ? I 4 : Put a window by the fireplace. I 4 ? I 5 : Have a window on the right-hand wall. I 5 ? I 6 : Have a bookshelf to the right of the window. I 6 ? I 1 : Have multiple television screens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Left: The four pairs we draw from a subset to have multiple outcomes from the same reference image. Each arrow represents a reference-to-target image pair with modification sentences. Right: An example of the four pairs with the same reference image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Number of examples per synset (sorted in ascending).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>D ? ? ? ? 8? 6 8 F. 5 Figure 12 .</head><label>8512</label><figDesc>? ? ? !? " # ? $ ? !%? &amp;? ' ? $ ? #? '? $ ? () *) + ,%? -? $ $ ? -. $ ? $ ? $ *? # '? &amp;? --*? / 0%# $ / ? ? ) &amp;) ? $ ? .) " '? $ ? &amp;$ ? &amp;? 1 ) $ ? 1 ? $ ? %# $ $ &amp;? ) ? ? $ ? &lt; ? + + = ? / &gt;$ ? ? $ ? . '? !? ?/ @ -% ? 64 ? 24 4 67&lt; ? + + = ? / A) .? $ ? $ ) ? %$ + / ? ) ? / B&amp;&amp;? ? $ -*) ? ? $ ? $ ? -) ) ? 1 ? $ ? .$ + / C 23 4 32? 64 64 6&lt; ? + + = ? / D*? $ ? &amp;? *? -%$ ) + / E ? $ ? $ ) ? FG51 72 ? 2 5728 04 F? ) ? F 42? 236? 2 5728 04 F&lt; 4 6H ? 231 4 4 67? $ ? $ ) $ ? = ? $ ? -.) ? $ ? . 6 ? 2H 5? 5? 5? 74 2M ? 36H 2 2? 42? 1 2 ? U ? 42? 26 2632V V V9H 21 65 4 32H M ? 42? G51 72 ? 2 572? 4 V V V ? ? ? E *? $ ? J1 ) -? K = ? K ? '$ ? $ ? ) $ ? K ? $ ? ?? W ? $ ? ) $ ? K ? X W W 'Y ? ? $ ? ? ? X ) ? ? -) ..&amp;? ") Y ? ? $ ? J1 ) -? K + $ ? $ ? = ? #%$ ? ? ? &amp; 1 1 ) -? !%? -? . -*? %.? $ ? &amp;-) #+ ASDZ[\\[,? ) ) "$ ? -$ $ = ? %-? ? ") !? &amp; 1 1 ) $ ? $ !.? 1 ? #] -$? ) ? ? ? ? ? 217 4? ! "4? 2#$%4? 7 1234? 627 8 ? 2 &amp;? ! "4? 1$&amp;7 ' (7 3? )4 ! 4 *4+ ? 2 ),48 ? 42*"? -.4)! 7 $ ? #(? 47 ! "48 ? ,87 ! 7 3? 2? )4 ! 4 *4+ ? $8 ? */ 7 *07 3? 1 $ 4? $' 2 ? ! "4? #.! ! $ 1 )2 ? #4/ $,3 49? 5 ? 6 6 6? 7 ? ? 89 6 ? ? : ;? 6 ? &lt; ? 8= ? &gt; ? 8? &lt; ? ? ? &lt; 7 ? ? 6@ ? A? B? &lt; ? :6 7 6 ? C ? ? D? C? 8 ? ? 7 ? D? 8 ? A? E? FGD? H? 8= ? 86H =? 6 ? ? ? I= ? A? E? FGD? H? 8= ? ? ? ? 6 ? 7 6=? ? ? JKLMLKNLE? :6 ? 4O? 5 ? 89 6 ? C ? 6 ? 8= ? ? ; ? ? ? &lt; 7 ? ? 6? P C? ? H ? 7 ? 8 ? 6 ? Q @ ? A? E? FGD? &lt;: ?? ? ? 89 6 ? R ? C ? C? ? 6? ? H? : 6? ? ? A? E? FGD? H? 8= ? 86H =? 6 ? ? ? I= ? A? S ? &gt; &gt; ? &lt;? 6? 7 &lt;? ? 7 P :Q ? &lt;? ? ? P 8 &lt;Q ? &lt; ? 4? R ? ? ?? 6? ? 6&lt; ? ? T ? 7 6=? T ? ; C: @ ? A? E? FGD? 6&lt;: ? ? &lt;? U 8?U ? A? R ? C ? ? 6? ? 6 8? ? 6? 7 &lt;? ? 7 P :Q ? ? ? P 8 &lt;Q ? &lt; ? ? ? 4V? R ? ? ?? 6? ? ? 86H =? 6 ? ? &lt;@ ? A? E? FGD? 6&lt;: ? ? &lt;? U 8?U ? A? R ? C ? ? 6? ? 6 8? ? 6? 7 &lt;? ? 7 P :Q ? ? ? P 8 &lt;Q ? &lt; ? ? ? ? W; ? C? =6H? 7 ? ? &lt;? 8 ? T ? B ? ? : ? 7 ? ? 8 &lt;? X F ? ? : ; ? ? ; ? ? ? ?? 6; ? ? ? ;? Y ? P G ?? =? ? ? 66 S Q P S ? Z C? ? Y&lt;: ? S Q ? ? [? ? 6 ? ? ? ? ? 6? ? 6 ? 7 ? ? ?? ? 8 6H? X ? ? &gt; ? ?? 6? C ? &lt; \ ? ? ? ? ?? 8? 6; ? ? ? &lt; 7 ? ? 6 \ ? B]D? ? &gt; ? 8? &lt; ? ? ? P : ? C ? &lt;? CQ ? _ ? 7 6=? ? ? : C? T ? 6? ? ? C ? ? 7 ? ? X F ? ? &gt; ? ?? 6? C ? &lt; \ ? ? ? ? ?? 8? 6; ? ? ? &lt; 7 ? ? 6 \ ? B]D? ? &gt; ? 8? &lt; ? ? ? P : ? C ? &lt;? CQ ? B6H =? = ? 6 ? ? T ? ? ? C C? ? ? 86H =? X ! #? $!#%" &amp; '%? ( ? %!))#%" &amp; '%? *+ #,%#? -'" ,-" ? !%? ," ? ,-./ ," ! 0# 1)2,&amp; + / -2 3 3 4&amp; .#'? ,? 57 7 77? 3 4567? ,'6? ,? 47 6? 7 77? 8 97 :;&lt; ? =&gt;&amp; -&gt;? 6#%-&amp; ?#%? %2#? 6#%&amp; #6? -&gt;,')#%@ A#? =,'" ? " ? &amp; '6B ? " &gt;#? ?#%" ? 2," -&gt;#6? 5 67 ? 3 4567? 2? " &gt;#? &amp; .#? -,'6&amp; 6," #%/ 3 ? 7 ? : 73 C&gt;%#? " &gt;#? &amp; 2,)#? " &gt;," B -'" ,&amp; '%? " &gt;#? 6#%&amp; #6? -&gt;,')#%? D 6#%-&amp; ?#6? &amp; '? " #E" F &lt; 2#,'=&gt;&amp; + #? #2,&amp; '%? ,%? -+ %#+ G? ,%? *%%&amp; ?+ #? " ? " &gt;#? H# # #'-#? I 2,)#/ J 757? 77? 7K549 7? ? :5 ? :7? 475 3 ? 7? 57? :7 7? ? 5 7? 75 ? 7? : ? ? 73 I ? G!? " &gt;&amp; '0? " =? ? 2 #? &amp; 2,)#%? , #? LMNOP P Q? ,**+ &amp; -,?+ #&lt; ? -&gt;%#? #&amp; " &gt;# / I ? -,'6&amp; 6," #%? -'" ,&amp; '? " =? ? 2 #? &amp; 6#'" &amp; -,+ ? &amp; 2,)#%&lt; ? -&gt;%#? #&amp; " &gt;# / 3 7 573 R%#? ?" " 2? ?!" " '%? ? %&gt; " -!" ? 0#G%? DS? ?EF ? " ? ',.&amp; )," #? ,2')? *,&amp; %/ T? S2? &amp; '? '? " &gt;#? &amp; 2,)#B ? &amp; )&gt;" ? -+ &amp; -0? '? &amp; " &lt; ? " &gt;#'?U*#'? &amp; 2,)#? &amp; '? '#=? " ,?/ V!?2&amp; " ? , " # ? &amp; '&amp; %&gt;? ,+ + ? *,&amp; %/ ? ? Y #2.#? ,+ + ? ?!" ? '#? 6)? ,'6? ,66? ,? =2,'? &gt;!))&amp; ')? &amp; " Y 7? 7? 4 ? 599 9 5 7? 5 67 ? 45673 ? D I " ? %&gt;!+ 6? -'" ,&amp; '%? " &gt;#? 6#%-&amp; ?#6? -&gt;,')#%&lt; ? ,'6? %" &amp; + + ? #%#2?+ #%? " &gt;#? H# # #'-#? I 2,)#? &amp; '? " &gt;# ? =,6? [:7 ? \94 77? 9? ] 7 T&gt;&amp; %? 2#%%,)#? &amp; %? '+ G? .&amp; %&amp; ?+ #? " ? G!? ,'6? =&amp; + + ? '" ? ?#? %&gt;='? " ? A 0# %/ !? -,'? " #%" ? -2*+ #" &amp; ')? " &gt;#? " ,%0? ?#+ =? ,'6? -+ &amp; -0? Y V!?2&amp; " Y ? &amp; '? 6# ? " ? * #.&amp; #=? " &gt;#? 6," ,? ,'6? 2," ? ? " &gt;#? %!?2&amp; " " #6? #%!+ " %/ \94 ? Snapshots of our collection interface (recommend viewing digitally by zooming in). (top) (Main) annotation, where we specifically require unique sentences within each subset. (middle) Auxiliary annotation, where we ask Amazon Mechanical Turk (AMT) workers of four detailed questions per pair. (bottom) Human performance (RecallSubset@1) evaluation on test-split, where we ask AMT workers to choose the most probable target image within the subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Remove the chair, make the dog sit in an open box. Remove all but one bird and have it facing right and putting food in its mouth.</figDesc><table><row><cell></cell><cell>Semantic aspect</cell><cell></cell><cell cols="2">Coverage (%) CIRR Fashion-IQ</cell><cell cols="2">Example (boldface added here for emphasis)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 Cardinality</cell><cell></cell><cell>29.3</cell><cell>-</cell><cell cols="3">Only one of the boars and the ground is browner.</cell><cell></cell></row><row><cell></cell><cell>2 Addition</cell><cell></cell><cell>15.2</cell><cell>15.7</cell><cell cols="2">Add human feet and a collar.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 Negation</cell><cell></cell><cell>11.9</cell><cell>4.0  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4 Direct Addressing</cell><cell></cell><cell>57.4</cell><cell>49.0  ?</cell><cell cols="3">Show some lemons with a glass of lemonade.</cell><cell></cell></row><row><cell></cell><cell>5 Compare &amp; Change</cell><cell></cell><cell>31.7</cell><cell>3.0</cell><cell cols="3">Same computer but different finish and black background.</cell><cell></cell></row><row><cell></cell><cell cols="2">6 Comparative Statement</cell><cell>51.7</cell><cell>32.0  ?</cell><cell cols="2">A bigger pub with more people on it.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">7 Statement with Conjunction</cell><cell>43.7</cell><cell>19.0  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">8 Spatial Relations &amp; Background 61.4</cell><cell>-</cell><cell cols="2">Change the sky to blue color.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>9 Viewpoint</cell><cell></cell><cell>12.7</cell><cell>-</cell><cell cols="3">Focus widely on all available cookies package.</cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. Sentence length (words)</cell><cell>11.3</cell><cell>5.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Nb. image subsets Nb. pairs Nb. pairs per subset Nb. images</figDesc><table><row><cell>Train</cell><cell>3,345</cell><cell>28,225</cell><cell>7.54</cell><cell>16,939</cell></row><row><cell>Val.</cell><cell>503</cell><cell>4,184</cell><cell>8.32</cell><cell>2,297</cell></row><row><cell>Test</cell><cell>503</cell><cell>4,148</cell><cell>8.25</cell><cell>2,316</cell></row><row><cell>Total</cell><cell>4,351</cell><cell>36,554</cell><cell>8.40</cell><cell>21,552</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Statistics of CIRR. Each reference-target image pair is associated with one annotation. For comparison, we also evaluate the following baselines, implemented by Vo et al. [40]: ? Random (theoretical): theoretical random guess. ? Random (init. ResNet): pretrained ImageNet [22] features, but random weights for others parameters. ? Image and text-only: substituting the combined imagetext feature with the reference image or text feature. ? Random image with text: randomly sampling images to pair with text during training and validation. ? Concatenation: replacing the image-text composition layer with a simple concatenation of features followed by a 2-layer perceptron with ReLU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 (</head><label>3</label><figDesc>rows 1-13) compares the retrieval performance of baseline and SoTA methods for both Recall and Recall Subset @K on CIRR.For global Recall, we notice that TIRG performs similar Recall@K Recall Subset @K (R@5 + R Subset @1)/2 Methods K = 1 K = 5 K = 10 K = 50 K = 1 K = 2 K = 3</figDesc><table><row><cell></cell><cell>1</cell><cell>Random (theoretical)</cell><cell>0.02</cell><cell>0.12</cell><cell>0.24</cell><cell>1.20</cell><cell>20.00</cell><cell>40.00</cell><cell>60.00</cell><cell>10.06</cell></row><row><cell>BASELINES</cell><cell>2 3 4 5 6</cell><cell>Random (init. ResNet) Image-only Text-only Random Image+Text Image+Text Concatenation</cell><cell>7.18 13.73 3.90 2.99 12.44</cell><cell>25.74 48.46 13.17 11.91 40.24</cell><cell>36.91 65.81 20.43 19.85 57.52</cell><cell>66.68 89.94 49.16 46.97 87.29</cell><cell>20.84 20.93 39.69 39.41 23.74</cell><cell>41.02 42.15 62.23 62.33 45.12</cell><cell>61.65 63.26 78.52 78.71 65.50</cell><cell>23.29 34.70 26.43 25.66 31.99</cell></row><row><cell></cell><cell>7</cell><cell>Human Performance  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.09</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>8</cell><cell>TIRG [40]</cell><cell>14.61</cell><cell>48.37</cell><cell>64.08</cell><cell>90.03</cell><cell>22.67</cell><cell>44.97</cell><cell>65.14</cell><cell>35.52</cell></row><row><cell></cell><cell>9</cell><cell>TIRG+LastConv [40]</cell><cell>11.04</cell><cell>35.68</cell><cell>51.27</cell><cell>83.29</cell><cell>23.82</cell><cell>45.65</cell><cell>64.55</cell><cell>29.75</cell></row><row><cell>SOTA</cell><cell cols="2">10 MAAF [8] 11 MAAF+BERT [8]</cell><cell>10.31 10.12</cell><cell>33.03 33.10</cell><cell>48.30 48.01</cell><cell>80.06 80.57</cell><cell>21.05 22.04</cell><cell>41.81 42.41</cell><cell>61.60 62.14</cell><cell>27.04 27.57</cell></row><row><cell></cell><cell cols="2">12 MAAF?IT [8]</cell><cell>9.90</cell><cell>32.86</cell><cell>48.83</cell><cell>80.27</cell><cell>21.17</cell><cell>42.04</cell><cell>60.91</cell><cell>27.02</cell></row><row><cell></cell><cell cols="2">13 MAAF?RP [8]</cell><cell>10.22</cell><cell>33.32</cell><cell>48.68</cell><cell>81.84</cell><cell>21.41</cell><cell>42.17</cell><cell>61.60</cell><cell>27.37</cell></row><row><cell></cell><cell cols="2">14 Ours (no init.)</cell><cell>15.18</cell><cell>43.36</cell><cell>60.48</cell><cell>87.64</cell><cell>33.81</cell><cell>56.99</cell><cell>75.40</cell><cell>38.59</cell></row><row><cell></cell><cell cols="2">15 Ours (init. OSCAR)</cell><cell>19.55</cell><cell>52.55</cell><cell>68.39</cell><cell>92.38</cell><cell>39.20</cell><cell>63.03</cell><cell>79.49</cell><cell>45.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>13 Ours (no init.) 14.38 34.66 13.64 33.56 16.44 38.34 14.82 35.52 25.17 14 Ours (init. OSCAR) 17.45 40.41 17.53 38.81 21.64 45.38 18.87 41.53 30.20</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Dress</cell><cell></cell><cell>Shirt</cell><cell cols="2">Toptee</cell><cell></cell><cell>Avg</cell><cell>(R@10 + R@50)/2</cell></row><row><cell></cell><cell>Methods</cell><cell cols="8">R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50</cell></row><row><cell>1</cell><cell>Image-only</cell><cell cols="2">4.20 13.29</cell><cell cols="2">4.51 14.47</cell><cell cols="2">4.13 14.30</cell><cell cols="2">4.28 14.20</cell><cell>9.15</cell></row><row><cell>2</cell><cell>Image+Text Concatenation</cell><cell cols="8">10.52 28.98 13.44 34.60 11.36 30.42 11.77 31.33</cell><cell>21.55</cell></row><row><cell>3</cell><cell>TIRG [40]</cell><cell cols="4">8.10 23.27 11.06 28.08</cell><cell cols="2">7.71 23.44</cell><cell cols="2">8.96 24.93</cell><cell>16.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49</cell><cell>29.39</cell></row><row><cell>9</cell><cell>VAL (init. GloVe) [5]</cell><cell cols="8">22.53 44.00 22.38 44.15 27.53 51.68 24.15 46.61</cell><cell>35.40</cell></row><row><cell cols="2">10 MAAF [8]</cell><cell>23.8</cell><cell>48.6</cell><cell>21.3</cell><cell>44.2</cell><cell>27.9</cell><cell>53.6</cell><cell>24.3</cell><cell>48.8</cell><cell>36.6</cell></row></table><note>4 TIRG+Side Information [12] 11.24 32.39 13.73 37.03 13.52 34.73 12.82 34.72 23.77 5 MRN [21] 12.32 32.18 15.88 34.33 18.11 36.33 15.44 34.28 24.86 6 FiLM [30] 14.23 33.34 15.04 34.09 17.30 37.68 15.52 35.04 25.28 7 TIRG [40] 14.87 34.66 18.26 37.89 19.08 39.62 17.40 37.39 27.40 8 Relationship [33] 15.44 38.08 18.33 38.63 21.10 44.77 18.29 40.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>mAP scores for SoTA methods (and ours) on CIRR. See Table 3 (corresp. row numbers) for comparison with Recall.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell>Nb. image</cell><cell>Nb.</cell><cell>Nb. pairs</cell><cell>Nb.</cell><cell></cell><cell cols="3">Pairs with auxiliary (%)</cell></row><row><cell></cell><cell>subsets</cell><cell>pairs</cell><cell>per subset</cell><cell>images</cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell></row><row><cell>Train</cell><cell cols="2">3,345 28,225</cell><cell cols="6">7.54 16,939 66.72 68.09 48.06 58.45</cell></row><row><cell>Val.</cell><cell>503</cell><cell>4,184</cell><cell>8.32</cell><cell cols="5">2,297 71.87 67.67 49.43 64.66</cell></row><row><cell>Test</cell><cell>503</cell><cell>4,148</cell><cell>8.25</cell><cell cols="5">2,316 69.62 69.01 46.44 63.00</cell></row><row><cell>Total</cell><cell cols="2">4,351 36,554</cell><cell cols="6">8.40 21,552 67.65 68.15 48.02 59.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Greedily add each image as ranked. Meanwhile, to ensure sufficient variations between images, skip an image if its ? i is within 0.002 of the last added image. We demonstrate the greedy process as below. In each step, curved arrow suggests a comparison of ? i and ? i+1 , added image is marked with a tick while skipped is crossed out. Form an image subset S = {I 1 , . . . , I 6 } if 6 images can be greedily added (true for this example), otherwise discard the entire set and restart at (a).</figDesc><table><row><cell></cell><cell>9981</cell><cell>0.8691</cell><cell>0.8663</cell><cell>0.8603</cell><cell>0.8490</cell><cell>0.8488</cell><cell>0.8456</cell><cell>0.8435</cell><cell>...</cell></row><row><cell cols="4">(b) Remove near-identical images with ? i ? 0.94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I 1</cell><cell>Removed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? i = 1.0</cell><cell>0.9981</cell><cell>0.8691</cell><cell>0.8663</cell><cell>0.8603</cell><cell>0.8490</cell><cell>0.8488</cell><cell>0.8456</cell><cell>0.8435</cell><cell>...</cell></row><row><cell cols="4">(c) Select the next top-20 ranked images (not fully shown below).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I 1</cell><cell cols="2">? Shifted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? top-20</cell></row><row><cell>? i = 1.0</cell><cell>0.8691</cell><cell>0.8663</cell><cell>0.8603</cell><cell>0.8490</cell><cell>0.8488</cell><cell>0.8456</cell><cell>0.8435</cell><cell>0.8421</cell><cell>...</cell></row><row><cell>(d) I 1</cell><cell>I 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? i = 1.0</cell><cell>0.8691</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row><row><cell></cell><cell></cell><cell>1.0?0.8691&gt;0.002</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I 1</cell><cell>I 2</cell><cell>I 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? i = 1.0</cell><cell>0.8691</cell><cell>0.8663</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row><row><cell></cell><cell></cell><cell cols="2">0.8691?0.8663&gt;0.002</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I 1</cell><cell>I 2</cell><cell>I 3</cell><cell>I 4</cell><cell>I 5</cell><cell>Skipped</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? i = 1.0</cell><cell></cell><cell></cell><cell></cell><cell>0.8490</cell><cell>0.8488</cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.8490?0.8488?0.002</cell><cell></cell><cell></cell><cell></cell></row><row><cell>I 1</cell><cell>I 2</cell><cell>I 3</cell><cell>I 4</cell><cell>I 5</cell><cell>Skipped</cell><cell>I 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>? i = 1.0</cell><cell></cell><cell></cell><cell></cell><cell>0.8490</cell><cell></cell><cell>0.8456</cell><cell></cell><cell></cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.8490?0.8456&gt;0.002</cell><cell></cell></row><row><cell cols="3">(e) I 1</cell><cell>I 2</cell><cell>I 3</cell><cell>I 4</cell><cell>I 5</cell><cell>I 6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>summarizes information we provide for each im-</cell></row><row><cell>age pair.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Comparison between CIRR (bolded) and existing datasets for composed image retrieval. CIRR is comparable in size (nb. pairs) while containing richer annotations of open-domain images. Simply describing the target image, not comparing the pair Having a large table in the center of the room.5Simple side-by-side comparisonThe left image shows a laptop on the wooden table, the right image has a flatscreen.</figDesc><table><row><cell>To avoid</cell><cell>Examples</cell></row><row><cell>1 Mentioning text/numbers</cell><cell>Text on the pillow says "LOVE".</cell></row><row><cell>2 Discussing photo editing properties</cell><cell>Crop the staircases out of the photo.</cell></row><row><cell>3 Subjective opinions</cell><cell>The dogs look very cute.</cell></row><row><cell>4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Negative results of TIRG and CIRPLANT on CIRR. Here, we show the RecallSubset rankings where we consider candidates from corresponding image subsets (see Sec. 5). First column shows the reference images. True targets are in green boxes. Each pair contains a main annotation (Main) and four auxiliary annotations (Q1-4) as explained in Sec. 4.1 and Sec. C. We demonstrate errors of the models where: (a) fails to associate text with both reference and target image; and (b-d) fails to identify and preserve implicit global visual similarity. We show that CIRR focuses on the challenging task of distinguishing harder negatives that require fine-grained visual reasoning. Let us note that the errors can be explicitly interpreted with our auxiliary annotations (bolded), which previous datasets cannot. This suggests that future work can leverage the auxiliary annotations for analysis of methods, and possibly training of models that account for implicit human ambiguities.</figDesc><table><row><cell>Q1 -Has marine animal in similar blue backdrop.</cell></row><row><cell>Q2 -Remove the blue thing on right.</cell></row><row><cell>Q3 -[N/A] Nothing worth mentioning</cell></row><row><cell>Q4 -[N/A] Nothing worth mentioning</cell></row><row><cell>(c)</cell></row><row><cell>Main -Remove the seashells and make the water green.</cell></row><row><cell>Q1 -Shows manta rays.</cell></row><row><cell>Q2 -Make the rays older, spread the rays further apart.</cell></row><row><cell>Q3 -View straight on.</cell></row><row><cell>Q4 -[N/A] Covered in main annotation</cell></row><row><cell>(d)</cell></row><row><cell>Main -More monkeys</cell></row><row><cell>Q1 -A group of monkeys side by side in same color.</cell></row><row><cell>Q2 -[N/A] Nothing worth mentioning</cell></row><row><cell>Q3 -More focused on the animals.</cell></row><row><cell>Q4 -[N/A] Nothing worth mentioning</cell></row><row><cell>Figure 11.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>if Nothing worth mentioning, begin with [cr1] if Covered in brief annotation.</figDesc><table><row><cell></cell><cell>Identifiers (keys)</cell><cell>Explanations</cell><cell>Content Details (values)</cell><cell>Examples</cell></row><row><cell>1</cell><cell>pairid</cell><cell>Unique pair id  *</cell><cell></cell><cell>12554</cell></row><row><cell>2</cell><cell>reference</cell><cell>Reference image</cell><cell></cell><cell>"dev-147-2-img0"</cell></row><row><cell>3</cell><cell>target hard</cell><cell>Target image  ?</cell><cell>Follow NLVR 2 [35] image naming conventions.</cell><cell>"dev-846-2-img0"</cell></row><row><cell>4</cell><cell>target soft</cell><cell>Target image with additional</cell><cell></cell><cell>{dev-846-2-img0": 1.0,</cell></row><row><cell></cell><cell></cell><cell>labeling (if exists)  ? ?</cell><cell></cell><cell>"dev-743-3-img0": -1.0}</cell></row><row><cell>5</cell><cell>caption</cell><cell>(Main) annotation</cell><cell></cell><cell>"Catch the crab in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>circular ring and place them</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>on the metal table."</cell></row><row><cell>6</cell><cell cols="2">caption extend Auxiliary annotation  ?</cell><cell></cell><cell></cell></row><row><cell>7 8</cell><cell>0 1</cell><cell>Q1 Q2</cell><cell>Begin with [c] if N/A.</cell><cell>"[c] None existed" "We don't see the gloved hands</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of the fisherman"</cell></row><row><cell>9</cell><cell>2</cell><cell>Q3</cell><cell cols="2">Begin with [cr0] "Focus on the net full of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>crabs"</cell></row><row><cell cols="2">10 3</cell><cell>Q4</cell><cell></cell><cell>"[cr0] Nothing worth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mentioning"</cell></row><row><cell cols="2">11 img set</cell><cell>Subset information</cell><cell></cell><cell></cell></row><row><cell cols="2">12 id</cell><cell>Unique subset id</cell><cell></cell><cell>106</cell></row><row><cell cols="2">13 members</cell><cell>Images within subset</cell><cell>Follow NLVR</cell><cell></cell></row></table><note>2 [35] image naming conventions.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See supp. mat. and our project website for details on auxiliary annotations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google/tirg, https://github. com/yahoo/maaf</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image search with text feedback by visiolinguistic attention learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Modality-agnostic attention fusion for visual search with text feedback. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural Naturalist: Generating fine-grained image comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaeser-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dialog-based interactive image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Fashion IQ Dataset: Retrieving images by combining side information and relative natural language feedback. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic spatially-aware fashion concept discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A recurrent vision-and-language bert for navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composed query image retrieval using locally bounded features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to describe differences between pairs of similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text-based image retrieval using progressive multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-andlanguage tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Support Vector Machine active learning for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Multimedia</title>
		<meeting>the Ninth ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval -an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">User term feedback in interactive text-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
