<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self Adversarial Training for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Chou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Chien</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self Adversarial Training for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a deep learning based approach to the problem of human pose estimation. We employ generative adversarial networks as our learning paradigm in which we set up two stacked hourglass networks with the same architecture, one as the generator and the other as the discriminator. The generator is used as a human pose estimator after the training is done. The discriminator distinguishes ground-truth heatmaps from generated ones, and back-propagates the adversarial loss to the generator. This process enables the generator to learn plausible human body configurations and is shown to be useful for improving the prediction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation from a single image is a challenging problem due to the limited information of 2D images and the large variations in configuration and appearance of body parts. Early work often tackles the problem using graphical models <ref type="bibr" target="#b8">[2,</ref><ref type="bibr" target="#b19">13,</ref><ref type="bibr" target="#b28">22]</ref> and random field inference <ref type="bibr" target="#b29">[23,</ref><ref type="bibr" target="#b42">35]</ref> with handcrafted image features. Despite the improvements made by those intriguing designs of models and algorithms, the bottleneck seems to be the lack of effective feature representations that are capable of characterizing different levels of visual cues and accounting for the varieties in appearance of people.</p><p>The situation has been changed along with the popularity of deep learning in computer vision. Deep neural nets have the ability to learn better feature representations. For example, a recent approach, stacked hourglass network <ref type="bibr" target="#b35">[28]</ref>, achieves state-of-the-art performance without the use of hand designed priors or graphical-model-style inference. The well-designed architecture, which supports repeated bottom-up, top-down inference across scales for large receptive field, helps the model to capture some correlations among human body parts. However, the model might predict human pose with implausible configuration due to severe occlusion or overlapping with other people nearby. In these situations, the model is forced to find some similar features which might be in the background or belong to another person. These challenging cases are much easier for human vision to recognize. Humans have the concepts of the structure and constraint of body parts, and are also good at associating these concepts with observed image features. Inspired by the success of generative adversarial networks on many topics, we incorporate a discriminator to take charge of checking the structural constraints of human body. We maintain the original pose estimator as the generator to capture important image features. It is worth noting that the architectures of our discriminator and generator are exactly the same. We use the adversarial training strategy to enable the discriminator to distinguish implausible poses and simultaneously to guide the generator. After the training is done, the generator is used as a pose estimator and the discriminator can be removed. The main contribution of this work is two folds: First, we design a deep ConvNet model to learn the structure and configuration of human body parts via adversarial training.</p><p>The training techniques of generative adversarial networks are used to train the proposed model for solving the human pose estimation problem. Second, we evaluate our method using LSP, MPII, and LIP datasets, with detailed analysis on the effects of different components in our design, and the experimental results show improved accuracy on all of those datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Pose Estimation</head><p>Many recent methods on human pose estimations use deep neural nets to predict the keypoints of human body in an image. DeepPose <ref type="bibr" target="#b45">[38]</ref>, one of the earliest deep-learning based approaches to human pose estimation, formulates the pose estimation problem as a regression problem using a standard convolutional architecture, and its performance is higher than classical approaches <ref type="bibr" target="#b8">[2,</ref><ref type="bibr" target="#b18">12,</ref><ref type="bibr" target="#b19">13,</ref><ref type="bibr" target="#b27">21,</ref><ref type="bibr" target="#b38">31,</ref><ref type="bibr" target="#b48">41]</ref>. Latest methods mostly aim to predict structural outputs, usually called heatmaps or support maps that characterize the probabilities of observing each keypoint at different locations. The exact location of a keypoint is further estimated by finding the maximum in an aggregation of heatmaps. Compared with direct-regression methods, heatmap-based methods better leverage the distributed properties of convolutional networks and are considered more suitable for training.</p><p>Some works incorporate graphical models, e.g. CRF, MRF, which may be used as a post-processing step <ref type="bibr" target="#b15">[9]</ref> or embedded into the network for end-to-end training <ref type="bibr" target="#b17">[11,</ref><ref type="bibr" target="#b47">40]</ref>. Powerful CNN architectures have been developed to capture the important cues and evidences of human parts. In <ref type="bibr" target="#b46">[39]</ref> and <ref type="bibr" target="#b35">[28]</ref>, a multi-stage scheme is employed to make the receptive field large enough for learning the long-range spatial relationships. Also, intermediate supervision is used to produce intermediate confidence maps and let them be refined through different stages. Several recent methods focus on solving the multi-person pose estimation problem. The methods of <ref type="bibr" target="#b25">[19,</ref><ref type="bibr" target="#b39">32]</ref> estimate poses of multiple people in a single image. They use deep networks to generate keypoint candidates and run integer linear programming (ILP) to group joints candidates for each person. The approach of Cao et al. <ref type="bibr" target="#b13">[7]</ref> predicts the multi-person keypoint heatmaps and the part affinity fields, and then uses a greedy algorithm to group the joints that belong to the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Adversarial Networks</head><p>Generative adversarial networks (GANs) flourish in generating natural images such as human faces and indoor scenes. With the introduction by Goodfellow et al. <ref type="bibr" target="#b22">[16]</ref>, the two-player minimax game allows unsupervised training of generative models and avoids the blur effect of using variational autoencoders. However, people concern about GANs being unstable and hard to train. Radford et al. <ref type="bibr" target="#b40">[33]</ref> introduce DCGAN, an all convolutional architecture which is easier to train. They propose some elements to increase the model stability such as eliminating the fully connected layer and employing batch normalization to prevent from losing diversity, i.e., mode collapsing. DCGAN uses an effective network configuration to make the training of GAN more feasible.</p><p>Recently, Arjovsky et al. <ref type="bibr" target="#b9">[3]</ref> propose Wasserstein GAN (WGAN), which does not require a special network design like DCGAN. WGAN uses the Wasserstein distance to replace the original loss function in GAN and solves the unreliable gradient problem in the original GAN. Using Wasserstein distance also provides an estimate of the quality of the generated samples. However, since WGAN satisfies the K-Lipschitz constraint by weight clipping, it pushes weights towards two values (the extremes of the clipping range) and is hard to tune the clipping parameters. Gulrajani et al. <ref type="bibr" target="#b23">[17]</ref> replace the weight clipping strategy by gradient penalty. Gradient penalty is an additional term in the loss function that directly enforces the discriminator's gradient norm around K. The result shows that the improved training strategy of <ref type="bibr" target="#b23">[17]</ref> is much faster and more stable than WGAN. Another branch of work uses autoencoders as discriminators such as EBGAN <ref type="bibr" target="#b49">[42]</ref>. EBGAN aims to match the autoencoder loss distribution while typical GANs try to match the data distribution. EBGAN still suffers from the same problem of classical GANs. Inherited from <ref type="bibr" target="#b49">[42]</ref>, Berthelot et al. <ref type="bibr" target="#b11">[5]</ref> present an equilibrium term, which is based on proportional control theory, to balance the discriminator and the generator. It also provides a convergence measure that can be used to determine if the model has collapsed or reached its final state.</p><p>Due to the success of GAN on generating images, it also draws attention to the field of supervised learning. The concept of conditional GAN <ref type="bibr" target="#b34">[27]</ref> is introduced for incorporating class information. Several methods combine the conditional GAN loss and the L1 or L2 distance between generated data and ground-truth data. The methods of <ref type="bibr" target="#b26">[20,</ref><ref type="bibr" target="#b30">24,</ref><ref type="bibr" target="#b37">30]</ref> use this solution to perform tasks of super-resolution, image inpainting, and image translation. They get promising results with respect to human vision. The examples described above are still all about generating natural images. They either generate a whole image based on certain constraints or generate an image patch. Another type of task is about generating heatmaps of labels as in semantic segmentation <ref type="bibr" target="#b32">[26]</ref>, saliency <ref type="bibr" target="#b36">[29]</ref> or human pose estimation <ref type="bibr" target="#b16">[10]</ref>. Adding the adversarial training strategy to this type of task seems to bring some benefits to it. In our work, we also try to use adversarial training techniques <ref type="bibr" target="#b11">[5]</ref>. to improve the performance of pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adversarial Training with the Stacked Hourglass Networks</head><p>Our model splits into two networks, the generator and the discriminator. The first network, generator, is a fully convolutional network with residual blocks and a conv-deconv architecture. After feeding forward through the generator, we get a set of heatmaps that indicate the confidence score at every location for each keypoint. The second network, discriminator, has the same architecture as the generator but it encodes the heatmaps along with the RGB image and decodes them into a new set of heatmaps in order to distinguish real heatmaps from fake ones. The framework of our model is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator</head><p>The goal of the generative network is to learn a mapping from a color image to keypoint heatmaps. The deep convolutional architecture allows itself to learn contextual feature representation from the input images. Additionally, the adversarial loss from the discriminative network is introduced and combined with the error between the generated heatmaps and the ground-truth heatmaps. This process enables the generator to learn not only the features and spatial dependencies from images but also the plausible human body configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Network Architecture</head><p>We use the state-of-the-art hourglass architecture <ref type="bibr" target="#b35">[28]</ref> as our base network. It is a fully convolutional network with residual modules as its building blocks. The network starts with an initial process of a 7 ? 7 convolution with stride 2, followed by several residual modules and max-pooling layers. The initial process reduces the resolution of the feature maps from 256 to 64. Then, a sequence of hourglass modules are stacked to predict the keypoint heatmaps. A single hourglass module is a bottom-up and top-down design to extract the features at every scale. For human pose estimation, it is essential to explore both the local evidence, such as a small region around the wrist, and the long-range relationships between joints. To maintain the information and to integrate global and local context concurrently, skip connections are used, and features at each resolution can be better preserved. A 4-stack hourglass architecture is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training the Generator</head><p>Training the generator is done by back-propagating the loss L MSE from generator itself and the adversarial loss L adv from the discriminator.</p><p>The generator consists of N stacks of hourglass modules. The expected output of each hourglass module contains M heatmaps, each of which is a 64 ? 64 map with a Gaussian peaked at the ground-truth location of the jth joint. The supervision is conducted at the end of each hourglass. The loss from the generator itself can be expressed as</p><formula xml:id="formula_0">L MSE = N i=1 M j=1 (C ij ?? ij ) 2 ,<label>(1)</label></formula><p>where C ij is the ground-truth heatmap of jth joints at the ith stack, and? ij is the generated heatmap. We calculate the mean square error between them to enforce the generator to learn the image features that are important for localizing the keypoints. In early stacks, local evidence is used since the receptive field is restricted to a small area. In later stacks, long-range spatial relationships will be considered since the receptive field has been enlarged through the numerous sequential convolutional operations. This training scheme is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>In addition to the traditional supervised loss described above, we add an adversarial loss, which can urge the generator to produce reasonable poses. The adversarial loss from the discriminator can be expressed as</p><formula xml:id="formula_1">L adv = M j=1 (? j ? D(? j , X)) 2 ,<label>(2)</label></formula><p>where? j is the output heatmaps of the generator's last hourglass stack, D is the discriminator, and X is an input image. The loss computes the error between generated heatmaps and reconstructed heatmaps. The detail of this equation will be explained in the next section. The total loss for generator is defined by</p><formula xml:id="formula_2">L G = L MSE + ? G L adv ,<label>(3)</label></formula><p>where ? G is a hyperparameter to control the weight of adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminator</head><p>The objective of the discriminator is to distinguish real data from generated data. The input of the discriminator contains either ground-truth heatmaps or generated heatmaps, and both of them are concatenated with the corresponding color image of the person. From the input pair, the discriminator should learn whether the pose described by the heatmaps is correct and corresponds to the person in the input color image. The discriminator attempts to reconstruct a new set of heatmaps. The qualities of the reconstructed heatmaps are determined by how they are similar to the input heatmaps, following the same notion as autoencoder. The loss is computed as the error between the input heatmaps and the reconstructed heatmaps. <ref type="figure">Figure 2</ref>. The framework of our adversarial networks. We incorporate a ConvNet-based pose estimator as the generator (on the left) with a discriminator (on the right) that aims to distinguish the generated heatmaps from the ground-truth heatmaps by reconstructing the input heatmaps. The generator and the discriminator have the same architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training the Discriminator</head><p>For each training image, the generated and ground-truth heatmaps will be fed to the discriminator separately. Two sets of heatmaps will be reconstructed for computing L real and L fake . In other words, at each iteration, the discriminator is updated using the accumulated gradient, which is computed with respect to L real and L fake .</p><p>When the input comprises ground-truth heatmaps, the discriminator is trained to recognize it and reconstruct a similar one, i.e., to minimize the error between the groundtruth heatmaps and the reconstructed ones. On the other hand, if the input comprises generated heatmaps, the discriminator is trained to reconstruct totally different ones, i.e., to drive the error between the generated heatmaps and the reconstructed ones as large as possible. The loss is ex-pressed as</p><formula xml:id="formula_3">L real = M j=1 (C j ? D(C j , X)) 2 , L fake = M j=1 (? j ? D(? j , X)) 2 , L D = L real ? k t L fake .<label>(4)</label></formula><p>The discriminator is optimized by the per-pixel loss L D . Given a set of heatmaps, which can refer to a particular pose, the discriminator will give a value to each pixel. The value is the error between the input and output heatmaps of the discriminator. The value means how good the confidence of this pixel is, in the discriminator's opinion. For example, if the confidence of the right knee is high nearby the left knee, a well-trained discriminator will produce a heatmap of the right knee that has a larger error at the location of left knee.</p><p>Since the discriminator is like a critic, it offers detailed 'comments' on the input heatmaps and suggests which parts in the heatmaps do not yield a real pose. This is different from the conventional GAN, which only judges the whole input being good or bad.</p><p>As mentioned in many papers, GAN is unstable and hard to train since the network easily collapses when the discriminator gets too good too quickly. Inspired by <ref type="bibr" target="#b11">[5]</ref>, we use a variable k t to control the balance between generator and discriminator. The variable is updated at every iteration t. The adaptive term k t is defined by</p><formula xml:id="formula_4">k t+1 = k t + ? k (? L real ? L fake ) ,<label>(5)</label></formula><p>where k t is bounded between 0 and 1, and ? k is a hyperparameter. As in Eq. (4), k t means how much emphasis is put on L fake . When the generator gets better than the discriminator, i.e., L fake is smaller than ? L real , the generated heatmaps are real enough to fool the discriminator. Hence, k t will increase, to make the term L fake more dominant, and thus the discriminator will be trained more on recognizing the generated heatmaps. The proportion it accelerates to train on L fake is according to how far the discriminator falls behind the generator, i.e., ? L real ? L fake . Similarly, when the discriminator gets better than the generator, k t decreases, to slow down the training on L fake so that the generator can keep up with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial Training</head><p>Based on generative adversarial networks (GANs), our training scheme is supervised learning plus a two-player game. The terms L fake in Eq. (4) and Eq. (2) have the same value except for the sign. The generator aims to minimize the distance between? and D(?, X) while the discriminator tries to maximize it. This is the adversarial part of this learning procedure. To distinguish poses, the discriminator seeks to capture the essential factor of real pose distribution during the process of reconstruction. At the same time, the generator seeks to produce higher-quality heatmaps of human pose so that it can deceive the discrminator and pass the inspection to let discriminator reconstruct similar heatmaps. In addition to the unsupervised training game, we preserve the traditional supervised learning to make the generator learn quicker and prevent the network from collapsing. Algorithm 1 summarizes the adversarial training process. Update generator with ?f G 11 while? still improves;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Inference</head><p>After the training is done, the discriminator can be removed. We use the generated heatmaps? = G(X) to infer the final result. To stabilize the predictions, we evaluate both the original image and its flipped version, and average their output heatmaps. As in the training phase, the output heatmap size of a joint is 64 ? 64. We first extract the location with the largest confidence score in each joint's heatmap. Then, we transform the location back to the original coordinate space with respect to the input image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on three benchmark datasets, Leeds Sports Pose Dataset (LSP) <ref type="bibr" target="#b27">[21]</ref>, MPII Human Pose Dataset <ref type="bibr" target="#b7">[1]</ref>, and Look Into Person Dataset (LIP) <ref type="bibr" target="#b21">[15]</ref>. In the following experiments, we use the same preprocessing and data augmentation settings. We randomly flip an input image horizontally, rotate it by an angle in [?30, 30] degrees, and scale it with factors in [0.75, 1.25]. During testing, we scale the LSP and LIP images uniformly across the whole datasets to make the person a suitable size in the image. For MPII images, we use the scale and center annotations provided with the images. We implement our methods using Torch7 libraries for deep learning. We set a batch size of 6 and train the network from scratch using the RMSprop optimization algorithm. The experiments are performed on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Leeds Sports Pose Dataset (LSP):</head><p>Our results of LSP are trained on the LSP plus LSPextended dataset. LSP consists of 11,000 poses for training and 1,000 for testing. The images are gathered from Flickr and contain people who are doing sports such as baseball, parkour, tennis, and so on. Each image is annotated with 14 keypoint locations. To make it easier to integrate with other datasets, we calculate the center and scale of annotated person and use it at the training phase. The label of this dataset is a little noisy since some occluded joints may not have location information or the location might be wrong. The noisy labels and the variations in poses of humans doing sports make this dataset quite challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? MPII Human Pose Dataset (MPII) :</head><p>MPII dataset contains about 25,000 images and over 40,000 annotated people. These data are divided into 30,000 images for training and 10,000 images for testing. Each person is annotated with 16 joints. The images are extracted from YouTube videos where the contents are everyday human activities. In comparison with other pose datasets, MPII has richer information such as activity label and fully unannotated video frames, and has higher image resolution. We only use keypoint locations during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Look Into Person Human Pose Dataset (LIP) :</head><p>LIP is the newest and largest dataset for human pose estimation. It contains 50,000 images with 19 semantic human part labels and 16 human keypoints. In the following experiments, we only use keypoints information. The dataset divides into 30,462 images for training set, 10,000 images for validation set, and 10,000 for test set. The images may contain full body, half body, or part of body, with heavy occlusions and of low resolution. The dataset is also used in CVPR 2017 workshop 'Visual Understanding of Humans in Crowd Scene' and the first 'Look Into Person (LIP) Challenge'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>The evaluations are based on two metrics. We use PCK to measure performance on LSP and LIP. For MPII, we use PCKh.</p><p>? Percentage of Correct Keypoints (PCK) <ref type="bibr" target="#b48">[41]</ref>:</p><p>PCK reports the percentage of correct detection that falls within a tolerance range. The tolerance range is a fraction of torso size. The equation can be expressed as</p><formula xml:id="formula_5">y i ?? i 2 y lhip ? y rsho 2 ? r ,<label>(6)</label></formula><p>where y i is the ground-truth location of the ith keypoint and? i is the predicted location of the ith keypoint. The fraction r is bounded between 0 and 1.</p><p>? Percentage of Correct Keypoints with respect to head (PCKh) <ref type="bibr" target="#b7">[1]</ref>:</p><p>PCKh is almost the same as PCK except for the tolerance range is a fraction of head size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We show in <ref type="figure">Fig. 6</ref> some qualitative results obtained using our method. <ref type="figure" target="#fig_5">Fig. 7</ref> shows a visualization of heatmaps. It can be seen in <ref type="figure" target="#fig_5">Fig. 7(a)</ref> that the predictions produced by the stacked hourglass network <ref type="bibr" target="#b35">[28]</ref> are mostly accurate, but the model is not very sure about its answers according to the heatmaps. Our method is able to refine the heatmaps, as shown in <ref type="figure" target="#fig_5">Fig. 7(b)</ref>.</p><p>? LSP: The comparisons between our results and others are reported in <ref type="table" target="#tab_0">Table 1</ref>. Our model shown in this table is trained with external data from the MPII training set. The score is computed at r = 0.2. As shown in <ref type="figure" target="#fig_1">Fig. 4.3</ref>, our approach gets the highest detection rate across all tolerance range. Furthermore, the improvement is even more obvious at tighter distance (between 0.05 and 0.1).</p><p>? MPII: <ref type="table">Table 2</ref> shows the PCKh performance of our method and previous methods at r = 0.5. Our model shown in this table is trained with external data from the LSP training set.</p><p>? LIP: <ref type="table" target="#tab_1">Table 3</ref> shows the final list of the CVPR 2017 LIP Human Pose Estimation Challenge. The challenge is finished and our method achieves the best result. For reference, both BUPTMM-POSE and Hybrid Pose Machine use methods that merge the predictions of Newell et al. <ref type="bibr" target="#b35">[28]</ref> and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>In this section, we present the effects of several components in our model. We conduct the experiments on the test set of the LSP dataset. We observe the accuracy through training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">GAN and Conditional GAN</head><p>We experiment on several network configurations. The settings differ in the number of stacks of the generator. The size of the discriminator is fixed (1-stack). As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, we find that GAN and conditional GAN perform almost equally in both 1-stack ( <ref type="figure" target="#fig_0">Fig. 10(a)</ref>) and 2-stack ( <ref type="figure" target="#fig_0">Fig. 10(b)</ref>). The discriminator seems to perform well even when the image of the person is not provided. A possible reason is that the implausible pose could be recognized by merely the pose information. The image of the person is an extra information, but the discriminator does not always need it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">With or without Adversarial Training</head><p>To investigate the benefit of adversarial training, we compare our method with the original stacked hourglass network. In <ref type="figure" target="#fig_0">Fig. 11(a)</ref>, the improvement of adding adversarial training is significant. Our method converges faster and ends at a higher accuracy. But when it comes to 2-stack hourglass, in <ref type="figure" target="#fig_0">Fig. 11(b)</ref>, the gain of adversarial training does not seem so obvious like 1-stack hourglass. The lines are staggered across training iterations, although at the end our method is a little higher than the original hourglass. The 8stack hourglass is the best setting released by the authors of <ref type="bibr" target="#b35">[28]</ref>, but in our experiment, in <ref type="figure" target="#fig_0">Fig. 11(c)</ref>, 4-stack hourglass plus a discriminator is a better choice. In this setting we decrease the learning rate by 10 ?1 at epoch 60. In <ref type="figure" target="#fig_0">Fig. 11(d)</ref>, we zoom in the part of curve after epoch 60. We find that the strategy of learning rate decay is helpful for both methods, but ours is a bit more stable and achieves better performance in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an adversarial network to solve the human pose estimation problem. The network is composed of a generator and a discriminator with the same architecture. The generator is responsible for predicting the heatamps of human body keypoints based on the image features, and the discriminator plays the role of critic that can distinguish implausible poses and give the generator useful hints to improve the heatmaps. The additional discriminator can be removed after the training is done, and therefore it does not affect the inference time. We evaluate our approach on three standard benchmark datasets and the results show that our approach is useful for improving the prediction accuracy. <ref type="figure">Figure 6</ref>. Qualitative results. The red and orange lines indicate the left side, and the blue line indicates the right side. Our method can generate more plausible and structural poses than <ref type="bibr" target="#b35">[28]</ref>.      <ref type="figure" target="#fig_0">Figure 11</ref>. PCK on the LSP dataset. The blue line is the approach of <ref type="bibr" target="#b35">[28]</ref> while the green line is ours. (a) 1-stack hourglass. (b) 2-stack hourglass. (c) 8-stack standard hourglass versus 4-stack hourglass plus a discriminator. In this setting we decrease the learning rate by 10 ?1 at epoch 60. (d) We zoom in the part of curve after epoch 60. We find that the strategy of learning rate decay is helpful for both methods, but ours is a bit more stable and achieves better performance in the end.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivation. (a) A deep network may produce incorrect estimations due to occlusion. (b) After incorporating adversarial training, the structural constraints of human body parts can be learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of 4-stack hourglass. The hourglass module consists of residual blocks (zoomed-in at bottom-left), pooling layers, upsampling layers, and skip connections. Between each pair of consecutive hourglass stacks, there is a transition block (yellow box) which produces intermediate heatmaps and adds them to the main trunk of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>An illustration of intermediate supervision. The mean squared error (MSE) loss is applied at the end of each hourglass module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 2 Forward 4 Forward 6 Forward 8 Update discriminator with ?f D 9</head><label>124689</label><figDesc>The adversarial training process. Input : An image X of a person and the corresponding ground-truth heatmaps C 1 do discriminator by D(C, X) 3 Compute gradient ?f D w.r.t. Eq. (4) generator by? = G(X) 5 Compute gradient ?f G w.r.t. Eq. (1) discriminator by D(?, X) 7 Accumulate gradient ?f D w.r.t. Eq. (4) Accumulate gradient ?f G w.r.t. Eq. (2)10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Summary of LG and LD. Losses in the orange box are used to update the generator. Losses in the blue box are used to update the discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Heatmaps visualization on the LSP dataset. (a) The predictions produced by the stacked hourglass network [28] are mostly accurate, but the heatmaps show that the model is not very sure about its answers. (b) Our method further refines the heatmaps and corrects the position of right shoulder. The heatmaps from left to right, top to bottom are left wrist, left elbow, left shoulder, right shoulder, right elbow, right wrist, left ankle, left knee, left hip, right hip, right knee, and right ankle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 2 .Figure 8 .</head><label>28</label><figDesc>Human pose estimation on the MPII dataset. (PCKh) Percentage of Correct Keypoints (PCK) on the LSP dataset. All methods are trained with external data from the MPII training set, in addition to the LSP training set. PC refers to the person-centric annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>PCKh on the MPII dataset. PCK on the LSP dataset. The blue line is the accuracy of GAN while the green line is of conditional GAN. (a) 1-stack hourglass. (b) 2-stack hourglass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Human pose estimation on the LSP dataset. (PCK) Methods Head Sho. Elb. Wri. Hip Knee Ank. Total Lifshitz et al. [25], ECCV'16 97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0 Pishchulin et al. [32] , CVPR'16 97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 Insafutdinov et al. [19], ECCV'16 96.8 95.2 89.3 84.4 88.4 83.4</figDesc><table><row><cell>78.0</cell><cell>88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Human pose estimation on the LIP dataset. (PCK)</figDesc><table><row><cell>Methods</cell><cell cols="3">Head Sho. Elb. Wri. Hip Knee Ank. Total</cell></row><row><cell>Hybrid Pose Machine</cell><cell>71.7 87.1 82.3 78.2 69.2 77.0</cell><cell>73.5</cell><cell>77.2</cell></row><row><cell>BUPTMM-POSE</cell><cell>90.4 87.3 81.9 78.8 68.5 75.3</cell><cell>75.8</cell><cell>80.2</cell></row><row><cell>Pyramid Stream Network</cell><cell>91.1 88.4 82.2 79.4 70.1 80.8</cell><cell>81.2</cell><cell>82.1</cell></row><row><cell>Ours</cell><cell>94.9 93.1 89.1 86.5 75.7 85.5</cell><cell>85.7</cell><cell>87.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Methods Head Sho. Elb. Wri. Hip Knee Ank. Total Pishchulin</title>
		<imprint/>
	</monogr>
	<note>et al. [31], ICCV&apos;13</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pishchulin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Insafutdinov</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<idno>CVPR&apos;16</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newell</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BE-GAN: boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno>abs/1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>abs/1611.08050</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1507.06550</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structure-sensitive learning and A new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1703.05446</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1704.00028</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1605.03170</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<idno>abs/1603.08212</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1611.08408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Salgan: Visual saliency prediction with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gir? I Nieto</surname></persName>
		</author>
		<idno>abs/1701.01081</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>abs/1602.00134</idno>
		<title level="m">Convolutional pose machines. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
