<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sauradip</forename><surname>Nag</surname></persName>
							<email>s.nag@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Surrey Institute for People-Centred Artificial Intelligence</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<email>y.song@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing temporal action detection (TAD) methods rely on generating an overwhelmingly large number of proposals per video. This leads to complex model designs due to proposal generation and/or perproposal action instance evaluation and the resultant high computational cost. In this work, for the first time, we propose a proposal-free Temporal Action detection model via Global Segmentation mask (TAGS). Our core idea is to learn a global segmentation mask of each action instance jointly at the full video length. The TAGS model differs significantly from the conventional proposal-based methods by focusing on global temporal representation learning to directly detect local start and end points of action instances without proposals. Further, by modeling TAD holistically rather than locally at the individual proposal level, TAGS needs a much simpler model architecture with lower computational cost. Extensive experiments show that despite its simpler design, TAGS outperforms existing TAD methods, achieving new state-of-the-art performance on two benchmarks. Importantly, it is ? 20? faster to train and ? 1.6? more efficient for inference. Our PyTorch implementation of TAGS is available at https://github.com/sauradip/TAGS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action detection (TAD) aims to identify the temporal interval (i.e., the start and end points) and the class label of all action instances in an untrimmed video <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref>. All existing TAD methods rely on proposal generation by either regressing predefined anchor boxes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>  <ref type="figure" target="#fig_0">(Fig. 1(a)</ref>) or directly predicting the start and end times of proposals <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>  <ref type="figure" target="#fig_0">(Fig. 1(b)</ref>). Centered around proposals, existing TAD methods essentially take a local view of the video and focus on each individual proposal for action instance temporal refinement and classification. Such an approach thus suffers from several fundamental limitations: (1) An excessive (sometimes exhaustive) number of proposals are usually required for good performance. For example, BMN <ref type="bibr" target="#b17">[18]</ref> generates ? 5000 proposals per video by exhaustively pairing start and end points predicted. Generating and evaluating such a large number of proposals means high computational costs arXiv:2207.06580v2 [cs.CV] 19 Aug 2022 for both training and inference. <ref type="bibr" target="#b1">(2)</ref> Once the proposals are generated, the subsequent modeling is local to each individual proposal. Missing global context over the whole video can lead to sub-optimal detection.</p><p>In this work, for the first time, we address these limitations by proposing a proposal-free TAD model. Our model, termed TAGS, learns a global segmentation mask of action instances at the full video length ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). By modeling TAD globally rather than locally, TAGS not only removes the need for proposal generation, and the associated design and computational complexity, it is also more effective. Concretely, instead of predicting the start/end points of each action instance, TAGS learns to predict an action segmentation mask of an entire video. Such a mask represents the global temporal structure of all action instances in the video; TAGS is thus intrinsically global context-aware.</p><p>Taking a proposal-free approach to TAD, our TAGS has a simpler model architecture design than existing methods. Specifically, it takes each local snippet (i.e., a short sequence of consecutive frames of a video) as a predictive unit. That is, taking as input a snippet feature representation for a given video, TAGS directly outputs the target action segmentation mask as well as class label concurrently. To facilitate global context modeling, we leverage self-attention <ref type="bibr" target="#b35">[36]</ref> for capturing necessary video-level inter-snippet relationship. Once the mask is generated, simple foreground segment classification follows to produce the final TAD result. To facilitate global segmentation mask learning, we further introduce a novel boundary focused loss that pays more attention to temporal boundary regions, and leverage mask predictive redundancy and inter-branch consistency for prediction enhancement. During inference, once the masks and class labels are predicted, top-scoring segments with refined boundary can then be selected via non-maximal suppression (NMS) to produce the final TAD result.</p><p>We make the following contributions. (I) We present a novel proposal-free TAD model based on global segmentation mask (TAGS) learning. To the best of our knowledge, this is the first model that eliminates the need for proposal generation/evaluation. As a result, it has a much simpler model design with a lower computational cost than existing alternatives. (II) We improve TAD feature representation learning with global temporal context using self-attention leading to context-aware TAD. (III) To enhance the learning of temporal boundary, we propose a novel boundary focused loss function, along with mask predictive redundancy and inter-branch consistency. (IV) Extensive experiments show that the proposed TAGS method yields new state-of-the-art performance on two TAD datasets (ActivityNet-v1.3 and THUMOS <ref type="bibr">'14)</ref>. Importantly, our method is also significantly more efficient in both training/inference. For instance, it is 20/1.6? faster than G-TAD <ref type="bibr" target="#b45">[46]</ref> in training and inference respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Although all existing TAD methods use proposals, they differ in how the proposals are generated. Anchor-based proposal learning methods These methods generate proposal based on a pre-determined set of anchors. Inspired by object detection in static images <ref type="bibr" target="#b29">[30]</ref>, R-C3D <ref type="bibr" target="#b41">[42]</ref> proposes to use anchor boxes. It follows the structure of proposal generation and classification in design. With similar model design, TURN <ref type="bibr" target="#b14">[15]</ref> aggregates local features to represent snippet-level features, which are then used for temporal boundary regression and classification. Later, GTAN <ref type="bibr" target="#b21">[22]</ref> improves the proposal feature pooling procedure with a learnable Gaussian kernel for weighted averaging. PBR-Net <ref type="bibr" target="#b19">[20]</ref> improves the detection performance using a pyramidal anchor based detection and fine-grained refinement using frame-level features. G-TAD <ref type="bibr" target="#b45">[46]</ref> learns semantic and temporal context via graph convolutional networks for better proposal generation. MUSES <ref type="bibr" target="#b20">[21]</ref> further improves the performance by handling intra-instance variations caused by shot change. VSGN <ref type="bibr" target="#b49">[50]</ref> focuses on short-action detection in a cross-scale multi-level pyramidal architecture. Note that these anchor boxes are often exhaustively generated so are high in number. Anchor-free proposal learning methods Instead of using pre-designed and fixed anchor boxes, these methods directly learn to predict temporal proposals (i.e., start and end times/points) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>. For example, SSN <ref type="bibr" target="#b51">[52]</ref> decomposes an action instance into three stages (starting, course, and ending) and employs structured temporal pyramid pooling to generate proposals. BSN <ref type="bibr" target="#b18">[19]</ref> predicts the start, end and actionness at each temporal location and generates proposals using locations with high start and end probabilities. Later, BMN <ref type="bibr" target="#b17">[18]</ref> additionally generates a boundary-matching confidence map to improve proposal generation. BSN++ <ref type="bibr" target="#b33">[34]</ref> further extends BMN with a complementary boundary generator to capture rich context. CSA <ref type="bibr" target="#b32">[33]</ref> enriches the proposal temporal context via attention transfer. Recently, ContextLoc <ref type="bibr" target="#b55">[56]</ref> further pushes the boundaries by adapting global context at proposal-level and handling the context-aware inter-proposal relations. While no pre-defined anchor boxes are required, these methods often have to exhaustively pair all possible locations predicted with high scores. So both anchor-based and anchor-free TAD methods have a large quantity of temporal proposals to evaluate. This results in complex model design, high computational cost and lack of global context modeling. Our TAGS is designed to address all these limitations by being proposal-free. . Given an untrimmed video, TAGS first extracts a sequence of T snippet features with a pre-trained video encoder (e.g., I3D <ref type="bibr" target="#b6">[7]</ref>), and conducts self-attentive learning at multiple temporal scales s to obtain snippet embedding with global context. Subsequently, with each snippet embedding, TAGS classifies different actions (output P s ? R (K+1)?T s with K the action class number) and predicts full-video-long foreground mask (output M s ? R T s ?T s ) concurrently in a two-branch design. During training, TAGS minimizes the difference of class and mask predictions against the ground-truth. For more accurate localization, an efficient boundary refinement strategy is further introduced, along with mask predictive redundancy and classification-mask consistency regularization. During inference, TAGS selects top scoring snippets from the classification output P , and then thresholds the corresponding foreground masks in M at each scale and then aggregates them to yield action instance candidates. Finally, softNMS is applied to remove redundant candidates.</p><p>Self-attention Our snippet representation is learned based on self-attention, which has been firstly introduced in Transformers for natural language processing tasks <ref type="bibr" target="#b35">[36]</ref>. In computer vision, non-local neural networks <ref type="bibr" target="#b40">[41]</ref> apply the core self-attention block from transformers for context modeling and feature learning. State-of-the-art performance has been achieved in classification <ref type="bibr" target="#b12">[13]</ref>, selfsupervised learning <ref type="bibr" target="#b8">[9]</ref>, semantic segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>, few-shot action recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref>, and object tracking <ref type="bibr" target="#b9">[10]</ref> by using such an attention model. Several recent works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref> also use Transformers for TAD. They focus on either temporal proposal generation <ref type="bibr" target="#b34">[35]</ref> or refinement <ref type="bibr" target="#b28">[29]</ref>. In this paper, we demonstrate the effectiveness of self-attention in a novel proposal-free TAD architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposal-Free Global Segmentation Mask</head><p>Our global segmentation mask (TAGS) model takes as input an untrimmed video V with a variable number of frames. Video frames are pre-processed by a feature encoder (e.g., a Kinetics pre-trained I3D network <ref type="bibr" target="#b6">[7]</ref>) into a sequence of localized snippets following the standard practice <ref type="bibr" target="#b17">[18]</ref>. To train the model, we collect a set of labeled video training set</p><formula xml:id="formula_0">D train = {V i , ? i }. Each video V i is labeled with temporal segmentation ? i = {(? j , ? j , y j )} Mi j=1</formula><p>where ? j /? j denote the start/end time, y j is the action category, and M i is the action instance number. Architecture As depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, a TAGS model has two key components: (1) a self-attentive snippet embedding module that learns feature representations with global temporal context (Sec. 3.1), and (2) a temporal action detection head with two branches for per-snippet multi-class action classification and binaryclass global segmentation mask inference, respectively (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-attentive multi-scale snippet embedding</head><p>Given a varying length untrimmed video V , following the standard practice <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b17">18]</ref> we first sample T equidistantly distributed temporal snippets (points) over the entire length and use a Kinetics pre-trained video encoder (e.g., a twostream model <ref type="bibr" target="#b37">[38]</ref>) to extract RGB X r ? R d?T and optical flow features X o ? R d?T at the snippet level, where d denotes the feature dimension. We then concatenate them as F = [X r ; X o ] ? R 2d?T . Each snippet is a short sequence of (e.g., 16 in our case) consecutive frames. While F contains local spatio-temporal information, it lacks a global context critical for TAD. We hence leverage the self-attention mechanism <ref type="bibr" target="#b35">[36]</ref> to learn the global context. Formally, we set the Q/K/V of a Transformer encoder as the features F/F/F . To model finer action details efficiently, we consider multiple temporal scales in a hierarchy. We start with the finest temporal resolution (e.g., sampling T = 800 snippets), which is progressively reduced via temporal pooling P (?) with kernel size k, stride s and padding p. For efficiency, we first apply temporal pooling:Q s = P (Q; ? Q ),K s = P (K; ? K ) andV s = P (V ; ? V ) with the scale s ? {1, 2, 4}. The self-attention then follows as:</p><formula xml:id="formula_1">A s i = F + sof tmax( F WQ s (F WK s ) ? ? d )(F WV s ),<label>(1)</label></formula><p>where WQ s , WK s , WV s are learnable parameters. In multi-head attention (MA) design, for each scale s we combine a set of n h independent heads A i to form a richer learning process. The snippet embedding E at scale s is obtained as:</p><formula xml:id="formula_2">E s = M LP ([A s 1 ? ? ? A s n h ] M A ) ? R T s ?C .<label>(2)</label></formula><p>The Multi-Layer Perceptron (MLP) block has one fully-connected layer with residual skip connection. Layer norm is applied before both the MA and MLP block. We use n h = 4 heads by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel action classification and global segmentation masking</head><p>Our TAD head consists of two parallel branches: one for multi-class action classification and the other for binary-class global segmentation mask inference.</p><p>Multi-class action classification Given the t-th snippet E s (t) ? R c (i.e, the t-th column of E s ), our classification branch predicts the probability p t ? R (K+1)?1 that it belongs to one of K target action classes or background. This is realized by a 1-D convolution layer H c followed by a softmax normalization.</p><p>Since a video has been encoded into T s temporal snippets, the output of the classification branch can be expressed in column-wise as:</p><formula xml:id="formula_3">P s := sof tmax(H c (E s )) ? R (K+1)?T s .<label>(3)</label></formula><p>Global segmentation mask inference In parallel to the classification branch, this branch aims to predict a global segmentation mask for each action instance of a video. Each global mask is action instance specific and class agnostic. For a training video, all temporal snippets of a single action instance are assigned with the same 1D global mask ? R T ?1 for model optimization (refer to <ref type="figure">Fig. 3(a)</ref>).</p><formula xml:id="formula_4">For each snippet E s (t), it outputs a mask prediction m t = [q 1 , ? ? ? , q T ] ? R T s ?1 with the k-th element q k ? [0, 1]</formula><p>indicating the foreground probability of k-th snippet conditioned on t-th snippet. This process is implemented by a stack of three 1-D conv layers as:</p><formula xml:id="formula_5">M s := sigmoid(H b (E s )) ? R T s ?T s ,<label>(4)</label></formula><p>where the t-th column of M is the segmentation mask prediction at the tth snippet. With the proposed mask signal as learning supervision, our TAGS model can facilitate context-aware representation learning, which brings clear benefit on TAD accuracy (see <ref type="table" target="#tab_4">Table 4</ref>). Remarks: Actionness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b51">52]</ref> is a popular localization method which predicts a single mask in shape of ? R T ?1 . There are several key differences between actionness and TAGS: (1) Our per-snippet mask model TAGS focuses on a single action instance per snippet per mask so that all the foreground parts of a mask are intrinsically related; In contrast, actionness does not. (2) TAGS breaks the single multi-instance 1D actionness problem into a multiple 1D single-instance mask problem (refer to <ref type="figure">Fig. 3(a)</ref>). This takes a divide-and-conquer strategy. By explicitly segmenting foreground instances at different temporal positions, TAGS converts the regression based actionness problem into a position aware classification task. Each mask, associated with a specific time t, focuses on a single action instance. On the other hand, one action instance would be predicted by multiple successive masks. This predictive redundancy, simply removable by NMS, provides rich opportunities for accurate detection. (3) Whilst learning a 2D actionness map, BMN <ref type="bibr" target="#b17">[18]</ref> relies on predicting 1D probability sequences which are highly noisy causing many false alarms. Further, its confidence evaluation cannot model the relations between candidates whilst our TAGS can (Eq. <ref type="formula">(7)</ref>). Lastly, our experiments in <ref type="table" target="#tab_8">Table 8</ref> validate the superiority of TAGS over actionness learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>Ground-truth labels. To train TAGS, the ground-truth needs to be arranged into the designed format. Concretely, given a training video with temporal in-Label Assignment Inference Strategy <ref type="figure">Fig. 3</ref>: Example of label assignment and model inference (see text for details).</p><p>tervals and class labels ( <ref type="figure">Fig. 3(a)</ref>), we label all the snippets (orange or blue squares) of a single action instance with the same action class. All the snippets off from action intervals are labeled as background. For an action snippet of a particular instance, its global mask is defined as the video-length binary mask of that action instance. Each mask is action instance specific. All snippets of a specific action instance share the same mask. For instance, all orange snippets ( <ref type="figure">Fig. 3(a)</ref>) are assigned with a T -length mask (eg. m 24 to m 38 ) with one in the interval of [q24, q38].</p><p>Learning objectives. The classification branch is trained by a combination of a cross-entropy based focal loss and a class-balanced logistic regression loss <ref type="bibr" target="#b11">[12]</ref>. For a training snippet, we denote y the ground-truth class label, p the classification output, and r the per-class regression output obtained by applying sigmoid on top of H c in Eq. (3) (discarded at inference time). The loss of the classification branch is then written as:</p><formula xml:id="formula_6">L c = ? 1 (1 ? p(y)) ? log(p y ) + (1 ? ? 1 ) log(r y ) ? ? |N | k?N (log(1 ? r(k))) ,<label>(5)</label></formula><p>where ? = 2 is a focal degree parameter, ? = 10 is a class-balancing weight, and N specifies a set of hard negative classes at size of K/10 where K is the toTAD action class number. We set the loss trade-off parameter ? 1 = 0.4. For training the segmentation mask branch, we combine a novel boundary IOU (bIOU) loss and the dice loss in <ref type="bibr" target="#b22">[23]</ref> to model two types of structured consistency respectively: mask boundary consistency and inter-mask consistency. Inspired by the boundary IOU metric <ref type="bibr" target="#b10">[11]</ref>, bIOU is designed particularly to penalize incorrect temporal boundary prediction w.r.t. the ground-truth segmentation mask. Formally, for a snippet location, we denote m ? R T ?1 the predicted segmentation mask, and g ? R T ?1 the ground-truth mask. The overall segmentation mask loss is formulated as: <ref type="figure">Fig. 4</ref>: An example of (a) ground-truth labels and (b) prediction along with an illustration of exploring (c) mask predictive redundancy (Eq. <ref type="formula">(7)</ref>) (d) classification-mask consistency ((Eq. <ref type="formula" target="#formula_11">(9)</ref>).</p><formula xml:id="formula_7">Lm = 1 ? ?(m, g) ?(m, g) + 1 ?(m, g) + ? ?m ? g? 2 c + ?2 1 ? m ? g T t=1 m(t) 2 + g(t) 2 ,<label>(6)</label></formula><p>where ?(m, g) = ?(m) ? ?(g) and ?(m, g) = ?(m) ? ?(g), ?(?) represents a kernel of size k (7 in our default setting, see more analysis in Suppl.) used as a differentiable morphological erosion operation <ref type="bibr" target="#b30">[31]</ref> on a mask and c specifies the ground-truth mask length. In case of no boundary overlap between the predicted and ground-truth masks, we use the normalized L 2 loss. The constant ? = e ?8 is introduced for numerical stability. We set the weight ? 2 = 0.4. Mask predictive redundancy Although the mask loss Eq. (6) above treats the global mask as a 2D binary mask prediction problem, it cannot always regulate the behaviour of individual 1D mask within an action instance. Specifically, for a predicted mask m t at time t, thresholding it at a specific threshold ? j ? ? can result in binarized segments of foreground and background:</p><formula xml:id="formula_8">?[j] = {(q i s , q i e , z i )} L i=1</formula><p>where q i s and q i e denotes the start and end of i-th segment, and z i ? {0, 1} indicates background or foreground. For a mask corresponding to an action snippet, ideally at least one of these {?[j]} should be closer to the ground truth. To explore this redundancy, we define a prediction scoring criterion with the outerinner-contrast <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref> as follows:</p><formula xml:id="formula_9">R(?[j]) = 1 L L i=1 ? ? ? ? ? ? 1 li q i e r=q i s ui(r) inside ? 1 ??li? + ??li? ? ? q i s ?1 r=q i s ???l i ? ui(r) + q i e +??l i ? r=q i e +1 ui(r) ? ? outside ? ? ? ? ? ? where u i (r) = m t [r], if z i = 1 (i.e., foreground) 1 ? m t [r]</formula><p>, otherwise (7) l i = q i e ? q i s + 1 is the temporal length of i-th segment, ? is a weight hyperparameter which is set to 0.25. We obtain the best prediction with the maximal score as j * = argmax(R(?[j])) (see <ref type="figure">Fig. 4(c)</ref>). Higher R(?[j * ]) means a better prediction. To encourage this best prediction, we design a prediction promotion loss function:</p><formula xml:id="formula_10">L pp = 1 ? R(?[j * ]) ? ?m t ? g t ? 2 ,<label>(8)</label></formula><p>where we set ? = 2 for penalizing lower-quality prediction stronger. We average this loss across all snippets of each action instance per training video. Classification-mask consistency In TAGS, there is structural consistency in terms of foreground between class and mask labels by design ( <ref type="figure">Fig. 4(a)</ref>). To leverage this consistency, we formulate a feature consistency loss as:</p><formula xml:id="formula_11">L f c = 1 ? cosine F clf ,F mask ,<label>(9)</label></formula><p>whereF clf = topk(argmax((P bin * E p )[: K, :])) is the features obtained from the top scoring foreground snippets obtained from the thresholded classification output P bin := ?(P ? ? c ) with ? c the threshold and E p obtained by passing the embedding E into a 1D conv layer for matching the dimension of P . The top scoring features from the mask output M are obtained similarly as:F mask = topk(?(1DP ool(E m * M bin ))) where M bin := ?(M ? ? m ) is a binarization of mask-prediction M , E m is obtained by passing the embedding E into a 1D conv layer for matching the dimension of M , * is element-wise multiplication, ?(.) is the binarization function, and ? is sigmoid activation. Our intuition is that the foreground features should be closer and consistent after the classification and masking process (refer to <ref type="figure">Fig. 4(d)</ref>).</p><p>Overall objective The overall objective loss function for training TAGS is defined as: L = L c + L m + L pp + L f c . This loss is calculated for each temporal scale s and finally aggregated over all the scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Inference</head><p>Our model inference is similar as existing TAD methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46]</ref>. Given a test video, at each temporal scale s the action instance predictions are first generated separately based on the classification P s and mask M s predictions and combined for the following post-processing. Starting with the top scoring snippets from P <ref type="figure">(Fig 3(b)</ref>), we obtain their segmentation mask predictions <ref type="figure">(Fig 3(c)</ref>) by thresholding the corresponding columns of M <ref type="figure">(Fig 3(d)</ref>). To generate sufficient candidates, we apply multiple thresholds ? = {? i } to yield action candidates with varying lengths and confidences. For each candidate, we compute its confidence score sc f inal by multiplying the classification score (obtained from the corresponding top-scoring snippet in P ) and the segmentation mask score (i.e., the mean predicted foreground segment in M ). Finally, we apply SoftNMS <ref type="bibr" target="#b2">[3]</ref> on top scoring candidates to obtain the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We conduct extensive experiments on two popular TAD benchmarks.</p><p>(1) ActivityNet-v1.3 <ref type="bibr" target="#b4">[5]</ref>    standard setting to split all videos into training, validation and testing subsets in ratio of 2:1:1.</p><p>(2) THUMOS14 <ref type="bibr" target="#b15">[16]</ref> has 200 validation videos and 213 testing videos from 20 categories with labeled temporal boundary and action class. Implementation details We use two pre-extracted encoders for feature extraction, for fair comparisons with previous methods. One is a fine-tuned twostream model <ref type="bibr" target="#b17">[18]</ref>, with downsampling ratio 16 and stride 2. Each video's feature sequence F is rescaled to T = 800/1024 snippets for AcitivtyNet/THUMOS using linear interpolation. The other is Kinetics pre-trained I3D model <ref type="bibr" target="#b6">[7]</ref> with a downsampling ratio of 5. Our model is trained for 15 epochs using Adam with learning rate of 10 ?4 /10 ?5 for AcitivityNet/THUMOS respectively. The batch size is set to 50 for ActivityNet and 25 for THUMOS. For classification-mask consistency, the threshold ? m /? p is set to 0.5/0.3 and in top?k to 40. In testing, we set the threshold set for mask ? = {0.1 ? 0.9} with step 0.05. We use the same set of threshold ? for mask predictive redundancy during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>Results on ActivityNet From <ref type="table">Table 1</ref>, we can make the following observations: (1) TAGS with I3D feature achieves the best result in average mAP. Despite the fact that our model is much simpler in architecture design compared to the existing alternatives. This validates our assumption that with proper global context modeling, explicit proposal generation is not only redundant but also less effective. (2) When using the relatively weaker two-stream (TS) features, our model remains competitive and even surpasses I3D based BU-TAL <ref type="bibr" target="#b50">[51]</ref>, A2Net <ref type="bibr" target="#b46">[47]</ref> and the very recent ContextLoc <ref type="bibr" target="#b55">[56]</ref> and MUSES <ref type="bibr" target="#b20">[21]</ref> by a significant margin. TAGS also surpasses a proposal refinement and strong G-TAD based approach CSA <ref type="bibr" target="#b32">[33]</ref> on avg mAP. (3) Compared to RTD-Net which employs an architecture similar to object detection Transformers, our TAGS is significantly superior. This validates our model formulation in exploiting the Transformer for TAD.</p><p>Results on THUMOS14 Similar conclusions can be drawn in general on THUMOS from <ref type="table">Table 1</ref>. When using TS features, TAGS achieves again the best results, beating strong competitors like TCANet <ref type="bibr" target="#b38">[39]</ref>, CSA <ref type="bibr" target="#b32">[33]</ref> by a clear margin. There are some noticeable differences: <ref type="bibr" target="#b0">(1)</ref> We find that I3D is now much more effective than two-stream (TS), e.g., 8.8% gain in average mAP over TS with TAGS, compared with 0.6% on ActivityNet. This is mostly likely caused by the distinctive characteristics of the two datasets in terms of action instance duration and video length. (2) Our method achieves the second best result with marginal edge behind MUSES <ref type="bibr" target="#b20">[21]</ref>. This is partly due to that MUSES benefits from additionally tackling the scene-changes.</p><p>(3) Our model achieves the best results in stricter IOU metrics (e.g., IOU@0.5/0.6/0.7) consistently using both TS and I3D features, verifying the effectiveness of solving mask redundancy.</p><p>Computational cost comparison One of the key motivations to design a proposal-free TAD model is to reduce the model training and inference cost. For comparative evaluation, we evaluate TAGS against two representative and recent TAD methods (BMN <ref type="bibr" target="#b17">[18]</ref> and G-TAD <ref type="bibr" target="#b45">[46]</ref>) using their released codes. All the methods are tested on the same machine with one Nvidia 2080 Ti GPU. We measure the convergence time in training and average inference time per video in testing. The two-stream video features are used. It can be seen in <ref type="table" target="#tab_2">Table 2</ref> that our TAGS is drastically faster, e.g., 20/25? for training and clearly quicker -1.6/1.8? for testing in comparison to G-TAD/BMN, respectively. We also notice that our TAGS needs less epochs to converge. <ref type="table" target="#tab_3">Table 3</ref> also shows that our TAGS has the smallest FLOPs and the least parameter number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study and further analysis</head><p>Transformers vs. CNNs We compare our multi-scale Transformer with CNN for snippet embedding. We consider two CNN designs: (1) a 1D CNN with 3 dilation rates (1, 3, 5) each with 2 layers, and (2) a multi-scale MS-TCN <ref type="bibr" target="#b13">[14]</ref>, and (3) a standard single-scale Transformer <ref type="bibr" target="#b35">[36]</ref>. <ref type="table" target="#tab_4">Table 4</ref> shows that the Transformers are clearly superior to both 1D-CNN and a relatively stronger MS-TCN. This suggests that our global segmentation mask learning is more compatible with  self-attention models due to stronger contextual learning capability. Besides, multi-scale learning with Transformer gives 0.4% gain in avg mAP validating the importance of larger snippets. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the gain almost saturates from 200 snippets, and finer scale only increases the computational cost.  Proposal-based vs. proposal-free We compare our proposal-free TAGS with conventional proposal-based TAD methods BMN <ref type="bibr" target="#b1">[2]</ref> (anchor-free) and R-C3D <ref type="bibr" target="#b41">[42]</ref> (anchor-based) via false positive analysis <ref type="bibr" target="#b0">[1]</ref>. We sort the predictions by the scores and take the top-scoring predictions per video. Two major errors of TAD are considered: (1) Localization error, which is defined as when a proposal/mask is predicted as foreground, has a minimum tIoU of 0.1 but does not meet the tIoU threshold. (2) Background error, which happens when a proposal/mask is predicted as foreground but its tIoU with ground truth instance is smaller than 0.1. In this test, we use ActivityNet. We observe in <ref type="figure" target="#fig_3">Fig. 5</ref> that TAGS has the most true positive samples at every amount of predictions. The proportion of localization error with TAGS is also notably smaller, which is the most critical metric for improving average mAP <ref type="bibr" target="#b0">[1]</ref>. This explains the gain of TAGS over BMN and R-C3D. Direction of improvement analysis Two subtasks are involved in TAD -temporal localization and action classification, each of which would affect the final performance. Given the two-branch design in TAGS, the performance effect of one subtask can be individually examined by simply assigning ground-truth to the other subtask's output at test time. From <ref type="table" target="#tab_7">Table 7</ref>, the following observations can be made: (1) There is still a big scope for improvement on both subtasks. (2) Regarding the benefit from the improvement from the other subtask, the classification subtask seems to have the most to gain at mAP@0.5, whilst the localization task can benefit more on the average mAP metric. Overall, this analysis suggests that further improving the efficacy on the classification subtask would be more influential to the final model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of components</head><p>We can see in <ref type="table" target="#tab_6">Table 6</ref> that without the proposed segmentation mask branch, the model will degrade significantly, e.g., a drop of 7.6% in average mAP. This is due to its fundamental capability of modeling the global temporal structure of action instances and hence yielding better action temporal intervals. Further, for TAGS we use a pre-trained UntrimmedNet (UNet) <ref type="bibr" target="#b36">[37]</ref> as an external classifier instead of using the classification branch, resulted in a 2-stage method. This causes a performance drop of 4.7%, suggesting that both classification and mask branches are critical for model accuracy and efficiency.  Global mask design We compare our global mask with previous 1D actionness mask <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref>. We integrate actionness with TAGS by reformulating the mask branch to output 1D actionness. From the results in <ref type="table" target="#tab_8">Table 8</ref>, we observe a significant performance drop of 11.5% in mAP@0.5 IOU. One reason is that the number of action candidates generated by actionness is drastically limited, leading to poor recall. Additionally, we visualize the cosine similarity scores of all snippet feature pairs on a random ActivityNet val video. As shown in <ref type="figure">Fig. 6</ref>, our single-instance mask (global mask) design learns more discriminating feature representation with larger separation between background and action, as compared to multi-instance actionness design. This validates the efficacy of our design in terms of jointly learning multiple per-snippet masks each with focus on a single action instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>In general, short foreground and background segments with the duration similar as or less than the snippet length would challenge snippet-based TAD methods. For instance, given short background between two foreground instances, our TAGS might wrongly predict it as part of the foreground. Besides, given a snippet with mixed background and foreground, TAGS tends to make a background prediction. In such cases, the ground-truth annotation involves uncertainty which however is less noted and investigated thus far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have presented the first proposal-free TAD model by Global Segmentation Mask (TAGS) learning. Instead of generating via predefined anchors, or predicting many start-end pairs (i.e., temporal proposals), our model is designed to estimate the full-video-length segmentation mask of action instances directly. As a result, the TAD model design has been significantly simplified with more efficient training and inference. With our TAGS learning, we further show that learning global temporal context is beneficial for TAD. Extensive experiments validated that the proposed TAGS yields new state-of-the-art performance on two TAD benchmarks, and with clear efficiency advantages on both model training and inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>All existing TAD methods, no matter whether (a) anchor-based or (b) anchor-free, all need to generate action proposals. Instead, (c) our global segmentation mask model (TAGS) is proposal-free.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture of our proposal-free Temporal Action detection model via Global Segmentation mask (TAGS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Performance comparison with state-of-the-art methods on THUMOS14 and ActivityNet-v1.3. The results are measured by mAP at different IoU thresholds, and average mAP in [0.3 : 0.1 : 0.7] on THUMOS14 and [0.5 : 0.05 : 0.95] on ActivityNet-v1.3. Actn = Actioness; PF = Proposal Free; Bkb = Backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>False positive profile of TAGS, BMN and R-C3D on ActivityNet. We use top up-to 10G predictions per video, where G is the number of ground truth action instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>has 19,994 videos from 200 action classes. We follow the I3D 68.6 63.8 57.0 46.3 31.8 52.8 56.3 36.8 9.6 36.5 TAGS (Ours) TS 61.4 52.9 46.5 38.1 27.0 44.0 53.7 36.1 9.5 35.9</figDesc><table><row><cell cols="2">Type Model</cell><cell>Bkb</cell><cell>THUMOS14</cell><cell></cell><cell cols="2">ActivityNet-v1.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">0.3 0.4 0.5 0.6 0.7 Avg. 0.5 0.75 0.95 Avg.</cell></row><row><cell></cell><cell>R-C3D</cell><cell cols="2">C3D 44.8 35.6 28.9 -</cell><cell>-</cell><cell cols="2">-26.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAD</cell><cell cols="5">I3D 53.2 48.5 42.8 33.8 20.8 39.8 38.2 18.3 1.3 20.2</cell></row><row><cell>Anchor</cell><cell>GTAN</cell><cell cols="2">P3D 57.8 47.2 38.8 -</cell><cell>-</cell><cell cols="2">-52.6 34.1 8.9 34.3</cell></row><row><cell></cell><cell>PBR-Net</cell><cell cols="5">I3D 58.5 54.6 51.3 41.8 29.5 -53.9 34.9 8.9 35.0</cell></row><row><cell></cell><cell>MUSES</cell><cell cols="5">I3D 68.9 64.0 56.9 46.3 31.0 53.4 50.0 34.9 6.5 34.0</cell></row><row><cell></cell><cell>VSGN</cell><cell cols="5">I3D 66.7 60.4 52.4 41.0 30.4 50.1 52.3 36.0 8.3 35.0</cell></row><row><cell></cell><cell>BMN</cell><cell cols="5">TS 56.0 47.4 38.8 29.7 20.5 38.5 50.1 34.8 8.3 33.9</cell></row><row><cell></cell><cell>DBG</cell><cell cols="4">TS 57.8 49.4 42.8 33.8 21.7 41.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>G-TAD</cell><cell cols="5">TS 54.5 47.6 40.2 30.8 23.4 39.3 50.4 34.6 9.0 34.1</cell></row><row><cell></cell><cell>BU-TAL</cell><cell cols="5">I3D 53.9 50.7 45.4 38.0 28.5 43.3 43.5 33.9 9.2 30.1</cell></row><row><cell></cell><cell>BSN++</cell><cell cols="5">TS 59.9 49.5 41.3 31.9 22.8 -51.2 35.7 8.3 34.8</cell></row><row><cell>Actn</cell><cell>GTAD+CSA</cell><cell cols="5">TS 58.4 52.8 44.0 33.6 24.2 42.6 51.8 36.8 8.7 35.7</cell></row><row><cell></cell><cell>BC-GNN</cell><cell cols="5">TS 57.1 49.1 40.4 31.2 23.1 40.2 50.6 34.8 9.4 34.3</cell></row><row><cell></cell><cell>TCANet</cell><cell cols="5">TS 60.6 53.2 44.6 36.8 26.7 -52.2 36.7 6.8 35.5</cell></row><row><cell></cell><cell>ContextLoc</cell><cell cols="5">I3D 68.3 63.8 54.3 41.8 26.2 -56.0 35.2 3.5 34.2</cell></row><row><cell></cell><cell>RTD-Net</cell><cell cols="5">I3D 68.3 62.3 51.9 38.8 23.7 -47.2 30.7 8.6 30.8</cell></row><row><cell>Mixed</cell><cell>A2Net</cell><cell cols="5">I3D 58.6 54.1 45.5 32.5 17.2 41.6 43.6 28.7 3.7 27.8</cell></row><row><cell></cell><cell cols="5">GTAD+PGCN I3D 66.4 60.4 51.6 37.6 22.9 47.8 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAGS (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analysis of model training and test cost.</figDesc><table><row><cell cols="2">Model Epoch</cell><cell>Train</cell><cell>Test</cell></row><row><cell>BMN</cell><cell>13</cell><cell cols="2">6.45 hr 0.21 sec</cell></row><row><cell>G-TAD</cell><cell>11</cell><cell cols="2">4.91 hr 0.19 sec</cell></row><row><cell>TAGS</cell><cell>9</cell><cell cols="2">0.26 hr 0.12 sec</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Analysis of model parameters # and FLOPs.</figDesc><table><row><cell>Model</cell><cell cols="2">Params (in M) FLOPs (in G)</cell></row><row><cell>BMN</cell><cell>5.0</cell><cell>91.2</cell></row><row><cell>GTAD</cell><cell>9.5</cell><cell>97.2</cell></row><row><cell>TAGS</cell><cell>6.2</cell><cell>17.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation of Transformer vs. CNN on ActivityNet.</figDesc><table><row><cell>Network</cell><cell>mAP</cell></row><row><cell></cell><cell>0.5 Avg</cell></row><row><cell>1D CNN</cell><cell>46.8 26.4</cell></row><row><cell>MS-TCN</cell><cell>53.1 33.8</cell></row><row><cell>Transformer</cell><cell>55.8 36.1</cell></row><row><cell cols="2">MS-Transformer 56.3 36.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation on snippet embedding design and multiple temporal scales.</figDesc><table><row><cell>Scale</cell><cell>Snippet</cell><cell>Params (in M)</cell><cell>Infer (in sec)</cell><cell>mAP 0.5 Avg</cell></row><row><cell>{1}</cell><cell>100</cell><cell>2.9</cell><cell cols="2">0.09 55.8 36.1</cell></row><row><cell cols="2">{1,2} 100,200</cell><cell>6.2</cell><cell cols="2">0.12 56.3 36.5</cell></row><row><cell cols="3">{1,2,4} 100,200,400 9.8</cell><cell cols="2">0.16 56.5 36.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Analysis of TAGS's two branches on ActivityNet.</figDesc><table><row><cell>Model</cell><cell>mAP</cell></row><row><cell></cell><cell>0.5 Avg</cell></row><row><cell>TAGS(Full)</cell><cell>56.3 36.5</cell></row><row><cell>w/o Mask Branch</cell><cell>45.8 28.9</cell></row><row><cell cols="2">w/o Class Branch + UNet 49.7 31.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Improvement analysis of TAGS on ActivityNet. Ground-truth class 61.0 43.8 ( ? 7.3%) + Ground-truth mask 69.2 48.5 ( ? 12.0%)</figDesc><table><row><cell>Model</cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell>0.5</cell><cell>Avg</cell></row><row><cell>TAGS (full)</cell><cell>56.3</cell><cell>36.5</cell></row><row><cell>+</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Analysis of mask design of TAGS on ActivityNet dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Fig. 6: Pairwise feature similarity.</cell></row><row><cell>Mask Design</cell><cell>mAP 0.5 Avg</cell><cell>Avg masks / video</cell></row><row><cell>Actionness</cell><cell>44.8 27.1</cell><cell>30</cell></row><row><cell cols="2">Our Global Mask 56.3 36.5</cell><cell>250</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Single-label multi-class image classification by deep logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<editor>AAAI.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning action completeness from points for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Temporal action localization with global segmentation mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised temporal action detection with proposal-free masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Zero-shot temporal action detection via vision-language prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Few-shot temporal action localization with query adaptive transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10552</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Temporalrelational crosstransformers for few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV. pp</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoloc: Weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Class semanticsbased attention for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07641</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12043</idno>
		<title level="m">Temporal action proposal generation with transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Oadtr: Online action detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<title level="m">Boundary-sensitive pre-training for temporal localization in videos. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Boundary-sensitive pre-training for temporal localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno>ICCV. pp. 7220-7230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Low-fidelity end-toend video encoder pre-training for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 1, 3, 5</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video self-stitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fewshot action recognition with prototype-centered attentive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08085</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Enriching local and global contexts for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
