<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Optimization as Data Augmentation for Large-scale Graphs *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<email>kong@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<email>guohao.li@kaust.edu.sa</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaust</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucong</forename><surname>Ding</surname></persName>
							<email>mcding@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
							<email>chenzhu@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaust</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
							<email>taylor@usna.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">US Naval Academy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Optimization as Data Augmentation for Large-scale Graphs *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation helps neural networks generalize better by enlarging the training set, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on manipulating graph topological structures by adding/removing edges, we offer a method to augment node features for better performance. We propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training. By making the model invariant to small fluctuations in input data, our method helps models generalize to out-of-distribution samples and boosts model performance at test time. FLAG is a general-purpose approach for graph data, which universally works in node classification, link prediction, and graph classification tasks. FLAG is also highly flexible and scalable, and is deployable with arbitrary GNN backbones and large-scale datasets. We demonstrate the efficacy and stability of our method through extensive experiments and ablation studies. We also provide intuitive observations for a deeper understanding of our method. We open source our implementation at https://github.com/devnkong/FLAG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have emerged as powerful architectures for learning and analyzing graph representations. The Graph Convolutional Network (GCN) <ref type="bibr" target="#b19">(Kipf and Welling, 2016</ref>) and its variants have been applied to a wide range of tasks, including visual recognition <ref type="bibr" target="#b31">(Shen et al., 2018)</ref>, meta-learning <ref type="bibr" target="#b9">(Garcia and Bruna, 2017)</ref>, social analysis <ref type="bibr" target="#b27">(Qiu et al., 2018;</ref><ref type="bibr" target="#b21">Li and Goldwasser, 2019)</ref>, and recommender systems <ref type="bibr" target="#b39">(Ying et al., 2018)</ref>. However, the training of GNNs on large-scale datasets usually suffers from overfitting, and realistic graph datasets often involve a high volume of out-of-distribution test nodes <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, posing significant challenges for prediction problems.</p><p>One promising solution to combat overfitting in deep neural networks is data augmentation <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>, which is commonplace in computer vision tasks. Data augmentations apply label-preserving transformations to the inputs, such as translations and reflections for images. Graph Classification Baseline +FLAG <ref type="figure">Figure 1</ref>: Generalization performance of FLAG on all three tasks. Left: node classification with GAT as baseline on ogbn-products; Middle: link prediction with hits@20 as metric (the higher the better) and GraphSAGE as baseline on ogbl-ddi; Right: graph classification with GIN as baseline on ogbg-molhiv. Plotted lines are attained by smoothing the original lines (the shallow ones), where smooth weights are 0.75, 0.75, and 0.5 respectively.</p><p>As a result, data augmentation effectively enlarges the training set while incurring negligible computational overhead. However, it remains an open problem how to effectively generalize the notion of data augmentation to GNNs. Transformations on images rely heavily on image structures , and it is challenging to design low-cost transformations that preserve semantic meaning for non-visual tasks like natural language processing <ref type="bibr" target="#b36">(Wei and Zou, 2019)</ref> and graph learning. Generally speaking, graph data for machine learning comes with graph structure (or edge features) and node features. In the limited cases where data augmentation can be done on graphs, it generally focuses exclusively on the graph structure by adding/removing edges <ref type="bibr" target="#b28">(Rong et al., 2019;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017;</ref><ref type="bibr" target="#b40">You et al., 2020;</ref><ref type="bibr" target="#b12">Godwin et al., 2022)</ref>. In the meantime, adversarial data augmentation, which applies small perturbations in the input feature space to maximially alter model outputs, is known to boost neural network robustness and promote resistance to adversarially chosen inputs <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b24">Madry et al., 2017)</ref>. Despite the wide belief that adversarial training harms standard generalization and leads to worse accuracy <ref type="bibr" target="#b33">(Tsipras et al., 2018;</ref><ref type="bibr" target="#b0">Balaji et al., 2019)</ref>, recently a growing amount of attention has been paid to using adversarial perturbations to augment datasets and ultimately alleviate overfitting. For example, <ref type="bibr" target="#b34">Volpi et al. (2018)</ref> and <ref type="bibr" target="#b32">Shu et al. (2020)</ref> showed adversarial data augmentation is a datadependent regularization that could help generalize to out-of-distribution samples, and its efficacy has been verified in domains including computer vision <ref type="bibr" target="#b38">(Xie et al., 2020)</ref>, language understanding <ref type="bibr" target="#b25">(Miyato et al., 2016;</ref><ref type="bibr" target="#b17">Jiang et al., 2019)</ref>, and visual question answering <ref type="bibr" target="#b8">(Gan et al., 2020)</ref>. Despite the success of adversarial augmentation in language and vision, it remains unclear how to effectively and efficiently improve GNNs' clean accuracy using adversarial augmentation.</p><p>Present work. We propose FLAG, Free Large-scale Adversarial Augmentation on Graphs, to tackle the overfitting problem. While existing literature focuses on modifying graph structures to augment datasets, FLAG works purely in the node feature space by adding adversarial perturbations (generated by gradient-based robust optimization algorithms), to the input node features with graph structures unchanged. FLAG leverages "free" adversarial training methods <ref type="bibr" target="#b29">(Shafahi et al., 2019)</ref> to conduct efficient adversarial training so that it is highly scalable to large datasets. The method also takes advantage of multi-scale adversarial augmentation to make the model fully generalized in the input feature space. We verify the effectiveness of our method on the Open Graph Benchmark (OGB) <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, which is a collection of large-scale, realistic, and diverse graph datasets for node, link, and graph property prediction tasks. We conduct extensive experiments across OGB datasets by applying FLAG to competitive GNN baselines and show that FLAG brings non-trivial improvements in most cases. For example, FLAG lifts the test accuracy of GAT on ogbn-products by an absolute value of 2.31%. FLAG is simple (easy to implement with a dozen lines of code in PyTorch), general (model-free and task-free), and efficient (able to bring salient improvement at tractable or even no extra cost). Our main contributions are summarized as follows:</p><p>? Method: To the best of our knowledge, our work is the first general-purpose feature-based data augmentation method on graph data, which is complementary to other regularizers (e.g., dropout) and topological augmentations. The novel method incorporates "free" and multi-scale techniques to craft feature augmentations more effectively.</p><p>? Experiments: We show the efficacy and scalability of our method through extensive experiments and ablation studies on large-scale datasets across node, link, and graph property prediction tasks. We validate that FLAG is superior to existing adversarial augmentation methods.</p><p>? Analysis: We provide observations and analysis to support our conjecture that the discrete vs. continuous distribution discrepancy of input features is the key to different effects (beneficial vs. harmful) of adversarial augmentations on model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>Graph Neural Networks (GNNs). We denote a graph as G(V, E) with initial node features x v for v ? V and edge features e uv for (u, v) ? E. GNNs are built on graph structures to learn representation vectors h v for every node v ? V and a vector h G for the entire graph G. Following <ref type="bibr" target="#b15">Hu et al. (2019)</ref>, formally the k-th iteration of message passing, or the k-th layer of GNN forward path is defined as:</p><formula xml:id="formula_0">msg (k) v = AGGREGATE (k) ? h (k?1) v , h (k?1) u , e uv , ?u ? N (v) h (k) v = COMBINE (k) ? h (k?1) v , msg (k) v ,<label>(1)</label></formula><p>where h (k) v is the embedding of node v at the k-th layer, e uv is the feature vector of the edge between node u and v, N (v) is node v's neighbor set, and h (0) v = x v . AGGREGATE(?) and COMBINE(?) functions are parameterized by neural networks.</p><p>To obtain the representation of the entire graph h G , the permutation-invariant READOUT(?) function pools node features from the final iteration K as:</p><formula xml:id="formula_1">h G = READOUT h (K) v | v ? V ,<label>(2)</label></formula><p>Existing graph regularizers mainly focus on augmenting graph structures by modifying edges <ref type="bibr" target="#b28">(Rong et al., 2019;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref>. GraphAT <ref type="bibr" target="#b7">(Feng et al., 2019)</ref>, BVAT <ref type="bibr" target="#b4">(Deng et al., 2019)</ref>, and LAT <ref type="bibr" target="#b18">(Jin and Zhang, 2019)</ref> are three semi-supervised methods on the node classification task. GraphAT promotes local smoothness by reinforcing the similarity between the predictions of perturbed nodes and their neighbors. BVAT proposed two graph VAT schemes to enhance the output smoothness of GCN; LAT virtually perturbed the first-layer embedding of a GCN classifier. The usage scenario of these methods is limited to node classification, while data augmentation should function regardless of tasks. Besides, the formulation of VAT <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref> utilized by these works involves both supervised clean and adversarial robust losses simultaneously. Practically this will consume at least twice the GPU memory as the baseline, making them not scalable to large-scale datasets. Overall, no work so far has considered general-purpose feature-based data augmentations for large-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this work, we investigate how to effectively improve the generalization of GNNs through a featurebased augmentation. Graph node features are usually constructed as discrete embeddings, such as binary bag-of-words vectors or categorical variables. As a result, standard hand-crafted augmentations, like flipping and cropping transforms used in computer vision, are not applicable to graphs node features.</p><p>By hunting for and stamping out small perturbations that cause the classifier to fail, one may hope that adversarial training could benefit standard accuracy <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b33">Tsipras et al., 2018;</ref><ref type="bibr" target="#b26">Miyato et al., 2018)</ref>. It is widely observed that when the data distribution is sparse and discrete, the beneficial effect of adversarial perturbations on generalization takes over <ref type="bibr" target="#b33">(Tsipras et al., 2018;</ref><ref type="bibr" target="#b8">Gan et al., 2020)</ref>. <ref type="bibr" target="#b34">Volpi et al. (2018)</ref> viewed adversarial perturbation as a data-dependent regularization, which could intuitively generalize to out-of-distribution samples. Highlighted by <ref type="bibr" target="#b16">Hu et al. (2020)</ref>, the out-of-distribution phenomenon of data is salient in the graph domain, and also considering the sparsity of labeled node samples in the semi-supervised node classification task, we view adversarial perturbation as a strong candidate method for input feature augmentation.</p><p>Min-Max Optimization. Adversarial training is the process of crafting adversarial data points, and then injecting them intro training data. This process is often formulated as the following min-max problem:</p><formula xml:id="formula_2">min ? E (x,y)?D max ? p? L (f ? (x + ?), y) ,<label>(3)</label></formula><p>where D is the data distribution, y is the label, ? p is some p -norm distance metric, is the perturbation budget, and L is the objective function. <ref type="bibr" target="#b24">Madry et al. (2017)</ref> showed that this saddlepoint optimization problem could be reliably tackled by Stochastic Gradient Descent (SGD) for the outer minimization and Projected Gradient Descent (PGD) for the inner maximization. In practice, the typical approximation of the inner maximization under an l ? -norm constraint is as follows,</p><formula xml:id="formula_3">? t+1 = ? ? ?? (? t + ? ? sign (? ? L (f ? (x + ? t ), y))) ,<label>(4)</label></formula><p>where the perturbation ? is updated iteratively, and ? ? ?? performs projection onto the -ball in the l ? -norm. For maximum robustness, this iterative updating procedure usually loops M times to craft the worst-case noise, which requires M forward and backward passes end-to-end. Afterwards the most vicious noise ? M is applied to the input feature, on which the model weight is optimized. The algorithm above is called PGD.</p><p>Multi-scale Augmentation. On visual tasks,  highlighted the importance of using diverse types of data augmentations such as random cropping, color distortion, and Gaussian blur. The authors showed that a single transformation is not sufficient to learn good representations. To fully exploit the generalizing ability and enhance the diversity and quality of adversarial perturbations, we propose to craft multi-scale augmentations. To realize this goal, we leverage the techniques below.</p><p>"Free" training. We leverage "free" adversarial training <ref type="bibr" target="#b29">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations. PGD is a powerful yet inefficient way of solving the min-max optimization. It runs M full forward and backward passes to craft a refined perturbation ? 1:M , but the model weights ? only get updated once using the final ? M . This process makes model training M times slower. In contrast, while computing the gradient for the perturbation ?, "free" training simultaneously produces the model parameter ? on the same backward pass. This enables a parameter update to be computed in parallel with a perturbation update at virtually no additional cost. The authors proposed to train on the same minibatch M times in a row to simulate the inner maximization in Eq. (3), while compensating by performing M times fewer epochs of training. The resulting algorithm yields accuracy and robustness competitive with standard adversarial training, but with the same runtime as clean training.</p><p>Besides the efficiency, the "free" method achieves our idea of optimizing ? with multi-scale augmentations. Note that X is augmented with additive perturbations ? 1:M , of which each can have a maximum scale of m?, m ? {1, ? ? ? , M }, in contrast to PGD whose perturbation is a single ? M with an M ? scaling. This greatly adds to the diversity of our augmentations. However, the "free" algorithm is suboptimal in terms of min-max optimization in that during the batch-replay process, the approximated perturbation computed to maximize the objective on ? t is used to robustly optimize ? t+1 rather than ? t . To tackle this problem, instead of directly updating ? using the "by-product" gradient attained from the gradient ascent step on ?, we accumulate the gradients ? ? L, and apply them to the model parameters all at once later. Formally, the optimization step is</p><formula xml:id="formula_4">? i+1 = ? i ? ? M M t=1 ? ? L (f ? (x + ? t ), y) ,<label>(5)</label></formula><p>where ? is learning rate and ? 1 is uniform noise. Note that the gradients in Eq.(5) are restored when crafting perturbation in Eq.(4). We save one backward pass and M times extra GPU memory through accumulating gradients (which is fully supported by PyTorch) during the batch replay process. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the effects of our design. We can see that PGD inevitably produces concentrated augmentations in terms of the magnitude, whereas our method produces perturbations with a broader range of sizes, which adds to the diversity and quality of the augmentations. Moreover on the node classification task, we propose to augment labeled vs. unlabeled nodes with diverse magnitudes of perturbations during training time to further diversify the augmentations. We call it Weighted perturbation. When classifying one target node, messages from the whole k-hop neighborhood are aggregated and combined into its embedding. It is natural to believe that a further neighbor should have lower impact, i.e. higher smoothness, on the final decision of the target node, which can also be intuitively reflected by the recursive message passing procedure of GNNs in Eq.(1). In practice we find that a larger perturbation for unlabeled nodes can be beneficial to the performance. Algorithm 1 summarizes the pseudo code of our method on node classification task. <ref type="figure">Figure 1</ref> illustrates the generalization ability of our proposed method.</p><p>Algorithm 1 FLAG: Free Large-scale Adversarial Augmentation on Graphs (Node Classification Task) Require: Graph G = (V, E), V l is the labeled node set; learning rate ? ; ascent steps M ; ascent step size ? v for labeled node, ? u for unlabeled, we assume the neighbors of labeled nodes are all unlabeled ones; L(?) as objective function; A(?) and C(?) denote the AGGREGATE and COMBINE functions in Eq.(1). The backward function at line 12 refers to back-propagation gradient computation for both model weights and noises.</p><formula xml:id="formula_5">1: Initialize (?, ?) 2: for v ? V l do 3: ? (0) v ? U (?? v , ? v ) 4: ? (0) u ? U (?? u , ? u ) 5: for t = 1 . . . M do 6: h (0) v ? x v + ? (t?1) v 7: h (0) u ? x u + ? (t?1) u 8: for k = 1 . . . K do 9: msg (k) v ? A (k) ? h (k?1) v , h (k?1) u , e uv , ?u ? N (v) 10: h (k) v ? C (k) ? h (k?1) v , msg (k) v 11: end for 12: L h (K) v , y .backward() 13: g (t) ?,? ? g (t?1) ?,? + 1 M ? grad(?, ?) 14: ? (t) v ? ? (t?1) v + ? v ? sign (grad (? v )) 15: ? (t) u ? ? (t?1) u + ? u ? sign (grad (? u )) 16:</formula><p>end for 17:</p><formula xml:id="formula_6">(?, ?) ? (?, ?) ? ? ? g (M ) ?,?</formula><p>18: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experiments to fully reveal the efficacy of our method. Datasets. We demonstrate FLAG's effectiveness through extensive experiments on the Open Graph Benchmark (OGB), which consists of a wide range of challenging large-scale datasets. <ref type="bibr" target="#b30">Shchur et al. (2018)</ref>, <ref type="bibr" target="#b6">Errica et al. (2019)</ref>, and <ref type="bibr" target="#b5">Dwivedi et al. (2020)</ref> showed that traditional graph datasets suffered from problems such as unrealistic and arbitrary data splits, highly limited data sizes, nonrigorous evaluation metrics, and common neglect of validation set, etc. In order to empirically study FLAG's effects in a fair and reliable manner, we conduct experiments on the OGB <ref type="bibr" target="#b16">(Hu et al., 2020)</ref> datasets, which have tackled those major issues and brought more realistic challenges to the graph research community.</p><p>Setup. FLAG drops the projection step when performing the inner maximization, in light of the positive effect of large perturbations on generalization <ref type="bibr" target="#b34">(Volpi et al., 2018)</ref>, and also to simplify hyperparameter search. Usually on images, the inner maximization has a norm constraint on the perturbation; the largest perturbation one can add is bounded by the hyperparameter , typically 8/255 under the l ? -norm. This encourages the visual imperceptibility of the perturbations, thus making defenses realistic and practical. However, graph node features or language word embeddings do not have an established distance threshold for imperceptibility, which makes the selection of highly heuristic. Note that, although the perturbation is no longer bounded by an explicit in FLAG, it is still implicitly bounded in the furthest distance that ? can reach, i.e. the step size ? times the number of ascending steps M . Also unless otherwise stated, all of the baseline test statistics come from the official OGB leaderboard website, and we conduct all of our experiments using publicly released implementations without touching the original model architecture or training setup for fair comparisons. We report mean and standard deviations from 10 runs with different random seeds. Following common practice on this benchmark, we report the test performance associated with the best validation result. We choose GCN, GraphSAGE, GAT, and GIN as our baseline models. In addition, we apply FLAG to the DeeperGCN model to demonstrate its effectiveness on the GNNs with significantly deeper depth. Our implementation always uses M = 3 ascent steps for simplicity. Following <ref type="bibr" target="#b13">Goodfellow et al. (2014)</ref>; <ref type="bibr" target="#b24">Madry et al. (2017)</ref>, we use sign(?) for gradient normalization.</p><p>Large-scale Node Property Prediction. We summarize the results of node classification in <ref type="table" target="#tab_0">Table 1</ref>. Notably, FLAG yields a 2.31% test accuracy lift for GAT, making GAT competitive on the ogbn-products dataset. Considering the specialty of not having input node features in ogbn-proteins, we provide detailed discussions on the effect of different node feature constructions in Section 5. ogbn-mag is a heterogeneous network where only "paper" nodes come with node features. We use the neighbor sampling mini-batch algorithm to train R-GCN and report its results in the <ref type="table" target="#tab_1">Table 2</ref>. Surprisingly, FLAG can also directly bring nontrivial accuracy improvement without special designs for heterogeneous graphs, which demonstrates its versatility. Large-scale Link Property Prediction. We evaluate our method on two OGB link prediction datasets, which are ogbl-ddi and ogbl-collab. The authors of OGB selected Hits@K as the official evaluation metric. We study the performance of FLAG with GCN and GraphSAGE as backbone on this task. We follow the practice of the baselines to train the models in the full-batch manner. Results are reported in <ref type="table" target="#tab_2">Table 3</ref>. We highlight that FLAG brings a salient boost to both GCN and GraphSAGE on the ogbl-ddi dataset.   <ref type="table" target="#tab_3">Table 4</ref> summarizes the test scores of GCN, GIN, and DeeperGCN on all four OGB graph property prediction datasets. "Virtual" means the model is augmented with virtual nodes <ref type="bibr" target="#b23">(Li et al., 2017;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017;</ref><ref type="bibr" target="#b16">Hu et al., 2020)</ref>. As adversarial perturbations are crafted by gradient ascent, it would be unnatural and suboptimal to add noises to discrete input node features <ref type="bibr" target="#b43">(Z?gner et al., 2018)</ref>. We firstly project discrete node features into the continuous space and then adversarially augment the hidden embeddings. On ogbg-molhiv, FLAG yields notable improvements, but when GCN has already been hurt by virtual nodes, FLAG appears to exaggerate the harm. On ogbg-molpcba, GIN-Virtual with FLAG receives an absolute value 1.31% test AP value increase. Besides node classification and link prediction, FLAG's strong effects on graph classification prove its high versatility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies and Discussions</head><p>Compatibility with graph structure regularizers. As our augmentation manipulates the input features, it is highly complementary to structure-based regularizers. We validate this point through the experiments below. We mainly focus on two widely-used topological augmentation methods to illustrate 1 : (i) Neighbor sampling <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> randomly samples neighbors for information aggregation. It not only contributes to GNN scalability but also acts as a structure regularizer. A fullbatch GraphSAGE reaches 78.50 ? 0.14% test accuracy on ogbn-products, and neighbor sampling  Compatibility with batch norm. Batch norm is appearing more and more frequently in top-performing GNNs. <ref type="bibr" target="#b38">Xie et al. (2020)</ref> argued that there was a potential risk, that adversarial examples could distort BN parameters away from natural distribution so to cause the adversarially trained model to fail on clean samples. The authors proposed to use dual batch norms (one for adversaries and the other for clean ones) at training time to better exploit the generalization ability of adversarial augmentations. To test the dual batch norm method on graph data, we run experiments as summarized in <ref type="table" target="#tab_4">Table 5</ref>. We find that the utilization of dual BN can produce a slight performance gain. As there is growing attention on using batch norms on GNNs, it will be interesting to see how to better synergize adversarial augmentation with batch norms in future research.</p><p>Comparison with other robust optimization methods. <ref type="table" target="#tab_5">Table 6</ref> shows performances with different adversarial augmentations. For PGD and "free", we compute 8 ascent steps for the innermaximization to make the attack strong enough, while for FLAG we only compute 3 steps. We can see that FLAG outperforms all other methods. We attribute that to the practice of our multiscale augmentation, which diversifies the scale range of feature perturbations, and helps the model see diverse input features to generalize better, especially on out-distribution samples. Although "free" method incorporates diversifying augmentations, but here the benefits are overwhelmed by the suboptimal problem.</p><p>Effects of weighted perturbation. The effects of biased perturbation are reported in <ref type="figure" target="#fig_2">Figure  3c</ref>. Generally speaking, when log 2 (? u /? l ) &gt; 0, which means that unlabeled nodes receive larger augmentations, the performance gains are more salient. The phenomenon supports our practice  of using weighted perturbation to promote multi-scale augmentations. Empirically we find that the benefit of weighted perturbation is more evident on ogbn-products than on ogbn-arxiv. Our understanding is that, ogbn-products is better suited with our practice of labeled vs. unlabeled split because of its high label sparsity compared with ogbn-arxiv (label rate 8% vs. 54%). When labeled nodes are more sparse, the neighborhood of labeled nodes will be more overwhelmed by unlabeled ones, where our approximation is more accurate. Hyperparameter sensitivity. <ref type="figure" target="#fig_2">Figure 3a</ref> and <ref type="figure" target="#fig_2">Figure 3b</ref> show the hyperparameter sensitivity of our method. Overall, our method is stable to yield consistent accuracy boost compared with baseline. Compatibility with mini-batch methods. Graph mini-batch algorithms are critical to training GNNs on large-scale datasets. We test how different algorithms will work with adversarial data augmentation with GraphSAGE as the backbone. From <ref type="table" target="#tab_7">Table 8</ref>, we see that neighbor sampling <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> and GraphSAINT <ref type="bibr" target="#b41">(Zeng et al., 2019)</ref> can all work with FLAG to further boost performance, while Cluster <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> suffers an accuracy drop.</p><p>Compatibility with dropout. Dropout is widely used in GNNs. <ref type="table" target="#tab_6">Table 7</ref> shows that, when trained without dropout, GAT accuracy drops steeply by a large margin. What is more, FLAG can further generalize GNN models together with dropout, similar to the phenomenon of image augmentations. It demonstrates that our method is fully compatible with this domain/model-agnostic regularizer.</p><p>Towards going "free". FLAG introduces tractable extra training overhead. We empirically show that, when we decrease the total number of training epochs to make it as fast as the standard GNN training pipeline, FLAG still brings significant performance gains. <ref type="table" target="#tab_5">Table 6</ref> shows that FLAG with fewer epochs still generalizes the baseline. Empirically, on a single Nvidia RTX 2080Ti, 100epoch vanilla GAT takes 88 mins, while FLAG (fast) in <ref type="table" target="#tab_5">Table 6</ref> takes 91 mins. We note that heuristics like early stopping and cyclic learning rates can further accelerate the adversarial training process <ref type="bibr" target="#b37">(Wong et al., 2020)</ref>, so there are abundant opportunities for further research on adversarial augmentation at lower or even no cost.</p><p>Towards going deep. Over-smoothing stops GNNs from going deep. FLAG shows its ability to boost both shallow and deep baselines, e.g., GCN and DeeperGCN. We carefully examine FLAG's effects on generalization when a GNN goes progressively deeper in <ref type="figure" target="#fig_3">Figure 4a</ref>. The experiments are conducted on ogbn-arxiv with GraphSAGE as the backbone, where a consistent improvement is evident. What if there's no node feature? One natural question can be raised: what if no input node features are provided? ogbn-proteins is a dataset without input node features. <ref type="bibr" target="#b16">Hu et al. (2020)</ref> proposed to average incoming edge features to obtain initial node features, while  used summation and achieved competitive results. Note that the GCN and GraphSAGE baselines in <ref type="table" target="#tab_0">Table 1</ref> use the "mean" node features as input and suffer an accuracy drop with FLAG; DeeperGCN leverages the "sum" and gets further improved. Interestingly, when DeeperGCN is trained with "mean" node features, it receives high invariance, so that even large magnitude perturbations will not change its result. The diverse behavior of adversarial augmentation implies the importance of node feature construction method selection.</p><p>Where Does the Boost Come from? It is now widely believed that model robustness appears to be at odds with clean accuracy. Despite the recent proliferation of literature in using adversarial data augmentation to promote standard performance, it is still unsettled where the boost or detriment of adversarial training comes from. Like one-hot word embeddings for language models, input node features usually come from discrete spaces, e.g., the bag-of-words binary features in ogbn-products. We conjecture that the diverse effects of adversarial training in different domains stem from differences in the input data distribution rather than model architectures. To ground our claim, we have the following observations. Observation 1: We utilize FLAG to augment MLPs (an architecture where adversarial training has adverse effects in the image domain), and successfully boost generalization. FLAG directly improves the test accuracy from 61.06 ? 0.08% to 62.41 ? 0.16% on ogbn-product, and from 55.50 ? 0.23% to 56.02 ? 0.19% on ogbn-arxiv.</p><p>Observation 2: In general, adversarial training hurts the clean accuracy in image classification, but <ref type="bibr" target="#b33">Tsipras et al. (2018)</ref> showed that CNNs could benefit from adversarial augmentations on MNIST, where the pixel values are closer to discrete distribution than other more natural image datasets.</p><p>Observation 3: To illustrate, we provide a simple example on the Cora <ref type="bibr" target="#b10">(Getoor, 2005)</ref> dataset. To simplify the scenario, we choose FGSM to craft adversarial augmentations for a GCN. By adding Gaussian noise with standard deviation ?, we simulate node features drawn from a continuous distribution. The result is summarized in <ref type="figure" target="#fig_3">Figure 4b</ref>. When ? = 0, the discrete distribution of node features persists. At this moment, a GCN with adversarial augmentation outperforms the non-augmented model. With increased noise level ?, the features are continuously distributed with large support and FGSM starts to harm the clean accuracy, which validates our conjecture. All these observations support our conjecture that data distribution has more to do with the effect of adversarial augmentation, while the lack of theoretical justification is a limitation of our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose FLAG, a simple, scalable, and general data augmentation method for better GNN generalization. Like widely-used image augmentations, FLAG can be easily incorporated into any GNN training pipeline. FLAG yields improvements over a range of GNN baselines. Besides extensive experiments, we also provide conceptual analysis to validate adversarial augmentation's different behavior on varied data types. The effects of adversarial augmentation on generalization are still not entirely understood, and we think this is a fertile space for future exploration. We summarize implementation details and selected hyperparameters in this section. Note that for ALL of our method, we fix the ascent step number M to 3 for simplicity. We leave more thorough step number search for future research. Experiments are done on hardware with Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz, and 128GB of RAM. If without mentioning, we use a single GeForce RTX 2080 Ti (11GB GPU memory). To highlight, for fair comparisons, we do not modify model architectures nor optimizing algorithms.  <ref type="table">Table 9</ref>: Node classification datasets statistics. * denotes we follow the split of <ref type="bibr" target="#b19">Kipf and Welling (2016)</ref>.</p><p>A.3.2 ogbg-molpcba GCN: perturbation step size ?=8e-03 for both the vanilla model and the one augmented by virtual node. GIN: perturbation step size ?=8e-03 for both the vanilla model and the one augmented by virtual node.</p><p>DeeperGCN: perturbation step size ?=8e-03 with virtual node added, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 ogbg-ppa</head><p>GCN: perturbation step size ?=2e-03, when virtual node is added we use a larger ?=5e-03.</p><p>GIN: perturbation step size ?=8e-03, when virtual node is added we use a smaller ?=5e-03. DeeperGCN: perturbation step size ?=8e-03, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory).</p><p>A.3.4 ogbg-code GCN: perturbation step size ?=8e-03 for both the vanilla model and the one augmented by virtual node.</p><p>GIN: perturbation step size ?=8e-03 for both the vanilla model and the one augmented by virtual node.</p><p>DeeperGCN: perturbation step size ?=8e-03 with virtual node added, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory). <ref type="table">Table 9</ref>, <ref type="table" target="#tab_0">Table 10, and Table 11</ref> summarize the dataset statistics for node classification, link prediction, and graph classification respectively. Datasets can be downloaded at https://ogb.stanford. edu/. <ref type="figure" target="#fig_5">Figure 6</ref> shows the loss landscape of GIN model. We can see that our method further regularizes the loss landscape.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Loss Landscape Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Augmentation distance distributions of FLAG and PGD. We run the test on ogbn-arxiv with GCN as backbone. Ascent steps are both set to 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results of GraphSAGE and GAT on the ogbn-products dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Test accuracy on ogbn-arxiv; (b) Performance gap on Cora</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Tom Goldstein were supported by DARPA GARD, Office of Naval Research, AFOSR MURI program, the DARPA Young Faculty Award, and the National Science Foundation Division of Mathematical Sciences. Additional support was provided by Capital One Bank and JP Morgan Chase. Guohao Li and Bernard Ghanem were supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding. 1 #M as ascent steps, alpha as ascent step size 2 #X denotes input node features, y denotes labels 3 def flag(gnn, X, y, optimizer, criterion, M, alpha) : .detach() + alpha*torch.sign(pert.grad.detach()) An abstract PyTorch Implementation of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Loss landscape visualization. The test is conducted on one random validation graph from ogbg-molhiv. Two models are GIN trained with FLAG and a vanilla GIN. (a) and (b) projects loss onto a random direction and the other adversarial direction, while (c) and (d) use two random directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Node property prediction test performance on ogbn-products, ogbn-proteins, and ogbn-arxiv datasets. Blank denotes no statistics on the leaderboard.ogbn-products ogbn-proteins ogbn-arxiv</figDesc><table><row><cell>Backbone</cell><cell>Test Acc</cell><cell>Test ROC-AUC</cell><cell>Test Acc</cell></row><row><cell>GCN</cell><cell>-</cell><cell>72.51?0.35</cell><cell>71.74?0.29</cell></row><row><cell>+FLAG</cell><cell>-</cell><cell>71.71?0.50</cell><cell>72.04?0.20</cell></row><row><cell>GraphSAGE</cell><cell>78.70?0.36</cell><cell>77.68 ?0.20</cell><cell>71.49?0.27</cell></row><row><cell>+FLAG</cell><cell>79.36?0.57</cell><cell>76.57?0.75</cell><cell>72.19?0.21</cell></row><row><cell>GAT</cell><cell>79.45?0.59</cell><cell>-</cell><cell>73.65?0.11</cell></row><row><cell>+FLAG</cell><cell>81.76?0.45</cell><cell>-</cell><cell>73.71?0.13</cell></row><row><cell>DeeperGCN</cell><cell>80.98?0.20</cell><cell>85.80?0.17</cell><cell>71.92?0.16</cell></row><row><cell>+FLAG</cell><cell>81.93?0.31</cell><cell>85.96?0.27</cell><cell>72.14?0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test performance on the heterogeneous OGB node property prediction dataset ogbn-mag.</figDesc><table><row><cell></cell><cell>ogbn-mag</cell></row><row><cell>Backbone</cell><cell>Test Acc</cell></row><row><cell>R-GCN</cell><cell>46.78?0.67</cell></row><row><cell>+FLAG</cell><cell>47.37?0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Link property prediction test performance on ogbl-ddi and ogbl-collab datasets.</figDesc><table><row><cell></cell><cell>ogbl-ddi</cell><cell>ogbl-collab</cell></row><row><cell>Backbone</cell><cell>Hits@20</cell><cell>Hits@50</cell></row><row><cell>GCN</cell><cell>37.07 ?5.07</cell><cell>44.75?1.07</cell></row><row><cell>+FLAG</cell><cell>51.41?3.76</cell><cell>46.22?0.81</cell></row><row><cell cols="3">GraphSAGE 53.90 ?4.74 48.10 ?0.81</cell></row><row><cell>+FLAG</cell><cell>63.31?6.06</cell><cell>48.44?0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Graph property test performance on ogbg-molhiv, ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.denotes the existence of virtual nodes; blank denotes no statistics on the leaderboard.</figDesc><table><row><cell></cell><cell>ogbg-molhiv</cell><cell cols="3">ogbg-molpcba ogbg-ppa ogbg-code</cell></row><row><cell>Backbone</cell><cell>Test ROC-AUC</cell><cell>Test AP</cell><cell>Test Acc</cell><cell>Test F1</cell></row><row><cell>GCN</cell><cell>76.06?0.97</cell><cell>20.20?0.24</cell><cell cols="2">68.39?0.34 31.63?0.18</cell></row><row><cell>+FLAG</cell><cell>76.83?1.02</cell><cell>21.16?0.17</cell><cell cols="2">68.38?0.47 32.09?0.19</cell></row><row><cell>GCN-Virtual</cell><cell>75.99?1.19</cell><cell>24.24?0.34</cell><cell cols="2">68.57?0.61 32.63?0.13</cell></row><row><cell>+FLAG</cell><cell>75.45?1.58</cell><cell>24.83?0.37</cell><cell cols="2">69.44?0.52 33.16?0.25</cell></row><row><cell>GIN</cell><cell>75.58?1.40</cell><cell>22.66?0.28</cell><cell cols="2">68.92?1.00 31.63?0.20</cell></row><row><cell>+FLAG</cell><cell>76.54?1.14</cell><cell>23.95?0.40</cell><cell cols="2">69.05?0.92 32.41?0.40</cell></row><row><cell>GIN-Virtual</cell><cell>77.07?1.49</cell><cell>27.03?0.23</cell><cell cols="2">70.37?1.07 32.04?0.18</cell></row><row><cell>+FLAG</cell><cell>77.48?0.96</cell><cell>28.34?0.38</cell><cell cols="2">72.45?1.14 32.96?0.36</cell></row><row><cell>DeeperGCN</cell><cell>78.58?1.17</cell><cell>27.81 ?0.38</cell><cell>77.12?0.71</cell><cell>-</cell></row><row><cell>+FLAG</cell><cell>79.42?1.20</cell><cell>28.42 ?0.43</cell><cell>77.52?0.69</cell><cell>-</cell></row></table><note>Large-scale Graph Property Prediction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test Accuracy on the ogbn-arxiv dataset with different BN methods.</figDesc><table><row><cell>Method</cell><cell>GCN</cell><cell>GraphSAGE</cell></row><row><cell>w/o BN</cell><cell cols="2">71.09?0.22 69.58?0.76</cell></row><row><cell>w/ BN</cell><cell cols="2">71.74?0.29 71.49?0.27</cell></row><row><cell>w/ BN +FLAG</cell><cell cols="2">72.04?0.20 72.19?0.21</cell></row><row><cell cols="3">w/ Dual BN +FLAG 72.11?0.23 72.21?0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Test performances on different datasets trained with different adversarial augmentations. Baselines are GAT, GraphSAGE, and GIN respectively. FLAG (fast) means the training epoch number is decreased to make our method trained as fast as the baseline. model to 78.70?0.36%. When FLAG is also used, the test accuracy is increased to 79.36 ? 0.57%. (ii) Virtual node<ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref> adds one synthetic node that connects to all existing nodes. Nearly all the numbers fromTable 4supports that our method works well with virtual node to generalize GNNs further. Here We highlight one representative group of experiments on ogbg-ppa with GIN as baseline. Vanilla GIN gets 68.92 ? 1.00% test accuracy. By adding virtual node alone, it goes to 70.37 ? 1.07%. When FLAG is further deployed, test accuracy reaches 72.45 ? 1.14%.</figDesc><table><row><cell></cell><cell cols="2">ogbn-products ogbl-ddi</cell><cell>ogbg-molhiv</cell></row><row><cell></cell><cell>Test Acc</cell><cell>Hits@20</cell><cell>Test ROC-AUC</cell></row><row><cell>Baseline</cell><cell>79.45?0.59</cell><cell>53.90?4.74</cell><cell>75.58?1.40</cell></row><row><cell>+PGD</cell><cell>80.96?0.41</cell><cell>62.02?6.56</cell><cell>76.14?1.62</cell></row><row><cell>+"Free"</cell><cell>79.42?0.84</cell><cell>58.61?6.0</cell><cell>74.93?1.29</cell></row><row><cell>+FLAG</cell><cell>81.76?0.45</cell><cell>63.31?6.06</cell><cell>76.54?1.14</cell></row><row><cell>+FLAG (fast)</cell><cell>80.64?0.74</cell><cell>-</cell><cell>-</cell></row><row><cell>alone generalizes the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Test Accuracy on the ogbn-products dataset.</figDesc><table><row><cell>Backbone</cell><cell>Test Acc</cell></row><row><cell>GAT w/o dropout</cell><cell>75.67?0.27</cell></row><row><cell>GAT w/ dropout</cell><cell>79.45?0.59</cell></row><row><cell cols="2">GAT w/ dropout +FLAG 81.76?0.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Test accuracy on ogbn-products with GraphSAGE trained with diverse mini-batch algorithms.</figDesc><table><row><cell></cell><cell>ogbn-products</cell></row><row><cell>Backbone</cell><cell>Test Acc</cell></row><row><cell>GraphSAGE w/ NS</cell><cell>78.70?0.36</cell></row><row><cell>+FLAG</cell><cell>79.36?0.57</cell></row><row><cell>GraphSAGE w/ Cluster</cell><cell>78.97?0.33</cell></row><row><cell>+FLAG</cell><cell>78.60?0.27</cell></row><row><cell>GraphSAGE w/ SAINT</cell><cell>79.08?0.24</cell></row><row><cell>+FLAG</cell><cell>79.60?0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Link prediction datasets statistics.</figDesc><table><row><cell>Name</cell><cell cols="5">#Graphs Avg #Nodes Avg #Edges #Tasks Train/Val/Test</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell>ogbg-molhiv</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>1</cell><cell>80/10/10</cell><cell>Binary classification</cell><cell>ROC-AUC</cell></row><row><cell cols="2">ogbg-molpcba 437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>128</cell><cell>80/10/10</cell><cell>Binary classification</cell><cell>AP</cell></row><row><cell>ogbg-ppa</cell><cell>158,100</cell><cell>243.4</cell><cell>2,266.1</cell><cell>1</cell><cell>49/29/22</cell><cell cols="2">Multi-class classification Accuracy</cell></row><row><cell>ogbg-code</cell><cell>452,741</cell><cell>125.2</cell><cell>124.2</cell><cell>1</cell><cell>90/5/5</cell><cell>Sub-token prediction</cell><cell>F1 score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Graph classification datasets statistics.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also tried DropEdge<ref type="bibr" target="#b28">(Rong et al., 2019)</ref> but it failed to yield performance gain in the first place.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08051</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced methods for knowledge discovery from complex data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple GNN regularisation for 3d molecular property prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1wVvweK3oIb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent adversarial training of graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Learning and Reasoning with Graph-Structured Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial training for free!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3358" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m">Prepare for the worst: Generalizing across domain shifts with adversarial batch normalization. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2009</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nodeaug: Semi-supervised node classification with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03994</idno>
		<title level="m">Fast is better than free: Revisiting adversarial training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphsaint</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<title level="m">Graph sampling based inductive learning method</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GraphSAGE: labeled perturbation step size ? l =8e-03, ? u /? l =2, and neighbor sampling is used for scalable training. GAT: labeled perturbation step size ? l =5e-03, ? u /? l =2, and neighbor sampling is used for scalable training. DeeperGCN: labeled perturbation step size ? l =5e-03</title>
	</analytic>
	<monogr>
		<title level="m">MLP: perturbation step size ?=2e-02, only labeled nodes are used in the training phase</title>
		<imprint/>
	</monogr>
	<note>? u /? l =2, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GraphSAGE: labeled perturbation step size ? l =1e-03, ? u /? l =1, and the model is trained in the full-batch manner. DeeperGCN: labeled perturbation step size ? l =8e-03</title>
	</analytic>
	<monogr>
		<title level="m">A.1.2 ogbn-proteins GCN: labeled perturbation step size ? l =1e-03, ? u /? l =1, and the model is trained in the full-batch manner</title>
		<imprint/>
	</monogr>
	<note>? u /? l =1, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GCN: labeled perturbation step size ? l =1e-03, ? u /? l =1, and the model is trained in the fullbatch manner. GraphSAGE: labeled perturbation step size ? l =1e-03, ? u /? l =1, and the model is trained in the full-batch manner. GAT: labeled perturbation step size ? l =1e-03, ? u /? l =2, and the model is trained in the fullbatch manner. DeeperGCN: labeled perturbation step size ? l =8e-03</title>
	</analytic>
	<monogr>
		<title level="m">A.1.3 ogbn-arxiv MLP: perturbation step size ?=2e-03, only labeled nodes are used in the training phase</title>
		<imprint/>
	</monogr>
	<note>? u /? l =1, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">labeled perturbation step size ? l =1e-04, ? u /? l =1, and the model is trained with neighbor sampling for scalability. both trained in the full-batch manner. During each gradient ascent loop negative edges are resampled for computing negative losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Gcn</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A.2.2 ogbl-collab GCN perturbation step size ?=3e-03, and GraphSAGE perturbation step size ? l =3e-03. Models are both trained in the full-batch manner. During each gradient ascent loop negative edges are resampled for computing negative losses</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GCN: perturbation step size ?=1e-02, when virtual node is added we use a smaller ?=1e-03. GIN: perturbation step size ?=5e-03, when virtual node is added we use a smaller ?=1e-03</title>
	</analytic>
	<monogr>
		<title level="m">DeeperGCN: perturbation step size ?=1e-02, and the model is trained on NVIDIA Tesla V100 (32GB GPU memory</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
