<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnable Boundary Guided Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Boundary Guided Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, we reduce natural accuracy degradation. We use the model logits from one clean model to guide learning of another one robust model, taking into consideration that logits from the well trained clean model embed the most discriminative features of natural data, e.g., generalizable classifier boundary. Our solution is to constrain logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with corresponding natural data. It lets the robust model inherit the classifier boundary of the clean model. Moreover, we observe such boundary guidance can not only preserve high natural accuracy but also benefit model robustness, which gives new insights and facilitates progress for the adversarial community. Finally, extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet testify to the effectiveness of our method. We achieve new state-of-the-art robustness on CIFAR-100 without additional real or synthetic data with auto-attack benchmark 1 . Our code is available at https: //github.com/dvlab-research/LBGAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved great success in many tasks, especially with the surge of neural architecture search <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3]</ref>. However, with the concern of security of deep models, several methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">37]</ref> have shown that deep models could be vulnerable to adversarial attack. Data that is intentionally created may easily fool strong classifiers.</p><p>In response to the vulnerability of deep neural networks, adversarial defense has become an essential topic in computer vision. There are now a sizable body of work exploring different ways to get adversarial settings, including defensive distillation <ref type="bibr" target="#b30">[30]</ref>, feature squeezing <ref type="bibr" target="#b53">[53]</ref>, randomization based methods <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b12">13]</ref> and augmenting the training 1 https://github.com/fra31/auto-attack Vanilla AT <ref type="bibr" target="#b27">[27]</ref> ALP <ref type="bibr" target="#b20">[21]</ref> TRADES <ref type="bibr" target="#b56">[56]</ref> LBGAT (Ours) Acc (%) Methods Natural Acc Robust Acc <ref type="figure">Figure 1</ref>: Model robustness on CIFAR-100 evaluated with 20 iterations PGD under white-box attack. "Natural Acc" represents classification accuracy on natural (clean) data. "Robust Acc" represents classification accuracy on adversarial data. Our method (LBGAT+TRADES with ? = 0) improves robustness with the least natural accuracy degradation.</p><p>with adversarial examples <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b43">43]</ref>, i.e., adversarial training. However, training a robust model is still challenging. Recently, adversarial training with PGD attack <ref type="bibr" target="#b27">[27]</ref> becomes an effective defense strategy. However, when plotting results of recent work <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">27]</ref> in <ref type="figure">Fig. 1</ref>, it is still noticeable that higher robustness is often accompanied with more accuracy degradation on natural data classification. Different from previous work that mainly pursues various ways to improve robustness, we meanwhile pursue accuracy preservation on natural data. In this paper, we propose a novel adversarial training scheme, which significantly improves classification accuracy on natural data. It also achieves high robustness under black-and white-box attack. We take advantage of logits from a clean model, which is trained only on natural data, to guide the learning of a robust model.</p><p>A conceptual illustration is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> to explain our motivation. As shown in (a), when only trained on natural (clean) data, the learned model M natural separates natural data (plotted in yellow) well. But it may fail to classify perturbed data and misclassifies the dark circle into the  In order to seek guidance from clean model M natural , we expect the logit output of adversarial example x adv from M robust to be similar to logits output of corresponding natural data x that goes through M natural . As plotted in <ref type="figure" target="#fig_1">Fig.  2(b)</ref>, the classifier boundary of our M robust is constrained by that of the clean model, which helps classify the clean data into correct categories. At the same time, adversarial examples are also correctly labeled, benefiting from the adversarial training scheme.</p><p>Instead of constraining M robust with the classifier boundary from one well trained static M natural , we further generalize our method to Learnable Boundary Guided Adversarial Training (LBGAT) by training M natural and our required model M robust at the same time to dynamically adjust the classifier boundary of M natural and learn the most robustness-friendly one to further help M robust enhance robustness. To show the flexibility of our method, we incorporate our model into state-of-the-art methods Adversarial Logit Pairing (ALP) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b56">[56]</ref> respectively and accomplish remarkable improvement over the baselines. Interestingly, in our exploration, we observe the classifier boundary guidance from M natural can also enhance model robustness, which gives us new insights and potentially facilitates progress for adversarial robustness.</p><p>We conduct experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet to evaluate the performance of our mod-els under both white-and black-box attacks. Our models achieve impressive performance on these datasets and outperform previous work in a large margin. Particularly, we achieve state-of-the-art model robustness on CIFAR-100 without extra real or synthetic data under current the most popular auto-attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Adversarial Attack</head><p>White-box Attack Szegedy et al. <ref type="bibr" target="#b39">[39]</ref> observed that CNNs are vulnerable to adversarial examples computed by the proposed box-constrained L-BFGS attack method. Goodfellow et al. <ref type="bibr" target="#b15">[16]</ref> attributed the existence of adversarial examples to the linear nature of networks, which yields the fast gradient sign method (FGSM) for efficiently generating adversarial examples. FGSM was further extended to different versions of iterative attack methods. Kurakin et al. <ref type="bibr" target="#b22">[23]</ref> showed that adversarial examples could exist in the physical world with an I-FGSM attack and iteratively applied FGSM multiple times with a small step size. Madry et al. <ref type="bibr" target="#b27">[27]</ref> proposed Projected Gradient Descent (PGD) method as a universal "first-order adversary", i.e., the most active attack utilizing the local first-order information about the network.</p><p>Dong et al. <ref type="bibr" target="#b13">[14]</ref> integrated the momentum term into an iterative process for attack, called MI-FGSM, to stabilize update of directions and escape from poor local maxima during iterations. This method obtains more transferable adversarial examples. Moreover, boundary-based methods like DeepFool <ref type="bibr" target="#b29">[29]</ref> and optimization-based methods like C&amp;W <ref type="bibr" target="#b3">[4]</ref> were also developed, making adversarial defense more challenging. Recently, the ensemble of diverse attack methods -auto-attack <ref type="bibr" target="#b9">[10]</ref> by Croce et al., consisting of APGD-CE <ref type="bibr" target="#b9">[10]</ref>, APGD-DLR <ref type="bibr" target="#b9">[10]</ref>, FAB <ref type="bibr" target="#b8">[9]</ref>, and Square Attack <ref type="bibr" target="#b0">[1]</ref>, became popular benchmark for testing model robustness.</p><p>Black-box Attack There are also many ways to explore the transferability of adversarial examples for the blackbox attack. Liu et al. <ref type="bibr" target="#b24">[25]</ref> was the first to study the transferability of targeted adversarial examples. They observed that a large proportion of target adversarial examples were able to transfer with their target labels using the proposed ensemble-based attack method. Dong et al. <ref type="bibr" target="#b13">[14]</ref> showed that iterative attack methods incorporating the momentum term achieved better transferability. Further, Xie et al. <ref type="bibr" target="#b52">[52]</ref> boosted the transferability of adversarial examples by creating diverse input patterns with random resize and random padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial Defense</head><p>Recent work focuses generally on developing defense methods to improve model robustness, including input transformation-based methods, randomization based methods <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b12">13]</ref>, and adversarial training <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b43">43]</ref>. Athalye et al. <ref type="bibr" target="#b1">[2]</ref> showed that adversarial training with PGD had withstood active attacks. Tram?r et al. <ref type="bibr" target="#b43">[43]</ref> raised model robustness under black-box attack by the proposed ensemble adversarial training, i.e., producing adversarial examples by static ensemble models. Madry et al. <ref type="bibr" target="#b27">[27]</ref> used the universal first-order adversary, i.e., PGD attack, to obtain adversarial examples in the course of adversarial training. Differently, Kannan et al. <ref type="bibr" target="#b20">[21]</ref> enhanced model robustness with adversarial logit pairing, which encourages the logits from natural images and adversarial examples to be similar to each other in the same model.</p><p>Moreover, Zhang et al. <ref type="bibr" target="#b56">[56]</ref> regularized the output from natural images and adversarial examples with the KLdivergence function, meanwhile using a variant of PGD attack. Xie et al. <ref type="bibr" target="#b50">[50]</ref> studied the effect of normalization in adversarial training and proposed the Mixture BN mechanism that uses separate batch normalization layers for natural data and adversarial examples in one model. It still requires the strong assumption of knowing whether an image is natural or adversarial, at inference time, which may not be that practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Knowledge Distillation</head><p>Knowledge distillation was first used in <ref type="bibr" target="#b18">[19]</ref> by Hinton et al., which was then widely applied to distill knowledge from a teacher model to a student model. The typical application of knowledge distillation is model compression, transferring from a large network or ensembles to a small network that better suits low-cost computing. Since this work, several methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b6">7]</ref> were proposed to further improve performance on model compressing and other tasks.</p><p>Goldblum et al. <ref type="bibr" target="#b14">[15]</ref> analyzed the application of knowledge distillation in adversarial training and proposed Adversarial Robust Distillation (ARD) to transfer robustness from a large adversarially trained model to a smaller one. In this paper, we propose to use one robustness-friendly boundary learned by one natural model, not necessarily large, to guide the adversarial training without cross-entropy loss. By this way, the robust model can sufficiently inherit the classifier boundary and thus preserves high accuracy on natural data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Boundary Guided Adversarial Training</head><p>As suggested by Madry et al. <ref type="bibr" target="#b27">[27]</ref>, projected gradient descent (PGD) is a universal first-order adversary. Robust methods to defense PGD might be able to resist attack stemming from other first-order methods as well. Similarly, we use adversarial training with PGD as</p><formula xml:id="formula_0">min ? E (x,y)?p data arg max ?L (?, x + ?, y)<label>(1)</label></formula><p>wherep data is the training data distribution,L(?, x, y) is the standard cross-entropy loss function with data point x and its corresponding true label y. ? represents parameters of the model, and the maximization with respect to ? is approximated using noisy BIM <ref type="bibr" target="#b22">[23]</ref>. We denote the adversarial example x + ? across the paper as x adv . Following previous work <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b27">27]</ref>, ? is bounded by l ? . Our expectation of the robust model is to achieve decent robustness and at the same time keep high accuracy on natural images. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we make use of logits from a clean model to help shape the classifier boundary of the robust model. The logits of our required robust model M robust with x adv taken as input should be similar to those of M natural taking x as input. This relation is expressed as</p><formula xml:id="formula_1">min ? E (x,y)?p data L M robust (x adv ), M natural (x) (2)</formula><p>where L is Mean Square Error (MSE) loss function in our experiments and M(x) denotes the logits of model M taking x as input. ? is the parameter of M robust . We randomly initialize M robust and off-line train M natural on natural data in our experiments.</p><p>Our method can be understood from the perspective of classifier boundary guidance. Here we give analysis of why our method can yield high performance on natural data.</p><p>Natural Classifier Boundary Guidance Since we assume that M natural is well trained on natural data, logits from M natural embed more discriminative features for classification, especially the classifier boundary. According to Eq. (2), when we impose the logits constraints, the system penalizes more on those pairs (x and x adv ) that have more substantial discrepancy in classification. Therefore, this logit guidance makes M robust inherit decent classifier boundary for adversarial data. Actually, the inherited classifier boundary is still applicable to natural data in following explanation.</p><p>It is noteworthy that the adversarial example x adv is located in the l ? ball of x. According to the min-max mechanism of PGD <ref type="bibr" target="#b27">[27]</ref>, when the adversarial training converges, the loss value corresponding to x adv is always larger than the loss value corresponding to x when passing x adv and x into the same model M robust . Therefore, when we pull x adv into the correct class with our proposed logits constraints, x is also squeezed into the correct class. Thus the inherited classifier boundary from M natural separates natural data well and preserves high natural accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learnable Boundary Guided Adv. Training</head><p>For Boundary Guided Adversarial Training (BGAT) method, M robust is constrained by logits of the static M natural . The well trained M natural has the most desirable classifier boundary for natural data. Thus, inheriting such classifier boundary, M robust tends to achieve high performance on natural images.</p><p>Nevertheless, the classifier boundary coming from static M natural might not be the most suitable choice for pursuing robustness. We generalize the BGAT method to Learnable Boundary Guided Adversarial Training (LBGAT) by training M natural and M robust simultaneously and collaboratively. The loss function is therefore changed from Eq.</p><p>(2) to</p><formula xml:id="formula_2">min ?,? * E (x,y)?p data L M robust (x adv ), M natural (x) + ? CE ?(M natural (x)), y<label>(3)</label></formula><p>where x adv is the adversarial example corresponding to its natural data x, and y is the true label. ?(?) is a softmax function. CE represents cross-entropy loss, M natural and M robust are parameterized by ? * and ? respectively. We use Mean Square Error (MSE) loss as L function. ? is the trade-off parameter. In this paper, we choose ? = 1. We randomly initialize M robust and M natural in our experiments.</p><p>Under the regularization of the proposed logits constraints, i.e., the L(?) loss item in Eq. (3), M natural adaptively learns one most robustness-friendly classifier boundary during the collaborative training. At the same time, it guarantees least performance degradation on natural data with CE(?) loss item in Eq. (3). Note there is no additional cross-entropy loss for optimizing M robust , which makes the classifier boundary be sufficiently inherited from M natural . More details are listed in Algorithm 1. Read mini-batch X = {x 1 , ..., x m }, Y = {y 1 , ..., y m } from training set; <ref type="bibr">6:</ref> Get adversarial examples X adv = {x adv 1 , ..., x adv m } by PGD attack with input X, Y ; <ref type="bibr">7:</ref> output n = M natural (X); <ref type="bibr">8:</ref> output r = M robust (X adv ); <ref type="bibr">9:</ref> loss ce = cross ? entropy(?(output n ), Y ); <ref type="bibr">10:</ref> loss reg = L(output n , output r ); 11: <ref type="bibr">13:</ref> until training converges</p><formula xml:id="formula_3">? * = ? * ? ? 2 m i=1 ? ? * (?loss ce + loss reg )/m; 12: ? = ? ? ? 2 m i=1 ? ? (?loss ce + loss reg )/m;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary Guidance Improving Robustness</head><p>Zhang et al. <ref type="bibr" target="#b56">[56]</ref> identified a trade-off between performance on natural data and robust accuracy. Xie et al. <ref type="bibr" target="#b48">[48]</ref> observed that adversarial examples were helpful to model generalization ability on natural images. However, using models trained only with natural data to enhance model robustness remains unexplored. We instead notice that proper classifier boundary learned by the naturally trained model not only helps preserve high natural accuracy but also enhances model robustness (2.44% improvement on CIFAR-100 dataset under the strongest auto-attack <ref type="bibr" target="#b9">[10]</ref> shown in <ref type="table">Table 5</ref>. We attribute the improvement to the guidance of natural classifier boundary with the following explanation.</p><p>Empirically, as shown in <ref type="figure">Fig. 1</ref>, an adversarially trained model usually suffer from natural accuracy degradation, which means the adversarially trained model can not model the relations among different classes as well as the naturally trained model.</p><p>For example, with an image of a dog, the naturally trained model can misclassify it as a cat with the probability of 0.5. Under some case, we can accept this result because some dogs are very like a cat in real life. However, the adversarially trained model can misclassify a dog into a truck with high confidence because attackers can change the prediction of an image into any other class. And this is not acceptable for us because a dog is very different from a truck. Thus, with the guide of classifier boundary from a naturally trained model, the adversarially trained model can avoid such issues to some degree in training optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Flexibility</head><p>Our method provides a new training scheme for adversarial training. It does not conflict or overlap with other adversarial training methods. We show the flexibility of our approach by using it in other state-of-the-art methods, e.g., Adversarial Logit Pairing (ALP) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b56">[56]</ref>. We validate the improvement over these baselines.</p><p>Combined with Adversarial Logit Paring Adversarial logit pairing (ALP) requires the logits of natural data x and the corresponding adversarial example x adv to be the same in one model, which is achieved by adding an extra mean square loss item between two logits output. We combine our BGAT with ALP as the loss of</p><formula xml:id="formula_4">min ? E (x,y)?p data L M robust (x adv ), M natural (x) + ? M SE M robust (x adv ), M robust (x)<label>(4)</label></formula><p>where ? is a trade-off parameter. ?(?) is a softmax function and y is the true label. ? is the parameter of M robust . We replace the cross-entropy loss item CE(?(M robust (x adv )), y) in the original ALP loss function with our Eq. (2).</p><p>Combined with TRADES The proposed TRADES algorithm <ref type="bibr" target="#b56">[56]</ref> explores the trade-off between model robustness and accuracy on natural data by optimizing one regularized surrogate loss. We use our BGAT in the TRADES algorithm as</p><formula xml:id="formula_5">min ? E (x,y)?p data L M robust (x adv ), M natural (x) + ? DKL ?(M robust (x adv ))||?(M robust (x))<label>(5)</label></formula><p>where ? is still a trade-off parameter. ? is the parameter of M robust . ?(?) is softmax function and y is the true label. D KL (?) is the boundary error term, pushing classifier boundary away from data point x, originally defined in TRADES <ref type="bibr" target="#b56">[56]</ref>. We replace the cross-entropy loss item of CE(?(M robust (x)), y) in original TRADES loss with our Eq. (2).</p><p>It is noted that our LBGAT method can also be combined with both ALP and TRADES methods by simply replacing the first loss item in Eqs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we verify the effectiveness of our methods by conducting both white-and black-box attack following the same experimental settings in <ref type="bibr" target="#b56">[56]</ref>, i.e., applying F GSM k (white-box or black-box) attack with 20 iterations, perturbation size = 0.031 with step size 0.003.  <ref type="bibr" target="#b11">[12]</ref>, which is with more complex data, is a miniature of ImageNet dataset. It has 200 classes. Each class has 500 training images, 50 validation images.</p><p>In our experiments, we resize the image to 32x32 and normalize pixel values to [0,1]. Following <ref type="bibr" target="#b56">[56]</ref>, we perform standard data augmentation including random crops with 4 pixels of padding and random horizontal flip during training.</p><p>Training Details. We use the same neural network architecture as <ref type="bibr" target="#b56">[56]</ref>, i.e., the wide residual network WRN-34-10. Following <ref type="bibr" target="#b56">[56]</ref>, We set perturbation = 0.031, perturbation step size ? 1 = 0.007, number of iterations K = 10, learning rate ? 2 = 0.1, batch size m = 128, and number of training epochs 100 with transition epochs {75, 90} on the training dataset. Similarly, SGD optimizer with momentum 0.9 and weight decay 2e ? 4 is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Natural Classifier Boundary Inheritance</head><p>To show the importance of boundary inheritance from M natural , we conduct ablation experiments with and without cross-entropy loss for M robust in Eq. (3). Experimental results are summarized in <ref type="table" target="#tab_2">Table 1</ref>. "w/o" additional crossentropy loss for M robust enjoys 2.05% higher robust accuracy than "w/", which further manifests vast importance of the natural classifier boundary inheritance. We also replace MSE loss with KL-Divergence loss in Eq. (3). KL-Divergence loss encourages the outputs of M robust and M natural to enjoy the same distribution while MSE loss encourages the outputs of M robust and M natural to have <ref type="figure">Figure 3</ref>: Feature Visualization for LBGAT and TRADES on 5 random selected classes. </p><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (a) Visualization for TRADES. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (b) Visualization for LBGAT.</formula><formula xml:id="formula_7">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (a) Visualization for TRADES. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (b) Visualization for LBGAT.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Separate Batch Normalization</head><p>Xie et al. pointed that clean and adversarial images are drawn from two different domains and disentangling the mixture distribution for normalization can enhance model robustness. However, in this paper, we explore the interaction of information from those two domains, i.e., using classifier boundary information from clean images to assist the learning for adversarial examples.</p><p>Here we go deeper to explore whether the convolution weights can be shared in M natural and M robust with experiments on CIFAR-100. The experimental results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Unfortunately, we observe robustness drops.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Effectiveness of Our Method</head><p>We first verify the effectiveness of our method compared with vanilla Adversarial Training (AT). Evaluation of model robustness is under the white-box attack using the same setting as described at the beginning of Sec. 4. Both our BGAT and LBGAT methods significantly outperform vanilla AT shown by results in <ref type="table" target="#tab_4">Table 3</ref>. As analyzed in Sec. 3.2, the BGAT method can achieve higher natural accuracy while the LBGAT method tends to have stronger robustness. Since we aim to achieve the strongest robustness while preserving natural accuracy as high as possible, we use LBGAT by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Combing with ALP and TRADES</head><p>To verify the flexibility of our method, we show that combined with our BGAT and LBGAT methods, ALP and TRADES further improve performance. For ALP, BGAT+ALP and LBGAT+ALP methods, we adopt ? = 1 following the setting in <ref type="bibr" target="#b20">[21]</ref>. For the TRADES method, we adopt ? = 6, with which TRADES achieves the best robustness, as demonstrated in <ref type="bibr" target="#b56">[56]</ref>. The evaluation is under the white-box attack following the same setting as described at the beginning of Sec. 4. We summarize the results in <ref type="table" target="#tab_5">Table 4</ref>. Equipped with regularization items of ALP and TRADES, our method can further enhance model robustness. For CIFAR-100, LBGAT+ALP outperforms ALP by 2.92% and 6.31% respectively on natural accuracy and robust accuracy under the whitebox attack respectively. Meanwhile, the BGAT+TRADES method also outperforms TRADES in terms of both natural accuracy and robustness under the white-box attack for CIFAR-10, which manifests the great flexibility of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robustness on CIFAR-10 and CIFAR-100</head><p>White-box Regular Attacks. We evaluate the robustness of our models under the white-box attack using the same setting as described at the beginning of Sec. 4. For CIFAR-10, our LBGAT+TRADES (? = 0) achieves 88.22% accuracy on natural images, which outperforms TRADES (? = 6) by 3.3% at the same time remaining 57.55% robust accuracy, 0.94% higher than that of TRADES (? = 6).</p><p>For CIFAR-100, our LBGAT+TRADES (? = 0) achieves 70.03% accuracy on natural images and 33.01% robust accuracy, improving TRADES (? = 6) by 13.53% and 2.08% respectively. Moreover, our LBGAT+TRADES (? = 6) further boosts robustness to 57.78% and 35.50% on CIFAR-10 and CIFAR-100 respectively.</p><p>We also apply several other regular attack methods, like FGSM and CW, to evaluate our models. Compared with TRADES, our proposed methods consistently achieve better accuracy on natural images and stronger robustness on both CIFAR-10 and CIFAR-100 datasets. The details of our results are presented in <ref type="table">Table 5</ref>. Note that the CW attack denotes using CW-loss within the PGD framework here. The  <ref type="table" target="#tab_8">Table 7</ref> evaluation under CW attack is also with 20 iterations, step size 0.003 and perturbation = 0.031.</p><p>White-box Auto-Attack (AA). Auto-Attack <ref type="bibr" target="#b9">[10]</ref> is to reliably evaluate model robustness with an ensemble of diverse strong attack methods, including APGD-CE, APGD-DLR, FAB, and Square Attack. We use the open-source code from <ref type="bibr" target="#b9">[10]</ref> to test our models with perturbation size 0.031. The results are listed in <ref type="table">Table 5</ref>. Compared with TRADES (? = 6), our LBGAT+TRADES (? = 0) model improves natural accuracy by 13.53% and 3.30% on CIFAR-100 and CIFAR-10 separately, while achieving comparable robustness. Our LBGAT+TRADES (? = 6) model further boosts robust accuracy, obtaining 29.34% and 53.14% on CIFAR-100 and CIFAR-10, outperforming TRADES (? = 6) by 2.44% and 0.5% respectively.</p><p>Black-box Attacks. We verify the robustness of our models under the black-box attack. We first train models without using adversarial training on the CIFAR-10 and CIFAR-100 datasets. The same network architectures that are specified at the beginning of this section, i.e., the WRN-34-10 architecture <ref type="bibr" target="#b54">[54]</ref>, are adopted. We denote these models by naturally trained models as (Natural). The accuracy of the naturally trained WRN-34-10 model is 95.80% on the CIFAR-10 dataset and 78.76% on the CIFAR-100 dataset. We also implement the method proposed in <ref type="bibr" target="#b56">[56]</ref> on both datasets with their open-source codebase. For both datasets, the F GSM k (black-box) method is applied to attack various defense models. We set = 0.031 and apply F GSM k (black-box) attack with 20 iterations with step size set to 0.003. Note that the setup is the same as that specified in the white-box attack. <ref type="table">Table 5</ref>: Comparison of our method with previous defense models under white-box attack on CIFAR-10 and CIFAR-100. We use ResNet18 as M natural for LBGAT method. Acc n represents accuracy on natural images while Acc r represents robustness of models. AA is the strongest attack, i.e., auto-attack <ref type="bibr" target="#b9">[10]</ref>. * denotes the model is WRN-34-20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense</head><p>Attack The results on CIFAR-100 are summarized in <ref type="table" target="#tab_8">Table 7</ref> of Appendix A. We use source models to generate adversarial perturbations where the perturbation directions are according to the gradients of the source models on the input images. Our models are more robust against blackbox attack transferred from naturally trained models and TRADES <ref type="bibr" target="#b56">[56]</ref>, while yielding stronger robustness under white-box attack and higher performance on natural images. Specifically, our best model is 12.83% and 8.60% higher than TRADES (? = 6) with the naturally trained model and robust model as the source model separately on CIFAR-100. For robustness under black-box attack with one robust source model, our model is tested under TRADES (? = 6) while TRADES is tested under our LBGAT trained model. More comparison between our method and TRADES is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, which exhibits results on the more challenging dataset CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness on Tiny-ImageNet.</head><p>To further demonstrate the effectiveness of our method on more complex data, we conduct experiments on Tiny ImageNet. <ref type="table">Table 6</ref> shows the experimental results. Our method is better than ALP and TRADES, surpassing baselines with a large margin. Specifically, our LB-GAT+TRADES (? = 0) outperforms the most robust baseline TRADES (? = 6) by 9.29% on natural data, meanwhile LBGAT+TRADES (? = 6) is 3.00% higher than it on adversarial data, which verifies the effectiveness of our approach again. <ref type="table">Table 6</ref>: Results on Tiny ImageNet <ref type="bibr" target="#b11">[12]</ref>. The same evaluation setting with CIFAR is applied under 20-iteration PGD white-box attack. We adopt ResNet18 as M natural for LB-GAT methods. Acc n represents accuracy on natural images while Acc r represents robustness of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed the Learnable Boundary Guided Adversarial Training (LBGAT) method, to improve model robustness without losing much accuracy on natural data. Our approach can be understood from the perspective of natural classifier boundary guidance. Moreover, an interesting phenomenon that the boundary guidance from a naturally trained model can also enhance model robustness is observed during our exploration. Finally, extensive experiments on CIFAR-10, CIFAR-100, and more challenging Tiny ImageNet datasets proved the effectiveness of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable Boundary Guided Adversarial Training Supplementary Material</head><p>A. Robustness under Black-box attack To further show the effectiveness of our method, we compare with more previous works. The experimental results are shown in <ref type="table">Table 8</ref>. On the more challlenging CIFAR-100 dataset, our method creates a new state-of-the-art (SOTA) on both robustness and natural accuracy. Specifically, our LBGAT (? = 0) model with WideResNet-34-10 architecture significantly outperforms previous SOAT method <ref type="bibr" target="#b5">[6]</ref> by 7.08% in the aspect of performance on natural data. Meanwhile, our method surpasses it with respect to model robustness. Further, our strongest model LBGAT (? = 6) with WideResNet-34-10 architecture enjoys 2.4% higher robustness than <ref type="bibr" target="#b5">[6]</ref>.</p><p>Moreover, It is worthy to note that our LBGAT (? = 6) model achieves even strong robustness than the model, by Hendrycks et al. <ref type="bibr" target="#b17">[18]</ref>, pre-trained on full ImageNet. At the same time, we also surpasses it in the aspect of natural accuracy. <ref type="table">Table 8</ref>: More comparisons under the strongest Auto-Attack on CIFAR-100 dataset. " ?" denotes numbers are directly copied from <ref type="bibr" target="#b9">[10]</ref>. " " denotes that the method has used additional unlabeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Classifier boundary of clean model (b) Classifier boundary of our robust model (c) Classifier boundary of robust model trained with previous methods Conceptual illustration of our method vs. previous adversarial training approaches. Solid lines denote real classifier boundary of the trained model, while the dotted line is the classifier boundary of the clean model M natural . Different shapes represent logits of images in various classes. Black color marks adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(4) and (5) with Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Feature Visualization for LBGAT and TRADES on 20 random selected classes.the same classifier boundary. After replacing MSE with KL-Divergence, we observe performance degradation.4.1.2 Feature VisualizationWe randomly sample 5 or 20 classes in CIFAR-100. The numbers in the pictures are class indexes. For each sampled class, we collect the logit features of clean images and the corresponding adversarial examples. As shown in the figures below, LBGAT can inherit a good classifier boundary from a naturally trained model, benefiting performance on both natural data and adversarial data of the adversarially trained model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>"White-box Robust Acc" represents classification accuracy under white-box attack. "Black-box Robust Acc" represents classification accuracy under black-box attack. Models on the right of the red line are evaluated with the clean model as the source one, while models on the left of the red line models are evaluated with the robust model as the source. More details are included in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Learnable Boundary Guided Adversarial Training (LBGAT) 1: Input: step size ? 1 and learning rate ? 2 , batch size m, number of iterations K in inner optimization, model M robust parameterized by ?, M natural parameterized by ? * . ? is one hyper-parameter. 2: Output: robust model M robust with ?. 3: Initialize M robust and M natural randomly or with pretrained configuration.</figDesc><table /><note>4: repeat5:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for boundary inheritance on CIFAR-10. 20 iterations PGD white-box attack is applied. We adopt ResNet18 as M natural for LBGAT method. Acc n represents accuracy on natural images while Acc r represents robustness of models.</figDesc><table><row><cell>Methods</cell><cell>Acc n</cell><cell>Acc r</cell></row><row><cell>vanilla AT</cell><cell cols="2">86.82% 52.87%</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="2">84.92% 56.61%</cell></row><row><cell cols="3">LBGAT (? = 0) (KL) 88.00% 56.10%</cell></row><row><cell>LBGAT (? = 0) w/</cell><cell cols="2">88.35% 55.50%</cell></row><row><cell>LBGAT (? = 0) w/o</cell><cell cols="2">88.22% 57.55%</cell></row><row><cell cols="3">Datasets. To evaluate the robustness of our models, we</cell></row><row><cell cols="3">conduct extensive experiments on CIFAR-10, CIFAR-100</cell></row><row><cell cols="3">and Tiny ImageNet datasets. CIFAR-10 dataset consists of</cell></row><row><cell cols="3">60,000 32x32 color images in 10 classes, with 6,000 images</cell></row><row><cell cols="3">per class. There are 50,000 training images and 10,000 test</cell></row><row><cell cols="3">images. CIFAR-100 has 100 classes containing 600 images</cell></row><row><cell cols="3">each. There are 500 training images and 100 testing images</cell></row><row><cell>per class. Tiny Imagenet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for separate batch normalization. Robustness is evaluated under auto-attack. ?denotes models trained with shared convolution and separate batch normalization.</figDesc><table><row><cell>Methods</cell><cell>Acc n</cell><cell>Acc r</cell><cell>Datasets</cell></row><row><cell>LBGAT (? = 0)</cell><cell cols="3">70.03% 27.05% CIFAR-100</cell></row><row><cell>LBGAT (? = 6)</cell><cell cols="3">60.43% 29.34% CIFAR-100</cell></row><row><cell cols="4">LBGAT (? = 0)  ? 64.89% 24.02% CIFAR-100</cell></row><row><cell cols="4">LBGAT (? = 6)  ? 60.62% 27.26% CIFAR-100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with vanilla AT method. For BGAT, we use the ensemble of WideResNet and InceptionRes-NetV2 as M natural . ResNet18 as M natural is for LBGAT on CIFAR-10 and CIFAR-100. Acc n represents accuracy on natural images, while Acc r represents the robustness of models.</figDesc><table><row><cell>Methods</cell><cell>Acc n</cell><cell>Acc r</cell><cell>Datasets</cell></row><row><cell cols="4">vanilla AT 60.90% 27.46% CIFAR-100</cell></row><row><cell>BGAT</cell><cell cols="3">67.72% 30.20% CIFAR-100</cell></row><row><cell>LBGAT</cell><cell cols="3">66.29% 34.30% CIFAR-100</cell></row><row><cell cols="4">vanilla AT 86.82% 52.87% CIFAR-10</cell></row><row><cell>BGAT</cell><cell cols="3">89.00% 55.40% CIFAR-10</cell></row><row><cell>LBGAT</cell><cell cols="3">87.08% 56.60% CIFAR-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Our method is supplementary to ALP and TRADES. For BGAT, we use the ensemble of WideRes-Net and InceptionResNetV2 model as M natural . ResNet18 is adopted as M natural for LBGAT+TRADES and LB-GAT+ALP. Acc n represents accuracy on natural images while Acc r represents robustness of models.</figDesc><table><row><cell>Methods</cell><cell>Acc n</cell><cell>Acc r</cell><cell>Datasets</cell></row><row><cell>ALP</cell><cell cols="3">59.75% 28.94% CIFAR-100</cell></row><row><cell>BGAT+ALP</cell><cell cols="3">63.46% 31.27% CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">62.67% 35.25% CIFAR-100</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="3">62.37% 25.31% CIFAR-100</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="3">56.51% 30.94% CIFAR-100</cell></row><row><cell>BGAT+TRADES (? = 0)</cell><cell cols="3">71.27% 28.70% CIFAR-100</cell></row><row><cell cols="4">LBGAT+TRADES (? = 0) 70.03% 33.01% CIFAR-100</cell></row><row><cell cols="4">LBGAT+TRADES (? = 6) 60.43% 35.50% CIFAR-100</cell></row><row><cell>ALP</cell><cell cols="3">85.55% 54.59% CIFAR-10</cell></row><row><cell>BGAT+ALP</cell><cell cols="3">86.58% 55.74% CIFAR-10</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">85.05% 57.60% CIFAR-10</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="3">88.64% 49.14% CIFAR-10</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="3">84.92% 56.61% CIFAR-10</cell></row><row><cell>BGAT+TRADES (? = 0)</cell><cell cols="3">89.06% 56.75% CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (? = 0) 88.22% 57.55% CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (? = 6) 81.98% 57.78% CIFAR-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of our method with previous defense models under black-box attack on CIFAR-100 and CIFAR-10. To rule out randomness, the numbers are averaged over 2 independently trained models. Acc n represents accuracy on natural images. BAcc r represents robustness under black-box attack. W Acc r represents robustness under white-box attack</figDesc><table><row><cell>Target Models</cell><cell>BAcc r</cell><cell>WAcc r</cell><cell>Acc n</cell><cell>Source Models</cell><cell>Dataset</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="3">61.29% 25.31% 62.37%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="3">55.52% 30.93% 56.51%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">61.38% 35.25% 62.67%</cell><cell>Natural</cell><cell>CIFAR100</cell></row><row><cell>LBGAT+TRADES (?=0)</cell><cell cols="3">68.35% 33.01% 70.03%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="5">42.32% 25.31% 62.37% LBGAT+TRADES (?=0) CIFAR-100</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="5">41.67% 30.93% 56.51% LBGAT+TRADES (?=0) CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">45.68% 35.25% 62.67%</cell><cell>TRADES (? = 6)</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT+TRADES (?=0)</cell><cell cols="3">50.27% 33.01% 70.03%</cell><cell>TRADES (? = 6)</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="3">87.00% 49.14% 88.64%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="3">83.30% 56.61% 84.92%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (? = 0) 87.20% 57.55% 88.22%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (? = 1)</cell><cell cols="4">66.18% 49.14% 88.64% LBGAT+TRADES(?=0)</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (? = 6)</cell><cell cols="5">67.18% 56.61% 84.92% LBGAT+TRADES (?=0) CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (? = 0) 68.45% 57.55% 88.22%</cell><cell>TRADES (?=6)</cell><cell>CIFAR-10</cell></row></table><note>B. Our Method Creates New SOTA Under the Strongest Auto-Attack on CIFAR-100</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Square attack: a query-efficient black-box adversarial attack via random search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Nicolas Flammarion, and Matthias Hein</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jacobian adversarially regularized networks for robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Yew Soon Ong, and Jie Fu</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01278</idno>
		<title level="m">Efficient robust training via backward smoothing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distilling knowledge via knowledge review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="5008" to="5017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial robustness: From self-supervised pre-training to fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimally distorted adversarial examples with a fast adaptive boundary attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and practical neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarially robust distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial logit pairing. CoRR, abs/1803.06373</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harnessing the vulnerability of latent layers in adversarially trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshitha</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data-free knowledge distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1710.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metric learning for adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE SP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial robustness through local linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overfitting in adversarially robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep model compression: Distilling knowledge from noisy teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhusan</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Sau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
		<idno>abs/1610.09650</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial training for free! In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Curls &amp; whey: Boosting black-box adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness through progressive hardening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chawin</forename><surname>Sitawarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supriyo</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09347</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilateral adversarial training: Towards fast training of more robust models against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Enhancing adversarial defense by k-winners-take-all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Intriguing properties of adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03787</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distributionally adversarial attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<idno>WideResNet-34-20 71.00% 27.66%</idno>
		<title level="m">Methods Model Acc n Acc r LBGAT (? = 0) Ours</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">LBGAT (? = 0) Ours WideResNet</title>
		<idno>34-10 70.03% 27.05%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">LBGAT (? = 6) Ours WideResNet</title>
		<idno>34-10 60.43% 29.34%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sitawarin</surname></persName>
		</author>
		<idno>38] ? WideResNet-34-10 62.82% 24.57%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno>6] ? WideResNet-34-10 62.15% 26.94%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
		<idno>18] ? WideResNet-28-10 59.23% 28.42%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C. More Comparisons Under the Strongest Auto-Attack on CIFAR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">We also compare with more previous methods on CIFAR-10 dataset. The experimental results are summarized in Table 9</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Our LBGAT (? = 0) model with WideResNet-34-10 architecture can consistently enjoy higher natural performance while keeping the strongest robustness. We observe that though many fast adversarial training methods</title>
		<imprint/>
	</monogr>
	<note>like [45, 35] are proposed to accelerate the training process, their performance are usually unsatisfied</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">More comparisons under the strongest Auto-Attack on CIFAR-10 dataset</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>denotes numbers are directly copied from [10]. &quot; * &quot; denotes the methods aiming to accelerate adversarial training</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-20 85.34% 53.42%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<idno>34-10 88.64% 48.11%</idno>
		<title level="m">TRADES (? = 1) WideResNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumari</surname></persName>
		</author>
		<idno>22] ? WideResNet-34-10 87.80% 49.12%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<idno>28] ? WideResNet-34-10 86.21% 47.41%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? *</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-10 87.20% 44.83%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shafahi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? *</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-10 86.11% 41.47%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<idno>5] ? WideResNet-34-10 93.79% 0.26%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>45] ? * WideResNet-28-10 92.80% 29.35%</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
