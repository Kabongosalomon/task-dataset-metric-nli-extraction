<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActionFormer: Localizing Moments of Actions with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
							<email>yin.li@wisc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ActionFormer: Localizing Moments of Actions with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>temporal action localization</term>
					<term>action recognition</term>
					<term>egocentric vision</term>
					<term>vision transformers</term>
					<term>video understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer-a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, Ac-tionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at https://github.com/happyharrycn/ actionformer_release.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying action instances in time and recognizing their categories, known as temporal action localization (TAL), remains a challenging problem in video understanding. Significant progress has been made in developing deep models for TAL. Most previous works have considered using action proposals <ref type="bibr" target="#b35">[36]</ref> or anchor windows <ref type="bibr" target="#b49">[50]</ref>, and developed convolutional <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b55">56]</ref>, recurrent <ref type="bibr" target="#b6">[7]</ref>, and graph <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b77">78]</ref> neural networks for TAL. Despite a steady progress on major benchmarks, the accuracy of existing methods usually comes at a price of modeling complexity, with increasingly sophisticated proposal generation, anchor design, loss function, network architecture, and output decoding process. In this paper, we adopt a minimalist design and develop a Transformer based model for TAL, inspired by the recent success of Transformers in NLP <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b18">19]</ref> and vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b10">11]</ref>. Originally developed for sequence data, Transformers use self-attention to model longrange dependencies, and thus are a natural fit for TAL in untrimmed videos. Our method, illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, adapts local self-attention to model temporal context in an input untrimmed videos, classifies every moment, and regresses their corresponding action boundaries. The result is a deep model trained using standard classification and regression loss, and can localize moments of actions in a single shot, without using action proposals or pre-defined anchor windows. Specifically, our model, dubbed ActionFormer, integrates local self-attention to extract a feature pyramid from an input video. Each location in the output pyramid represents a moment in the video, and is treated as an action candidate. A lightweight convolutional decoder is further employed on the feature pyramid to classify these candidates into foreground action categories, and to regress the distance between a foreground candidate and its action onset and offset. The results can be easily decoded into actions with their labels and temporal boundaries. Our method thus provides a single-stage anchor-free model for TAL.</p><p>We show that such a simple model, with proper design, can be surprisingly powerful for TAL. In particular, ActionFormer establishes a new state of the art across several major TAL benchmarks, surpassing previous works by a significant margin. For example, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THU-MOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer reaches an average mAP of 36.6% on ActivityNet 1.3. More importantly, ActionFormer shows impressive results on EPIC-Kitchens 100 for egocentric action localization, with a boost of over 13.5 absolute percentage points in average mAP.</p><p>Our work is based on simple techniques, supported by favourable empirical results, and validated by extensive ablation experiments, at our best. Our main contributions are summarized as follows. First, we are among the first to propose a Transformer based model for single-stage anchor-free TAL. Second, we study key design choices of developing Transformer models for TAL, and demonstrate a simple model that works surprisingly well. Finally, our model achieves stateof-the-art results across major benchmarks and offers a solid baseline for TAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Two-stage TAL. These approaches first generate candidate video segments as action proposals, and further classify the proposals into actions and refine their temporal boundaries. Previous works focused on action proposal generation, by either classifying anchor windows <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref> or detecting action boundaries <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b83">84]</ref>, and more recently using a graph representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b74">75]</ref> or Transformers <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b65">66]</ref>. Others have integrated proposal generation and classification into a single model <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b13">14]</ref>. More recent effort investigates the modeling of temporal context among proposals using graph neural networks <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83]</ref> or attention and self-attention mechanisms <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58]</ref>. Similar to previous approaches, our method considers the modeling of long-term temporal context, yet uses a self-attention within a Transformer model. Different from previous approaches, our model detects actions without using proposals.</p><p>Single-stage TAL. Several recent works focused on single-stage TAL, seeking to localize actions in a single shot without using action proposals. Many of them are anchor-based (e.g., using anchor windows sampled from sliding windows). Lin et al. <ref type="bibr" target="#b36">[37]</ref> presented the first single-stage TAL using convolutional networks, borrowing ideas from a single-stage object detector <ref type="bibr" target="#b43">[44]</ref>. Buch et al. <ref type="bibr" target="#b6">[7]</ref> presented a recurrent memory module for single-stage TAL. Long et al. <ref type="bibr" target="#b49">[50]</ref> proposed to use Gaussian kernels to dynamically optimize the scale of each anchor, based on a 1D convolutional network. Yang et al. <ref type="bibr" target="#b76">[77]</ref> explored the combination of anchor-based and anchor-free models for single-stage TAL, again using convolutional networks. More recently, Lin et al. <ref type="bibr" target="#b34">[35]</ref> proposed an anchor-free singlestage model by designing a saliency-based refinement module incorporated in convolutional network. Similar ideas were also explored in video grounding <ref type="bibr" target="#b80">[81]</ref>.</p><p>Our model falls into the category of single-stage TAL. Indeed, our formulation follows a minimalist design of sequence labeling by classifying every moment and regressing their action boundaries, previously discussed in <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b34">35]</ref>. The key difference is that we design a Transformer network for action localization. The result is a single stage anchor-free model that outperforms all previous methods. A concurrent work from Liu et al. <ref type="bibr" target="#b45">[46]</ref> also used Transformer for TAL, yet considered a set prediction problem similar to DETR <ref type="bibr" target="#b10">[11]</ref>.</p><p>Spatial-temporal Action Localization. A related yet different task, known as spatial-temporal action localization, is to detect the actions both temporally and spatially, in the form of moving bounding boxes of an actor. It is possible that TAL might be used as a first step for spatial-temporal localization. Girdhar et al. <ref type="bibr" target="#b24">[25]</ref> proposed to use Transformer for spatial-temporal action localization. While both our work and <ref type="bibr" target="#b24">[25]</ref> use Transformer, the two models differ significantly. We consider a sequence of video frames as the inputs, while <ref type="bibr" target="#b24">[25]</ref> used a set of 2D object proposals. Moreover, our work addresses a different task of TAL.</p><p>Object Detection. TAL models have been heavily influenced by the developments of object detection models. Some of our model design, including the multiscale feature representation and convolutional decoder, is inspired by feature pyramid network <ref type="bibr" target="#b38">[39]</ref> and RetinaNet <ref type="bibr" target="#b39">[40]</ref>. Our training using center sampling also stems from recent single-stage object detectors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b81">82]</ref>.</p><p>Vision Transformer. Transformer models were originally developed for NLP tasks <ref type="bibr" target="#b63">[64]</ref>, and has demonstrated recent success for many vision tasks. ViT <ref type="bibr" target="#b19">[20]</ref> presented the first pure Transformer-based model that can achieve state-of-theart performances on image classification. Subsequent works, including DeiT <ref type="bibr" target="#b60">[61]</ref>, T2T-ViT <ref type="bibr" target="#b78">[79]</ref>, Swin Transformer <ref type="bibr" target="#b47">[48]</ref>, Focal Transformer <ref type="bibr" target="#b75">[76]</ref> and PVT <ref type="bibr" target="#b68">[69]</ref>, have further pushed the envelope, resulting in vision Transformer backbones with impressive results on classification, segmentation, and detection tasks. Transformer have also been explored in object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b67">68]</ref>, semantic segmentation <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b14">15]</ref>, and video representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b22">23]</ref>. Our model builds on these developments and presents one of the first Transformer models for TAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ActionFormer: A Simple Transformer Model for Temporal Action Localization</head><p>Given an input video X, we assume that X can be represented using a set of feature vectors X = {x 1 , x 2 , . . . , x T } defined on discretized time steps t = {1, 2, . . . , T }, where the total duration T varies across videos. For example, x t can be the feature vector of a video clip at moment t extracted from a 3D convolutional network. The goal of temporal action localization is to predict the action label Y = {y 1 , y 2 , . . . , y N } based on the input video sequence X. Y consists of N action instances y i , where N also varies across videos. Each instance y i = (s i , e i , a i ) is defined by its starting time s i (onset), ending time e i (offset) and its action label a i , where s i ? [1, T ], e i ? [1, T ], a i ? {1, .., C} (C pre-defined categories) and s i &lt; e i . The task of TAL is thus a challenging problem of structured output prediction.</p><p>A Simple Representation for Action Localization. Our method builds on an anchor-free representation for action localization, inspired by <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b34">35]</ref>. The key idea is to classify each moment as either one of the action categories or the background, and further regress the distance between this time step and the action's onset and offset. In doing so, we convert the structured output prediction problem (X = {x 1 , x 2 , ..., x T } ? Y = {y 1 , y 2 , . . . , y N }) into a more approachable sequence labeling problem</p><formula xml:id="formula_0">X = {x 1 , x 2 , ..., x T } ?? = {? 1 ,? 2 , ...,? T }.<label>(1)</label></formula><p>The output? t = (p(a t ), d s t , d e t ) at time t is defined as ? p(a t ) consists of C values, with each representing a binomial variable indicating the probability of action category a t (? {1, 2, . . . , C}) at time t. This can be considered as the outputs of C binary classification.  Intuitively, this formulation considers every moment t in the video X as an action candidate, recognizes the action's category a t , and estimates the distances between current step and the action boundaries (d s t and d e t ) if an action presents. Action localization results can be readily decoded from? t = (p(a t ), d s t , d e t ) by a t = arg max p(a t ), s t = t ? d s t , e t = t + d e t .</p><p>Method Overview. Our model -ActionFormer learns to label an input video sequence f (X) ??. Specifically, f is realized using a deep model. ActionFormer follows an encoder-decoder architecture proven successful in many vision tasks, and decomposes f as h ? g. Here g : X ? Z encodes the input into a latent vector Z, and h : Z ?? subsequently decodes Z into the sequence label?. <ref type="figure" target="#fig_2">Fig. 2</ref> presents an overview of our model. Importantly, our encoder g is parameterized by a Transformer network <ref type="bibr" target="#b63">[64]</ref>. Our decoder h adopts a lightweight convolutional network. To capture actions at various temporal scales, we design a multi-scale feature representation Z = {Z 1 , Z 2 , . . . , Z L } forming a feature pyramid with varying resolutions. Note that our model operates on a temporal axis defined by feature grids rather than the absolute time, allowing it to adapt to videos with different frame rates. We now describe the details of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encode Videos with Transformer</head><p>Our model first encodes an input video X = {x 1 , x 2 , . . . , x T } into a multiscale feature representation Z = {Z 1 , Z 2 , . . . , Z L } using an encoder g. The encoder g consists of (1) a projection function using a convolutional network that embeds each feature (x t ) into a D-dimensional space; and (2) a Transformer network that maps the embedded features to the output feature pyramid Z.</p><p>Projection. Our projection E is a shallow convolutional network with ReLU as the activation function, defined as</p><formula xml:id="formula_2">Z 0 = [E(x 1 ), E(x 2 ), . . . , E(x T )] T ,<label>(3)</label></formula><p>where E(x i ) ? R D is the embedded feature of x i . Adding convolutions before a Transformer network was recently found helpful to better incorporate local context for time series data <ref type="bibr" target="#b31">[32]</ref> and to stabilize the training of vision Transformers <ref type="bibr" target="#b70">[71]</ref>. An position embedding <ref type="bibr" target="#b63">[64]</ref> E pos ? R T ?D can be optionally added. However, we find that doing so will decrease the performance of the model, and have thus removed position embeddings in our model by default.</p><p>Local Self-Attention. The Transformer network further takes Z 0 as input. The core of a Transformer is self-attention <ref type="bibr" target="#b63">[64]</ref>. We briefly introduce the key idea to make the paper self-contained. Concretely, self-attention computes a weighted average of features with the weight proportional to a similarity score between pairs of input features.</p><formula xml:id="formula_3">Given Z 0 ? R T ?D with T time steps of D dimensional features, Z 0 is projected using W Q ? R D?Dq , W K ? R D?D k , and W V ? R D?Dv</formula><p>to extract feature representations Q, K, and V, referred to as query, key and value respectively with D k = D q . The outputs Q, K, V are computed as</p><formula xml:id="formula_4">Q = Z 0 W Q , K = Z 0 W K , V = Z 0 W V .<label>(4)</label></formula><p>The output of self-attention is given by</p><formula xml:id="formula_5">S = softmax QK T / D q V,<label>(5)</label></formula><p>where S ? R T ?D and softmax is performed row-wise. A multiheaded selfattention (MSA) further adds several self-attention operations in parallel.</p><p>A main advantage of MSA is the ability to integrate temporal context across the full sequence, yet such a benefit comes at the cost of computation. A vanilla MSA has a complexity of O(T 2 D + D 2 T ) in both memory and time, and is thus highly inefficient for long videos. There has been several recent work on efficient self-attention <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b15">16]</ref>. Here we adapt the local self-attention from <ref type="bibr" target="#b15">[16]</ref> by limiting the attention within a local window. Our intuition is the temporal context beyond a certain range is less helpful for action localization. Such a local self-attention significantly reduces the complexity to O(W 2 T D + D 2 T ) with W the local window size (? T ). Importantly, local self-attention is used in tandem with the multiscale feature representation Z = {Z 1 , Z 2 , . . . , Z L }, using the same window size on each pyramid level. With this design, a small window size (19) on a downsampled feature map (16x) will cover a large temporal range (304).</p><p>Multiscale Transformer. We now present the design of our Transformer encoder. Our Transformer has L Transformer layers with each layer consisting of alternating layers of local multiheaded self-attention (MSA) and MLP blocks.</p><p>Moreover, LayerNorm (LN) is applied before every MSA or MLP block, and residual connection is added after every block. GELU is used for the MLP. To capture actions at different temporal scales, a downsampling operator ?(?) is optionally attached. This is given b?</p><formula xml:id="formula_6">Z ? = ? ? MSA(LN(Z ??1 )) + Z ??1 ,? ? =? ? MLP(LN(Z ? )) +Z ? , Z ? = ?(? ? ), ? = 1 . . . L,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">Z ??1 ,Z ? ,? ? ? R T ??1 ?D and Z ? ? R T ? ?D . T ??1 /T ? is the downsampling ratio.</formula><p>? ? and? ? are learnable per-channel scaling factors as in <ref type="bibr" target="#b61">[62]</ref>. The downsampling operator ? is implemented using a strided depthwise 1D convolution due to its efficiency. We use 2x downsampling for our model. Our Transformer block is shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (right). Our model further combines several Transformer blocks with downsampling in between, resulting in a feature pyramid Z = {Z 1 , Z 2 , . . . , Z L }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding Actions in Time</head><p>Next, our model decodes the feature pyramid Z from the encoder g into the sequence label? = {? 1 ,? 2 , . . . ,? T } using the decoder h. Our decoder is a lightweight convolutional network with a classification and a regression head.</p><p>Classification Head. Given the feature pyramid Z, our classification head examines each moment t across all L levels on the pyramid, and predicts the probability of action p(a t ) at every moment t. <ref type="bibr" target="#b3">4</ref> This is realized using a lightweight 1D convolutional network attached to each pyramid level with its parameters shared across all levels. Our classification network is implemented using 3 layers of 1D convolutions with kernel size=3, layer normalization (for the first 2 layers), and ReLU activation. A sigmoid function is attached to each output dimension to predict the probability of C action categories. Adding layer normalization slightly boosts the performance as we will demonstrate in our ablation.</p><p>Regression Head. Similar to our classification head, our regression head examines every moment t across all L levels on the pyramid. The difference is that the regression head predicts the distances to the onset and offset of an action (d s t , d e t ), only if the current time step t lies in an action. An output regression range is pre-specified for each pyramid level. The regression head, again, is implemented using a 1D convolutional network following the same design of the classification network, except that a ReLU is attached at the end for distance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ActionFormer: Model Design</head><p>Putting things together, ActionFormer is conceptually simple: each feature on the feature pyramid Z outputs an action score p(a) and the corresponding temporal boundaries (s, e), which are then used to decode an action candidate.</p><p>Notwithstanding the simplicity, we find that several key architecture designs are important to ensure a strong performance. We discuss these design choices here.</p><p>Design of the Feature Pyramid. A critical component of our model is the design of the temporal feature pyramid Z = {Z 1 , Z 2 , . . . , Z L }. The design choices include (1) the number of levels within the pyramid; (2) the downsampling ratio between successive feature maps; and (3) the output regression range of each pyramid level. Inspired by the design of feature pyramid in modern object detectors (FPN <ref type="bibr" target="#b38">[39]</ref> and FCOS <ref type="bibr" target="#b59">[60]</ref>), we simplify our design choices by using a 2x downsampling of the feature maps, and roughly enlarging the output regression range by 2 accordingly. We explore different design choices in our ablation.</p><p>Loss Function. Our model outputs (p(a t ), d s t , d e t ) for every moment t, including the probability of action categories p(a t ) and the distances to action boundaries (d s t , d e t ). Our loss function, again following minimalist design, only has two terms: (1) L cls a focal loss <ref type="bibr" target="#b39">[40]</ref> for C way binary classification; and (2) L reg a DIoU loss <ref type="bibr" target="#b86">[87]</ref> for distance regression. The loss is defined for each video X as</p><formula xml:id="formula_8">L = t (L cls + ? reg 1 ct L reg ) /T + ,<label>(7)</label></formula><p>where T + is the total number of positive samples. 1 ct is an indicator function that denotes if a time step t is within an action, i.e., a positive sample. L is applied to all levels on the output pyramid, and averaged across all video samples during training. ? reg is a coefficient balancing the classification and regression loss. We set ? reg =1 by default and study the choice of ? reg in our ablation.</p><p>Importantly, L cls uses Focal loss <ref type="bibr" target="#b59">[60]</ref> to recognize C action categories. Focal loss naturally handles imbalanced samples -there are much more negative samples than positive ones. Moreover, L reg adopts a differentiable IoU loss <ref type="bibr" target="#b54">[55]</ref>. L reg is only enabled when the current time step contains a positive sample. Center Sampling. During training, we find it helpful to adapt a center sampling strategy similar to <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b81">82]</ref>, as we will show in our ablation study. Specifically, when determining the positive samples, only time steps within an interval around the center of an action are considered positive, where the duration of interval is proportional to the feature stride of the current pyramid level ?. More precisely, given an action centered at c, any time step t ? [c ? ?T /T ? , c + ?T /T ? ] at the pyramid level ? is considered as positive, where ? = 1.5. Center sampling does not impact model inference, yet encourages higher scores around action centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Training. Following <ref type="bibr" target="#b24">[25]</ref>, we use Adam <ref type="bibr" target="#b30">[31]</ref> with warm-up for training. The warm-up stage is critical for model convergence and good performance, as also pointed out by <ref type="bibr" target="#b41">[42]</ref>. When training with variable length input, we fix the maximum input sequence length, pad or cropped the input sequences accordingly, and add proper masking for operations in the model. This is equal to training with sliding windows as in <ref type="bibr" target="#b76">[77]</ref>. Varying the maximum input sequence length during training has little impact to the performance, as shown in our ablation.</p><p>Inference. At inference time, we feed the full sequences into the model, as no position embeddings are used in the model. Our model takes the input video X, and outputs {(p(a t ), d s t , d e t ))} for every time step t across all pyramid levels. Each time step t further decodes an action instance (e t = t ? d s t , s t = t + d e t , p(a t )). e t and s t are the onset and offset of the action, and p(a t ) is an action confidence score. The result action candidates are further processed using Soft-NMS <ref type="bibr" target="#b5">[6]</ref> to remove highly overlapping instances, leading to the final outputs of actions.</p><p>Network Architecture. We used 2 convolutions for projection, 7 Transformer blocks for the encoder (all using local attention and with 2x downsampling for the last 5), and separate classification and regression heads as the decoder. The regression range on each pyramid level was normalized by the stride of the features. More details are presented in the appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We now present our experiments and results. Our main results include benchmarks on THUMOS14 <ref type="bibr" target="#b28">[29]</ref>, ActivityNet-1.3 <ref type="bibr" target="#b9">[10]</ref> and EPIC-Kitchens 100 <ref type="bibr" target="#b17">[18]</ref>. Moreover, we provide extensive ablation studies of our model.</p><p>Evaluation Metric. For all datasets, we report the standard mean average precision (mAP) at different temporal intersection over union (tIoU) thresholds, widely used to evaluate TAL methods. tIoU is defined as the intersection over union between two temporal windows, i.e., the 1D Jaccard index. Given a tIoU threshold, mAP computes the mean of average prevision across all action categories. An average mAP is also reported by averaging across several tIoUs.</p><p>Baseline and Comparison. For our main results on THUMOS14 <ref type="bibr" target="#b28">[29]</ref> and ActivityNet-1.3 <ref type="bibr" target="#b9">[10]</ref>. We compare to a strong set of baselines, including both two-stage (e.g., G-TAD <ref type="bibr" target="#b74">[75]</ref>, BC-GNN <ref type="bibr" target="#b3">[4]</ref>, TAL-MR <ref type="bibr" target="#b83">[84]</ref>) and single-stage (e.g., A2Net <ref type="bibr" target="#b76">[77]</ref>, GTAN <ref type="bibr" target="#b49">[50]</ref>, AFSD <ref type="bibr" target="#b34">[35]</ref>, TadTR <ref type="bibr" target="#b45">[46]</ref>) methods for TAL. Our close competitors are those single-stage methods. Despite our best attempt for a fair comparison, we recognize some of our baselines used different setups (e.g., video features). Our experiment setup follows previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b83">84]</ref>. And our intention here is to compare our results to the best results previously reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on THUMOS14</head><p>Dataset. THUMOS14 <ref type="bibr" target="#b28">[29]</ref> dataset contains 413 untrimmed videos with 20 categories of actions. The dataset is divided into two subsets: validation set and test set. The validation set contains 200 videos and the test set contains 213 videos. Following the common practice <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>, we use the validation set for training and report results on the test set.</p><p>Experiment Setup. We used two-stream I3D <ref type="bibr" target="#b11">[12]</ref> pretrained on Kinetics to extract the video features on THUMOS14, following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b83">84]</ref>. mAP@[0.3:0.1:0.7] was used to evaluate our model. A window size of 19 was used for local selfattention based on our ablation. Further details are described in the appendix C. To show that our method can adapt to different video features, we also consider the pre-training method from <ref type="bibr" target="#b1">[2]</ref> using an R(2+1)D network <ref type="bibr" target="#b62">[63]</ref>.</p><p>Results. , with an mAP of 71.0% at tIoU=0.5 and an mAP of 43.9% at tIoU=0.7, outperforming all previous methods by a large margin (+14.1% mAP at tIoU=0.5 and +12.8% mAP at tIoU=0.7). Our results stay on top of all single-stage methods, and also beats all previous two-stage methods, including the latest ones from <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b57">58]</ref>. Note that our method significantly outperforms the concurrent work of TadTR <ref type="bibr" target="#b45">[46]</ref>, which also designed a Transformer model for TAL. With the combination of a simple design and a strong Transformer model, our method establishes new state of the art on THUMOS14, crossing the 65% average mAP for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on ActivityNet-1.3</head><p>Dataset. ActivityNet-1.3 <ref type="bibr" target="#b9">[10]</ref> is a large-scale action dataset which contains 200 activity classes and around 20,000 videos with more than 600 hours. The dataset is divided into three subsets: 10,024 videos for training, 4,926 for validation, and 5,044 for testing. Following the common practice in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b74">75]</ref>, we train our model on the training set and report the performance on the validation set. Experiment Setup. We used two-stream I3D <ref type="bibr" target="#b11">[12]</ref> for feature extraction.Following <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b74">75]</ref>, the extracted features were downsampled into a fixed length of 160 using linear interpolation. For evaluation, we used mAP@[0.5:0.05:0.95] and also reported the average mAP. A window size of 11 was used for local self-attention. Further implementation details can be found in the appendix C. Moreover, we combined external classification results from <ref type="bibr" target="#b85">[86]</ref> following <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b79">80]</ref>. Similarly, we consider the pre-training method from <ref type="bibr" target="#b1">[2]</ref>.</p><p>Results.  <ref type="bibr" target="#b45">[46]</ref>. Our results are worse than TCANet [52]-a latest two-stage method using stronger SlowFast features <ref type="bibr" target="#b23">[24]</ref> that are not publicly available. We conjecture our method will also benefit from better features. Nonetheless, our model clearly demonstrates state-of-the-art results on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on EPIC-Kitchens 100</head><p>Dataset. EPIC-Kitchens 100 is the largest egocentric action dataset. The dataset contains 100 hours of videos from 700 sessions capturing cooking activities in different kitchens. In comparison to ActivityNet-1.3, EPIC-Kitchens 100 has less number of videos, yet many more instances per video (average 128 vs. 1.5 on ActivityNet-1.3). In comparison to THUMOS14, EPIC-Kitchens is 3 times larger in terms of video hours and more than 10 times larger in terms of action instances. These egocentric videos also include significant camera motion. This dataset thus poses new challenges for TAL.</p><p>Experiment Setup. We used a SlowFast network <ref type="bibr" target="#b23">[24]</ref> pre-trained on EPIC-Kitchens for feature extraction. This model is provided by <ref type="bibr" target="#b17">[18]</ref>. Our model was trained on the training set and evaluated on the validation set. A window size of 9 was used for local self-attention. For evaluation, we used mAP@[0.1:0.1:0.5] and report the average mAP following <ref type="bibr" target="#b17">[18]</ref>. In this dataset, an action is defined as a combination of a verb (action) and a noun (object). As this dataset was recently released, we are only able to compare our methods to BMN <ref type="bibr" target="#b35">[36]</ref> and G-TAD <ref type="bibr" target="#b74">[75]</ref>, both using the same SlowFast features provided by <ref type="bibr" target="#b17">[18]</ref>. Again, implementation details are described in the appendix C.</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> presents the results. Our method achieves an average mAP ([0.1:0.1:0.5]) of 23.5% and 21.9% for verb and noun, respectively. Our results again largely outperform the strong baselines of BMN <ref type="bibr" target="#b35">[36]</ref> and G-TAD <ref type="bibr" target="#b74">[75]</ref> by over 13.5% in absolute percentage points. An interesting observation is that the gaps between our results and BMN / G-TAD are much larger on EPIC-Kitchens 100. A possible reason is that ActivityNet has a small number of actions per video (1.5), leading to imbalanced classification for our model; only a few moments (around the action center) are labeled positive while all rest are negative. We adapt ActionFormer for EPIC-Kitchens 100 2022 Action Detection challenge. By combining features from SlowFast <ref type="bibr" target="#b23">[24]</ref> and ViViT <ref type="bibr" target="#b2">[3]</ref>, ActionFormer achieves 21.36% / 20.95% average mAP for actions on the validation / test set. Our results ranked 2nd with a gap of 0.32 average mAP to the top solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Experiments</head><p>We conduct extensive ablations on THUMOS14 to understand our model design. Results are reported using I3D features with a fixed random seed for training. Further ablations on loss weight, maximum input length during training, input temporal feature resolution and error analysis can be found in the appendix A.</p><p>Baseline: A Convolutional Network. Our ablation starts by re-implementing a baseline anchor-free method (AF Base) as described in <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b34">35]</ref>  <ref type="table" target="#tab_5">(Table 3a</ref> row 1-2). This baseline shares the same action representation as our model, yet uses a 1D convolutional network as the encoder. We roughly match the number of layers and parameters of this baseline to our model. See the appendix C for more details. This baseline achieves an average mAP of 46.6% on THUMOS14 <ref type="table" target="#tab_5">(Table 3a row</ref> 3), outperforms the numbers reported in [35] by 6.2%. We attribute the difference to variations in architectures and training schemes. This baseline, when using score fusion, reaches 52.9% average mAP <ref type="table" target="#tab_5">(Table 3a row</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4).</head><p>Transformer Network. Our next step is to simply replace the 1D convolutional network with our Transformer model using vanilla self-attention. This model achieves an average mAP of 62.7% <ref type="table" target="#tab_5">(Table 3a</ref> row 5) -a major boost of 16.1%. We note that this model already outperforms the best reported results (56.9% mAP at tIoU=0.5 from <ref type="bibr" target="#b44">[45]</ref>). This result shows that our Transformer model is very powerful for TAL, and serves as the main course of performance gain.</p><p>Layer Norm, Center Sampling, Position Encoding, &amp; Score Fusion. We further add layer norm in the classification and regression heads, apply center sampling during training, and explore position encoding as well as score fusion <ref type="table" target="#tab_5">(Table 3a row</ref>  <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Adding layer norm boosts the average mAP by 2.7%, and using center sampling further improves the performance by 1.4%. The commonly used position encoding, however, does not bring performance gain. We postulate that our projection using convolutions as well as the depthwise convolutions in our Transformer blocks already leak the location information, as also pointed <ref type="table" target="#tab_5">Table 3</ref>. Ablation studies on THUMOS14. We report mAP at tIoU=0.5 and 0.7, and the average mAP in [0.3 : 0.1 : 0.7]. Results are without score fusion unless specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone LN CTR PE Fusion 0.5 0.7 Avg AF Base <ref type="bibr">[</ref> out in <ref type="bibr" target="#b71">[72]</ref>. Further fusing the classification scores will decrease the largely performance. As a reference, when replacing the vanilla self-attention with a local version (window size=19), the average mAP remains the same.</p><p>Window Size for Local Self-Attention. Next, we study the effects of window size for local self-attention in our model. We vary the window size, re-train the model, and present both model accuracy, complexity (in GMACs), and run time in <ref type="table" target="#tab_5">Table 3b</ref>. All results are reported without score fusion. Due to our design of a multiscale feature pyramid, even using the global self-attention only leads to 26% increase in MACs when compared to the baseline convolutional network. Reducing the window size cuts down the MACs yet maintains a similar accuracy. In addition to MACs, we also evaluate the normalized run time of these models on GPUs, where the base convolutional model is set to 1.0x. In spite of similar MACs, Transformer-based models are roughly 2x slower in run time compared to a convolution-based model (AF Base). It is known that self-attention is not easily parallelizable on GPUs. Also, our current implementation of Transformer used PyTorch primitives <ref type="bibr" target="#b50">[51]</ref> without leveraging customized CUDA kernels.</p><p>Feature Pyramid. Further, we study the design of the feature pyramid. As discussed in Sec. 3.3, our design space is specified by (1) the number of pyramid  <ref type="formula" target="#formula_1">(2)</ref> an initial regression range for the first pyramid level. We vary these parameters and report results in <ref type="table" target="#tab_5">Table 3c</ref>. We follow our best design with a local window size=19 and layer norm, and center sampling. First, we disable the feature pyramid and attach the heads to the feature map with the highest resolution. This is done by setting the number of pyramid to 1 with an initial regression range of [0, +?), Removing the feature pyramid results in a major performance drop <ref type="bibr">(-19</ref>.3% in average mAP), suggesting that using feature pyramid is critical for our model. Next, we set the number of pyramid levels to 3 and experiment with different initial regression ranges. The best results are achieved with the range of [0, 4). Further increase of the range decreases the mAP scores. Finally, we fix the initial regression range to [0, 4) and increase the number of pyramid levels. The performance of our method generally increases with more pyramid levels, yet is saturated when using 6 levels.</p><p>Result Visualization. Finally, we visualize the outputs of our model (before Soft-NMS) in <ref type="figure" target="#fig_3">Fig. 3</ref>, including the action scores, and the regression outputs weighted by the action scores (as a weighted histogram). Our model outputs a strong peak near the center of an action, potentially due to the employment of center sampling during training. The regression of action boundaries seems less accurate. We conjecture that our regression heads can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this paper, we presented ActionFormer-a Transformer-based method for temporal action localization. ActionFormer has a simple design, falls into the category of single-stage anchor-free method, yet achieves impressive results across several major TAL benchmarks including THUMOS14, ActivityNet-1.3, and the more recent EPIC-Kitchens 100 (egocentric videos). Through our experiments, we showed that the power of ActionFormer lies in our orchestrated design, in particular the combination of local self-attention and a multiscale feature representation to model longer range temporal context in videos. We hope that our model, notwithstanding its simplicity, can shed light on the task of temporal action localization, as well as the broader field of video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>In the appendix, we describe (1) additional ablation experiments (Sec. A); (2) further error analysis of our results (Sec. B); (3) implementation details and how to reproduce our results (Sec. C); (4) additional visualizations of our results (Sec. D); and (5) limitation of our approach and furture directions (Sec. E). For sections, figures, tables, and equations, we use numbers (e.g.,, Sec. 1) to refer to the main paper and capital letters (e.g.,, Sec. A) to refer to this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Ablation Experiments</head><p>Here we present additional ablation experiments, as mentioned in Sec. 4.4 of the main paper. These are omitted from the main paper due to lack of space. All experiments are reported on THUMOS14, consistent with our ablation experiments in the main paper. We follow our best design and use a local window size=19 with layer norm, center sampling, and score fusion enabled.</p><p>Loss Weight. We provide additional ablation on the loss weight ? reg in Eq. 7. Specifically, we varied the loss weight ? reg ? [0.2, 0.5, 1, 2, 5], retrained the model, and reported the mAP scores. The results are presented in <ref type="table" target="#tab_5">Table A</ref>. For a large range of ? reg , our model has quite stable results with a maximum gap of 1.4% in average mAP. ? reg = 1 yields the best results, as we used in all our experiments. Maximum Input Sequence Length during Training. A possible explanation of our superior results is that our model might benefit from training using a long sequence (2304 time steps as in our previous experiments). Here we examine the effects of maximum input sequence length during training. <ref type="table" target="#tab_5">Table B</ref> reports mAP scores for different training sequence lengths. The results of our model remain fairly consistent even with much shorter input sequence length. Note that when truncating an input sequence, our training scheme is equal to training with sliding windows as in <ref type="bibr" target="#b76">[77]</ref>. The differences are (1) the windows are dynamically sampled rather than pre-generated; (2) windows without foreground actions are removed. When using a input sequence length of 512, similar to what was considered in <ref type="bibr" target="#b76">[77]</ref> (512), our method only has a minor drop in average mAP (-1.1%) and significantly outperforms <ref type="bibr" target="#b76">[77]</ref>. Temporal Feature Resolution. Some of the previous works considered video features with lower temporal resolution. For example, a feature stride of 8 was used by PGCN <ref type="bibr" target="#b79">[80]</ref> and ContextLoc <ref type="bibr" target="#b88">[89]</ref>. To understand the effects of temporal feature resolution, we downsample our input I3D features and study the performance variation when using different feature strides. <ref type="table" target="#tab_5">Table C</ref> report the results. When using a lower resolution (stride=8), the results of our model only drop slightly (-0.5% in average mAP). Further reducing the resolution (e.g.,, stride=16) leads to larger performance degradation, yet our results remains favourable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Error Analyses</head><p>We present further analyses of our results on THUMOS14 using the tool provided by <ref type="bibr" target="#b0">[1]</ref>. We refer the readers to <ref type="bibr" target="#b0">[1]</ref> for more details.</p><p>Metrics. In <ref type="bibr" target="#b0">[1]</ref>, several characteristic metrics were defined given a dataset (e.g.,  into five length groups: Extra Small (XS: (0, 3]), Small (S: <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6]</ref>), Medium (M: <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12]</ref>), Long (L: <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b17">18]</ref>), and Extra Long (XL: &gt; 18). Moreover, number of instances refers to the total count of instances (from the same class) in a video. This number is further divided into four parts, including Extra Small (XS: 1); Small (S: <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref>); Medium (M: <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b79">80]</ref>); Large (L: &gt; 80).</p><p>Results and Analyses. <ref type="figure">Fig. A</ref> presents the false negative profiling. In <ref type="figure">Fig.  A</ref>, we breakdown the false negative rates under the different coverage, length, and the number of instances. Our results have similar false negative rates across different coverage categories, yet have much higher false negative rates on action instances that are either very shot or very long (length), and on videos that contains many action instances (#instances). These action instances and videos are naturally more challenging. <ref type="figure" target="#fig_5">Fig. B</ref> presents the sensitivity analysis of our results, i.e.,, normalized mAP at tIoU=0.5 under different characteristic metrics (left) and the variance of mAP across categories (right). Our model performs better on simple context scenarios, including XS/S/M/L coverage, S/M length and XS #instances, and worse on more complicated scenarios. The trend is similar to the false negative profiling in <ref type="figure">Fig. A</ref>. Moreover, our model is robust across different categories in coverage, length and #instances with small variances.</p><formula xml:id="formula_9">1G 2G 3G 4G 5G 6G 7G 8G 9G 10G</formula><p>Top Predictions Removing Error Impact </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>We now present implementation details including the network architecture, training and inference. Further details can be found in our code.</p><p>Network Architecture. We present our network architecture in <ref type="table" target="#tab_5">Table D</ref>, as described in Sec. 3. In the ablation study (Sec. 4), we also considered a baseline that replaces the Transformer Units in <ref type="table" target="#tab_5">Table D</ref> with convolution blocks, following the design of a bottleneck block in ResNet <ref type="bibr" target="#b26">[27]</ref>. Specifically, a stack of three 1D convolutional layers were used. The kernel size of three convolutional layers were 1, 3 and 1, respectively. The expansion factor of the bottleneck block was 2. We added an extra strided convolutional layer with kernel size=1 and stride=2 to perform downsampling when necessary.</p><p>Training Details. For training, we considered both fixed length inputs (Ac-tivityNet) and variable length inputs (THUMOS14, ActivityNet, and EPIC-Kitchens 100). For variable length inputs, we capped the input length to 2304 (around 5 minutes on THUMOS14 and around 20 minutes on EPIC-Kitchens 100), and randomly selected a subset of consecutive clips from an input video. Position embedding was disabled by default except for ActivityNet. Model EMA <ref type="bibr" target="#b27">[28]</ref> and gradient clipping were also implemented to further stabilize the training. Hyperparameters were slightly different across datasets and discussed later in our experiment details.</p><p>Inference Details. For fixed length inputs (ActivityNet-1.3), we fed the full sequence into our model. For variable length inputs (THUMOS14 and EPIC- <ref type="table" target="#tab_5">Table D</ref>. The architecture of our model. Our network consists of (1) a Transformer encoder (first row block) and (2) a lightweight convolutional decoder with the classification / regression heads (last row block). For each layer, we list the layer name, layer parameters, the input to the layer, and the output feature size. We also include its regression range (in seconds for THUMOS14 and EPIC-Kitchens 100 and in number of time steps for ActivityNet-1.3). For convolutional layers, k is the kernel size of 1D convolutions and s is the stride, and ci, co is the input and output feature channel, respectively. For Transformer Unit, ds is the downsampling ratio. T is the temporal length of input sequence and D is the input feature dimension. For classification head, the output dimension is the number of action categories. For regression head, the output dimension is 2, i.e.,, distances to action onset and offset. Kitchens 100), we sent the full sequence into the model. When using position embeddings in our ablation study, we adopted the technique from <ref type="bibr" target="#b19">[20]</ref>. Specifically, for input sequences shorter than the training sequence length (2304), we fed the full sequence into our model and clipped the position embedding using the actual length of the video. For input sequences longer than the training sequence length, we again fed the full sequence into our model, yet used linear interpolation to upsample the position embeddings.</p><p>Score Fusion. For our experiments on THUMOS14 and ActivityNet-1.3, we sometimes consider score fusion using external classification scores. Specifically, given an input video, the top-2 video-level classes given by external classification scores were assigned to all detected action instances in this video, where the action scores from our model were multiplied with the external classification scores. Each detected action instance from our model thus creates two action instances. We refer the readers to <ref type="bibr" target="#b79">[80]</ref> (Appendix E) for a more detailed description of the score fusion strategy.</p><p>Experiment Details. Our experiment details vary across datasets, as each dataset includes videos of different resolution and frame rate, and considers different types of features. We now describe our experiment details for THUMOS14, ActivityNet-1.3, and EPIC-Kitchens 100.</p><p>? THUMOS14: We used two-stream I3D <ref type="bibr" target="#b11">[12]</ref> pretrained on Kinetics to extract the video features on THUMOS14, following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b83">84]</ref>. We fed 16 consecutive frames as the input to I3D, used a sliding window with stride 4 and extracted 1024-D features before the last fully connected layer. The two-stream features were further concatenated (2048-D) as the input to our model. mAP@[0.3:0.1:0.7] was used to evaluate our model. Our model was trained for 50 epochs with a linear warmup of 5 epochs. The initial learning rate was 1e-4 and a cosine learning rate decay is used. The mini-batch size was 2, and a weight decay of 1e-4 was used. ? ActivityNet-1.3: We used two-stream I3D <ref type="bibr" target="#b11">[12]</ref> and TSP <ref type="bibr" target="#b1">[2]</ref> for feature extraction, and increased the stride of the sliding window to 16. Following <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b74">75]</ref>, the extracted features were downsampled into a fixed length of 160 and 192 using linear interpolation for I3D and TSP features, respectively. For evaluation, we used mAP@[0.5:0.05:0.95] and also reported the average mAP.</p><p>Our model was trained for 15 epochs with a linear warmup of 5 epochs. The learning rate was 1e-3, the mini-batch size was 16, and the weight decay was 1e-4. For ActivityNet, we find it is helpful to train our model to generate proposals by considering all actions from a single category, and then use external classification scores for the recognition. This strategy was also used in previous single-stage TAL methods <ref type="bibr" target="#b34">[35]</ref>. ? EPIC-Kitchens 100: We used a SlowFast network <ref type="bibr" target="#b23">[24]</ref> pre-trained on EPIC-Kitchens for feature extraction. This model is provided by <ref type="bibr" target="#b17">[18]</ref>. We fed 32 frame window with a stride of 16 to extract 2304-D features. Our model was trained on the training set and evaluated on the validation set. A window size of 9 was used for local self-attention. For evaluation, we used mAP@[0.1:0.1:0.5] and report the average mAP following <ref type="bibr" target="#b17">[18]</ref>. Our model was trained for 30 epochs with learning rate 1e-4, mini-batch size 2, and weight decay of 1e-4.</p><p>Reproducibility of Our Results. All results reported in the paper were obtained with the same random seed using PyTorch 1.10, CUDA 10.2 and CUDNN 7.6.5 on an NVIDIA Titan Xp GPU, using deterministic GPU computing routines. On the same machine, our code will always produce the same results when using the same random seed. Across machines/GPUs and computing environments, we have observed minor variation of average mAP scores (up to 0.5% average mAP on THUMOS, less than 0.2% average mAP on ActivityNet, and under 0.8% average mAP on EPIC Kitchens), yet those minor variations do not erode the clear performance gains of our method. Our code is made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Visualizations</head><p>Further, we present more visualizations of our results in <ref type="figure" target="#fig_3">Fig. D, extending Fig.  3</ref> of the main paper. Our model is able to detect the occurrence of actions and estimate their temporal boundaries for the most of the cases (see the first column of <ref type="figure" target="#fig_8">Fig. D)</ref>. The major failure modes of our model, as demonstrated in the second column of <ref type="figure" target="#fig_8">Fig. D</ref>, include (1) incorrect classification of action centers, i.e., background confusion (classification errors); (2) inaccurate regression of the action's onset and offset (localization errors). We plan to address these issues in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations and Future Work</head><p>A main limitation of our method is the use of pre-extracted video features, also faced by many previous approaches. Another limitation is the need for many human labeled videos for training and the constraint of a pre-defined vocabulary of actions. Interesting future directions include pre-training for action localization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b73">74]</ref>, and learning from videos and text corpus <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b29">30]</ref> without human labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?Fig. 1 .</head><label>1</label><figDesc>Work was done when visiting UW Madison. arXiv:2202.07925v2 [cs.CV] 28 Aug 2022 An illustration of our Action-Former. We propose a Transformer based model to localize action instances in time (top) by (1) classifying every moment into action categories and (2) estimating their distances to action boundaries (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? d s t &gt; 0 and d e t &gt; 0 are the distance between the current time t to the action's onset and offset, respectively. d s t and d e t are not defined if the time step t lies on the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our ActionFormer. Our method builds a Transformer based model to detect an action instance by classifying every moment and estimating action boundaries. Specifically, ActionFormer first extracts a sequence of video clip features, and embeds each of these features. The embedded features are further encoded into a feature pyramid using a multi-scale Transformer (right). The feature pyramid is then examined by shared classification and regression heads, producing an action candidate at every time step. Our method provides a single-stage anchor-free model for temporal action localization with strong performance across several datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of our results. From top to bottom: (1) input video frames; (2) action scores at each time step; (3) histogram of action onsets and offsets computed by weighting the regression outputs using action scores. See more in the appendix D levels and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>THUMOS14), including coverage, length, and the number of instances. Specifically, coverage presents the relative length of the actions (compared to the whole video), categorized into five bins: Extra Small (XS: (0, 0.02]), Small (S: (0.02, 0.04]), Medium (M: (0.04, 0.06]), Large (L:(0.06, 0.08]), and Extra Large (XL: (0.08, 1.0]). Length denotes the absolute length (in seconds) of actions, organized False negative (FN) profiling of our results on THUMOS14 using<ref type="bibr" target="#b0">[1]</ref>. This figure shows the FN rates under different video contents. From this figure we can find that our model will suffer from extra short or extra long instances. Also, our model will suffer from video inputs which have a large number of action instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. B .</head><label>B</label><figDesc>Sensitive analysis of our results on THUMOS14 using<ref type="bibr" target="#b0">[1]</ref>. Left: normalized mAP at tIoU=0.5 under different video contents. Right: The relative normalized mAP change at tIoU=0.5 with respect to different characteristics of the ground truth instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. C .</head><label>C</label><figDesc>False positive (FP) profiling of our results on THUMOS14 using<ref type="bibr" target="#b0">[1]</ref>. Left: FP error breakdown when considering the predictions for the top-10 ground-truth (G) instances. Right: The impact of error types. Localization error and background confusion are the top two error types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. D .</head><label>D</label><figDesc>More visualization of our outputs. For each item from top to bottom: (1) the input video frames; (2) action scores at each time step; (3) histogram of action onsets and offsets computed by weighting the regression outputs using the action scores. Left: successful cases; Right: failure cases. This figure is best viewed in color and when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on THUMOS14 and ActivityNet1.3. We report mAP at different tIoU thresholds. Average mAP in [0.3:0.1:0.7] is reported on THUMOS14 and [0.5:0.05:0.95] on ActivityNet1.3. Best results are in bold and second best underlined. Our method outperforms previous methods on THUMOS14 by a large margin, and beats previous methods when using the same features on ActivityNet1.3.</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell>Feature</cell><cell cols="6">THUMOS14 0.3 0.4 0.5 0.6 0.7 Avg. 0.5 0.75 0.95 Avg. ActivityNet1.3</cell></row><row><cell></cell><cell>BMN [36]</cell><cell>TSN [65]</cell><cell cols="6">56.0 47.4 38.8 29.7 20.5 38.5 50.1 34.8 8.3 33.9</cell></row><row><cell></cell><cell>DBG [34]</cell><cell>TSN [65]</cell><cell cols="6">57.8 49.4 39.8 30.2 21.7 39.8 -</cell><cell>---</cell></row><row><cell></cell><cell>G-TAD [75]</cell><cell>TSN [65]</cell><cell cols="6">54.5 47.6 40.3 30.8 23.4 39.3 50.4 34.6 9.0 34.1</cell></row><row><cell></cell><cell>BC-GNN [4]</cell><cell>TSN [65]</cell><cell cols="6">57.1 49.1 40.4 31.2 23.1 40.2 50.6 34.8 9.4 34.3</cell></row><row><cell></cell><cell>TAL-MR [84]</cell><cell>I3D [12]</cell><cell cols="6">53.9 50.7 45.4 38.0 28.5 43.3 43.5 33.9 9.2 30.2</cell></row><row><cell></cell><cell>P-GCN [80]</cell><cell>I3D [12]</cell><cell cols="4">63.6 57.8 49.1 -</cell><cell>-</cell><cell>-48.3 33.2 3.3 31.1</cell></row><row><cell></cell><cell cols="8">P-GCN [80]+TSP [2] R(2+1)D [63] 69.1 63.3 53.5 40.4 26.0 50.5 -</cell><cell>---</cell></row><row><cell></cell><cell>TSA-Net [26]</cell><cell>P3D [53]</cell><cell cols="6">61.2 55.9 46.9 36.1 25.2 45.1 48.7 32.0 9.0 31.9</cell></row><row><cell>Two-Stage</cell><cell>MUSES [45]</cell><cell>I3D [12]</cell><cell cols="6">68.9 64.0 56.9 46.3 31.0 -50.0 35.0 6.6 34.0</cell></row><row><cell></cell><cell>TCANet [52]</cell><cell>TSN [65]</cell><cell cols="6">60.6 53.2 44.6 36.8 26.7 44.3 52.3 36.7 6.9 35.5</cell></row><row><cell></cell><cell>TCANet [52]</cell><cell cols="2">SlowFast [24] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-54.3 39.1 8.4 37.6</cell></row><row><cell></cell><cell>BMN-CSA [58]</cell><cell>TSN [65]</cell><cell cols="6">64.4 58.0 49.2 38.2 27.8 47.7 52.4 36.2 5.2 35.4</cell></row><row><cell></cell><cell>ContextLoc [89]</cell><cell>I3D [12]</cell><cell cols="6">68.3 63.8 54.3 41.8 26.2 50.9 56.0 35.2 3.6 34.2</cell></row><row><cell></cell><cell>VSGN [83]</cell><cell>TSN [65]</cell><cell cols="6">66.7 60.4 52.4 41.0 30.4 50.2 52.4 36.0 8.4 35.1</cell></row><row><cell></cell><cell>VSGN [83]</cell><cell>I3D [12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-52.3 35.2 8.3 34.7</cell></row><row><cell></cell><cell cols="3">VSGN [83]+TSP [2] R(2+1)D [63] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-53.3 36.8 8.1 35.9</cell></row><row><cell></cell><cell>RTD-Net [59]</cell><cell>I3D [12]</cell><cell cols="6">68.3 62.3 51.9 38.8 23.7 49.0 47.2 30.7 8.6 30.8</cell></row><row><cell></cell><cell>A 2 Net [77]</cell><cell>I3D [12]</cell><cell cols="6">58.6 54.1 45.5 32.5 17.2 41.6 43.6 28.7 3.7 27.8</cell></row><row><cell></cell><cell>GTAN [50]</cell><cell>P3D [53]</cell><cell cols="4">57.8 47.2 38.8 -</cell><cell>-</cell><cell>-52.6 34.1 8.9 34.3</cell></row><row><cell></cell><cell>PBRNet [43]</cell><cell>I3D [12]</cell><cell cols="6">58.5 54.6 51.3 41.8 29.5 -54.0 35.0 9.0 35.0</cell></row><row><cell>Single-Stage</cell><cell>AFSD [35]</cell><cell>I3D [12]</cell><cell>67.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3 62.4 55.5 43.7 31.1 52.0 52.4 35.3 6.5 34.4 TadTR [46] I3D [12] 62.4 57.4 49.2 37.8 26.3 46.6 49.1 32.6 8.5 32.3 Ours I3D [12] 82.1 77.8 71.0 59.4 43.9 66.8 53.5 36.2 8.2 35.6 Ours+TSP [2] R(2+1)D [63] 73.4 67.4 59.1 46.7 31.5 55.6 54.7 37.8 8.4 36.6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>(left) summarizes the results. Our method achieves an average mAP of 66.8% ([0.3 : 0.1 : 0.7])</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on EPIC-Kitchens 100 validation set. We report mAP at different tIoU thresholds and the average mAP in [0.1:0.1:0.5]. All methods used the same SlowFast features. Our method outperforms all baselines by a large margin.</figDesc><table><row><cell cols="2">Task Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5 Avg</cell></row><row><cell></cell><cell cols="3">BMN [36,18] 10.8 9.8</cell><cell>8.4</cell><cell>7.1</cell><cell>5.6</cell><cell>8.4</cell></row><row><cell>Verb</cell><cell cols="4">G-TAD [75] 12.1 11.0 9.4</cell><cell>8.1</cell><cell>6.5</cell><cell>9.4</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">26.6 25.4 24.2 22.3 19.1 23.5</cell></row><row><cell></cell><cell cols="3">BMN [36,18] 10.3 8.3</cell><cell>6.2</cell><cell>4.5</cell><cell>3.4</cell><cell>6.5</cell></row><row><cell>Noun</cell><cell cols="4">G-TAD [75] 11.0 10.0 8.6</cell><cell>7.0</cell><cell>5.4</cell><cell>8.4</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">25.2 24.1 22.7 20.5 17.0 21.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>(right) shows the results. With I3D features, our method reaches an average mAP of 35.6% ([0.5 : 0.05 : 0.95]), outperforming all pre- vious methods using the same features by at least 0.6%. This boost is significant as the result is averaged across many tIoU thresholds, including those tight ones e.g., 0.95. Using the pre-training method from TSP [2] largely improves our re- sults (36.6% average mAP). Our model thus outperforms the best method with the same features [83] by a major margin (+0.7%). Again, our method outper- forms TadTR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc>Ablation study on loss weight. We report mAP at tIoU=0.5 and 0.7, and the average mAP in [0.3 : 0.1 : 0.7] on THUMOS14 by varying the loss weight ?reg in Eq. 7.</figDesc><table><row><cell cols="2">Method ?reg 0.5 0.7 Avg</cell></row><row><cell>Ours</cell><cell>0.2 69.7 40.9 65.6</cell></row><row><cell>Ours</cell><cell>0.5 71.3 42.4 66.7</cell></row><row><cell>Ours</cell><cell>1 71.0 43.9 66.8</cell></row><row><cell>Ours</cell><cell>2 69.5 43.9 66.2</cell></row><row><cell>Ours</cell><cell>5 69.1 43.0 65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B .</head><label>B</label><figDesc>Ablation study on maximum input sequence length during training. We report mAP at tIoU=0.5 and 0.7, and the average mAP in [0.3 : 0.1 : 0.7] on THUMOS14 by varying the maximum input length Tmax for training.</figDesc><table><row><cell cols="2">Method Tmax 0.5 0.7 Avg</cell></row><row><cell>Ours</cell><cell>576 69.6 42.5 65.7</cell></row><row><cell>Ours</cell><cell>1152 71.0 42.7 66.3</cell></row><row><cell>Ours</cell><cell>2304 71.0 43.9 66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table C .</head><label>C</label><figDesc>Ablation study on temporal feature resolution.</figDesc><table><row><cell></cell><cell></cell><cell>We report mAP at</cell></row><row><cell cols="3">tIoU=0.5 and 0.7, and the average mAP in [0.3 : 0.1 : 0.7] on THUMOS14 by varying</cell></row><row><cell>the feature stride.</cell><cell></cell><cell></cell></row><row><cell cols="3">Method stride 0.5 0.7 Avg</cell></row><row><cell>Ours</cell><cell>4</cell><cell>71.0 43.9 66.8</cell></row><row><cell>Ours</cell><cell>8</cell><cell>69.8 43.9 66.3</cell></row><row><cell>Ours</cell><cell cols="2">16 65.8 38.4 61.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>32?512,. . ., T?512] conv k=3, s=1 (ci = 512, co = 512) transformer1,...,transformer6[T/32?512,. . ., T?512] conv k=3, s=1 (ci = 512, co = output) transformer1,...,transformer6 [T/32?output,. . ., T?output] -</figDesc><table><row><cell></cell><cell>Name</cell><cell>Layer</cell><cell>Input</cell><cell>Output Size (T ? D)</cell><cell>Regression Range</cell></row><row><cell></cell><cell>input clip</cell><cell>-</cell><cell>-</cell><cell>T?D</cell><cell>-</cell></row><row><cell></cell><cell>projection1</cell><cell>conv k=3, s=1 (ci = D, co = 512)</cell><cell>input clip</cell><cell>T ? 512</cell><cell>-</cell></row><row><cell></cell><cell cols="2">projection2 conv k=3, s=1 (ci = 512, co = 512)</cell><cell>projection1</cell><cell>T ? 512</cell><cell>-</cell></row><row><cell></cell><cell>transformer0</cell><cell>Transformer Unit, ds=1</cell><cell>projection2</cell><cell>T ? 512</cell><cell>-</cell></row><row><cell></cell><cell>transformer1</cell><cell>Transformer Unit, ds=1</cell><cell>transformer0</cell><cell>T ? 512</cell><cell>[0, 4)</cell></row><row><cell></cell><cell>transformer2</cell><cell>Transformer Unit, ds=2</cell><cell>transformer1</cell><cell>T/2 ?512</cell><cell>[4, 8)</cell></row><row><cell>encoder</cell><cell>transformer3</cell><cell>Transformer Unit, ds=2</cell><cell>transformer2</cell><cell>T/4 ?512</cell><cell>[8, 16)</cell></row><row><cell></cell><cell>transformer4</cell><cell>Transformer Unit, ds=2</cell><cell>transformer3</cell><cell>T/8 ?512</cell><cell>[16, 32)</cell></row><row><cell></cell><cell>transformer5</cell><cell>Transformer Unit, ds=2</cell><cell>transformer4</cell><cell>T/16 ?512</cell><cell>[32, 64)</cell></row><row><cell></cell><cell>transformer6</cell><cell>Transformer Unit, ds=2</cell><cell>transformer5</cell><cell>T/32 ?512</cell><cell>[64, +?)</cell></row><row><cell>(heads) decoder</cell><cell>cls / reg nets</cell><cell cols="2">conv k=3, s=1 (ci = 512, co = 512) transformer1,...,transformer6</cell><cell>[T/</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Without loss of clarity, we drop the index of the pyramid ?.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TSP: Temporally-sensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12373</biblScope>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niebles Carlos</surname></persName>
		</author>
		<idno>93.1-93.12</idno>
		<editor>Brit. Mach. Vis. Conf</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SST: Singlestream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the Kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
	<note>IEEE Conf. Comput. Vis. Pattern Recog</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Augmented transformer with adaptive graph for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16024</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the Faster-RCNN architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic DETR: End-to-end object detection with dynamic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<title level="m">Rescaling egocentric vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">North American Asso. Comput. Lin. pp</title>
		<imprint>
			<biblScope unit="page" from="4171" to="4186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scale matters: Temporal scale aggregation network for precise action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The THUMOS challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. and Image Under</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst. vol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep concept-wise temporal convolutional networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BMN: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia. pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI. vol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11612" to="11619" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12596" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10271</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst. vol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CDC: Convolutionalde-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Class semanticsbased attention for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13739" to="13748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13526" to="13535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12043</idno>
		<title level="m">Temporal action proposal generation with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">PnP-DETR: Towards efficient visual analysis with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4661" to="4670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">End-toend video instance segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14138" to="14148" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Boundary-sensitive pre-training for temporal localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7220" to="7230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">G-TAD: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Acgnet: Action complement graph network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI. vol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3090" to="3098" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Tokens-to-token ViT: Training vision transformers from scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dense regression network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10287" to="10296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchorbased and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Video self-stitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13658" to="13667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. LNCS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12353</biblScope>
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">CUHK &amp; ETHZ &amp; SIAT submission to ActivityNet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Distance-IoU loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Enriching local and global contexts for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13516" to="13525" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
