<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalization is known to help the optimization of deep neural networks. Curiously, different architectures require specialized normalization methods. In this paper, we study what normalization is effective for Graph Neural Networks (GNNs). First, we adapt and evaluate the existing methods from other domains to GNNs. Faster convergence is achieved with InstanceNorm compared to BatchNorm and LayerNorm. We provide an explanation by showing that InstanceNorm serves as a preconditioner for GNNs, but such preconditioning effect is weaker with BatchNorm due to the heavy batch noise in graph datasets. Second, we show that the shift operation in In-stanceNorm results in an expressiveness degradation of GNNs for highly regular graphs. We address this issue by proposing GraphNorm with a learnable shift. Empirically, GNNs with Graph-Norm converge faster compared to GNNs using other normalization. GraphNorm also improves the generalization of GNNs, achieving better performance on graph classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2017). Empirically, GNNs have succeeded in a variety of tasks such as computational chemistry <ref type="bibr" target="#b30">(Stokes et al., 2020)</ref>, recommendation systems <ref type="bibr">(Ying et al., 2018)</ref>, and visual question answering <ref type="bibr" target="#b23">(Santoro et al., 2017)</ref>. Theoretically, existing works have studied GNNs through the lens of expressive power <ref type="bibr" target="#b19">(Keriven &amp; Peyr?, 2019;</ref><ref type="bibr">Xu et al., 2019;</ref><ref type="bibr" target="#b25">Sato et al., 2019;</ref><ref type="bibr">Loukas, 2020;</ref><ref type="bibr">Ying et al., 2021)</ref>, generalization <ref type="bibr" target="#b27">(Scarselli et al., 2018;</ref><ref type="bibr" target="#b6">Du et al., 2019b;</ref><ref type="bibr">Xu et al., 2020)</ref>, and extrapolation <ref type="bibr">(Xu et al., 2021)</ref>. However, the optimization of GNNs is less well understood, and in practice, the training of GNNs is often unstable and the convergence is slow <ref type="bibr">(Xu et al., 2019)</ref>.</p><p>In this paper, we study how to improve the training of GNNs via normalization. Normalization methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b33">Ulyanov et al., 2016;</ref><ref type="bibr" target="#b5">Ba et al., 2016;</ref><ref type="bibr">Salimans &amp; Kingma, 2016;</ref><ref type="bibr">Xiong et al., 2020;</ref><ref type="bibr">Salimans et al., 2016;</ref><ref type="bibr">Miyato et al., 2018;</ref><ref type="bibr" target="#b36">Wu &amp; He, 2018;</ref><ref type="bibr" target="#b24">Santurkar et al., 2018)</ref>. Curiously, no single normalization helps in every domain, and different architectures require specialized methods. For example, Batch normalization (BatchNorm) is a standard component in computer vision <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015)</ref>; Layer normalization (LayerNorm) is popular in natural language processing <ref type="bibr" target="#b5">(Ba et al., 2016;</ref><ref type="bibr">Xiong et al., 2020)</ref>; Instance normalization (InstanceNorm) has been found effective for style transfer tasks <ref type="bibr" target="#b33">(Ulyanov et al., 2016)</ref> . This motivates the question: What normalization methods are effective for GNNs?</p><p>We take an initial step towards answering the question above. First, we adapt the existing methods from other domains, including BatchNorm, LayerNorm, and InstanceNorm, to GNNs and evaluate their performance with extensive experiments on graph classification tasks. We observe that our adaptation of InstanceNorm to GNNs, which for each individual graph normalizes its node hidden representations, obtains much faster convergence compared to BatchNorm and LayerNorm. We provide an explanation for the success of InstanceNorm by showing that the shift operation in InstanceNorm serves as a preconditioner of the graph aggregation operation. Empirically, such preconditioning makes the optimization curvature smoother and makes the training more efficient. We also explain why the widely used arXiv:2009.03294v3 <ref type="bibr">[cs.</ref>LG] 11 Jun 2021  <ref type="figure">Figure 1</ref>. Overview. We evaluate and understand BatchNorm, LayerNorm, and InstanceNorm, when adapted to GNNs. InstanceNorm trains faster than LayerNorm and BatchNorm on most datasets (Section 3.1), as it serves as a preconditioner of the aggregation of GNNs (1a, Section 3.2). The preconditioning effect is weaker for BatchNorm due to heavy batch noise in graphs (1b, Section 3.3). We propose GraphNorm with a learnable shift to address the limitation of InstanceNorm. GraphNorm outperforms other normalization methods for both training speed ( <ref type="figure">Figure 2</ref>) and generalization <ref type="table">(Table 1</ref>, 2).</p><p>BatchNorm does not bring the same level of acceleration. The variance of the batch-level statistics on graph datasets is much larger if we apply the normalization across graphs in a batch instead of across individual graphs. The noisy statistics during training may lead to unstable optimization.</p><p>Second, we show that the adaptation of InstanceNorm to GNNs, while being helpful in general, has limitations. The shift operation in InstanceNorm, which subtracts the mean statistics from node hidden representations, may lead to an expressiveness degradation for GNNs. Specifically, for highly regular graphs, the mean statistics contain graph structural information, and thus removing them could hurt the performance. Based on our analysis, we propose Graph-Norm to address the issue of InstanceNorm with a learnable shift (Step 2 in <ref type="figure">Figure 1</ref>). The learnable shift could learn to control the ideal amount of information to preserve for mean statistics. Together, GraphNorm normalizes the hidden representations across nodes in each individual graph with a learnable shift to avoid the expressiveness degradation while inheriting the acceleration effect of the shift operation.</p><p>We validate the effectiveness of GraphNorm on eight popular graph classification benchmarks. Empirical results confirm that GraphNorm consistently improves the speed of converge and stability of training for GNNs compared to those with BatchNorm, InstanceNorm, LayerNorm, and those without normalization. Furthermore, GraphNorm helps GNNs achieve better generalization performance on most benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Closely related to our work, InstanceNorm <ref type="bibr" target="#b33">(Ulyanov et al., 2016)</ref> is originally proposed for real-time image generation. Variants of InstanceNorm are also studied in permutation equivalent data processing <ref type="bibr">(Yi et al., 2018;</ref><ref type="bibr" target="#b32">Sun et al., 2020)</ref>. We instead adapt InstanceNorm to GNNs and find it helpful for the training of GNNs. Our proposed GraphNorm builds on and improves InstanceNorm by addressing its expressiveness degradation with a learnable shift.</p><p>Few works have studied normalization in the GNN literature. <ref type="bibr">Xu et al. (2019)</ref> adapts BatchNorm to GIN as a plug-in component. A preliminary version of <ref type="bibr" target="#b8">Dwivedi et al. (2020)</ref> normalizes the node features with respect to the graph size. Our GraphNorm is size-agnostic and significantly differs from the graph size normalization. More discussions on other normalization methods are in Appendix E.</p><p>The reason behind the effectiveness of normalization has been intensively studied. While scale and shift are the main components of normalization, most existing works focus on the scale operation and the "scale-invariant" property: With a normalization layer after a linear (or convolutional) layer, the output values remain the same as the weights are scaled. Hence, normalization decouples the optimization of direction and length of the parameters <ref type="bibr" target="#b22">(Kohler et al., 2019)</ref>, implicitly tunes the learning rate <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b14">Hoffer et al., 2018;</ref><ref type="bibr" target="#b2">Arora et al., 2018b;</ref><ref type="bibr">Li &amp; Arora, 2019)</ref>, and smooths the optimization landscape <ref type="bibr" target="#b24">(Santurkar et al., 2018)</ref>. Our work offers a different view by instead showing specific shift operation has the preconditioning effect and can accelerate the training of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We begin by introducing our notations and the basics of GNNs. Let G = (V, E) denote a graph where V = {v 1 , v 2 , ? ? ? , v n }, n is the number of nodes. Let the feature vector of node v i be X i . We denote the adjacency matrix of a graph as</p><formula xml:id="formula_0">A ? R n?n with A ij = 1 if (v i , v j ) ? E and 0 otherwise. The degree matrix associated with A is defined as D = diag (d 1 , d 2 , . . . , d n ) where d i = n j=1 A ij .</formula><p>Graph Neural Networks. GNNs use the graph structure and node features to learn the representations of nodes and graphs. Modern GNNs follow a neighborhood aggregation strategy <ref type="bibr" target="#b31">(Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017;</ref><ref type="bibr" target="#b34">Velickovic et al., 2018;</ref><ref type="bibr">Monti et al., 2017;</ref><ref type="bibr">Ying et al., 2021)</ref>, where the representation of a node is iteratively updated by aggregating the representation of its neighbors. To be concrete, we denote h (k) i as the representation of v i at the k-th layer and define h (0) i = X i . We use AGGREGATE to denote the aggregation function in the k-th layer:</p><formula xml:id="formula_1">h (k) i = AGGREGATE (k) h (k?1) i , h (k?1) j : v j ? N (v i ) ,<label>(1)</label></formula><p>where N (v i ) is the set of nodes adjacent to v i . Different GNNs can be obtained by choosing different AGGREGATE functions. Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> can be defined in matrix form as:</p><formula xml:id="formula_2">H (k) = ReLU W (k) H (k?1) Q GCN ,<label>(2)</label></formula><p>where ReLU stands for rectified linear unit,</p><formula xml:id="formula_3">H (k) = h (k) 1 , h (k) 2 , ? ? ? , h (k) n ? R d (k)</formula><p>?n is the feature matrix at the k-th layer where d (k) denotes the feature dimension, and W (k) is the parameter matrix in layer k. Q GCN = D ? 1 2?D ? 1 2 , where? = A + I n andD is the degree matrix of?. I n is the identity matrix.</p><p>Graph Isomorphism Network (GIN) <ref type="bibr">(Xu et al., 2019)</ref> is defined in matrix form as</p><formula xml:id="formula_4">H (k) = MLP (k) W (k) H (k?1) Q GIN ,<label>(3)</label></formula><p>where MLP stands for multilayer perceptron, ? (k) is a learnable parameter and Q GIN = A + I n + ? (k) I n .</p><p>For a K-layer GNN, the outputs of the final layer, i.e., h (K) i ,i = 1, ? ? ? , n, will be used for prediction. For graph classification tasks, we can apply a READOUT function, e.g., summation, to aggregate node features h</p><formula xml:id="formula_5">(K) i to obtain the entire graph's representation h G = READOUT h (K) i v i ? V .</formula><p>A classifier can be applied upon h G to predict the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization.</head><p>Generally, given a set of values {x 1 , x 2 , ? ? ? , x m }, a normalization operation first shifts each x i by the mean ?, and then scales them down by standard deviation ?:</p><formula xml:id="formula_6">x i ? ? xi?? ? + ?, where ? and ? are learnable parameters, ? = 1 m m i=1 x i and ? 2 = 1 m m i=1 (x i ? ?)</formula><p>2 . The major difference among different existing normalization methods is which set of feature values the normalization is applied to. For example, in computer vision, BatchNorm normalizes the feature values in the same channel across different samples in a batch. In NLP, LayerNorm normalizes the feature values at each position in a sequence separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluating and Understanding Normalization for GNNs</head><p>In this section, we first adapt and evaluate existing normalization methods to GNNs. Then we give an explanation of the effectiveness of the variant of InstanceNorm, and show why the widely used BatchNorm fails to have such effectiveness. The understanding inspires us to develop better normalization methods, e.g., GraphNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adapting and Evaluating Normalization for GNNs</head><p>To investigate what normalization methods are effective for GNNs, we first adapt three typical normalization methods, i.e., BatchNorm, LayerNorm, and InstanceNorm, developed in other domain to GNNs. We apply the normalization after the linear transformation as in previous works <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr">Xiong et al., 2020;</ref><ref type="bibr">Xu et al., 2019)</ref>. The general GNN structure equipped with a normalization layer can be represented as:</p><formula xml:id="formula_7">H (k) = F (k) Norm W (k) H (k?1) Q ,<label>(4)</label></formula><p>where F (k) is a function that applies to each node separately, Q is an n ? n matrix representing the neighbor aggregation, and W (k) is the weight/parameter matrix in layer k. We can instantiate Eq. (4) as GCN and GIN, by setting proper F (k) and matrix Q. For example, if we set F (k) to be ReLU and set Q to be Q GCN (Eq. (2)), then Eq. (4) becomes GCN with normalization; Similarly, by setting F (k) to be MLP (k) and Q to be Q GIN (Eq. (3)), we recover GIN with normalization.</p><p>We then describe the concrete operations of the adaptations of the normalization methods. Consider a batch of graphs {G 1 , ? ? ? , G b } where b is the batch size. Let n g be the number of nodes in graph G g . We generally denote? i,j,g as the inputs to the normalization module, e.g., the j-th feature value of node v i of graph G g , i = 1, ? ? ? , n g , j = 1, ? ? ? , d, g = 1, ? ? ? , b. The adaptations take the general form:</p><formula xml:id="formula_8">Norm ? i,j,g = ? ?? i,j,g ? ? ? + ?,<label>(5)</label></formula><p>where the scopes of mean ?, standard deviation ?, and affine parameters ?, ? differ for different normalization methods. For BatchNorm, normalization and the computation of ? and ? are applied to all values in the same feature dimension across the nodes of all graphs in the batch as in Xu et al.</p><p>(2019), i.e., over dimensions g, i of? i,j,g . To adapt Layer-Norm to GNNs, we view each node as a basic component, resembling words in a sentence, and apply normalization to all feature values across different dimensions of each node, i.e., over dimension j of? i,j,g . For InstanceNorm, we regard each graph as an instance. The normalization is then applied to the feature values across all nodes for each individual graph, i.e., over dimension i of? i,j,g .</p><p>In <ref type="figure">Figure 2</ref> we show training curves of different normalization methods in graph classification tasks. We find that LayerNorm hardly improves the training process in most tasks, while our adaptation of InstanceNorm can largely boost the training speed compared to other normalization methods. The test performances have similar trends. We summarize the final test accuracies in <ref type="table">Table 1</ref>. In the following subsections, we provide an explanation for the success of InstanceNorm and its benefits compared to BatchNorm, which is currently adapted in many GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Shift in InstanceNorm as a Preconditioner</head><p>As mentioned in Section 1.1, the scale-invariant property of the normalization has been investigated and considered as one of the ingredients that make the optimization efficient. In our analysis of normalizations for GNNs, we instead take a closer look at the shift operation in the normalization. Compared to the image and sequential data, the graph is explicitly structured, and the neural networks exploit the structural information directly in the aggregation of the neighbors, see Eq.</p><p>(1). Such uniqueness of GNNs makes it possible to study how the shift operation interplays with the graph data in detail.</p><p>We show that the shift operation in our adaptation of In-stanceNorm serves as a preconditioner of the aggregation in GNNs and hypothesize this preconditioning effect can boost the training of GNNs. Though the current theory of deep learning has not been able to prove and compare the convergence rate in the real settings, we calculate the convergence rate of GNNs on a simple but fully characterizable setting to give insights on the benefit of the shift operation.</p><p>fWe first formulate our adaptation of InstanceNorm in the matrix form. Mathematically, for a graph of n nodes, denote N = I n ? 1 n 11 . N is the matrix form of the shift operation, i.e., for any vector z = [z 1 , z 2 , ? ? ? , z n ] ? R n , z N = z ? 1 n n i=1 z i 1 . Then the normalization together with the aggregation can be represented as 1</p><formula xml:id="formula_9">Norm W (k) H (k?1) Q = S W (k) H (k?1) Q N, (6) where S = diag 1 ?1 , 1 ?2 , ? ? ? , 1 ? d (k)</formula><p>is the scaling, and Q is the GNN aggregation matrix. Each ? i is the standard deviation of the values of the i-th features among the nodes in the graph we consider. We can see that, in the matrix form, shifting feature values on a single graph is equivalent to multiplying N as in Eq. (6). Therefore, we further check how this operation affects optimization. In particular, we examine the singular value distribution of QN . The following theorem shows that QN has a smoother singular value distribution than Q, i.e., N serves as a preconditioner of Q.</p><p>Theorem 3.1 (Shift Serves as a Preconditioner of Q). Let Q, N be defined as in Eq. (6), 0 ? ? 1 ? ? ? ? ? ? n be the singular values of Q. We have ? n = 0 is one of the singular values of QN , and let other singular values of QN be 0 ? ? 1 ? ? 2 ? ? ? ? ? ? n?1 . Then we have</p><formula xml:id="formula_10">? 1 ? ? 1 ? ? 2 ? ? ? ? ? ? n?1 ? ? n?1 ? ? n ,<label>(7)</label></formula><p>where ? i = ? i or ? i = ? i?1 only if there exists one of the right singular vectors ? i of Q associated with ? i satisfying 1 ? i = 0.</p><p>The proof can be found in Appendix A.1.</p><p>We hypothesize that precoditioning Q can help the optimization. In the case of optimizing the weight matrix W (k) , we can see from Eq. (6) that after applying normalization, the term Q in the gradient of W (k) will become QN which makes the optimization curvature of W (k) smoother, see Appendix A.5 for more discussions. Similar preconditioning effects are believed to improve the training of deep learning models <ref type="bibr" target="#b7">(Duchi et al., 2011;</ref><ref type="bibr" target="#b20">Kingma &amp; Ba, 2015)</ref>, and classic wisdom in optimization has also shown that preconditioning can accelerate the convergence of iterative methods <ref type="bibr" target="#b4">(Axelsson, 1985;</ref><ref type="bibr">Demmel, 1997)</ref>. Unfortunately, current theoretical toolbox only has a limited power on the optimization of deep learning models. Global convergence rates have only been proved for either simple models, e.g., linear models <ref type="bibr" target="#b1">(Arora et al., 2018a)</ref>, or extremely overparameterized models <ref type="bibr">(Du et al., 2018;</ref><ref type="bibr" target="#b0">Allen-Zhu et al., 2019;</ref><ref type="bibr">Du et al., 2019a;</ref><ref type="bibr">Cai et al., 2019;</ref><ref type="bibr" target="#b6">Du et al., 2019b;</ref><ref type="bibr">Zou et al., 2020)</ref>. To support our hypothesis that preconditioning may suggest better training, we investigate a simple but characterizable setting of training a linear GNN using gradient descent in Appendix A.2. In this setting, we prove that: 1 Standard normalization has an additional affine operation after shifting and scaling. Here we omit it in Eq. 6 for better demonstration. Adding this operation will not affect the theoretical analysis.  <ref type="figure">Figure 3</ref>. Singular value distribution of Q and QN for sampled graphs in different datasets using GIN. More visualizations can be found in Appendix D.1 randomness of data generation, the parameter w Shift t of the model with shift at step t converges to the optimal parameter w Shift * linearly:</p><formula xml:id="formula_11">w Shift t ? w Shift * 2 = O ? t 1 ,</formula><p>where ? 1 is the convergence rate.</p><p>Similarly, the parameter w Vanilla t of the vanilla model converges linearly, but with a slower rate:</p><formula xml:id="formula_12">w Vanilla t ? w Vanilla * 2 = O ? t 2 and ? 1 &lt; ? 2 ,</formula><p>which indicates that the model with shift converges faster than the vanilla model.</p><p>The proof can be found in Appendix A.2. To check how much the matrix N improves the distribution of the spectrum of matrix Q in real practice, we sample graphs from different datasets for illustration, as showed in <ref type="figure">Figure 3</ref> (more visualizations for different types of graph can be found in Appendix D.1). We can see that the singular value distribution of QN is much smoother, and the condition number is improved. Note that for a multi-layer GNN, the normalization will be applied in each layer. Therefore, the overall improvement of such preconditioning can be more significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Heavy Batch Noise in Graphs Makes BatchNorm Less Effective</head><p>The above analysis shows the adaptation of InstanceNorm has the effect of preconditioning the aggregation of GNNs. Then a natural question is whether a batch-level normalization for GNNs (Xu et al., 2019) has similar advantages. We show that BatchNorm is less effective in GNNs due to heavy batch noise on graph data.</p><p>In BatchNorm, the mean ? B and standard deviation ? B are calculated in a sampled batch during training, which can be viewed as random variables by the randomness of sampling. During testing, the estimated dataset-level statistics (running mean ? D and standard deviation ? D ) are used instead of the batch-level statistics <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015)</ref>. To apply Theorem 3.1 to BatchNorm for the preconditioning effect, one Std value <ref type="figure">Figure 4</ref>. Batch-level statistics are noisy for GNNs. We plot the batch-level/dataset-level mean/standard deviation of models trained on PROTEINS (graph classification) and CIFAR10 (image classification). We observe that the deviation of batch-level statistics from dataset-level statistics is rather large for the graph task, while being negligible in image task.</p><formula xml:id="formula_13">CIFAR10-layer-3 dataset-level ? batch-level ? (max) batch-level ? (min) dataset-level ? batch-level ? (max) batch-level ? (min)</formula><p>could potentially view all graphs in a dataset as subgraphs in a super graph. Hence, Theorem 3.1 applies to BatchNorm if the batch-level statistics are well-concentrated around dataset-level statistics, i.e., ? B ? ? D and ? B ? ? D . However, the concentration of batch-level statistics is heavily domain-specific. While <ref type="bibr" target="#b28">Shen et al. (2020)</ref> find the variation of batch-level statistics in typical networks is small for computer vision, the concentration of batch-level statistics is still unknown for GNNs.</p><p>We study how the batch-level statistics ? B , ? B deviate from the dataset-level statistics ? D , ? D . For comparison, we train a 5-layer GIN with BatchNorm on the PROTEINS dataset and train a ResNet18 <ref type="bibr" target="#b13">(He et al., 2016)</ref> on the CIFAR10 dataset. We set batch size to 128. For each epoch, we record the batch-level max/min mean and standard deviation for the first and the last BatchNorm layer on a randomly selected dimension across batches. In <ref type="figure">Figure 4</ref>, pink line denotes the dataset-level statistics, and green/blue line denotes the max/min value of the batch-level statistics. We observe that for image tasks, the maximal deviation of the batch-level statistics from the dataset-level statistics is negligible <ref type="figure">(Figure 4)</ref> after a few epochs. In contrast, for the graph tasks, the variation of batch-level statistics stays large during training. Intuitively, the graph structure can be quite diverse and the a single batch cannot well represent the entire dataset. Hence, the preconditioning property also may not hold for Batch-Norm. In fact, the heavy batch noise may bring instabilities to the training. More results may be found in Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Normalization</head><p>Although we provide evidence on the indispensability and advantages of our adaptation of InstanceNorm, simply normalizing the values in each feature dimension within a graph does not consistently lead to improvement. We show that in some situations, e.g., for regular graphs, the standard shift (e.g., shifting by subtracting the mean) may cause information loss on graph structures.</p><p>We consider r-regular graphs, i.e., each node has a degree r. We first look into the case that there are no available node features, then X i is set to be the one-hot encoding of the node degree <ref type="bibr">(Xu et al., 2019)</ref>. In a r-regular graph, all nodes have the same encoding, and thus the columns of H (0) are the same. We study the output of the standard shift operation in the first layer, i.e., k = 1 in Eq. (6). From the following proposition, we can see that when the standard shift operation is applied to GIN for a r-regular graph described above, the information of degree is lost:</p><p>Proposition 4.1. For a r-regular graph with one-hot encodings as its features described above, we have for GIN,</p><formula xml:id="formula_14">Norm W (1) H (0) Q GIN = S W (1) H (0) Q GIN N = 0,</formula><p>i.e., the output of normalization layer is a zero matrix without any information of the graph structure.</p><p>Such information loss not only happens when there are no node features. For complete graphs, we can further show that even each node has different features, the graph structural information, i.e., adjacency matrix A, will always be ignored after the standard shift operation in GIN:</p><p>Proposition 4.2. For a complete graph (r = n ? 1), we have for GIN, Q GIN N = ? (k) N , i.e., graph structural in- formation in Q will be removed after multiplying N .</p><p>The proof of these two propositions can be found in Appendix A. Similar results can be easily derived for other architectures like GCN by substituting Q GIN with Q GCN . As we can see from the above analysis, in graph data, the mean statistics after the aggregation sometimes contain structural information. Discarding the mean will degrade the expressiveness of the neural networks. Note that the problem may not happen in image domain. The mean statistics of image data contains global information such as brightness. Removing such information in images will not change the semantics of the objects and thus will not hurt the classification performance.</p><p>This analysis inspires us to modify the current normalization method with a learnable parameter to automatically control how much the mean to preserve in the shift operation. Combined with the graph-wise normalization, we name our new method Graph Normalization, i.e., Graph-Norm. For each graph G, we generally denote value? i,j as the inputs to GraphNorm, e.g., the j-th feature value of node v i , i = 1, ? ? ? , n, j = 1, ? ? ? , d. GraphNorm takes the following form:</p><formula xml:id="formula_15">GraphNorm ? i,j = ? j ?? i,j ? ? j ? ? ? ? j + ? j , (8) where ? j = n i=1? i,j n ,? 2 j = n i=1 (?i,j??j??j) 2 n</formula><p>, and ? j , ? j are the affine parameters as in other normalization methods.</p><p>By introducing the learnable parameter ? j for each feature dimension j, we are able to learn how much the information we need to keep in the mean. It is easy to see that Graph-Norm has stronger expressive power than InstanceNorm. Formally, we have the following fact:</p><p>Fact 1 (GraphNorm is strictly more expressive than InstanceNorm). If ? j = 1, ? j = 0, then there does not exist ? j , ? j such that for any ? i,j n i=1 that the normalization is applied to, for any i,</p><formula xml:id="formula_16">GraphNorm {?j ,?j ,?j } ? i,j = ? j ?? i,j ??j ??? ?j +? j = ? j ? hi,j ??j ?j +? j = InstanceNorm {? j ,? j } ? i,j , where ? j = n i=1? i,j n ,? 2 j = n i=1 (?i,j??j??j) 2 n , ? 2 j = n i=1 (?i,j??j) 2 n .</formula><p>To validate our theory and the proposed GraphNorm in realworld data, we conduct an ablation study on two typical datasets, PROTEINS and IMDB-BINARY. As shown in <ref type="figure">Figure 5</ref>, the graphs from PROTEINS and IMDB-BINARY exhibit irregular-type and regular-type graphs, respectively. We train GIN/GCN using our adaptation of InstanceNorm and GraphNorm under the same setting in Section 5. The training curves are presented in <ref type="figure">Figure 5</ref>. The curves show that using a learnable ? slightly improves the convergence on PROTEINS, while significantly boost the training on IMDB-BINARY. This observation verify that shifting the feature values by subtracting the mean may lose information, especially for regular graphs. And the introduction of learnable shift in GraphNorm can effectively mitigate the expressive degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate and compare both the training and test performance of GraphNorm with other normalization methods on graph classification benchmarks.  Results. We plot the training curves of GIN with Graph-Norm and other normalization methods 2 on different tasks in <ref type="figure">Figure 2</ref>. The results on GCN show similar trends, and are provided in Appendix D.3. As shown in <ref type="figure">Figure 2</ref> For reference, we explain the possible reasons of higher test accuracy in two folds. First, as shown in <ref type="figure">Figure 2</ref>, using proper normalization helps the model find a minimum with a higher training accuracy. Second, as suggested by <ref type="bibr" target="#b12">Hardt et al. (2016)</ref>, faster training leads to smaller generalization gap. Since the test accuracy equals the training accuracy plus the generalization, these two views together suggest better normalization leads to better test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>In this subsection, we summarize the results of some ablation studies, including BatchNorm with learnable shift, BatchNorm with running statistics and the effect of batch size. Due to the space limitation, the detailed results can be found in Appendix D.</p><p>BatchNorm with learnable shift. We conduct experiments on BatchNorm to investigate whether simply introducing a learnable shift can already improve the existing normalization methods without concrete motivation of overcoming expressiveness degradation. Specifically, we equip BatchNorm with a similar learnable shift as GraphNorm and evaluate its performance. We find that the learnable shift cannot further improve upon BatchNorm (See Appendix D), which suggests the introduction of learnable shift in GraphNorm is critical.</p><p>BatchNorm with running statistics. We study the variant of BatchNorm which uses running statistics to replace the batch-level mean and standard deviation (Similar idea is also proposed in Yan et al. <ref type="formula" target="#formula_1">(2019)</ref>). At first glance, this method may seem to be able to mitigate the problem of large batch noise. However, the running statistics change a lot during training, and using running statistics disables the model to back-propagate the gradients through mean and standard deviation. Results in Appendix D show this variant has even worse performance than BatchNorm.</p><p>The effect of batch size. We further compare the Graph-Norm with BatchNorm with different batch sizes <ref type="bibr">(8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref>. As shown in Appendix D, our GraphNorm consistently outperforms the BatchNorm on all the settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we adapt and evaluate three well-used normalization methods, i.e., BatchNorm, LayerNorm, and Instan-ceNorm to GNNs. We give explanations for the successes and failures of these adaptations. Based on our understanding of the strengths and limitations of existing adaptations, we propose Graph Normalization, that builds upon the adaptation of InstanceNorm with a learnable shift to overcome the expressive degradation of the original InstanceNorm.</p><p>Experimental results show GNNs with GraphNorm not only converge faster, but also achieve better generalization performance on several benchmark datasets.</p><p>Though seeking theoretical understanding of normalization methods in deep learning is challenging <ref type="bibr" target="#b2">(Arora et al., 2018b)</ref> due to limited understanding on the optimization of deep learning models and characterization of real world data, we take an initial step towards finding effective normalization methods for GNNs with theoretical guidance in this paper. The proposed theories and hypotheses are motivated by several simple models. And we are not able to give concrete theoretical results to problems such as: the convergence rate of general GNNs with normalization, the spectrum of Q normalized by learnable shift, etc. We believe the analyses of more realistic but complicated settings, e.g., the dynamics of GraphNorm on deep GNNs, are good future directions.</p><p>Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.</p><p>Cai, T., Gao, R., Hou, J., Chen, S., Wang, D., He, D., Zhang, Z., and Wang, L. A gram-gauss-newton method learning overparameterized deep neural networks for regression problems. arXiv preprint arXiv:1905.11675, 2019.</p><p>Chen, Y., Tang, X., Qi, X., Li, C.-G., and Xiao, R. Learning graph normalization for graph neural networks, 2020.</p><p>Defferrard, M., Bresson, X., and Vandergheynst, P.  <ref type="formula" target="#formula_1">(2012))</ref>). Let S ? R (n?1)?(n?1) be symmetric, y ? R n and a ? R be given, and let R = S y y a ? R n?n . Let ? 1 ? ? 2 ? ? ? ? ? ? n be the eigenvalues of R and ? 1 ? ? 2 ? ? ? ? ? ? n?1 be the eigenvalues of S. Then</p><formula xml:id="formula_17">? 1 ? ? 1 ? ? 2 ? ? ? ? ? ? n?1 ? ? n?1 ? ? n ,<label>(9)</label></formula><p>where ? i = ? i only when there is a nonzero z ? R n?1 such that Sz = ? i z and y z = 0; if ? i = ? i?1 then there is a nonzero z ? R n?1 such that Sz = ? i?1 z, y z = 0.</p><p>Using Lemma A.1, the theorem can be proved as below.</p><p>Proof. For any matrices P, R ? R n?n , we use P ? R to denote that the matrix P is similar to the matrix R. Note that if P ? R, the eigenvalues of P and R are the same. As the singular values of P are equal to the square root of the eigenvalues of P P , we have the eigenvalues of Q Q and that of N Q QN are ? 2 i n i=1 and ? 2 i n i=1 , respectively. Note that N is a projection operator onto the orthogonal complement space of the subspace spanned by 1, and N can be decomposed as N = U diag ? ? 1, ? ? ? , 1 ?n?1 , 0 ? ? U where U is an orthogonal matrix. Since 1 is the eigenvector of N associated with eigenvalue 0, we have</p><formula xml:id="formula_18">U = U 1 1 ? n 1 ,<label>(10)</label></formula><p>where U 1 ? R n?(n?1) satisfies U 1 1 = 0 and U 1 U 1 = I n?1 .</p><p>Then</p><formula xml:id="formula_19">we have N Q QN = U diag (1, ? ? ? , 1, 0) U Q QU diag (1, ? ? ? , 1, 0) U ? diag (1, ? ? ? , 1, 0) U Q QU diag (1, ? ? ? , 1, 0). Let D = diag (1, ? ? ? , 1, 0) = I n?1 0 0 0 ,<label>(11)</label></formula><formula xml:id="formula_20">B = I n?1 0 ,<label>(12)</label></formula><formula xml:id="formula_21">C = Q Q,<label>(13)</label></formula><p>where 0 = ? ? 0, ? ? ? , 0 ?n?1 ? ? .</p><p>We have</p><formula xml:id="formula_22">N Q QN ? DU C U D (14) = D U 1 1 ? n 1 C U 1 1 ? n 1 D (15) = D U 1C U 1 1 ? n U 1C 1 1 ? n 1 C U 1 1 n 1 C 1 D (16) = B 0 0 U 1C U 1 1 ? n U 1C 1 1 ? n 1 C U 1 1 n 1 C 1 B 0 0 (17) = U 1C U 1 0 0 0 .<label>(18)</label></formula><p>Using Lemma A.1 and taking R = U C U and S = U 1C U 1 , we have the eigenvalues of U 1C U 1 are interlacing between the eigenvalues of U C U . Note that the eigenvalues of DU C U D are ? 2 1 ? ? 2 2 ? ? ? ? ? ? 2 n?1 and ? 2 n = 0, and by Eq. (18), the eigenvalues of DU C U D contain the eigenvalues of U 1C U 1 and 0. Since the eigenvalues of U C U are ? 2 1 ? ? 2 2 ? ? ? ? ? ? 2 n (By similarity of U C U andC), we then have</p><formula xml:id="formula_23">? 2 1 ? ? 2 1 ? ? 2 2 ? ? ? ? ? ? 2 n?1 ? ? 2 n?1 ? ? 2 n .<label>(19)</label></formula><p>Moreover, the equality holds only when there is a nonzero z ? R n?1 that satisfies</p><formula xml:id="formula_24">U 1C U 1 z = ?z,<label>(20)</label></formula><formula xml:id="formula_25">1 C U 1 z = 0,<label>(21)</label></formula><p>where ? is one of ? 2 i s. Since U 1 forms an orthogonal basis of the orthogonal complement space of 1 and Eq. (21) is equivalent to "CU 1 z lies in the orthogonal complement space", we have that there is a vector y ? R n?1 such that</p><formula xml:id="formula_26">CU 1 z = U 1 y.<label>(22)</label></formula><p>Substituting this into Eq. (20), we have</p><formula xml:id="formula_27">U 1 U 1 y = ?z.<label>(23)</label></formula><p>Since U 1 U 1 = I n?1 , the equation above is equivalent to</p><formula xml:id="formula_28">y = ?z,<label>(24)</label></formula><p>which meansC</p><formula xml:id="formula_29">U 1 z = U 1 y = ?U 1 z,<label>(25)</label></formula><p>i.e., U 1 z is the eigenvector ofC associated with ?. By noticing U 1 z lies in the orthogonal complement space of 1 and the eigenvector ofC is right singular vector of Q, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Concrete example of the acceleration</head><p>To get more intuition on how the preconditioning effect of the shift can accelerate the training of GNNs, we provide a concrete example showing that shift indeed improves the convergence rate. Note that the global convergence rate of widely-used deep GNNs on general data remains highly unexplored, and the existing works mainly focus on some simplified case, e.g., GNTK <ref type="bibr" target="#b6">(Du et al., 2019b)</ref>. To make things clear without loss of intuition, we focus on a simple linear GNN applied to a well-specified task where we are able to explicitly compare the convergence rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1. SETTINGS</head><p>Data. We describe each sample, i.e., graph, with n nodes by a tuple G = {X, Q, p, y}, where ? X ? R d?n is the feature matrix of the graph, where d is the dimension of the of each feature vector.</p><p>? Q ? R n?n representing the matrix representing the neighbor aggregation as Eq. (4). Note that this matrix depends on the aggregation scheme used by the chosen architecture, but for simplicity, we model this as a part of data structure.</p><p>? p ? R n?1 is a weight vector representing the importance of each node. This will be used to calculate the READOUT step. Note that this vector is not provided in many real-world datasets, so the READOUT step usually takes operations such as summation.</p><p>? y ? R is the label.</p><p>The whole dataset S = {G 1 , ? ? ? , G m } consists of m graphs where G i = {X i , Q i , p i , y i }. We make the following assumptions on the data generation process:</p><p>Assumption 1 (Independency). We assume X i , Q i , p i are drawn from three independent distributions in an i.i.d. manner, e.g., X 1 , ? ? ? , X m are i.i.d..</p><p>Assumption 2 (Structure of data distributions). For clearness and simplicity of statement, we assume the number of nodes in each graph G i are the same, we will use n to denote this number and we further assume n = d. We assume that the distribution of p i satisfies E pp = I n , Ep = 0, which means the importance vector is non-degenerate. Let EXQ = Y , we assume that Y is full ranl. We make the following assumptions on XQ: 1 Y ?1 XQ = 0, which ensures that there is no information in the direction 1 Y ?1 ; there is a constant ? 1 such that</p><formula xml:id="formula_30">E(XQ ? Y )(XQ ? Y ) ? 1 I d and E(XQ ? Y )N (XQ ? Y ) ? 1 I d ,</formula><p>where ? 1 characterizes the noise level; none of the eigenvectors of Y Y is orthogonal to 1.</p><p>Remark 1. A few remarks are in order, firstly, the assumption that each graph has the same number of nodes and the number n is equal to feature dimension d can be achieved by "padding", i.e., adding dummy points or features to the graph or the feature matrix. The assumption that 1 Y ?1 XQ = 0 is used to guarantee that there is no information loss caused by shift (1 Y ?1 Y N Y = 0). Though we make this strong assumption to ensure no information loss in theoretical part, we introduce "learnable shift" to mitigate this problem in the practical setting. The theory taking learnable shift into account is an interesting future direction. Assumption 3 (Boundness). We make the technical assumption that there is a constant b such that the distributions of</p><formula xml:id="formula_31">X i , Q i , p i ensures X i Q i p i ? ? b.<label>(26)</label></formula><p>Model. We consider a simple linear graph neural network with parameter w ? R d?1 :</p><formula xml:id="formula_32">f Vanilla w (X, Q, p) = w XQp.<label>(27)</label></formula><p>Then, the model with shift can be represented as:</p><formula xml:id="formula_33">f Shift w (X, Q, p) = w XQN p,<label>(28)</label></formula><p>where N = I n ? 1 n 11 .</p><p>Criterion. We consider using square loss as training objective, i.e.,</p><formula xml:id="formula_34">L(f ) = m i=1 1 2 (f (X i , Q i , p i ) ? y i ) 2 .<label>(29)</label></formula><p>Algorithm. We consider using gradient descent to optimize the objective function. Let the initial parameter w 0 = 0. The update rule of w from step t to t + 1 can be described as:</p><formula xml:id="formula_35">w t+1 = w t ? ?? w L(f wt ),<label>(30)</label></formula><p>where ? is the learning rate. Theorem A.1. Under Assumption 1,2,3, for any &gt; 0 there exists constants C 1 , C 2 , such that for ? 1 &lt; C 1 , m &gt; C 2 , with probability 1 ? , the parameter w Vanilla t of vanilla model converges to the optimal parameter w Vanilla * linearly:</p><formula xml:id="formula_36">w Vanilla t ? w Vanilla * 2 ? O ? t 1 ,<label>(31)</label></formula><p>while the parameter w Shfit t of the shifted model converges to the optimal parameter w Shfit * linearly:</p><formula xml:id="formula_37">w Shift t ? w Shfit * 2 ? O ? t 2 ,<label>(32)</label></formula><p>where</p><formula xml:id="formula_38">1 &gt; ? 1 &gt; ? 2 ,<label>(33)</label></formula><p>which indicates the shifted model has a faster convergence rate.</p><p>Proof. We firstly reformulate the optimization problem in matrix form.</p><p>Notice that in our linear model, the representation and structure of a graph G i = {X i , Q i , p i , y i } can be encoded as a whole in a single vector, i.e., z Vanilla </p><formula xml:id="formula_39">i = X i Q i p i ? R d?1</formula><formula xml:id="formula_40">L(f w ) = 1 2 Z w ? y 2 2 ,<label>(34)</label></formula><p>where y = [y 1 , ? ? ? , y m ] ? R m?1 .</p><p>Then the gradient descent update can be explicitly writen as:</p><formula xml:id="formula_41">w t+1 = w t ? ? ZZ w t ? Zy (35) = (I d ? ?ZZ )w t + ?Zy,<label>(36)</label></formula><p>which converges to w * = ZZ ? Zy according to classic theory of least square problem <ref type="bibr" target="#b15">(Horn &amp; Johnson, 2012)</ref>, where ZZ ? is the Moore-Penrose inverse of ZZ .</p><p>By simultaneously subtracting w * in the update rule, we have</p><formula xml:id="formula_42">w t+1 ? w * = I d ? ?ZZ (w t ? w * ) .<label>(37)</label></formula><p>So the residual of w t is</p><formula xml:id="formula_43">w t ? w * = I d ? ?ZZ t w * (38) ? I d ? ?ZZ t w * .<label>(39)</label></formula><p>Let ? max (A) and ? min (A) be the maximal and mininal positive eigenvalues of A, respectively. Then the optimial learning rate (the largest learning rate that ensures I d ? ?ZZ is positive semidefinite) is ? = 1 ?max(ZZ ) . Under this learning rate we have the convergence rate following Eq. (39):</p><formula xml:id="formula_44">w t ? w * ? I d ? ?ZZ t w * (40) ? 1 ? ? min ZZ ? max (ZZ ) t w * .<label>(41)</label></formula><p>For now, we show that the convergence rate of the optimization problem with vanilla model depends on ?min(ZZ ) ?max(ZZ ) .</p><p>Follwing the same argument, we can show the convergence rate of the optimization problem with shifted model depends on ?min(Z Shift Z Shfit ) ?max(Z Shift Z Shfit ) . We then aim to bound this term, which we call effective condition number.</p><p>Similarly, we investigate the effective condition number for ZZ first, and the analysis of Z Shift Z Shift follows the same manner. As multiplying a constant does not affect the effective condition number, we first scale ZZ by 1 m and expand it as:</p><formula xml:id="formula_45">1 m ZZ = 1 m m i=1 z i z i ,<label>(42)</label></formula><p>which is the empirical estimation of the covariance matrix of the combined feature. By concentration inequality, we know this quantity is concentrated to the covariance matrix, i.e.,</p><formula xml:id="formula_46">E z zz = E X,Q,p XQp (XQp) = E X,Q XQ E pp (XQ) = E X,Q XQ(XQ) (By Assumption 1) = Y Y + E X,Q (XQ ? Y )(XQ ? Y )</formula><p>.</p><formula xml:id="formula_47">Noticing that 0 E X,Q (XQ ? Y )(XQ ? Y ) ? 1 I d by Assumption 2, and Y is full rank, we can conclude that ? max Y Y ? ? max E z zz ? ? max Y Y + ? 1 , and ? min Y Y ? ? min E z zz ? ? min Y Y + ? 1 by Weyl's inequality.</formula><p>By similar argument, we have that 1 m Z Shift Z Shift concentrates to</p><formula xml:id="formula_48">E z Shift z Shift z Shift =E X,Q (XQ)N 2 (XQ) =E X,Q (XQ)N (XQ) (N 2 = N ) =Y N Y + E X,Q (XQ ? Y )N (XQ ? Y ) .</formula><p>By Assumption 2, we have</p><formula xml:id="formula_49">0 =1 Y ?1 E z Shift z Shift z Shift =1 Y ?1 Y N Y + E X,Q (XQ ? Y )N (XQ ? Y ) =1 Y ?1 E X,Q (XQ ? Y )N (XQ ? Y ) , which means E X,Q (XQ ? Y )N (XQ ? Y ) has the same eigenspace as Y N Y with respect to eigen- value 0. Combining with 0 E X,Q (XQ ? Y )N (XQ ? Y ) ? 1 I d , we have ? max Y N Y ? ? max E z Shift z Shift z Shift ? ? max Y N Y + ? 1 , and ? min Y N Y ? ? min E z Shift z Shift z Shift ? ? min Y N Y + ? 1 .</formula><p>It remains to bound the finite sample error, i.e., 1 m ZZ ? E z zz 2 and 1 m Z Shift Z Shfit ? E z zz 2 . These bounds can be obtained by the following lemma: Lemma A.2 (Corollary 6.1 in <ref type="bibr" target="#b35">Wainwright (2019)</ref>). Let z 1 , ? ? ? , z m be i.i.d. zero-mean random vectors with covariance matrix ? such that z 2 ? ? b almost surely. Then for all ? &gt; 0, the sample covariance matrix? = 1</p><formula xml:id="formula_50">m m i=1 z i z i satisfies Pr ? ? ? 2 ? ? ? 2d exp ? ? 2 2b ( ? 2 + ?) .<label>(43)</label></formula><p>By this lemma, we further have Lemma A.3 (Bound on the sample covariance matrix). Let z 1 , ? ? ? , z m be i.i.d. zero-mean random vectors with covariance matrix ? such that z 2 ? ? b almost surely. Then with probability 1 ? , the sample covariance matrix</p><formula xml:id="formula_51">? = 1 m m i=1 z i z i satisfies ? ? ? 2 ? O log(1/ ) m ,<label>(44)</label></formula><p>where we hide constants b, ? 2 , d in the big-O notation and highlight the dependence on the number of samples m.</p><p>Combining with previous results, we conclude that:</p><formula xml:id="formula_52">? max Y Y ? O log(1/ ) m ?? max 1 m ZZ ?? max Y Y + ? 1 + O log(1/ ) m ; ? min Y Y ? O log(1/ ) m ?? min 1 m ZZ ?? min Y Y + ? 1 + O log(1/ ) m ; ? max Y N Y ? O log(1/ ) m ?? max 1 m Z Shift Z Shift ?? max Y N Y + ? 1 + O log(1/ ) m ? min Y N Y ? O log(1/ ) m ?? min 1 m Z Shift Z Shift ?? min Y N Y + ? 1 + O log(1/ ) m .</formula><p>By now, we have transfered the analysis of ZZ and Z Shift Z Shfit to the analysis of Y Y and Y N Y . And the positive eigenvalues of Y N Y is interlaced between the positive eigenvalues of Y Y by the same argument as Theorem 3.1.</p><formula xml:id="formula_53">Concretely, we have ? min Y Y ? ? min Y N Y ? ? max Y N Y ? ? max Y Y .</formula><p>Noticing that none of the eigenvectors of Y Y is orthogonal to 1, the first and last equalies can not be achieved,</p><formula xml:id="formula_54">so ? min Y Y &lt; ? min Y N Y ? ? max Y N Y &lt; ? max Y Y .</formula><p>Finally, we can conclude for small enough ? 1 and large enough m, with probability ,</p><formula xml:id="formula_55">? min 1 m ZZ ?? min Y Y + ? 1 + O log(1/ ) m &lt;? min Y N Y ? O log(1/ ) m ?? min 1 m Z Shift Z Shift ?? max 1 m Z Shift Z Shift ?? max Y N Y + ? 1 + O log(1/ ) m &lt;? max Y Y ? O log(1/ ) m ?? max 1 m ZZ . So ? 2 = 1 ? ? min Z Shift Z Shift ? max (Z Shift Z Shift ) &lt;? 1 = 1 ? ? min ZZ ? max (ZZ ) ,</formula><p>where ? 1 , ? 2 are the constants in the statement of the theorem. This inequality means the shifted model has better convergence speed by Eq. (41).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Proposition 4.1</head><p>Proof. For r-regular graph, A = r ? I n and Q GIN = r + 1 + ? (1) I n . Since H (0) is given by one-hot encodings of node degrees, the row of H (0) can be represented as c ? 1 where c = 1 for the r-th row and c = 0 for other rows. By the associative property of matrix multiplication, we only need to show H (0) Q GIN N = 0. This is because, for each row c ? 1 Q GIN N = c ? 1 (r + 1 + ? (1) )I n I n ? 1 n 11</p><formula xml:id="formula_56">(45) = c r + 1 + ? (1) 1 ? 1 ? 1 n 11 = 0.<label>(46)</label></formula><p>A.4. Proof of Proposition 4.2</p><p>Proof.</p><formula xml:id="formula_57">Q GIN N = (A + I n + ? (k) I n )N == (11 + ? (k)In )N = ? (k) N,<label>(47)</label></formula><p>A.5. Gradient of W (k)</p><p>We first calculate the gradient of W (k) when using normalization. Denote Z (k) = Norm W (k) H (k?1) Q and L as the loss. Then the gradient of L w.r.t. the weight matrix</p><formula xml:id="formula_58">W (k) is ?L ?W (k) = H (k?1) QN ? S ?L ?Z (k) ,<label>(48)</label></formula><p>where ? represents the Kronecker product, and thus H (k?1) QN ? S is an operator on matrices.</p><p>Analogously, the gradient of W (k) without normalization consists a H (k?1) Q ? I n term. As suggested by Theorem 3.1, QN has a smoother distribution of spectrum than Q, so that the gradient of W (k) with normalization enjoys better optimization curvature than that without normalizaiton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>Detailed of the datasets used in our experiments are presented in this section. Brief statistics of the datasets are summarized in <ref type="table" target="#tab_7">Table 3</ref>. Those information can be also found in <ref type="bibr">Xu et al. (2019)</ref> and <ref type="bibr" target="#b16">Hu et al. (2020)</ref>.</p><p>Bioinformatics datasets. PROTEINS is a dataset where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn. NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 discrete labels. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels. PTC is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels.</p><p>Social networks datasets. IMDB-BINARY is a movie collaboration dataset. Each graph corresponds to an egonetwork for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn betwen two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre graph it is derived from. REDDIT-BINARY is a balanced dataset where each graph corresponds to an online discussion thread and nodes correspond to users. An edge was drawn between two nodes if at least one of them responded to another's comment. The task is to classify each graph to a community or a subreddit it belongs to. COLLAB is a scientific collaboration dataset, derived from 3 public collaboration datasets, namely, High Energy Physics, Condensed Matter Physics and Astro Physics. Each graph corresponds to an ego-network of different researchers from each field. The task is to classify each graph to a field the corresponding researcher belongs to.</p><p>Large-scale Open Graph Benchmark: ogbg-molhiv.</p><p>Ogbg-molhiv is a molecular property prediction dataset, which is adopted from the the MOLECULENET <ref type="bibr" target="#b37">(Wu et al., 2017)</ref>. Each graph represents a molecule, where nodes are atoms and edges are chemical bonds. Both nodes and edges have associated diverse features. Node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features. Edge features are 3-dimensional, containing bond type, stereochemistry as well as an additional bond feature indicating whether the bond is conjugated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Experimental Setup</head><p>Network architecture. For the medium-scale bioinformatics and social network datasets, we use 5-layer GIN/GCN with a linear output head for prediction followed <ref type="bibr">Xu et al. (2019)</ref> with residual connection. The hidden dimension of GIN/GCN is set to be 64. For the large-scale ogbg-molhiv dataset, we also use 5-layer GIN/GCN <ref type="bibr">(Xu et al., 2019)</ref> architecture with residual connection. Following <ref type="bibr" target="#b16">Hu et al. (2020)</ref>, we set the hidden dimension as 300.  <ref type="bibr" target="#b18">(Ivanov &amp; Burnaev, 2018)</ref>. We report the accuracies reported in the original paper <ref type="bibr">(Xu et al., 2019)</ref>. For the large-scale ogbg-molhiv dataset, we use the baselines in <ref type="bibr" target="#b16">Hu et al. (2020)</ref>, including the Graph-agnostic MLP model, GCN <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> and <ref type="bibr">GIN (Xu et al., 2019)</ref>. We also report the roc-auc values reported in the original paper <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>.</p><p>Hyper-parameter configurations. We use Adam <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> optimizer with a linear learning rate decay schedule. We follow previous work <ref type="bibr">Xu et al. (2019)</ref> and <ref type="bibr" target="#b16">Hu et al. (2020)</ref> to use hyper-parameter search (grid search) to select the best hyper-parameter based on validation performance. In particular, we select the batch size ? {64, 128}, the dropout ratio ? {0, 0.5}, weight decay ? {5e ? 2, 5e ? 3, 5e ? 4, 5e ? 5} ? {0.0}, the learning rate ? {1e ? 4, 1e ? 3, 1e ? 2}. For the drawing of the training curves in <ref type="figure">Figure 2</ref>, for simplicity, we set batch size to be 128, dropout ratio to be 0.5, weight decay to be 0.0, learning rate to be 1e-2, and train the models for 400 epochs for all settings.</p><p>Evaluation. Using the chosen hyper-parameter, we report the averaged test performance over different random seeds (or cross-validation). In detail, for the medium-scale datasets, following <ref type="bibr">Xu et al. (2019)</ref>, we perform a 10-fold cross-validation as these datasets do not have a clear trainvalidate-test splitting format. The mean and standard deviation of the validation accuracies across the 10 folds are reported. For the ogbg-molhiv dataset, we follow the official setting <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>. We repeat the training process with 10 different random seeds.</p><p>For all experiments, we select the best model checkpoint with the best validation accuracy and record the corresponding test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Visualization of the singular value distributions</head><p>As stated in Theorem 3.1, the shift operation N serves as a preconditioner of Q which makes the singular value distribution of Q smoother. To check the improvements, we sample graphs from 6 median-scale datasets (PROTEINS, NCI1, MUTAG, PTC, IMDB-BINARY, COLLAB) for visualization, as in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Visualization of noise in the batch statistics</head><p>We show the noise of the batch statistics on the PROTEINS task in the main body. Here we provide more experiment details and results.</p><p>For graph tasks (PROTEINS, PTC, NCI1, MUTAG, IMDB-BINARY datasets), we train a 5-layer GIN with BatchNorm as in <ref type="bibr">Xu et al. (2019)</ref> and the number of sub-layers in MLP is set to 2. For image task (CIFAR10 dataset), we train a ResNet18 <ref type="bibr" target="#b13">(He et al., 2016)</ref>. Note that for a 5-layer GIN model, it has four graph convolution layers (indexed from 0 to 3) and each graph convolution layer has two BatchNorm layers; for a ResNet18 model, except for the first 3?3 convolution layer and the final linear prediction layer, it has four basic layers (indexed from 0 to 3) and each layer consists of two basic blocks (each block has two BatchNorm layers). For image task, we set the batch size as 128, epoch as 100, learning rate as 0.1 with momentum 0.9 and weight decay as 5e-4. For graph tasks, we follow the setting of <ref type="figure">Figure 2</ref> (described in Appendix C).</p><p>The visualization of the noise in the batch statistics is obtained as follows. We first train the models and dump the model checkpoints at the end of each epoch; Then we randomly sample one feature dimension and fix it. For each model checkpoint, we feed different batches to the model and record the maximum/minimum batch-level statistics (mean and standard deviation) of the feature dimension across different batches. We also calculate dataset-level statistics.</p><p>As <ref type="figure">Figure 4</ref> in the main body, pink line denotes the dataset-level statistics, and green/blue line denotes the maximum/minimum value of the batch-level statistics respectively. First, we provide more results on PTC, NCI1, MU-TAG, IMDB-BINARY tasks, as in <ref type="figure">Figure 8</ref>. We visualize the statistics from the first (layer-0) and the last (layer-3) BatchNorm layers in GIN for comparison. Second, we further visualize the statistics from different BatchNorm layers (layer 0 to layer 3) in GIN on PROTEINS and ResNet18 in CIFAR10, as in <ref type="figure">Figure 9</ref>. Third, we conduct experiments to investigate the influence of the batch size. We visualize the statistics from BatchNorm layers under different settings of batch sizes <ref type="bibr">[8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64]</ref>, as in <ref type="figure">Figure 10</ref>. We can see that the observations are consistent and the batch statistics on graph data are noisy, as in <ref type="figure">Figure 4</ref> in the main body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Training Curves on GCN</head><p>As <ref type="figure">Figure 2</ref> in the main body, we train GCNs with different normalization methods (GraphNorm, InstanceNorm, Batch-Norm and LayerNorm) and GCN without normalization in graph classification tasks and plot the training curves in <ref type="figure">Figure 6</ref>. It is obvious that the GraphNorm also enjoys the fastest convergence on all tasks. Remarkably, GCN with InstanceNorm even underperforms GCNs with other normalizations, while our GraphNorm with learnable shift significantly boosts the training upon InstanceNorm and achieves the fastest convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Further Results of Ablation Study</head><p>BatchNorm with learnable shift. We conduct experiments on BatchNorm to investigate whether simply introducing a learnable shift can already improve the existing normalization methods without concrete motivation of overcoming expressiveness degradation. Specifically, we equip Batch-Norm with a similar learnable shift (?-BatchNorm for short) as GraphNorm and evaluate its performance. As shown in <ref type="figure">Figure 12</ref>, the ?-BatchNorm cannot outperform the Batch-Norm on the three datasets. Moreover, as shown in <ref type="figure">Figure 5</ref> in the main body, the learnable shift significantly improve upon GraphNorm on IMDB-BINARY dataset, while it cannot further improve upon BatchNorm, which suggests the introduction of learnable shift in GraphNorm is critical.</p><p>BatchNorm with running statistics. We study the variant of BatchNorm which uses running statistics (MS-BatchNorm for short) to replace the batch-level mean and standard deviation (similar idea is also proposed in Yan et al. <ref type="formula" target="#formula_1">(2019)</ref>). At first glance, this method may seem to be able to mitigate the problem of large batch noise. However, the running statistics change a lot during training, and using running statistics disables the model to back-propagate the gradients through mean and standard deviation. Thus, we also train GIN with BatchNorm which stops the backpropagation of the graidients through mean and standard deviation (DT-BatchNorm for short). As shown in <ref type="figure">Figure  12</ref>, both the MS-BatchNorm and DT-BatchNorm underperform the BatchNorm by a large margin, which shows that the problem of the heavy batch noise cannot be mitigated by simply using the running statistics.</p><p>The effect of batch size. We further compare the Graph-Norm and BatchNorm with different batch sizes <ref type="bibr">(8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref>. As shown in <ref type="figure">Figure 11</ref>, our GraphNorm consistently outperforms the BatchNorm on all the settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other Related Works</head><p>Due to space limitations, we add some more related works on normalization and graph neural networks here. Zou et al.  . Batch-level statistics are noisy for GNNs of different depth. We plot the batch-level mean/standard deviation and datasetlevel mean/standard deviation of different BatchNorm layers (from layer 0 to layer 3) in different checkpoints. We use a five-layer GIN on PROTEINS and ResNet18 on CIFAR10 for comparison.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for vanilla model in Eq. (27), and z Shift i = X i Q i N p i ? R d?1 for shifted model in Eq. (28). We call z i and z Shift i "combined features". Let Z Vanilla = z Vanilla 1 , ? ? ? , z Vanilla m ? R d?m and Z Shift = z Shift 1 , ? ? ? , z Shift m ? R d?m be the matrix of combined features of valinna linear model and shifted linear model respectively. For clearness of the proof, we may abuse the notations and use Z to represent Z Vanilla . Then the objective in Eq. (29) for vanilla linear model can be reformulated as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Training performance of GCN with different normalization methods and GCN without normalization in graph classification tasks. Singular value distribution of Q and QN . Graph samples from PROTEINS, NCI1, MUTAG, PTC, IMDB-BINARY, COLLAB are presented. Batch-level statistics are noisy for GNNs (Examples from PTC, NCI1, MUTAG, IMDB-BINARY datasets). We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of the first (layer 0) and the last (layer 3) BatchNorm layers in different checkpoints. GIN with 5 layers is employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Training performance of GIN/GCN with GraphNorm and BatchNorm with batch sizes of (8, 16, 32, 64) on PROTEINS and REDDITBINARY datasets. Training performance of GIN with GraphNorm and variant BatchNorms (?-BatchNorm, MS-BatchNorm and DT-BatchNorm) on PROTEINS, PTC and IMDB-BINARY datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>PROTEINS (Tree-type Graphs) IMDBBINARY (Regular-type Graphs)</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell>PROTEINS</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell>IMDBBINARY</cell></row><row><cell>Training Accuracy</cell><cell>0.6 0.8</cell><cell></cell><cell cols="2">GraphNorm+GIN GraphNorm+GCN InstanceNorm+GIN</cell><cell>Training Accuracy</cell><cell>0.6 0.8</cell><cell></cell><cell>GraphNorm+GIN GraphNorm+GCN InstanceNorm+GIN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">InstanceNorm+GCN</cell><cell></cell><cell></cell><cell></cell><cell>InstanceNorm+GCN</cell></row><row><cell></cell><cell>0.4</cell><cell>0</cell><cell>1500</cell><cell>3000</cell><cell></cell><cell>0.4</cell><cell>0</cell><cell>1500</cell><cell>3000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Iterations</cell></row><row><cell cols="9">Figure 5. Comparison of GraphNorm and InstanceNorm on</cell></row><row><cell cols="9">different types of graphs. Top: Sampled graphs with different</cell></row><row><cell cols="9">topological structures. Bottom: Training curves of GIN/GCN</cell></row><row><cell cols="7">using GraphNorm and InstanceNorm.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Settings. We use eight popularly used benchmark datasets of different scales in the experiments(Yanardag &amp; Vishwanathan, 2015; Xu et al., 2019), including four mediumscale bioinformatics datasets (MUTAG, PTC, PROTEINS, NCI1), three medium-scale social network datasets (IMDB-BINARY, COLLAB, REDDIT-BINARY), and one largescale bioinformatics dataset ogbg-molhiv, which is recently released on Open Graph Benchmark (OGB)<ref type="bibr" target="#b16">(Hu et al., 2020)</ref>. Dataset statistics are summarized inTable 1. We use two typical graph neural networks GIN (Xu et al., 2019) and GCN<ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> for our evaluations. Specifically, we use a five-layer GCN/GIN. For GIN, the number of sub-layers in MLP is set to 2. Normalization is applied to each layer. To aggregate global features on top of the network, we use SUM readout for MUTAG, PTC, PRO-TEINS and NCI1 datasets, and use MEAN readout for other datasets, as inXu et al. (2019). Details of the experimentalTable 1. Test performance of GIN/GCN with various normalization methods on graph classification tasks.</figDesc><table><row><cell>Datasets</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>IMDB-B</cell><cell>RDT-B</cell><cell>COLLAB</cell></row><row><cell># graphs</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>1000</cell><cell>2000</cell><cell>5000</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Avg # nodes</cell><cell>17.9</cell><cell>25.5</cell><cell>39.1</cell><cell>29.8</cell><cell>19.8</cell><cell>429.6</cell><cell>74.5</cell></row><row><cell cols="3">WL SUBTREE (SHERVASHIDZE ET AL., 2011) 90.4 ? 5.7 59.9 ? 4.3</cell><cell>75.0 ? 3.1</cell><cell cols="4">86.0 ? 1.8 73.8 ? 3.9 81.0 ? 3.1 78.9 ? 1.9</cell></row><row><cell>DCNN (ATWOOD &amp; TOWSLEY, 2016)</cell><cell>67.0</cell><cell>56.6</cell><cell>61.3</cell><cell>62.6</cell><cell>49.1</cell><cell>-</cell><cell>52.1</cell></row><row><cell>DGCNN (ZHANG ET AL., 2018)</cell><cell>85.8</cell><cell>58.6</cell><cell>75.5</cell><cell>74.4</cell><cell>70.0</cell><cell>-</cell><cell>73.7</cell></row><row><cell>AWL (IVANOV &amp; BURNAEV, 2018)</cell><cell>87.9 ? 9.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">74.5 ? 5.9 87.9 ? 2.5 73.9 ? 1.9</cell></row><row><cell>GIN+LAYERNORM</cell><cell cols="2">82.4 ? 6.4 62.8 ? 9.3</cell><cell>76.2 ? 3.0</cell><cell cols="4">78.3 ? 1,7 74.5 ? 4,4 82.8 ? 7.7 80.1 ? 0.8</cell></row><row><cell>GIN+BATCHNORM ((XU ET AL., 2019))</cell><cell cols="2">89.4 ? 5.6 64.6 ? 7.0</cell><cell>76.2 ? 2.8</cell><cell cols="3">82.7 ? 1.7 75.1 ? 5.1 92.4 ? 2.5</cell><cell>80.2 ? 1.9</cell></row><row><cell>GIN+INSTANCENORM</cell><cell cols="2">90.5 ? 7.8 64.7 ? 5.9</cell><cell>76.5 ? 3.9</cell><cell cols="4">81.2 ? 1.8 74.8 ? 5.0 93.2 ? 1.7 80.0 ? 2.1</cell></row><row><cell>GIN+GraphNorm</cell><cell cols="2">91.6 ? 6.5 64.9 ? 7.5</cell><cell cols="5">77.4 ? 4.9 81.4 ? 2.4 76.0 ? 3.7 93.5 ? 2.1 80.2 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Test performance on OGB.</figDesc><table><row><cell>Datasets</cell><cell>OGBG-MOLHIV</cell></row><row><cell># graphs</cell><cell>41,127</cell></row><row><cell># classes</cell><cell>2</cell></row><row><cell>Avg # nodes</cell><cell>25.5</cell></row><row><cell>GCN (Hu et al., 2020)</cell><cell>76.06 ? 0.97</cell></row><row><cell>GIN (Hu et al., 2020)</cell><cell>75.58 ? 1.40</cell></row><row><cell>GCN+LayerNorm</cell><cell>75.04 ? 0.48</cell></row><row><cell>GCN+BatchNorm</cell><cell>76.22 ? 0.95</cell></row><row><cell>GCN+InstanceNorm</cell><cell>78.18 ? 0.42</cell></row><row><cell>GCN+GraphNorm</cell><cell>78.30 ? 0.69</cell></row><row><cell>GIN+LayerNorm</cell><cell>74.79 ? 0.92</cell></row><row><cell>GIN+BatchNorm</cell><cell>76.61 ? 0.97</cell></row><row><cell>GIN+InstanceNorm</cell><cell>77.54 ? 1.27</cell></row><row><cell>GIN+GraphNorm</cell><cell>77.73 ? 1.29</cell></row><row><cell cols="2">settings are presented in Appendix C.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, Graph-Norm enjoys the fastest convergence on all tasks. Compared to BatchNorm used in Xu et al. (2019), GraphNorm converges in roughly 5000/500 iterations on NCI1 and PTC datasets, while the model using BatchNorm does not even converge in 10000/1000 iterations. Remarkably, though In-stanceNorm does not outperform other normalization methods on IMDB-BINARY, GraphNorm with learnable shift significantly boosts the training upon InstanceNorm and achieves the fastest convergence. We also validate the test</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pp. 3844-3852, 2016. Demmel, J. W. Applied numerical linear algebra, volume 56. Siam, 1997. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685. PMLR, 2019a. Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-5124, 2017. Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems, pp. 901-909, 2016. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y. Do transformers really perform bad for graph representation?, 2021.Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural networks for web-scale recommender systems. In kdd, pp.</figDesc><table><row><cell>A. Proofs</cell><cell></cell></row><row><cell>A.1. Proof of Theorem 3.1</cell><cell></cell></row><row><cell>We first introduce the Cauchy interlace theorem:</cell><cell></cell></row><row><cell>Lemma A.1 (Cauchy interlace theorem (Theorem 4.3.17</cell><cell></cell></row><row><cell>in Horn &amp; Johnson</cell><cell></cell></row><row><cell>974-983, 2018.</cell><cell></cell></row><row><cell>Zhang, M., Cui, Z., Neumann, M., and Chen, Y. An end-</cell><cell></cell></row><row><cell>to-end deep learning architecture for graph classification.</cell><cell>Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip,</cell></row><row><cell>pp. 4438-4445, 2018.</cell><cell>S. Y. A comprehensive survey on graph neural networks.</cell></row><row><cell></cell><cell>IEEE Transactions on Neural Networks and Learning</cell></row><row><cell>Zhang, Z., Cui, P., and Zhu, W. Deep learning on graphs:</cell><cell>Systems, 2020.</cell></row><row><cell>A survey. IEEE Transactions on Knowledge and Data</cell><cell></cell></row><row><cell>Engineering, 2020.</cell><cell>Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,</cell></row><row><cell></cell><cell>C., Zhang, H., Lan, Y., Wang, L., and Liu, T.-Y. On</cell></row><row><cell>Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth-</cell><cell>layer normalization in the transformer architecture. arXiv</cell></row><row><cell>ing in gnns. In International Conference on Learning</cell><cell>preprint arXiv:2002.04745, 2020.</cell></row><row><cell>Representations, 2020. URL https://openreview.</cell><cell></cell></row><row><cell>net/forum?id=rkecl1rtwB.</cell><cell></cell></row><row><cell>Feng,</cell><cell></cell></row><row><cell>J. Effective training strategies for deep graph neural</cell><cell></cell></row><row><cell>networks, 2020a.</cell><cell>Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K.,</cell></row><row><cell>Zhou, K., Huang, X., Li, Y., Zha, D., Chen, R., and Hu, X.</cell><cell>and Jegelka, S. What can neural networks reason about?</cell></row><row><cell>Towards deeper graph neural networks with differentiable</cell><cell>In International Conference on Learning Representations,</cell></row><row><cell>group normalization. arXiv preprint arXiv:2006.06972,</cell><cell>2020. URL https://openreview.net/forum?</cell></row><row><cell>2020b.</cell><cell>id=rJxbJeHFPS.</cell></row><row><cell>Zou, D., Hu, Z., Wang, Y., Jiang, S., Sun, Y., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pp. 11249-</cell><cell>Xu, K., Zhang, M., Li, J., Du, S. S., Kawarabayashi, K.-I., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021.</cell></row><row><cell>11259, 2019.</cell><cell>Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y., and Sun, J.</cell></row><row><cell>Zou, D., Cao, Y., Zhou, D., and Gu, Q. Gradient descent op-timizes over-parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.</cell><cell>Towards stabilizing batch statistics in backward propaga-tion of batch normalization. In International Conference on Learning Representations, 2019.</cell></row><row><cell></cell><cell>Yanardag, P. and Vishwanathan, S. Deep graph kernels.</cell></row><row><cell></cell><cell>In Proceedings of the 21th ACM SIGKDD International</cell></row><row><cell></cell><cell>Conference on Knowledge Discovery and Data Mining,</cell></row><row><cell></cell><cell>pp. 1365-1374, 2015.</cell></row><row><cell></cell><cell>Yang, C., Wang, R., Yao, S., Liu, S., and Abdelzaher, T.</cell></row><row><cell></cell><cell>Revisiting" over-smoothing" in deep gcns. arXiv preprint</cell></row><row><cell></cell><cell>arXiv:2003.13663, 2020.</cell></row></table><note>Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Rep- resentations, 2018. Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- ergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020. Li, Z. and Arora, S. An exponential learning rate sched- ule for deep learning. arXiv preprint arXiv:1910.07454, 2019. Loukas, A. How hard is to distinguish graphs with graph neural networks? In Advances in neural information processing systems, 2020. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec- tral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=B1QRgziT-.Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for train- ing gans. In Advances in neural information processing systems, pp. 2234-2242, 2016.Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, pp. 5453-5462, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https:// openreview.net/forum?id=ryGs6iA5Km.Yi, K. M., Trulls, E., Ono, Y., Lepetit, V., Salzmann, M., and Fua, P. Learning to find good correspondences. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2666-2674, 2018.Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. Zhou, K., Dong, Y., Lee, W. S., Hooi, B., Xu, H., and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Summary of statistics of benchmark datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="8">MUTAG PTC PROTEINS NCI1 IMDB-B RDT-B COLLAB OGBG-MOLHIV</cell></row><row><cell># graphs</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>1000</cell><cell>2000</cell><cell>5000</cell><cell>41127</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Avg # nodes</cell><cell>17.9</cell><cell>25.5</cell><cell>39.1</cell><cell>29.8</cell><cell>19.8</cell><cell>429.6</cell><cell>74.5</cell><cell>25.5</cell></row><row><cell>Avg # edges</cell><cell>57.5</cell><cell>72.5</cell><cell>184.7</cell><cell>94.5</cell><cell>212.8</cell><cell>1425.1</cell><cell>4989.5</cell><cell>27.5</cell></row><row><cell>Avg # degrees</cell><cell>3.2</cell><cell>3.0</cell><cell>4.7</cell><cell>3.1</cell><cell>10.7</cell><cell>3.3</cell><cell>66.9</cell><cell>2.1</cell></row><row><cell cols="4">Baselines. For the medium-scale bioinformatics and so-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">cial network datasets, we compare several competitive base-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">lines as in Xu et al. (2019), including the WL subtree kernel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">model (Shervashidze et al., 2011), diffusion-convolutional</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">neural networks (DCNN) (Atwood &amp; Towsley, 2016), Deep</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Graph CNN (DGCNN) (Zhang et al., 2018) and Anonymous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Walk Embeddings (AWL)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>We analyze more general aggregation schemes, e.g., those in GIN, and understand the effect of the shift through the distribution of spectrum. Some concurrent and independent works(Li et al., 2020; Chen et al., 2020;  Zhou et al., 2020b;a) also seek to incorporate normalization schemes in GNNs, which show the urgency of developing normalization schemes for GNNs. In this paper, we provide several insights on how to design a proper normalization for GNNs. Before the surge of deep learning, there are also many classic architectures of GNNs such as Scarselli et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>used normalization to stabilize the training process</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of GNNs. Zhao &amp; Akoglu (2020) introduced PAIRNORM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>to prevent node embeddings from over-smoothing on the</cell></row><row><cell>7UDLQLQJ$FFXUDF\ 7UDLQLQJ$FFXUDF\</cell><cell>,WHUDWLRQV 3527(,16 ,WHUDWLRQV 5('',7%,1$5&lt;</cell><cell>7UDLQLQJ$FFXUDF\ 7UDLQLQJ$FFXUDF\</cell><cell>,WHUDWLRQV 1&amp;, ,WHUDWLRQV &amp;2//$%</cell><cell>node classification task. Our GraphNorm focuses on ac-celerating the training and has faster convergence speed on graph classification tasks. Yang et al. (2020) interpreted the effect of mean subtraction on GCN as approximating ,WHUDWLRQV 7UDLQLQJ$FFXUDF\ 37&amp; ,WHUDWLRQV 087$* 7UDLQLQJ$FFXUDF\ ,WHUDWLRQV the Fiedler vector. (2008); Bruna et al. (2013); Defferrard et al. (2016) that are 7UDLQLQJ$FFXUDF\</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>not mentioned in the main body of the paper. We refer the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>readers to Zhou et al. (2018); Wu et al. (2020); Zhang et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(2020) for surveys of graph representation learning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Batch-level statistics are noisy for GNNs of different batch sizes. We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of different BatchNorm layers (layer 0 and layer 3) in different checkpoints. Specifically, different batch sizes(8, 16, 32, 64)  are chosed for comparison. GIN with 5 layers is employed.</figDesc><table><row><cell>0HDQYDOXH 7UDLQLQJ$FFXUDF\</cell><cell>3527(,16%6OD\HU 3527(,16%6</cell><cell>0HDQYDOXH 7UDLQLQJ$FFXUDF\</cell><cell>3527(,16%6OD\HU 3527(,16%6</cell><cell>6WGYDOXH 7UDLQLQJ$FFXUDF\</cell><cell>3527(,16%6OD\HU 3527(,16%6</cell><cell>6WGYDOXH 7UDLQLQJ$FFXUDF\</cell><cell>3527(,16%6OD\HU 3527(,16%6</cell></row><row><cell cols="4">(SRFKV (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU GDWDVHWOHYHO 0HDQYDOXH 3527(,16%6OD\HU (SRFKV (SRFKV (SRFKV 0HDQYDOXH 3527(,16%6OD\HU (SRFKV 0HDQYDOXH 3527(,16%6OD\HU EDWFKOHYHO PD[ EDWFKOHYHO PLQ ,WHUDWLRQV ,WHUDWLRQV 5('',7%,1$5&lt;%6 ,WHUDWLRQV 7UDLQLQJ$FFXUDF\ 5('',7%,1$5&lt;%6 *UDSK1RUP*,1 Figure 10. ,WHUDWLRQV 0HDQYDOXH 0HDQYDOXH 0HDQYDOXH 7UDLQLQJ$FFXUDF\ *UDSK1RUP*&amp;1</cell><cell cols="4">(SRFKV (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU GDWDVHWOHYHO 6WGYDOXH 6WGYDOXH 6WGYDOXH EDWFKOHYHO PD[ EDWFKOHYHO PLQ (SRFKV (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU (SRFKV 3527(,16%6OD\HU ,WHUDWLRQV ,WHUDWLRQV ,WHUDWLRQV 5('',7%,1$5&lt;%6 6WGYDOXH 6WGYDOXH 6WGYDOXH 7UDLQLQJ$FFXUDF\ ,WHUDWLRQV 7UDLQLQJ$FFXUDF\ 5('',7%,1$5&lt;%6 %DWFK1RUP*,1 %DWFK1RUP*&amp;1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The graph size normalization in the preliminary version of<ref type="bibr" target="#b8">Dwivedi et al. (2020)</ref> does not show significant improvement on the training and test performance, so we do not report it.performance and report the test accuracy inTable 1,2. The results show that GraphNorm also improves the generalization on most benchmarks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Mozhi Zhang and Ruosong Wang for helpful suggestions on the paper; Zhiyuan Li and Kaifeng Lyu for helpful discussion on the literature of normalization methods; and Prof. Yang Yuan for support of computational resources. This work was supported by National Key R&amp;D Program of China (2018YFB1402600), Key-Area Research and Development Program of Guangdong Province (No. 2019B121204008), BJNSF (L172037), Beijing Academy of Artificial Intelligence, Project 2020BD006 supported by PKU-Baidu Fund, NSF CAREER award (1553284) and NSF III (1900933).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convergence analysis of gradient descent for deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Theoretical analysis of auto rate-tuning by batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03981</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of preconditioned iterative methods for linear systems of algebraic equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Axelsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="187" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2160" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7092" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neymeyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="806" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4083" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The vapnikchervonenkis dimension of graph and recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powernorm</surname></persName>
		</author>
		<title level="m">Rethinking batch normalization in transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bloom-Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acne: Attentive context normalization for robust permutation-equivariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11286" to="11295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">High-dimensional statistics: A nonasymptotic viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moleculenet</surname></persName>
		</author>
		<idno>abs/1703.00564</idno>
		<ptr target="http://arxiv.org/abs/1703.00564" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
