<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Logic Rules for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Ru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
							<email>sunchangzhi@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
							<email>fengjiangtao@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
							<email>lqiu@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<email>lilei@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Bytedance</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Logic Rules for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. Lo-giRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectationmaximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that LogiRE significantly outperforms several strong baselines in terms of relation performance (?1.8 F1 score) and logical consistency (over 3.3 logic score). Our code is available at https://github. com/rudongyu/LogiRE. * corresponding authors. ? Work is done while at ByteDance. [1] Britain's Prince Harry is engaged to his US partner Meghan Markle. ... [2] Harry spent 10 years in the army and has this year, with his elder brother William, ... [3] The last major royal wedding took place In 2011, when Kate Middleton and Prince William were married. Entities: UK, Harry, William, Kate Relations: royalty_of(Harry, UK), sibling_of(William, Harry), spouse_of(Kate, William), royalty_of(Kate, UK) ... Rule: Figure 1: An example of relation identification by utilizing rules. The three labeled sentences describe the relations royalty_of (Harry,UK), sib-ling_of (William,Harry), and spouse_of(Kate,William), respectively. The identification of the relation roy-alty_of (Kate,UK) requires the synthesis of information in three sentences. It can be easily derived from the demonstrated rule and the other three relations.</p><p>implicitly captures long-range dependencies. According to the input structure, we can divide the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model.</p><p>The sequence-based model first leverages different sequence encoder (e.g., BERT (Devlin et al.,  2019), RoBERTa (Liu et al., 2019)) to obtain token representations, and then computes relation representations by various pooling operations, e.g., average pooling <ref type="bibr" target="#b28">(Yao et al., 2019;</ref><ref type="bibr" target="#b27">Xu et al., 2021)</ref>, attentive pooling <ref type="bibr" target="#b35">(Zhou et al., 2021)</ref>. To further capture long-range dependencies, graph-based models are proposed. By constructing a graph, words or entities that are far away can become neighbor nodes. On top of the sequence encoder, the graph encoder (e.g., GNN) can aggregate information from all neighbors, thus capturing longer dependencies. Various forms of graphs are proposed, including dependency tree (Peng et al., 2017; Zhang arXiv:2111.05407v1 [cs.CL] 9 Nov 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting relations from a document has attracted significant research attention in information extraction (IE). Recently, instead of focusing on sentencelevel <ref type="bibr" target="#b14">(Socher et al., 2012;</ref><ref type="bibr" target="#b4">dos Santos et al., 2015;</ref><ref type="bibr" target="#b5">Han et al., 2018;</ref><ref type="bibr">Wang et al., 2021a,b)</ref>, researchers have turned to modeling directly at the document level <ref type="bibr" target="#b22">(Wang et al., 2019;</ref><ref type="bibr" target="#b29">Ye et al., 2020;</ref><ref type="bibr" target="#b35">Zhou et al., 2021)</ref>, which provides longer context and requires more complex reasoning. Early efforts focus mainly on learning a powerful relation (i.e., entity pair) representation, which et al., 2018), co-reference graph <ref type="bibr" target="#b13">(Sahu et al., 2019)</ref>, mention-entity graph <ref type="bibr" target="#b31">Zeng et al., 2020)</ref>, entity-relation bipartite graph  and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious "black boxes".</p><p>Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in <ref type="figure">Fig. 1</ref>, the identification of royalty_of(Kate,UK) requires information in all three sentences. The demonstrated logic rule can be applied to directly obtain this relation from the three relations locally extracted in each sentence. Reasoning over rules bypasses the difficulty of capturing long-range dependencies and interprets the result with intrinsic correlations. If the model could automatically learn rules and use them to make predictions, then we would get better relation extraction performance and enjoy more interpretation.</p><p>In this paper, we propose LogiRE, a novel probabilistic model modeling intrinsic interactions among relations by logic rules. Inspired by RNN-Logic <ref type="bibr" target="#b11">(Qu et al., 2021)</ref>, we treat logic rules as latent variables. Specifically, LogiRE consists of a rule generator and a relation extractor, which are simultaneously trained to enhance each other. The rule generator provides logic rules that are used by the relation extractor for prediction, and the relation extractor provides some supervision signals to guide the optimization of the rule generator, which significantly reduces the search space. In addition, the proposed relation extractor is model agnostic, so it can be used as a plug-and-play technique for any existing relation extractors. Those two modules can be efficiently optimized with the EM algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies between entities and output relations in a document and enjoy better interpretation. Our main contributions are listed below:</p><p>? We propose a novel probabilistic model for relation extraction by learning logic rules. The model can explicitly capture dependencies between entities and output relations, while enjoy better interpretation.</p><p>? We propose an efficient iterative-based method to optimize LogiRE based on the EM algorithm.</p><p>? Empirical results show that LogiRE significantly outperforms several strong baselines in terms of relation performance (?1.8 F1 score) and logical consistency (over 3.3 logic score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For document-level relation extraction, prior efforts on capturing long-range dependencies mainly focused on two directions: pursuing stronger sequence representation <ref type="bibr" target="#b9">(Nguyen and Verspoor, 2018;</ref><ref type="bibr" target="#b19">Verga et al., 2018;</ref><ref type="bibr" target="#b34">Zheng et al., 2018)</ref> or including prior for interactions among entities as graphs . For more powerful representations, they introduced pre-trained language models <ref type="bibr" target="#b22">(Wang et al., 2019;</ref><ref type="bibr" target="#b29">Ye et al., 2020)</ref>, leveraged attentions for context pooling <ref type="bibr" target="#b35">(Zhou et al., 2021)</ref>, or integrated the scattered information according to a hierarchical level <ref type="bibr" target="#b18">(Tang et al., 2020)</ref>. Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence , mentions of the same entities <ref type="bibr" target="#b31">Zeng et al., 2020)</ref>, etc. <ref type="bibr" target="#b8">Nan et al. (2020)</ref>; <ref type="bibr" target="#b27">Xu et al. (2021)</ref> directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model <ref type="bibr" target="#b0">(Andor et al., 2016;</ref>. In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations.</p><p>Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them <ref type="bibr" target="#b12">(Qu and Tang, 2019;</ref> concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems <ref type="bibr" target="#b6">(Hu et al., 2016;</ref><ref type="bibr" target="#b21">Wang and Poon, 2018)</ref> combined logic rules and neural networks to benefit from regularization on deep learning approaches. These efforts demonstrated the effectiveness of integrating neural networks with logical reasoning. Despite doc-RE providing a suitable scenario for logical reasoning (with relations serving as predicates and entities as variables), and a relation extractor . For a given document D and a query triple , we treat the required logic rules as latent variables , aiming to identify the corresponding truth value . During inference, we sample from the rule generator for the latent rule set and use the relation extractor to predict given the rules. The overall objective (maximizing the likelihood) is optimized by the EM algorithm. In the E-step, we estimate the approximate posterior ( ); In the M-step, we maximize a lower bound of the likelihood w.r.t. , . no existing work attempted to learn and utilize rules in this field. Using hand-crafted rules, ; <ref type="bibr" target="#b26">Wu et al. (2020)</ref> achieved great success on sent-level information extraction tasks. However, the rules were predefined and limited to lowlevel operations, restricting their applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe the proposed method LogiRE that learns logic rules for document-level relation extraction. We first define the task of document-level relation extraction and logic rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level Relation Extraction</head><p>Given a set of entities E with mentions scattered in a document D, we aim to extract a set of relations R. A relation is a triple (?, , ) ? R (also denoted by (?, )), where ? ? E is the head entity, ? E is the tail entity and is the relation type describing the semantic relation between two entities. Let T be the set of possible relation types (including reverse relation types). For simplicity, we define a query = (?, , ) and aim to model the probabilistic distribution ( | , D), where ? {?1, 1} is a binary variable indicating whether (?, , ) is valid or not, and ?, ? E, ? T . In this paper, bold letters indicate variables.</p><p>Logic Rule We extract relations from the document by learning logic rules, where logic rules in this work have the conjunctive form:</p><formula xml:id="formula_0">?{ } =0 ( 0 , ) ? 1 ( 0 , 1 ) ? ? ? ? ? ( ?1 , ) ? E,</formula><p>? T and is the rule length. This form can express a wide range of common logical relations such as symmetry and transferability. For example, transferability can be expressed as</p><formula xml:id="formula_1">?{ 0 , 1 , 2 } ( 0 , 2 ) ? ( 0 , 1 ) ? ( 1 , 2 )</formula><p>Inspired by RNNLogic <ref type="bibr" target="#b11">(Qu et al., 2021)</ref>, to infer high-quality logic rules in the large search space, we separate rule learning and weight learning and treat the logic rules as the latent variable. LogiRE consists of two main modules: the rule generator and the relation extractor, which are simultaneously trained to enhance each other. Given the query = (?, , ) in the document D, on the one hand, the rule generator adopts an auto-regressive model to generate a set of logic rules based on , which was used to help the relation extractor make the final decision; on the other hand, the relation extractor can provide some supervision signals to update the rule generator with posterior inference, which greatly reduces the search space with high-quality rules.</p><p>Unlike existing methods to capture the interactions among relations in the document by learning powerful representations, we introduce a novel probabilistic model LogiRE (Sec. 3.1, <ref type="figure" target="#fig_0">Fig. 2)</ref>, which explicitly enhances the interaction by learning logic rules. LogiRE uses neural networks to parameterize the rule generator and the relation extractor (Sec. 3.2), optimized by the EM algorithm in an iterative manner (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We formulate the document-level relation extraction in a probabilistic way, where a set of logic rules is assigned as a latent variable . Given a query variable = ( , , ) in the document D, we define the target distribution ( | , D) as below 1 :</p><formula xml:id="formula_2">, ( | ) = ?? ( | , ) ( | )</formula><p>where is the distribution of the rule generator which defines a prior over the latent variable conditioned on a query (we assume the distribution of is independent from the document D), and is the relation extractor which gives the probability of conditioned on the query , latent , and the document D. Given the gold label * of the query in the document D, the objective function is to maximize the likelihood as follows:</p><formula xml:id="formula_3">L ( , )= log , ( * | )<label>(1)</label></formula><p>Due to the existence of latent variables in the objective function L, we use the EM algorithm for optimization (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterization</head><p>We use neural networks to parameterize the rule generator and the relation extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule Generator</head><p>The rule generator defines the distribution ( | ). For a query , the rule generator generates a set of logic rules denoted by for predicting the truth value of the query . Formally, given a query = (?, , ), we generate logic rules that takes the form of ? 1 ? ? ? ? ? . Such relation sequences [ 1 , . . . , ] can be effectively modeled by an autoregressive model. In this work, we employ a Transformer-based autoregressive model AutoReg to parameterize the rule generator, which sequentially generates each relation . In this process, the probabilities of generated rules are simultaneously computed. Next, 1 For simplicity, we omit D in distributions , and .</p><p>we assume that the rule set obeys a multinomial distribution with rules independently sampled from the distribution AutoReg (rule| ):</p><formula xml:id="formula_4">( | ) ? Multi( | , AutoReg (rule| )),</formula><p>where Multi denotes multinomial distribution, is a hyperparameter for the size of the set and AutoReg defines a distribution over logic rules conditioned on the query . 2</p><p>Relation Extractor The relation extractor defines ( | , ). It utilizes a set of logic rules to get the truth value of corresponding to the query . For each query , a rule ? is able to find different grounding paths on the document D. For example, Alice father ? ??? ?Bob spouse ? ???? ?Cristin is a grounding path for the rule mother( 0 , 2 ) ? father( 0 , 1 )?spouse( 1 , 2 ). Following the product t-norm fuzzy logic <ref type="bibr" target="#b2">(Cignoli et al., 2000)</ref>, we score each rule as follows:</p><formula xml:id="formula_5">(rule)= max path? P (rule) (path) path: 0 (?) 1 ? ? 1 2 ? ? 2 ? ? ? ? ? ? ( ) (path)= =1 ( ?1 , , )</formula><p>where P (rule) is the set of grounding paths which start at ? and end at following a rule.</p><p>( ?1 , , ) is the confidence score obtained by any existing relation models. <ref type="bibr">3</ref> To get the probability (fuzzy truth value) of , we synthesize the evaluation result of each rule in the latent rule set . The satisfaction of any rule body will imply the truth of . Accordingly, we take the disjunction of all rules in as the target truth value. Following the principled sigmoid-based fuzzy logic function for disjunction <ref type="bibr" target="#b15">(Sourek et al., 2018;</ref>, we define the fuzzy truth value as:</p><formula xml:id="formula_6">( | , )= Sigmoid( ? score ( , )) score ( , )= ( ) + ?? rule? ( , rule) (rule)</formula><p>where ( ) and ( , rule) are learnable scalar weights.</p><p>( ) is a bias term for balancing the score of positive and negative cases.</p><p>( , rule) estimates the score, namely, the quality of a specific rule.</p><p>(rule) evaluates the accessibility from the head entity ? to the tail entity through the meta path defined by rule's body. Applying logic rules and reasoning over the rules enable the relation extractor to explicitly modeling the long-range dependencies as the interactions among entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>To optimize the likelihood L ( , ) (Eq. 1), we update the rule generator and the relation extractor alternately in an iterative manner, namely the EM algorithm. The classic EM algorithm estimates the posterior of the latent variable according to current parameters in the E-step; The parameters are updated in the M-step with obeys the estimated posterior. However, in our setting, it is difficult to compute the exact posterior ( | , ) due to the large space of . To tackle this challenge, we seek an approximate posterior ( ) by a second-order Taylor expansion. This modified version of posterior forms a lower bound on log , ( | ), since the difference between them is a KL divergence and hence positive:</p><formula xml:id="formula_7">log , ( | ) E ( ) log , ( , | ) , ( | , ) ? lower bound E ( ) log , ( , | ) ( ) = KL ( )|| , ( | , ) ? 0</formula><p>Once we get ( ), we can maximize this lower bound of log , ( | ). E-step Given the current parameters , , E-step aims to compute the posterior of according to the current parameters , . However, the exact posterior , ( | , ) is nontrivial due to its intractable partition function (space of is large). In this work, we aim to seek an approximate posterior ( ).</p><p>By approximating the likelihood with the secondorder Taylor expansion, we can obtain a conjugate form of the posterior as a multinomial distribution. The detailed derivation is listed in Appendix. A. Formally, we first define (rule) as the score function estimating the quality of each rule: For each instance, use the rule generator to generate a set of logic rules?(|?| = ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Calculate the rule score (rule) of each rule for approximating the posterior of rule:</p><p>(rule| ). ? E-step 4:</p><p>For each instance, update the rule generator AutoReg based on the sampled rules from (rule| ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>For each instance, update the relation extractor based on generated logic rulesf rom the updated rule generator.</p><p>? M-step 6: end while which servers as the prior probability for each rule. The other is based on the relation extractor, and it takes into account the contribution of the current rule to the final correct answer * . Next, we us? (rule| ) to denote the posterior distribution of the rule given the query :</p><formula xml:id="formula_8">(rule| ) ? exp ( (rule))</formula><p>Thus the approximate posterior also obeys a multinomial distribution.</p><p>( ) ? Multi ( ,?(rule| )) M-step After obtaining the ( ), M-step is to maximize the lower bound log , ( | ) with respect to both and . Formally, given each data instance ( * , , D) and the ( ), the objective is to maximize</p><formula xml:id="formula_9">L lower = L G E ( ) [log ( | )] + L R E ( ) [log , ( * | , )]</formula><p>where L G , L R are the objective of the rule generator and the relation extractor, repectively. For the objective L , it can be further converted equally as</p><formula xml:id="formula_10">L G = E?( rule| ) [AutoReg (rule| )]</formula><p>To compute the expectation term of L G we sample from the current prior ( | ) for a sampl? , and evaluate the score of each rule as (rule), normalized score over (rule) are regarded as the approximated?(rule| ). Then we use sampled rules to update the AutoReg (rule| ). Intuitively,  we update the rule generator ( | ) to make it consistent with the high-quality rules identified by the approximated posterior.</p><p>For the objective L , we update the relation extractor according to the logic rules sampled from the updated rule generator. The logic rules explicitly capturing more interactions between relations can be fused as input to the relation extractor, which yields better empirical results and enjoys better interpretation. Finally, we summarize the optimization procedure in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on multi-relational document-level relation extraction datasets: Do-cRED <ref type="bibr" target="#b28">(Yao et al., 2019)</ref> and DWIE <ref type="bibr" target="#b30">(Zaporojets et al., 2020)</ref>. The statistics of the two datasets are listed in <ref type="table" target="#tab_1">Table 1</ref>. Pre-processing details of DWIE are described in Appendix B.</p><p>Evaluation Besides the commonly used F1 metric for relation extraction, we also include other two metrics for comprehensive evaluation of the models: ign F1, logic. ign F1 was proposed in <ref type="bibr" target="#b28">(Yao et al., 2019)</ref> for evaluation with triples appearing in the training set excluded. It avoids information leakage from the training set. We propose logic for evaluation of logical consistency among the prediction results. Specifically, we use the 41 pre-defined rules on the DWIE dataset to evaluate whether the predictions satisfy these gold rules. The rules have a similar form to logic rules defined in Sec. 3. We name the precision of these rules on predictions as logic score. Note that these rules are independent of the rule learning and utilization in Sec. 3 but only used for logic evaluation.</p><p>Experimental Settings The rule generator in our experimental settings is implemented as a transformer with a two-layer encoder and a two-layer decoder, hidden size set to 256. We empirically find the tiny structure is enough for modeling the required rule set. We set the size of the latent rule set to 50. We limit the maximum length of logic rules to 3 in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We compare our LogiRE with the following baselines on document-level RE. The baselines are also used as corresponding backbone models in our framework. <ref type="bibr" target="#b28">Yao et al. (2019)</ref> proposed to apply four state-of-the-art sentence-level RE models to document-level relation extraction: CNN, LSTM, BiLSTM, and Context-Aware. <ref type="bibr" target="#b31">(Zeng et al., 2020)</ref> proposed GAIN to leverage both mention-level graph and aggregated entity-level graph to simulate the inference process in document-level RE, using graph neural networks. <ref type="bibr" target="#b35">Zhou et al. (2021)</ref> proposed ATLOP, using adaptive thresholding to learn a better adjustable threshold and enhancing the representation of entity pairs with localized context pooling. The implementation details of the baselines are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Our LogiRE outperforms the baselines on all of the three metrics. (We mainly analyze the results on DWIE with all three metrics can be evaluated. The results on DocRED are demonstrated in <ref type="table" target="#tab_4">Table 3</ref> and discussed in Sec. 4.3.)</p><p>Our LogiRE consistently outperforms various backbone models. It outperforms various baselines on the DWIE dataset as shown in <ref type="table" target="#tab_3">Table 2</ref>. We achieve 2.02 test ign F1 and 1.84 test F1 improvements on the current SOTA, ATLOP. The compatibility between LogiRE and various backbone models shows the generalization ability of our LogiRE. The consistent improvements on both sequencebased and graph-based models empirically verified the benefits of explicitly injecting logic rules to document-level relation extraction.</p><p>The improvements on graph-based models indicate the effectiveness of modeling interactions among multiple relations and entities. Despite graph-based models provide graphs  consisting of connections among mentions, entities, and sentences, they seek more powerful representations which implicitly model the intrinsic connections. Our Lo-giRE instead builds explicit interactions among the entities and relations through the meta path determined by the rules. The improvements on the current SOTA for graph-based model empirically proved the superiority of such explicit modeling.   Our model achieves better logical consistency compared with the baselines. The results show that LogiRE achieves up to 18.78 enhancement on the logic metric. Even on the graph-based model, GAIN, we obtain a significant improvement of 5.03 on logical consistency. The improved logic score shows that the predictions of LogiRE are more consistent with the regular logic patterns in the data. These numbers are evidence of the strength of our iterative-based optimization approach by introducing logic rules as latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis &amp; Discussion</head><p>We analyze the results on DocRED data and discuss the superiority of our LogiRE on capturing long-range dependencies and interpretability. The capability of capturing long-range dependencies is studied by inspecting the inference performance on entity pairs of various distances. The interpretability is verified by checking the logic rules learned by our rule generator and the case study on predictions. relatively shorter dependencies in DocRED and the logical inconsistency caused by incomplete annotations. a) Shorter Dependencies in DocRED Shorter dependencies in DocRED lower the demand for capturing long-range correlations among entities and relations. We show the distribution of distance between entity pairs in <ref type="figure" target="#fig_2">Fig. 3</ref>. 79.26% of entity pairs in DocRED have distances less than 100 tokens. The examples in DocRED are less difficult on capturing long-range dependencies. More analysis and comparison can be found in <ref type="bibr" target="#b30">Zaporojets et al. (2020)</ref>. The representation-based approaches can already perform well in such cases. The benefits of modeling long-range dependencies through logical reasoning will be smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on DocRED Results</head><p>b) Logical Inconsistency in DocRED The justification of predictions after reasoning may be not accurate because of missing annotations. We calculated the error rate of a few easy-to-verify logic rules as shown in <ref type="table">Table.</ref> 4. The 7 rules, selected implication rule error rate father(?, ) ? spouse( , ) ? mother <ref type="bibr">(?, )</ref> 24.07% replaces ?1 (?, ) ? replaced_by <ref type="bibr">(?, )</ref> 22.22% capital ?1 (?, ) ? capital_of(?, )</p><p>28.24% father ?1 (?, ) ? child(?, ) 10.26% followed ?1 (?, ) ? follows <ref type="bibr">(?, )</ref> 22.40% capital ?1 (?, ) ? capital_of(?, )</p><p>28.24% P150 ?1 (?, ) ? P131(?, )</p><p>19.71%  by case study, have a considerable part (12.96%) of labeled relations may participate in as atoms. However, the statistics in the table demonstrated that all the 7 rules have error rates higher than 10%. The numbers indicated that a notable partition of true relations are missing. The results obtained by reasoning over logic rules may be wrongly justified since the data is not exhaustively annotated.</p><p>According to the analysis above, our LogiRE has greater potential than that demonstrated as the overall performance on DocRED.</p><p>Logic rules are shortcuts for comprehension. The performance enhancement of our LogiRE becomes more prominent when the distance between entity pairs gets longer. We plot the performance of ATLOP and ATLOP-based LogiRE on the DWIE dataset with four groups of entity pair distances in <ref type="figure" target="#fig_3">Fig. 4</ref>. The distance is calculated as the number of Relation extraction for entity pairs with longer distances in between generally performs worse. As shown in the figure, the performance starts to drop as the distance surpasses 100 tokens, indicating the difficulty of modeling long-range dependencies. The redundant information in a long context impedes accurate semantic mapping through powerful representations. This issue increases the complexity of modeling and limits the potential of representation-based approaches.</p><formula xml:id="formula_11">played_by(?, ) ? plays_in( , ) ? character_in (?, ) (parent_of(?, ) ? child_of(?, ) ? spouse_of(?, )) ? royalty_of( , ) ? royalty_of(?, ) event_in2 ?1 (?, ) ? event_in0( , ) ? in0(?, ) minister_of(?, ) ? in0( , ) ? citizen_of(?, ) member_of ?1 (?, ) ? agent_of( , ) ? based_in0(?, )</formula><p>Our framework with latent logic rules injected can effectively alleviate this problem. The performance drop of our LogiRE is smaller when the distance between entities gets larger. For entity pairs of distances larger than 400, our LogiRE achieves up to 4.47 enhancement on test ign F1. By reasoning over local logic units (atoms in rules), we ignore the noisy background information in the text but directly integrate high-level connections among concepts to get the answer.</p><p>The reasoning process of our LogiRE is in line with the comprehension way of we human beings when reading long text. We construct basic concepts and connections between (local logic atoms) for each local part of the text. When the collected information is enough to fit some prior knowledge (logic rules), we deduct new cognition from the existing knowledge. Our LogiRE provides shortcuts for modeling long text semantics by adding logic reasoning to naive semantics mapping.</p><p>Interpretability by Generating Rules Our Lo-giRE enjoys better interpretability with the generated latent rule set. After the EM optimization, we can sample from the rule generator for high-quality rules that may contribute to the final predictions. Besides the gold rules previously shown for evaluating logic, LogiRE mines more logic rules from the data, as shown in <ref type="table">Table.</ref> 5. These logic rules explicitly reveal the interactions among entities and relations in the same document as regular patterns. LogiRE is more transparent, exhibiting the latent rules by the rule generator.</p><p>Case Study <ref type="figure">Fig. 5</ref> shows a few inference cases of our LogiRE, including two positive examples and a negative one. As shown in the first two examples, LogiRE can complete the missing relations in the backbone model's outputs by utilizing logical rules. The soft logical reasoning can remedy the defects of representation-based approaches under specific circumstances. However, the extra reasoning may also exacerbate errors by reasoning over wrongly estimated logic units. The third example shows such a case. The wrongly estimated atom in0(Vega, Germany) leads to one more wrong relation extracted by reasoning. Fortunately, such errors in our LogiRE will be more controllable because of the transparency in the logical reasoning part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a probabilistic model Lo-giRE, which utilizes rules and conducts reasoning over the rules for document-level relation extraction. The logic rules are treated as latent variables. We utilize the EM algorithm to efficiently maximize the overall likelihood. By injecting rules to the relation extraction framework, our LogiRE explicitly models the long-range dependencies in docRE as interactions among relations and entities, thus enjoying better interpretability. Empirical results and analysis show that LogiRE outperforms strong baselines on overall performance, logical consistency, and capability for capturing long-range dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overview of LogiRE. LogiRE consists of two modules: a rule generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>rule) = log AutoReg (rule| )+ * 2 1 ( ) + ( , rule) (rule) Intuitively, (rule) evaluates rule quality in two factors. One is based on the rule generator , Algorithm 1 EM Optimization for L ( , ) 1: while not converge do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>In comparison with the significant improvements on DWIE, the enhancement of LogiRE on DocRED is less significant. Our analysis shows that the reasons are Distribution of Distance Between Entity Pairs in DocRED</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance gaps between ATLOP and LogiRE-ATLOP for entity pairs with different distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Document-level RE Datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results on DWIE. (The underlined statistics pass a t-test for significance with value &lt; 0.01.)</figDesc><table><row><cell>Model</cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>ign F1</cell><cell>F1</cell></row><row><cell>GAIN</cell><cell>57.93</cell><cell>60.07</cell></row><row><cell>GAIN + LogiRE</cell><cell cols="2">58.62(+0.69) 60.61(+0.54)</cell></row><row><cell>ATLOP</cell><cell>59.14</cell><cell>61.13</cell></row><row><cell cols="3">ATLOP + LogiRE 59.48(+0.34) 61.45(+0.32)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison on DocRED. The improvements are less significant with reasons analyzed in Sec. 4.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: The logical inconsistency in the DocRED (for</cell></row><row><cell cols="7">conciseness, P150 represents the relation 'contains ad-</cell></row><row><cell cols="7">ministrative territorial entity' and P131 represents the</cell></row><row><cell cols="7">relation 'located in the administrative territorial entity').</cell></row><row><cell cols="7">The shown easy-to-verify gold rules have high error</cell></row><row><cell cols="7">rates in DocRED while a considerable part of rela-</cell></row><row><cell cols="7">tions (12.96%) are involved in as atoms in shown rules.</cell></row><row><cell cols="7">Those missing annotations make the learning of logic</cell></row><row><cell cols="7">rules difficult. Inconsistent patterns or statistics be-</cell></row><row><cell cols="7">tween training and test may lead to unfair evaluation</cell></row><row><cell cols="6">of relation extraction performance.</cell><cell></cell></row><row><cell>65 70 75 80</cell><cell>75.88 76.91 68.36 69.31</cell><cell>76.98 78.88</cell><cell>69</cell><cell>71.13</cell><cell>66.73 68.93</cell><cell>ATLOP test F1 LogiRE test F1 ATLOP test ign F1 LogiRE test ign F1</cell></row><row><cell>40 45 60 Score 55 50</cell><cell>[0, 100)</cell><cell cols="4">[100, 200) Entity Pair Distances [200, 400) 58.41 60.8</cell><cell>[400, inf) 41.84 51.72 47.87 46.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Example rules extracted from LogiRE trained on the DWIE dataset. tokens in between the nearest mentions of an entity pair. Results indicate that our LogiRE performs better on capturing long dependencies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc> year s ago that he would find success in Ger many. I n his home countr y, I r aq, ......, forcing him and his family to leave M osul and go to Ger many in search of safety and religious freedom. ...... I t took only three months for the I r aqi family's request for asylum to be recognized ...... .....funded by the Z?r ich-based film company Vega and directed by Ger man film maker M ar kus I mboden, is to be launched in Ger many on Thur sday........ Inference cases of our LogiRE on DWIE by using ATLOP as the backbone model. The grey arrows are relations extracted by the backbone model, solid lines representing true relations while dashed lines representing false relations. The green arrows are new relations correctly extracted by logical reasoning. The blue arrows indicate the potential reasoning paths. We also demonstrate a negative case. In the third example, the red arrow represents a wrong relation extracted by reasoning over wrongly estimated atoms.</figDesc><table><row><cell>Documents</cell><cell></cell><cell></cell><cell cols="3">Extr acted Relations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gpe0</cell></row><row><cell></cell><cell>I r aq</cell><cell cols="2">in0</cell><cell>M osul</cell><cell>in0-x</cell><cell>I r aqi</cell></row><row><cell></cell><cell cols="2">citizen_of</cell><cell></cell><cell>Rani</cell><cell>citizen_of-x</cell></row><row><cell>.......Cisse saves win for Freibur g I n Sunday's other Bundesliga</cell><cell></cell><cell></cell><cell></cell><cell cols="2">appear s_in</cell><cell>Bundesliga</cell></row><row><cell>match, Senegal str iker Papiss Demba Cisse broke a scoreless deadlock with a goal in the 91st minute to give Freibur g a 1-0</cell><cell cols="2">Cisse member _of</cell><cell cols="2">Freibur g</cell><cell>appear s_in</cell></row><row><cell>win over Hoffenheim........</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">won_vs</cell><cell>Hoffenheim</cell></row><row><cell></cell><cell>Z?r ich</cell><cell cols="2">based_in2</cell><cell>Vega</cell><cell>in0</cell><cell>Ger many</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">based_in0</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>..</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The generative process of a rule set is quite intuitively similar to a translation model, and we simply generate rules with AutoReg to form .3 This is why our approach is plug-and-play.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The SJTU team is supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (61772333). We thank Xinbo Zhang, Jingjing Xu, Yuxuan Song, Wenxian Shi and other anonymous reviewers for their insightful and detailed comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Approximation of the True Posterior</head><p>The exact posterior of the latent rule set is difficult to be directly calculated because of the large space. In this section, we provide the detailed derivation for the approximate posterior.</p><p>The approximation is obtained by the following second-order Taylor expansion:</p><p>By such approximation, we can decompose the posterior to each rule in the latent rule set. We first define the score for each rule:</p><p>Then, it's easy to obtain that the approximated posterior ( ) and the prior are conjugate distributions.</p><p>where is the normalization factor.</p><p>B Implementation Details DWIE Dataset Preprocessing The original DWIE dataset <ref type="bibr" target="#b30">(Zaporojets et al., 2020)</ref> is designed for four sub-tasks in the information extraction, including named entity recognition, coreference resolution, relation extraction, and entity linking.</p><p>In this paper, we focus on the document-level RE task. We only use the dataset for document-level relation extraction. The original dataset published 802 documents with 23130 entities in total, 702 for train and 100 for test. In our setting, we remove the entities without mentions in the context. After the cleaning, we have 700 documents for train and 99 documents for test. The training set is then randomly split into two parts: 602 documents for train and 98 for development. The statistics of the preprocessed dataset are shown in <ref type="table">Table 1</ref> of the main body.</p><p>Baselines We use their published open-source code to implement the baselines <ref type="bibr" target="#b28">(Yao et al., 2019;</ref><ref type="bibr" target="#b31">Zeng et al., 2020;</ref><ref type="bibr" target="#b35">Zhou et al., 2021)</ref>, as well as the backbone models in our framework. The pretrained language models used in GAIN and AT-LOP follows the original paper <ref type="bibr" target="#b31">(Zeng et al., 2020;</ref><ref type="bibr" target="#b35">Zhou et al., 2021)</ref>, using the pre-trained bert-baseuncased and bert-base-cased models respectively. The hyperparameters reserve the same as in their papers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06042</idno>
		<title level="m">Globally normalized transition-based neural networks</title>
		<meeting><address><addrLine>Slav Petrov, and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Basic fuzzy logic is the logic of continuous t-norms and their residua</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cignoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>Godo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Torrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="112" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>C?cero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1247</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2245" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2410" to="2420" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2018 workshop</title>
		<meeting>the BioNLP 2018 workshop<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00049</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rnnlogic: Learning logic rules for reasoning on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic logic neural networks for reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lifted relational neural networks: Efficient learning of latent relational structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Sourek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojtech</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Zelezny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Kuzelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="69" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extracting entities and relations with joint minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hin: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12084</biblScope>
			<biblScope unit="page">197</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep probabilistic logic: A unifying framework for indirect supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1891" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating deep learning with logic fusion for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sinno Jialin Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9225" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ENPAR:enhancing entity and entity pair representations for joint entity relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2877" to="2887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UniRE: A unified label space for entity relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="220" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Weighted MaxSAT for Aspect-based Opinion Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meixi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno Jialin</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5618" to="5628" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.582</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7170" to="7186" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dwie: an entity-centric dataset for multi-task document-level information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klim</forename><surname>Zaporojets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12626</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient probabilistic logic reasoning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An effective neural model extracting document level chemical-induced disease relations from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
