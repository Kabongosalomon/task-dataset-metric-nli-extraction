<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woojeong</forename><surname>Jin</surname></persName>
							<email>woojeong.jin@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>yu.cheng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<email>yelong.shen@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FEWVLM, relatively smaller than recent fewshot learners. For FEWVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FEWVLM with prompt-based learning outperforms Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref> which is 31? larger than FEWVLM by 18.2% point and achieves comparable results to a 246? larger model, PICa . In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github. com/woojeongjin/FewVLM</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Raffel et al., 2020;</ref><ref type="bibr" target="#b2">Brown et al., 2020;</ref><ref type="bibr" target="#b18">Radford et al., 2021)</ref>. Such large PLMs can learn a new task with a few examples or generalize to a new task without fine-tuning on any training examples, i.e., few-shot and zero-shot learn- * Work was mainly done while interning at Microsoft Azure AI.  <ref type="figure">Figure 1</ref>: Examples of VQA and Captioning tasks.</p><p>In our setup, we convert the tasks into generative tasks in which models need to generate target text given input text and an image.</p><p>ing <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b18">Radford et al., 2021;</ref><ref type="bibr" target="#b26">Tsimpoukelli et al., 2021)</ref>. Few-shot learning overcomes the challenges of data-hungry supervised learning, where collecting human-labeled data is costly and slow. However, recent few-shot models such as GPT3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>, Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref>, and PICa  are too large to deploy in small or moderate computing machines due to their gigantic model sizes In this paper, we study low-resource learning of VL tasks with our proposed method, FEWVLM, a moderate-sized vision-language model, in which we fine-tune the model with no or a handful of training examples. For FEWVLM, we pre-train a sequence-to-sequence transformer model <ref type="bibr" target="#b20">Raffel et al., 2020)</ref> with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). This setup is more practical in that training and inference can be run economically using standard computing hardware and  it is expensive to obtain a large number of quality training examples in the real world. In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks <ref type="bibr" target="#b18">Radford et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021a,b;</ref><ref type="bibr" target="#b2">Brown et al., 2020</ref>).</p><p>To extend the success to VL tasks, we aim to answer the following questions for prompt-based low-resource VL learning. Q1) How does prompt design affect zero/few-shot learning on new tasks? Q2) Does prompt design still matter given larger training? Q3) How do different pre-training objectives affect zero/few-shot learning? To answer these questions, we explore various prompt formats including hand-crafted and noisy prompts on zero/few-shot VL learning datasets. In addition, we study pre-training objectives on few-shot tasks inspired by <ref type="bibr" target="#b20">Raffel et al. (2020)</ref>: prefix language modeling (PrefixLM) inspired by <ref type="bibr" target="#b20">Raffel et al. (2020)</ref> and masked language modeling (MaskedLM). To this end, we investigate the model's performance on few-shot VL tasks including visual question answering <ref type="bibr" target="#b8">(Goyal et al., 2017;</ref><ref type="bibr" target="#b16">Marino et al., 2019;</ref><ref type="bibr" target="#b9">Hudson and Manning, 2019)</ref>, captioning <ref type="bibr" target="#b0">(Agrawal et al., 2019;</ref><ref type="bibr" target="#b32">Young et al., 2014)</ref>  <ref type="figure">(Fig. 1)</ref>, and mini-ImageNet <ref type="bibr" target="#b29">(Vinyals et al., 2016)</ref>.</p><p>In our empirical analysis, our FEWVLM with prompt-based learning outperforms Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref> which is 31? larger than FEWVLM by 18.2% point on zero-shot VQAv2 and achieves comparable results to a 246? larger model, PICa . Furthermore, we observe that (1) prompts significantly affect zero-shot performance but marginally affect fewshot performance on new tasks ( ?6.2 and ?6.3), (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data ( ?6.5), and (3) MaskedLM helps few-shot VQA tasks while PrefixLM boosts captioning performance ( ?6.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision-language few-shot learning. Recently, several few-shot learners on vision-language tasks were proposed including GPT <ref type="bibr" target="#b19">(Radford et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>, Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref>, PICa , and SimVLM . Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref> is a large language model based on GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>, and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text. Their approach shows the fewshot capability on visual question answering and image classification tasks. Similarly, PICa  uses <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples. It converts images into textual descriptions so that GPT-3 can understand the images. SimVLM  is trained with prefix language modeling on weakly-supervised datasets. It demonstrates its effectiveness on a zero-shot captioning task. While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes.</p><p>Language model prompting. Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks <ref type="bibr" target="#b18">Radford et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021a,b;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>. Among them, GPT models <ref type="bibr" target="#b19">(Radford et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>   We pretrain FEWVLM with masked language modeling (MaskedLM) and prefix language modeling (Pre-fixLM).</p><p>or task demonstrations in NLP tasks. In light of this direction, prompt-based approaches improve small pre-trained models in few-shot text classification tasks <ref type="bibr">Schick and Sch?tze, 2021a,b)</ref>. CLIP <ref type="bibr" target="#b18">(Radford et al., 2021)</ref> also explores prompt templates for image classification which affect zero-shot performance. We follow these core ideas so we aim to improve zero-shot and few-shot performance using prompts in visionlanguage tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis Setup</head><p>In this work, we study the zero-shot and few-shot performance of vision-language models L. We introduce our analysis setup: problem formulation, analysis questions, downstream tasks and datasets, evaluation metrics, and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>For zero-shot tasks, a pre-trained VL model L have no access to training set D train and development set D dev , and directly makes inference on the test instances D test . For few-shot tasks, we compose a dev set D dev from training data and ensure that |D train | = |D dev | following <ref type="bibr" target="#b17">Perez et al. (2021)</ref>;  to tune the hyper-parameters and select the model. We limit the sizes of training and development sets to meet the goal of learning from limited data. The size of D train and D dev are small -i.e., we set the size of both to 16 in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis Questions</head><p>We aim to answer the following questions in this study through experiments on multiple VL datasets. Q1) How does prompt design affect zero/fewshot learning on new tasks? Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains <ref type="bibr">Schick and Sch?tze, 2021a,b;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>. For this question, we test several ad-hoc prompts on vision-language tasks and analyze how large zero-shot and few-shot performance is affected by different prompts, hand-crafted and noisy prompts, in Sec. 6.5. Q2) Does prompt design still matter given larger training data? As we will see in our experiments, prompts affect the zero/few-shot performance. However, prompts may have different effects when models are given different sizes of training data. To answer this question, we train models with different sizes of training data and various prompts, and compare the performance between different prompts.</p><p>Q3) How do different pre-training objectives affect zero/few-shot performance? We study two different pre-training objectives on few-shot performance: prefix language modeling (PrefixLM) inspired by <ref type="bibr" target="#b20">Raffel et al. (2020)</ref> and masked language modeling (MaskedLM). In this setup, we pre-train our model with different objectives and test the model on zero-shot and few-shot tasks in Sec. 6.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Downstream Tasks and Datasets</head><p>In this work, we mainly focus on three tasks: visual question answering, captioning, and categorical learning. The visual question answering task requires models to answer a question to a given context image. We convert the visual question answering task into a generation task so that the model can generate answers in the zero-shot setting. The captioning task requires a model to generate descriptions for a given context image. The categorical learning requires a model to choose the correct category or class. We evaluate our model in an open-ended fashion to quantify fast learning of categories, in which it must generate correct labels unlike other classification methods.</p><p>We include VQAv2 <ref type="bibr" target="#b8">(Goyal et al., 2017)</ref>, OK-VQA <ref type="bibr" target="#b16">(Marino et al., 2019)</ref>, and GQA (Hudson <ref type="table">Table 1</ref>: Hand-crafted prompts. We study hand-crafted prompts on zero-shot and few-shot tasks.</p><p>[Q] and [A] refer to question text and answer text, respectively. &lt;text_1&gt; is a sentinel token. We append image features to input text. Target prompts are "[A]" and "&lt;text_1&gt; [A]" in VQA. We use caption text as a target prompt in captioning. and <ref type="bibr" target="#b9">Manning, 2019)</ref> for visual question answering tasks, and NoCaps <ref type="bibr" target="#b0">(Agrawal et al., 2019)</ref>, and Flickr30k <ref type="bibr" target="#b32">(Young et al., 2014)</ref> for image captioning. <ref type="bibr">1</ref> We use Karpathy split <ref type="bibr" target="#b10">(Karpathy and Li, 2015)</ref> for Flickr30k, which re-splits train and val images into 29,000 / 1,014 / 1,000 for train / validation / test. For categorical learning, we include miniImageNet <ref type="bibr" target="#b29">(Vinyals et al., 2016)</ref>, a meta learning dataset. Following <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref>, we use only meta test data to evaluate FEWVLM in a few-shot manner and test on 5-way k-shot setup, where 5 classes and k examples per class are given. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metrics</head><p>To evaluate few-shot performance, we randomly sample 5 different training and dev splits and measure average performance on the 5 splits. We finetune the vision-language models with 200 epochs for the few-shot setup and choose the best checkpoint on the dev set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Baselines</head><p>We evaluate strong zero/few-shot vision-language learners for comparison: Frozen <ref type="bibr" target="#b26">(Tsimpoukelli et al., 2021)</ref>, PICa  for VQA datasets and SimVLM  for captioning datasets. We include Unified VLP  for few-shot VQAv2 and Flickr30k. Also, we compare them with fully fine-tuned models L f ull as upper bounds of few-shot models for each task; these models are fine-tuned on the entire datasets while few-shot models can access a small amount of data. For fully fine-tuned models L f ull , we borrow numbers from Uniter large  for VQAv2, Oscar <ref type="bibr" target="#b14">(Li et al., 2020b)</ref> for GQA, SimVLM  and VinVL <ref type="bibr" target="#b33">(Zhang et al., 2021)</ref> for NoCaps CIDER and SPICE respectively, and Unified VLP  for Flickr30k captioning. We include VL-T5 no-vqa as a baseline which is pre-trained without visual question answering datasets . For miniImageNet, we include Frozen and AFHN <ref type="bibr" target="#b12">(Li et al., 2020a)</ref>. Frozen is designed for few-shot learning while AFHN is for meta learning, which is smaller and faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Before diving into the analysis, we introduce our model, FEWVLM, to do zero/few-shot learning on VL tasks and answer the analysis questions we raised. We introduce FEWVLM architecture and pre-training objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder-decoder Vision-language Model</head><p>We adopt an encoder-decoder architecture <ref type="bibr" target="#b27">Vaswani et al., 2017)</ref>, to encode visual and text inputs and generate target text. We represent an input image with 36 object regions from a Faster R-CNN (Ren et al., 2015) trained on Visual Genome <ref type="bibr" target="#b11">(Krishna et al., 2017)</ref>. The sets of region representations are fed into the encoder by appending them to the text . We train the model parameters ? by minimizing the negative log-likelihood of target text y tokens given input text x and image v:</p><formula xml:id="formula_0">L ? = ? |y| i=1 log P ? (y i |y &lt;i , x, v).</formula><p>(1)</p><p>The model is not task-specific, so it is a good option for zero/few-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training Objectives</head><p>We pre-train the models with both prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the PrefixLM and MaskedLM. Prefix language modeling. We include prefix language modeling (PrefixLM) following <ref type="bibr" target="#b20">Raffel et al. (2020)</ref>. Given an image and a span of text, this objective randomly splits the text into two separate components; the former component with the given image is used as inputs to the encoder and the latter component is used as target text to be generated by the decoder. Masked language modeling. We follow  to do masked language modeling. This objective is to replace random spans with numbered sentinel tokens, e.g., &lt;text_1&gt;, and then the masked text is fed into the encoder. Then the decoder generates the masked spans as target text. We randomly mask 15% of input text tokens and replace them with sentinel tokens.</p><p>Pre-training data. To pre-train FEWVLM, we collect image-caption data from MS COCO <ref type="bibr" target="#b15">(Lin et al., 2014;</ref><ref type="bibr" target="#b3">Chen et al., 2015)</ref> and Visual Genome (VG) <ref type="bibr" target="#b11">(Krishna et al., 2017)</ref>. The pre-training datasets contains 9.18M image-text pairs and 180K distinct images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Low-resource Adaptation</head><p>In downstream tasks, we train our model with few-shot examples. <ref type="figure" target="#fig_0">Fig. 2</ref> shows an illustration of FEWVLM in inference time. Given a prompt template P, we first get input text and target text using the template x, y = P(input, label). Then we train model parameters by minimizing the negative log-likelihood in Eq. (1). In inference, we use the same prompt and the model generates the label text. Here we obtain the final label by removing the target prompt template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prompt Design</head><p>Prompts affect the performance of the visionlanguage model ; we study the effect of different prompts on the zero-shot and fewshot performance on downstream tasks. <ref type="table" target="#tab_14">Tables 1  and 11</ref> show prompts we used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Visual Question Answering.</head><p>The visual question answering tasks (VQA, OK-VQA, and GQA) require models to answer a question to a given context image. Recent approaches <ref type="bibr" target="#b25">Tan and Bansal, 2019;</ref><ref type="bibr" target="#b24">Su et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2019</ref><ref type="bibr" target="#b14">Li et al., , 2020b</ref> tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates. Instead, we approach the visual question answering tasks as a generation task so that the model can produce the answers without introducing any task-specific heads. In this setup, prompts act as constraints to guide the models to generate proper formats of answers; models might generate a sentence for VQA, which is not the correct format, without prompts. Therefore, we study several prompts for input and output as shown in Tables 1 and 11; we explore hand-crafted prompts <ref type="table">(Table 1)</ref> and noisy prompts for ablation study <ref type="table" target="#tab_14">(Table 11)</ref>. Hand-crafted prompts. For input prompts, we explore three different templates: "question:</p><p>[Q] answer:" and with the &lt;text_1&gt; sentinel token at the end. Similarly to masked language modeling, we expect models to generate words thanks to the sentinel token. For target prompts, we explore two different templates: "[A]" (an answer) and "&lt;text_1&gt; <ref type="bibr">[A]</ref>" (an answer with a sentinel token). Here, we aim to mimic MaskedLM's target text format, so the similar format helps the model quickly adapt to the new task. We call each prompt ID as in <ref type="table">Table 1</ref>. Noisy prompts. To understand the effect of noisy prompts in zero/few-shot learning, we include irrelevant prompts, noisy tokens, and random sentences as in <ref type="table" target="#tab_14">Table 11</ref>. Irrelevant prompts are random questions or instructions that mislead models to answer wrong questions or follow irrelevant instructions. Noisy tokens are randomly selected from T5's vocabulary, so we test how robust our model is to random tokens. Finally, random sentences are captions from MS COCO and this gives false information to models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Captioning.</head><p>In NoCaps and Flickr30k, we explore three handcrafted input prompts: "a picture of ", "a photo of ", and "an image of ". We study the effect of different    word choices in this captioning task. While the three different words have similar meanings, they show different performance in zero-shot and fewshot tasks as we will see in our experiments.. For target prompts, we just train the model with the original caption without any additional prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">MiniImageNet</head><p>In miniImageNet, we train our model with a handcrafted input prompt, "This is &lt;text_1&gt;," and target prompt, "&lt;text_1&gt; [A]." We compare our model with and without prompts in this dataset to study whether prompts are helpful in categorical learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>In this section, we first discuss our main results on zero-shot and few-shot tasks and then answer the questions we raised: does prompt design matter in zero/few-shot learning?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Details</head><p>For pre-training, we set batch size 1,280 and 800 for FEWVLM base and FEWVLM large , respectively and pre-train them with 30 epochs. We use learning rate 1e-4 with 5% linear warmup. For few-shot learning, we train models with 200 epochs, learning rate 5e-5 and 5% linear warmup and choose the best checkpoint on the dev set. For FEWVLM, we use "question:</p><p>[Q] answer &lt;text_1&gt;" (P3) as an input prompt and "&lt;text_1&gt; [A]" as a target prompt for visual question answering, and "an image of" (Q3) as an input prompt for captioning, which show the best performance. We will study the effect of different prompts in Sec. 6.5. The sizes of of D train and D dev are 16 on VQA and captioning tasks. For miniImageNet, we use 'This is &lt;text_1&gt;," and "&lt;text_1&gt; [A]" as input and target prompts. In this data, we test with {1, 3, 5}-shots per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance on Zero-shot Learning</head><p>We evaluate the existing models in a zero-shot manner, in which models do not have access to any training data. <ref type="table" target="#tab_5">Tables 2 and 4</ref> show the results on VQA and captioning datasets, respectively. First, FEWVLM with the hand-crafted prompt (P3) achieves better performance than other baselines on VQA datasets. In particular, our FEWVLM base significantly outperforms Frozen which is about 31? larger than ours. Also, PICa based on GPT3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> shows the best performance on OK-VQA. It is noticeable that our FEWVLM large , the 246? smaller model, achieves the comparable result to PICa. Compared to VL-T5 no-vqa which is the same architecture as ours, FEWVLM base improves VQAv2 performance by about 30% point. As we will see in the later section, our pre-training objectives and the prompts boost the VQA performance. On NoCaps, SimVLM huge shows the best performance. Our FEWVLM base significantly improves the performance compared to VL-T5 no-vqa . As we will see in the later section, our pre-training objectives and the prompts boost the VQA and captioning performance. <ref type="table" target="#tab_6">Tables 3 and 5</ref> show the few-shot performance on VQA and captioning datasets. Sizes of training and validation sets are 16 for FEWVLM, VL-T5 no-vqa , and Unified VLP; and Frozen and PICa use 4 and 16 in-context demonstration examples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance on Few-shot Learning</head><p>On VQAv2 and OK-VQA, PICa shows the best performance while our FEWVLM large achieves the comparable result on VQAv2. OK-VQA requires external knowledge to answer unlike other VQA datasets, so larger models and large pre-training data (prior knowledge) are necessary to improve. Interestingly, FEWVLM * base , which is trained with 4 training examples, outperforms Frozen. On captioning data, FEWVLM base notably outperforms VL-T5 no-vqa by 31.1% point on NoCaps CIDEr.</p><p>Unified VLP slightly underperforms FEWVLM on Flickr30k captioning task. We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task ).  <ref type="table" target="#tab_9">Table 6</ref> shows results on miniImageNet, where models must choose the correct class for each image. We train and evaluate FEWVLM in an generative manner; the model must generate correct label text to get the credit. FEWVLM significantly outperforms Frozen in all shots. Note that we train FEWVLM with a few training samples while Frozen uses them as in-context demonstration. Interestingly, FEWVLM with a hand-crafted prompt improves performance a lot on the 1-shot case, while it marginally improves on the 5-shot case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Study of Prompt Design</head><p>Here we examine the effect of different prompts on FEWVLM base in <ref type="table" target="#tab_10">Table 7</ref>  <ref type="bibr">and Figs. 6,</ref><ref type="bibr">5,</ref><ref type="bibr">and 4</ref>. We test the model on VQAv2 and Flickr30k datasets. <ref type="table" target="#tab_10">Table 7</ref> shows the zero-shot performance on VQAv2 and Flickr30k. We observe that zero-shot results are remarkably affected by input prompts on both datasets. For input prompts, &lt;text_1&gt; in P1 and P3 helps the zero-shot predictions significantly compared to "no prompt" and P2. We conjecture that &lt;text_1&gt; guides the model to predict masked spans similarly to MaskedLM, so it improves the performance. On Flickr30k, we examine different word choices of prompts: "a picture of" (Q1), "a photo of" (Q2), and "an image of" (Q3). For instance, using "an image of" outperforms using no prompt by 21.4 point. It is noticeable that different word choices significantly affect the zero-shot results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Zero-shot Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Few-shot Predictions</head><p>We study various input prompts including irrelevant prompts, noisy tokens, and random sentences on VQAv2 <ref type="figure" target="#fig_2">(Fig. 4)</ref>. First, noisy prompts and no prompt achieve near 0 accuracy on the zero-shot setting. In few-shot predictions, FEWVLM with FEWVLM is trained with our best hand-crafted prompt (P3), irrelevant prompts, noisy tokens and random sentences. We list the prompt templates in <ref type="table" target="#tab_14">Table 11</ref> of appendix. We use "&lt;text_1&gt; [A]" as our target prompt. noisy prompts learns as quickly as hand-crafted prompts given larger data. For example, our model with noisy prompts achieves comparable results to the best hand-crafted prompt. Among all different types of noisy prompts, random sentences deteriorate performance the most. This is because the random sentences come from captions in MS COCO, so the model might choose the answer from wrong captions not from images. Interestingly, no prompt outperforms the other noisy prompts and even shows similar to or better than the handcrafted prompt with larger training data. We also observe a similar phenomenon on Flickr30k; no prompt performs similar to hand-crafted prompts in <ref type="figure" target="#fig_3">Fig. 5</ref>. We investigate different target prompts with handcrafted input prompts on various training sizes. In addition, we explore two different target prompts, "&lt;text_1 [A]" and " <ref type="bibr">[A]</ref>." We try to mimic the MaskedLM's target text format, so we add "&lt;text_1" to target prompt on VQA. This might help the model's fast adaptation to a new task since they share the same target prompt. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we notice an interesting phenomenon; the target prompt "[A]" shows a larger variance than the other suggesting that introducing "&lt;text_1" helps the model quickly adapt to a new task. However, both prompts show similar results given larger training data, e.g., 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Pre-training Objectives</head><p>We investigate how pre-training objectives affect different tasks. We pre-train FEWVLM with different pre-training objectives: masked language modeling (MaskedLM) and prefix language modeling (PrefixLM).</p><p>In <ref type="table" target="#tab_11">Table 8</ref>, we observe that MaskedLM helps VQA tasks while PrefixLM helps captioning tasks in zero-shot and few-shot settings. We conjecture that MaskedLM is to predict spans, which is analogous to predict correct answers to questions, and PrefixLM is to generate the rest of the given prefix, which is similar to captioning tasks. In other words, if the pre-training task is similar to the downstream tasks, then it will help performance further. When pre-training with both objectives, they create a synergetic effect and thus improve cross-task generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present FEWVLM, a few-shot prompt-based learner on vision-language tasks. On diverse datasets, FEWVLM outperforms baselines and shows comparable results to PICa which is 246? larger than ours. We observe that prompts are vital in zero-shot and few-shot tasks and each pre-training objective helps different few-shot tasks. Also, we find out that models with larger training data are not significantly affected by noisy prompts. Future work includes exploring automatic prompt generation and diverse formats of few-shot tasks such as multiple-choice VQA. Finding optimal prompts require exhaustive engineering to achieve the best performance and leads to impressive results. We leave the exploration of these directions to future investigations.   A Model Architectures <ref type="table" target="#tab_12">Table 9</ref> shows model parameters in our model, FEWVLM. FEWVLM base and FEWVLM large is based on VL-T5  and T5 <ref type="bibr" target="#b20">(Raffel et al., 2020)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COCO Captioning</head><p>We evaluate our model with COCO captioning data. We use Karpathy split <ref type="bibr" target="#b10">(Karpathy and Li, 2015)</ref> for MS COCO captioning, which re-splits train and val images into 113,287 / 5000 / 5000 for train / validation / test. <ref type="table" target="#tab_13">Table 10</ref> shows the results on COCO. sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Prompt Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effect of Pre-training Data</head><p>We pre-train our model with different datasets: MS COCO and Visual Genome (VG), and Conceptual Captions (CC). We investigate which pre-training dataset helps the downstream tasks in a few-shot manner. In <ref type="table" target="#tab_5">Table 12</ref>, we observe that MS COCO and VG datasets are more helpful to the downstream tasks than CC.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of FEWVLM. This shows inference of FEWVLM with prompt-based learning. Given a prompt template, we convert the question text into input text. The prompt helps the model generate correct answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Pre-training objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>VQAv2 results on noisy prompts. We investigate different prompts on various training sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Flickr30k results on hand-crafted prompts. We investigate different hand-crafted prompts (Q1, Q2, and Q3) on various training sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>VQAv2 results on different target prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>VQAv2 results on hand-crafted prompts and the target prompt "&lt;text_1&gt; [A]".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Tables 7, 8, and 9 show the results of each prompt on VQAv2 and Flickr30k with various training VQAv2 results on hand-crafted prompts and the target prompt "[A]"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Encoder Transformer Decoder What position is this man playing?</head><label></label><figDesc>arXiv:2110.08484v2 [cs.CV] 15 Mar 2022</figDesc><table><row><cell>Prompts</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[Q]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>question: [Q] answer: &lt;text_1&gt; [Q] &lt;text_1&gt;</cell><cell>&lt;text_1&gt; Transformer &lt;s&gt; question: What position is this man playing? answer: &lt;text_1&gt;</cell><cell>&lt;text_1&gt; pitcher</cell><cell>pitcher &lt;/s&gt;</cell></row><row><cell></cell><cell>Faster R-CNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>For NoCaps task, it does not have training data. Thus we use the training data from COCO captioning in the experiments following Wang et al. (2021). We evaluate on the VQAv2 validation set, GQA test-dev, OK-VQA test set, test set of Karpathy split for Flickr30k captioning, and NoCaps validation set. We adopt accuracy for VQA datasets and miniImageNet, and CIDEr (Vedantamet al., 2015)  and SPICE<ref type="bibr" target="#b1">(Anderson et al., 2016)</ref> as evaluation metrics for captioning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot VQA results. We test models without any training examples. VL-T5 no-vqa is pre-trained without VQA datasets. Compared to larger models, Frozen and PICa-Full, our models outperform them or show the comparable results.</figDesc><table><row><cell>Model</cell><cell>Model size</cell><cell>VQAv2</cell><cell>OK-VQA</cell><cell>GQA</cell></row><row><cell>Unified VLP</cell><cell>122M</cell><cell>0.0</cell><cell>-</cell><cell>-</cell></row><row><cell>VL-T5 no-vqa</cell><cell>224M</cell><cell>13.5</cell><cell>5.8</cell><cell>6.3</cell></row><row><cell>Frozen</cell><cell>7B</cell><cell>29.5</cell><cell>5.9</cell><cell>-</cell></row><row><cell>PICa</cell><cell>175B</cell><cell>-</cell><cell>17.5</cell><cell>-</cell></row><row><cell>FEWVLM base</cell><cell>224M</cell><cell>43.4</cell><cell>11.6</cell><cell>27.0</cell></row><row><cell>FEWVLM large</cell><cell>740M</cell><cell>47.7</cell><cell>16.5</cell><cell>29.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Few-shot VQA results. We report average performance over 5 different splits. The size of training and validation sets are 16 for our FEWVLM and VL-T5 no-vqa , and Frozen and PICa use 4 and 16 in-context training examples, respectively. For the fair comparison to Frozen, we include FEWVLM * base with 4 training and validation examples.</figDesc><table><row><cell>Model</cell><cell>Model size</cell><cell>VQAv2</cell><cell>OK-VQA</cell><cell>GQA</cell></row><row><cell>Unified VLP</cell><cell>122M</cell><cell>24.3</cell><cell>-</cell><cell>-</cell></row><row><cell>VL-T5 no-vqa</cell><cell>224M</cell><cell>31.8</cell><cell>12.7</cell><cell>19.6</cell></row><row><cell>Frozen</cell><cell>7B</cell><cell>38.2</cell><cell>12.6</cell><cell>-</cell></row><row><cell>PICa</cell><cell>175B</cell><cell>54.3</cell><cell>43.3</cell><cell>-</cell></row><row><cell>FEWVLM  *  base</cell><cell>224M</cell><cell>45.1</cell><cell>14.5</cell><cell>26.9</cell></row><row><cell>FEWVLM base</cell><cell>224M</cell><cell>48.2</cell><cell>15.0</cell><cell>32.2</cell></row><row><cell>FEWVLM large</cell><cell>740M</cell><cell>51.1</cell><cell>23.1</cell><cell>35.7</cell></row><row><cell>Fine-tuned L f ull</cell><cell>-</cell><cell>72.6</cell><cell>-</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot captioning results. We use the CIDEr and SPICE metrics for evaluation.</figDesc><table><row><cell>Model</cell><cell>Model size</cell><cell cols="2">NoCaps</cell><cell cols="2">Flickr30k</cell></row><row><cell></cell><cell></cell><cell cols="4">CIDEr SPICE CIDEr SPICE</cell></row><row><cell>Unified VLP</cell><cell>122M</cell><cell>-</cell><cell>-</cell><cell>24.9</cell><cell>7.2</cell></row><row><cell>VL-T5 no-vqa</cell><cell>224M</cell><cell>4.4</cell><cell>5.3</cell><cell>2.6</cell><cell>2.0</cell></row><row><cell>SimVLM huge</cell><cell>-</cell><cell>101.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FEWVLM base</cell><cell>224M</cell><cell>42.2</cell><cell>8.5</cell><cell>31.0</cell><cell>10.0</cell></row><row><cell>FEWVLM large</cell><cell>740M</cell><cell>47.7</cell><cell>9.1</cell><cell>36.5</cell><cell>10.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Few-shot captioning results. We report average performance over 5 different splits. We use the CIDEr and SPICE metrics for evaluation.</figDesc><table><row><cell>Model</cell><cell>Model size</cell><cell cols="2">NoCaps</cell><cell cols="2">Flickr30k</cell></row><row><cell></cell><cell></cell><cell cols="4">CIDEr SPICE CIDEr SPICE</cell></row><row><cell>Unified VLP</cell><cell>122M</cell><cell>-</cell><cell>-</cell><cell>28.8</cell><cell>9.4</cell></row><row><cell>VL-T5 no-vqa</cell><cell>224M</cell><cell>22.0</cell><cell>6.8</cell><cell>12.8</cell><cell>8.3</cell></row><row><cell>FEWVLM base</cell><cell>224M</cell><cell>48.6</cell><cell>10.0</cell><cell>32.6</cell><cell>12.8</cell></row><row><cell>FEWVLM large</cell><cell>740M</cell><cell>53.1</cell><cell>10.4</cell><cell>37.0</cell><cell>13.5</cell></row><row><cell>Fine-tuned L f ull</cell><cell>-</cell><cell>112.2</cell><cell>13.1</cell><cell>67.4</cell><cell>17.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>5-way miniImageNet results. We evaluate FEWVLM in a generative manner. The shot represents the number of training examples per class.</figDesc><table><row><cell>Model</cell><cell>Model size</cell><cell>1 shot</cell><cell>3 shots</cell><cell>5 shots</cell></row><row><cell>Frozen</cell><cell>7B</cell><cell>14.5</cell><cell>34.7</cell><cell>33.8</cell></row><row><cell>FEWVLM base (no prompt)</cell><cell>224M</cell><cell>48.0</cell><cell>75.0</cell><cell>82.6</cell></row><row><cell>FEWVLM base</cell><cell>224M</cell><cell>57.0</cell><cell>78.0</cell><cell>84.2</cell></row><row><cell>FEWVLM large</cell><cell>740M</cell><cell>57.1</cell><cell>78.3</cell><cell>84.4</cell></row><row><cell>AFHN</cell><cell>-</cell><cell>62.3</cell><cell>-</cell><cell>78.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Zero-shot results of hand-crafted prompts. We test different input prompts in zero-shot predictions. We use a CIDEr metric for Flickr30k. Note that zeroshot setting does not require target prompts.</figDesc><table><row><cell></cell><cell>no prompt</cell><cell>P1</cell><cell>P2</cell><cell>P3</cell></row><row><cell>VQAv2</cell><cell>3.7</cell><cell>9.9</cell><cell>19.0</cell><cell>43.4</cell></row><row><cell></cell><cell>no prompt</cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell></row><row><cell>Flickr30k</cell><cell>9.6</cell><cell>15.2</cell><cell>25.6</cell><cell>31.0</cell></row><row><cell cols="2">6.4 MiniImageNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results on different pre-training objectives.</figDesc><table><row><cell cols="4">We test our pre-training objectives to investigate how it</cell></row><row><cell cols="4">affects zero-shot and few-shot performance. We train</cell></row><row><cell cols="4">FEWVLM base with 16 training and validation exam-</cell></row><row><cell>ples.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Objective</cell><cell>VQAv2</cell><cell>GQA</cell><cell>Flickr30k</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CIDEr</cell></row><row><cell>Zero-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MaskedLM</cell><cell>42.4</cell><cell>25.1</cell><cell>4.6</cell></row><row><cell>PrefixLM</cell><cell>11.9</cell><cell>6.7</cell><cell>26.8</cell></row><row><cell>MaskedLM + PrefixLM</cell><cell>43.4</cell><cell>27.0</cell><cell>31.0</cell></row><row><cell>Few-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MaskedLM</cell><cell>46.0</cell><cell>31.4</cell><cell>18.5</cell></row><row><cell>PrefixLM</cell><cell>40.8</cell><cell>27.6</cell><cell>31.8</cell></row><row><cell>MaskedLM + PrefixLM</cell><cell>48.2</cell><cell>32.2</cell><cell>32.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Model architectures.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">FEWVLM base FEWVLM large</cell></row><row><cell># Layers</cell><cell>12+12</cell><cell>24+24</cell></row><row><cell>Hidden dimension</cell><cell>768</cell><cell>1,024</cell></row><row><cell>FF hidden size</cell><cell>3,072</cell><cell>4,096</cell></row><row><cell># Attention head</cell><cell>12</cell><cell>16</cell></row><row><cell>Attention head size</cell><cell>64</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>COCO captioning results. We use the CIDEr and SPICE metrics for evaluation.</figDesc><table><row><cell cols="2">Model</cell><cell>Model size</cell><cell cols="2">Zero-shot</cell><cell cols="2">Few-shot</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">CIDEr SPICE CIDEr SPICE</cell></row><row><cell cols="2">VL-T5 no-vqa</cell><cell>224M</cell><cell>4.9</cell><cell>2.0</cell><cell>43.0</cell><cell>10.8</cell></row><row><cell cols="2">SimVLM huge</cell><cell>-</cell><cell>102.3</cell><cell>22.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FEWVLM base</cell><cell>224M</cell><cell>84.5</cell><cell>16.2</cell><cell>98.7</cell><cell>18.9</cell></row><row><cell cols="2">FEWVLM large</cell><cell>740M</cell><cell>92.1</cell><cell>17.3</cell><cell>100.4</cell><cell>19.1</cell></row><row><cell cols="2">Unified VLP (fully supervised)</cell><cell>122M</cell><cell>-</cell><cell>-</cell><cell>117.7</cell><cell>21.3</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACC on VQAv2</cell><cell>10 20 30 40</cell><cell></cell><cell></cell><cell></cell><cell cols="2">no prompt P1 P2 P3</cell></row><row><cell></cell><cell cols="6">0 10 20 30 50 100 200 300 Training size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Prompt templates. We test different input prompts on VQAv2.[Q] refers to input question text. We use &lt;text_1&gt; [A] as target text. We append image features to input text.</figDesc><table><row><cell>Input prompt template</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Few-shot results on different pre-training datasets. We examine different pre-training datasets on each downstream tasks.</figDesc><table><row><cell>Dataset</cell><cell>VQAv2</cell><cell>GQA</cell><cell>Flickr30k</cell></row><row><cell>MS COCO, VG</cell><cell>48.2</cell><cell>32.2</cell><cell>32.6</cell></row><row><cell>Conceptual Captions</cell><cell>36.7</cell><cell>25.9</cell><cell>22.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We include COCO captioning results on Sec. B of Appendix.2 For VQA and captioning, we include k samples in total, not per class.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">nocaps: novel object captioning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00904</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="8947" to="8956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>December 6-12, 2020, virtual</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1504.00325</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1931" to="1942" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.670</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6325" to="6334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00686</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298932</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01348</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13467" to="13476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1908.03557</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OK-VQA: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00331</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/2105.11447</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VL-BERT: pretraining of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2106.13884</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299087</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2108.10904</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2109.05014</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
