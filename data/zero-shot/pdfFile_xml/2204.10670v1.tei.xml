<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARAMIXER: PARAMETERIZING MIXING LINKS IN SPARSE FACTORS WORKS BETTER THAN DOT-PRODUCT SELF-ATTENTION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Khalitov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PARAMIXER: PARAMETERIZING MIXING LINKS IN SPARSE FACTORS WORKS BETTER THAN DOT-PRODUCT SELF-ATTENTION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>parameterization ? mixing links ? sparse ? matrix factorization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-Attention is a widely used building block in neural modeling to mix long-range data elements. Most self-attention neural networks employ pairwise dot-products to specify the attention coefficients. However, these methods require O(N 2 ) computing cost for sequence length N . Even though some approximation methods have been introduced to relieve the quadratic cost, the performance of the dotproduct approach is still bottlenecked by the low-rank constraint in the attention matrix factorization. In this paper, we propose a novel scalable and effective mixing building block called Paramixer. Our method factorizes the interaction matrix into several sparse matrices, where we parameterize the non-zero entries by MLPs with the data elements as input. The overall computing cost of the new building block is as low as O(N log N ). Moreover, all factorizing matrices in Paramixer are full-rank, so it does not suffer from the low-rank bottleneck. We have tested the new method on both synthetic and various real-world long sequential data sets and compared it with several state-of-the-art attention networks. The experimental results show that Paramixer has better performance in most learning tasks. 3</p><p>Keywords parameterization ? mixing links ? sparse ? matrix factorization 1 Introduction</p><p>Transformer models have been widely used on many tasks such as text classification [1], text summarization, promoter region prediction <ref type="bibr" target="#b1">[2]</ref>, and image classification <ref type="bibr" target="#b2">[3]</ref>. The main engine in Transformer is the self-attention mechanism, which can work in parallel to mix long-range tokens in a long sequence. This fundamental innovation eliminated the sequential dependency in recurrent neural networks and was used as a building block for many powerful models, such as Bert <ref type="bibr" target="#b3">[4]</ref>, GPT[5]  and Ernie[6].</p><p>However, the original self-attention is not scalable because it requires computing and storing all pairwise dot-products, which incurs O(N 2 ) cost for sequence length N . The scalability issue significantly restricted the application of neural models based on self-attention.</p><p>Various methods have been introduced to alleviate the quadratic cost of full attention. Some of them attempt to shorten the sequence length <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, even though much information is lost. Others try to break up the softmax by a certain kernel factorization. Another family of methods sparsify the attention matrix with predefined attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, most Transformer variants stick to the dot-product self-attention, of which the expressive power is restricted by the low-rank bottleneck <ref type="bibr" target="#b12">[13]</ref> because the dimensionality of the dot-product space is much smaller than the sequence length. Therefore, they cannot accurately model the transformation if the attention is intrinsically high-rank. This paper proposes a scalable and effective attention building block called Paramixer without dot-product and softmax. Our method directly parameterizes the mixing links in several sparse factors to form an attention matrix, where all * Equal contribution ? Corresponding author, zhirong.yang@ntnu.no 3 https://github.com/wiedersehne/Paramixer arXiv:2204.10670v1 [cs.</p><p>LG] 22 Apr 2022</p><p>Parameterizing Mixing Links in Sparse Factors Works Better than Dot-Product Self-AttentionA PREPRINT factorizing matrices are full-rank. Therefore Paramixer does not suffer from the low-rank bottleneck. We present two ways to specify the non-zero positions in each sparse factor. Both lead to an economical approximation of the full attention matrix, with the computing cost as low as O(N log N ). As a result, our method can easily model very long sequential data.</p><p>We have tested Paramixer on various sequence data sets and compared it with many popular self-attention neural networks based on dot-products. The experimental results show that Paramixer gets the best performance on very long sequence tasks, including synthetic data inference, Genome classification, and character-level long document classification. Paramixer also achieves state-of-art accuracy on the public Long Range Arena benchmark tasks.</p><p>We organize the rest of the paper as follows. Section 2 investigates dot-product self-attention and its related work. Section 3 introduces the development clue and model architecture of Paramixer. The experimental settings and results are presented in Section 4, and we conclude the paper in Section 5.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-attention (SA) is a building block in neural networks which enables long range interaction between elements in a sequence. The most widely used SA architecture is called Transformer <ref type="bibr" target="#b0">[1]</ref>, where the self-attention matrix is constructed using scaled dot-product followed by softmax. Given an input sequence of N elements encoded in X ? R N ?d , a self-attention building block calculates a weighted average of feature representations V ? R N ?dv , where the weights are the result of scaled dot-product of Q ? R N ?D and K ? R N ?D : Q = XW q , K = XW k , and V = XW v . Then the self-attention in Transformer is</p><formula xml:id="formula_0">Self-Attention(Q, K, V ) = AV,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">A = softmax QK T ? D<label>(2)</label></formula><p>and the softmax applies row-wise on the scaled dot-products.</p><p>The attention matrix in Eq. 2 is not scalable because it requires computing and storing N 2 attention values, which is infeasible for a large N . The quadratic cost becomes a significant bottleneck when applying self-attention applications for long sequences. Many research teams have proposed Transformer variants to relieve this problem.</p><p>The first branch of methods attempts to reduce N . The Linformer approximation <ref type="bibr" target="#b13">[14]</ref> uses random projection to reduce the rows of K and V from N to r with r &lt; N . However, because r ? ?2 with the approximation error bound, Linformer has to use a large r to achieve satisfactory approximation quality. Another method called Enformer <ref type="bibr" target="#b7">[8]</ref> shortens the sequence length by convolutional network pooling.</p><p>The second branch of methods tries to break up the softmax by a certain kernel factorization A ? ?(Q)?(K) T , where ? Q and ? K ? R N ?r with r &lt; N . Then by the association rule of multiplication, we can calculate ?(Q) ?(K) T V and avoid the quadratic cost. For example, Nystr?mformer <ref type="bibr" target="#b14">[15]</ref> uses a few landmarks as surrogates to construct ?(Q) and ?(K). Linear Transformer <ref type="bibr" target="#b15">[16]</ref> directly chooses ?(x) = elu(x) + 1. Performer <ref type="bibr" target="#b16">[17]</ref> uses r orthogonal random features to obtain ?(Q) and ?(K). Random features were also used in another work <ref type="bibr" target="#b17">[18]</ref>, with gating mechanism combined.</p><p>The third branch of methods chooses only a subset of (i, j) pairs in each attention layer. These include Sparse Transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> that use a set of neighboring tokens, Sinkhorn Transformer <ref type="bibr" target="#b11">[12]</ref> that uses blockwise sparsity, Longformer <ref type="bibr" target="#b8">[9]</ref> that uses dilated sparse connections, and BigBird <ref type="bibr" target="#b1">[2]</ref> that uses both blockwise and dilated sparse connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Paramixer</head><p>Despite many variants of Transformers, there are still several drawbacks because they stick to dot-product + softmax or their approximation. Below we discuss the disadvantages of these two components and propose our solution without them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Drawbacks of dot-products and softmax</head><p>The first drawback is the low-rank bottleneck: dot-product self-attention requires D N ; otherwise, the Q and K matrices are unaffordable for a large N . Bhojanapalli et al. have shown that the low-rank bottleneck restricts the Each node in the circular graph represents a sequence element. The links between nodes correspond to the non-zero entries in W (m) (here m = 1) output from f <ref type="bibr">(m)</ref> . Note that the sparse structure of all factors in CHORD is the same, while it varies at different m's in CDIL.</p><p>representation power and attention performance <ref type="bibr" target="#b12">[13]</ref>. However, their workaround that uses D = N does not apply for long sequences due to the quadratic cost.</p><p>Another limitation comes from the pairwise definition of dot-product. Although dot-products have a theoretical connection to Reproducing Kernel Hilbert Space (RKHS), self-attention cannot benefit from more than two factors in the matrix product because the kernels are defined in pairs.</p><p>Another key component, softmax, in Transformer is also problematic. First, such nonlinearity over dot-products leads to quadratic cost. Comprehensive approximation methods are required to approximate softmax for more economical matrix products.</p><p>Second, softmax limits the mixing capability. The softmax layer output probabilities and the mixing result are thus constrained in the convex hull of the existing elements. The constraint is more severe in sparse attention methods such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, where the mixing result is in a rather constrained convex hull of a few involved elements.</p><p>Moreover, softmax is incompatible with sparse attention. The latter was introduced to relieve the quadratic computing cost caused by softmax. However, after each sparse attention layer with softmax, the network can output probabilities for only a few sequence elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterizing mixing links</head><p>Seeing the drawbacks of dot-products and softmax, we rethink self-attention as a transformation block and redesign the neural model. First, we drop the softmax because it is not compulsory for the mixing function but limits the mixing capability. Then the transformation becomes V new = AV for an unconstrained mixing matrix A.</p><p>Next, we consider parameterizing the mixing links or coefficients for each matrix row</p><formula xml:id="formula_2">A i: = f (X i ; ?), where f : R d ? R N is a neural network with weights ?.</formula><p>However, such simple parameterization does not solve the quadratic cost. We thus consider a sparse factorization of the mixing matrix A:</p><formula xml:id="formula_3">A = M m=1 W (m) ,<label>(3)</label></formula><p>where each sparse factor W (m) is a full-rank sparse square matrix. Then we parameterize each sparse factor W (m) as</p><formula xml:id="formula_4">W (m) i: = f (m) (X i ; ? m ),<label>(4)</label></formula><p>where i = 1, . . . , N and f : R d ? R K is a Multilayer Perceptron (MLP) that outputs the K non-zero entries 4 of each row in W (m) . If the total number of non-zeros in the factors is much smaller than N 2 , we obtain a new economical self-attention method without dot-product and softmax. We call the new method Paramixer.</p><p>There are different ways or protocols to specify the sparse structure or the non-zero entries. We have studied two protocols and present them below. For short, we abbreviate f The first protocol CHORD modifies from an algorithm with the same name in peer-to-peer lookup service <ref type="bibr" target="#b18">[19]</ref>, where the i-th row of each sparse factor W (m) is parameterized as</p><formula xml:id="formula_5">(m) ik def = f (m) (X i ; ? m ) k .</formula><formula xml:id="formula_6">W (m) ij = ? ? ? ? ? f (m) i1 if j = i f (m) ik if j = i + 2 k?2 mod N 0 otherwise.<label>(5)</label></formula><p>where j = 1, . . . , N and k = 1, ..., K. That is, each row has K non-zeros, and in total all W (m) 's have M N K non-zeros. In this work, we follow the original CHORD algorithm to use K = M = log N . Therefore the number of stored entries is N log 2 N .</p><p>Each factor W (m) can be treated as the adjacency matrix of a directed graph, where the (i, j)-th non-zero entry can be seen as a link from i to j. The graph visualization and the parameterization are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The product of the factorizing matrices corresponds to the connections in the circular graph after multiple sparse factors. Theorem 2 in <ref type="bibr" target="#b18">[19]</ref> guarantees that the graph will become complete with high probability after M = log N sparse factors. That is, the resulting A matrix will become full after the matrix product.</p><p>The second protocol CDIL (Circular DILated) originates from Temporal Convolution Networks (TCN) <ref type="bibr" target="#b19">[20]</ref>, where we modify the dilated connections at both sides and along a circular graph to ensure the receptive fields are symmetric. The i-th row of W (m) is parameterized as</p><formula xml:id="formula_7">W (m) ij = ? ? ? ? ? f (m) i1 if j = i f (m) ik if j = i + p k?1 2 m mod N 0 otherwise,<label>(6)</label></formula><p>where p = 1, 2, ..., K?1 2 , ?1, ?2, ? K?1 2 and k = 2, . . . , K. Similar to CHORD, the CDIL protocol includes self links but the other links appear on both sides, and the dilation 2 m varies at different m's. The CDIL protocol and parameterization are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In CDIL, each factor contains KN non-zero entries, and in total, there are KN log N non-zeros if M = log N , which is more economical than CHORD if K &lt; log N . It can be proven that the product A = M m=1 W (m) is a full matrix with the CDIL protocol.</p><p>The following proposition states that the factorizing matrices constructed by the two protocols are full-rank. The proof in given the supplemental document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Paramixer neural networks</head><p>A Paramixer block transforms an N ? d tensor X to another tensor of the same size. See <ref type="figure" target="#fig_1">Figure 2</ref>. Here we use an MLP g : R d ? R d with weights ? to mix the columns of the input tensor, which is similar to the product with W v in Transformer. In this work, we used simple three-layer MLPs (Linear-GELU-Linear) for both f (m) 's and g. Stacking L such blocks forms a neural network backbone, denoted by ParamixerNet, for representation learning.</p><p>We have tried two options for parameterization in multi-block setting:</p><formula xml:id="formula_8">W (l,m) = f (l,m) X (l?1) ; ? (l) m and W (l,m) = f (l,m) X (0) ; ? (l)</formula><p>m , where the superscript l indexes the l-th block and X (l?1) is the input to the l-th block, with X (0) = X + PE. We find that the latter option makes the network easier to train and works better.</p><p>The ParamixerNet output can then be fed to a loss function J , and overall, the learning task can be formulated as the following optimization problem:</p><formula xml:id="formula_9">minimize ? J (ParamixerNet(X + PE; ?)),<label>(7)</label></formula><p>where ? = ?</p><formula xml:id="formula_10">(l) 1 , . . . , ? (l) M , ? (l) L l=1</formula><p>. The optimization can be implemented with back-propagation, and a gradient-based algorithm such as Adam <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted four groups of experiments. In the first group, we demonstrate the scalability of Paramixer on long synthetic sequences with lengths up to tens of thousands of positions. Then, we tested the performance of Paramixer on the pubic Long Range Arena benchmark data sets. In the third group, we built a character-level document classification task to evaluate if Paramixer can handle real-world long text sequences with tens of thousands of tokens on average. Finally, we showcase if Paramixer performs well in modeling long genome sequences. We ran all experiments on a Linux machine with 3?NVIDIA Tesla V100 32GB, Intel Xeon Gold 6240 CPU @ 2.60GHz processors, with 754GB of system memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Scalability Test</head><p>In this section, we examine the scalability of Paramixer and compare its performance with several competitors. We used two synthetic data sets composed of long sequences for supervised learning tasks. An experimental setup was inspired by <ref type="bibr" target="#b21">[22]</ref>, where similar synthetic sequences appeared for scalability tests. The details of both tasks are given below:</p><p>? Adding Problem. This is a sequence regression task. Each element of an input sequence is a pair of numbers</p><formula xml:id="formula_11">(a i , b i ), where a i ? U (?1, 1), b i ? {0, 1}, i = 1, .</formula><p>. . , N . We generated signals at two randomly selected positions t 1 and t 2 such that b t1 = b t2 = 1 and b i = 0 elsewhere. The learning target is y = 0.5 + a t1 + a t2 4 .</p><p>For example, an input sequence [(0.5, 1), (?0.2, 0), (0.2, 1), (?0.8, 0), (0.6, 1)] will have the learning target y = 0.825. Unlike <ref type="bibr" target="#b21">[22]</ref>, we did not restrict the t 1 and t 2 choice and made the task more challenging. That is, the relevant signals can appear either locally or at a great distance from each other. In evaluation, a network prediction? is considered correct if |y ??| &lt; 0.04. ? Temporal Order. This is a sequence classification task. Each sequence consists of randomly chosen symbols from the alphabet {a, b, c, d, X, Y }, where the first four are noise symbols. Each sequence has two signal symbols, either X or Y , which appear at two arbitrary positions. The four target classes correspond to the ordered combinations of the signal symbols (X,</p><formula xml:id="formula_12">X), (X, Y ), (Y, X), and (Y, Y ). For example, an input sequence [a, d, Y, c, b, a, Y, c, d]</formula><p>should be classified as Class 3.</p><p>We generated data of different sequence lengths for each problem: from N = 128 to N = 2 <ref type="bibr" target="#b14">15</ref> , progressively increasing the length by the factor of two. For each sequence length, a model can access 100 000 training sequences and 5 000 testing instances for evaluation.</p><p>We compared Paramixer with a group of popular methods based on scaled dot-product attention (referred as X-formers), including Linformer <ref type="bibr" target="#b6">[7]</ref>, Performer <ref type="bibr" target="#b16">[17]</ref>, Reformer <ref type="bibr" target="#b22">[23]</ref> and Nystr?mformer, which all <ref type="bibr" target="#b14">[15]</ref> have claimed to be scalable. For completeness, we also included the original Transformer <ref type="bibr" target="#b0">[1]</ref>. We used the open-source PyTorch implementations 5 of these models.</p><p>We fine-tuned the main hyperparameters in a standard cross-validation manner for Paramixer and X-formers, including the number of layers and heads, dimensionality of the token embedding, and query/key/value dimensions. For the Temporal Order problem, we directly fed the data instances to the embedding layers. For the Adding problem, the input   data was only two-dimensional, and one of them was real-valued. Directly using such a low-dimensional embedding space would limit the expressive power. So we added a linear layer to augment the dimensionality to allow sufficient freedom for the scaled dot-products in the X-former architectures. All the models were optimized using the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with the learning rate of 0.001 using a batch size of 40.</p><p>The results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. For the Adding problem, we see that all models obtain 100% accuracy for N ? 256. In summary, Paramixer has better scalability than the attention neural networks based on scaled dot-products. When scaled to tens of thousands, Paramixer still gives very high prediction accuracy. The results suggest we can evaluate our model on more real-world tasks with very long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long Range Arena Public Bechmark</head><p>Next, we evaluate the performance of Paramixer on Long Range Arena (LRA), a publicly available benchmark for modeling long sequential data <ref type="bibr" target="#b23">[24]</ref>. The details of the tasks are the following:</p><p>? ListOps. ListOps is a classification task designed for measuring the ability of models to parse hierarchically constructed data <ref type="bibr" target="#b25">[26]</ref>. Each sequence is composed of operators, digits, and left or right brackets. The brackets define lists of items. Each operator in a sequence takes the items in a list as input and returns a digit.</p><p>? Text Classification. We use the IMDb Review dataset <ref type="bibr" target="#b26">[27]</ref> which requires the model to classify each review as positive or negative. The task uses a character-level representation for each sequence, which makes the tasks more challenging than the word-level version. We truncated or padded every sequence to a fixed length (N = 4k).</p><p>? Image Classification. This task is to classify images into one of ten classes. Each image is flattened to form a sequence of length 1024. Unlike conventional computer vision, the task requires the predictors to treat the grayscale levels (0-255) as categorical values. That is, each image becomes a sequence of symbols with an alphabet size of 256. Two example matrices are shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>? Pathfinder. This task is motivated by cognitive psychology <ref type="bibr" target="#b27">[28]</ref>, and constructed using synthetic images. Each image (size 32 ? 32) contains two highlighted endpoints and some path-like patterns. The models need to classify whether there is a path consisting of dashes between two highlighted points. Similar to the Image Classification task, the predictors must flatten the image to a sequence of symbols with length 1024. Two example matrices are shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>For a fair comparison, we followed the experiment settings in the original paper <ref type="bibr" target="#b23">[24]</ref> and evaluated multi-block ParamixerNet in the above tasks. We constructed Paramixer in variable blocks and used cross-validation to report the model with the best hyperparameters. We ran the model four times with a different random seed for each task.</p><p>We compared Paramixer with many X-former architectures in prediction accuracies. If a method has different implementations, we quote all the alternatives and their results. The results show that Paramixer beats all the other self-attention-based transformers on all the tasks, having the best classification accuracy. Such stable cross-task wins suggest that forming an attention matrix with parameterizing mixing links works better than those based on scaled dot-products.</p><p>Remarkably, Paramixer has achieved state-of-the-art performance on Text and Pathfinder tasks. For Text Classification, ParamixerNet achieves 83.32%, which is 14.92 p.p. higher than the best Transformer variant, Transformer-LS (68.4%).</p><p>Our method also wins with the accuracy of 80.49% on Pathfinder, where it gains about 5 percentage points higher than the runner-up model. The compelling improvement brought by ParamixerNet is a probable cause of the high-rank nature of Natural Language <ref type="bibr" target="#b28">[29]</ref>. For Paramixer, we reported two variants for this task, using the CHORD and CDIL protocols independently. As shown in <ref type="table" target="#tab_2">Table 1</ref>, CHORD wins on ListOps and Pathfinder, while CDIL gains the best results on Text and Image. However, CHORD still has a competitive accuracy on Text (83.12%) and Image (45.01%), which demonstrates that Paramixer has more stable performance using the CHORD protocol. Consequently, we used CHORD as a default protocol for the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Long Document Classification</head><p>The goal of this subsection is to study the benefits of modeling long sequences in NLP tasks. The character-level text classification task from the LRA benchmark is not long enough to conclude this pattern, so we seek for Longer Document Data with tens of thousands of tokens.</p><p>Long-document-dataset is a publicly available dataset. It contains a collection of academic papers, which are parsed using the arXiv sanity preserver program and published on Github <ref type="bibr" target="#b29">[30]</ref>. There are eleven diverse research areas a paper may belong to. Similar to the source article, we used only four classes of documents: cs.AI, cs.NE, math.AC, math.GR, having a final set of 11 956 documents in total. We used 70% of the data for training, 20% for validation, and 10% for the test. Unlike the original study <ref type="bibr" target="#b29">[30]</ref>, we transformed each article into a sequence of characters and finally got a challenging task with an average training sequence length of 52 112.</p><p>We truncate every sequence to a fixed length. Since NLP task is known to benefit from using longer sequence length, we tested Paramixer on sequence lengths of 16 384 and 32 768 to see if Paramixer can be in favor of using longer context. We studied other transformer-like variants as well.</p><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, Paramixer outperforms the follow-up transformer-based competitors by 12.19 and 16.86 percentage points on two sequence lengths, respectively. Paramixer achieves higher accuracy when using 32k tokens, which signals a better generalization from using more extended context. Among all the competitors, only Linformer, Transformer-LS, and Nystr?mformer can handle the sequences of this length to a certain degree. Nevertheless, the gap in performance between sequence lengths 32k and 16k is severe, suggesting that the low-rank factorization weakens the prediction power on high-rank NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Genome Classification</head><p>Inspired by a recent rise in using deep learning models in biological applications, such as Chromatin-profile prediction <ref type="bibr" target="#b1">[2]</ref> and genome analysis <ref type="bibr" target="#b7">[8]</ref>. However, directly processing genome and protein sequences with tens of thousands of positions are problematic. The standard preprocessing pipeline includes chunking a long sequence into many fragments, which are modeled independently. This approach is associated with inevitable information loss caused by the long-distant nature of interaction processes in the DNA sequences <ref type="bibr" target="#b30">[31]</ref>. We tested our design on this type of data. By design, Paramixer can treat a full sequence as an input without the need for its preliminary segmentation, allowing it to capture highly non-local effects.</p><p>We built two data sets for this task. The first group of DNA sequences were downloaded from NONCODEv6 [32] 6 . We used human and fruitfly DNA sequences to construct a binary-classification task. We filtered out the sequences shorter than 5k and thus got 9 536 human DNA sequences and 4401 fruitfly DNA sequences with mean sequence lengths 10 586 and 9 793, respectively. We name the data set HFDNA. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, the two species in HFDNA have similar sequence length distributions, which means they can not be directly distinguished based on a sequence length  The other data set, namely MTcDNA (mouse and turtles' cDNA), was built following the same preprocessing/split strategy. The original cDNA sequences were downloaded from Ensembl genome browser 7 <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. The task is to predict a binary class, given either mouse or turtles cDNA sequence. The mouse class presented in 12 300 cDNA sequences with a mean sequence length of 7 235, including two sub-species: Mus musculus and Mus spretus. The turtle class contains 4 193 cDNA sequences of Chelonoidis abingdonii and Gopherus agassizii with a mean sequence length of 7 072. The percentage histograms of sequence lengths are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. It is clearly seen that the two species have a big overlap in terms of sequence length, which makes it hard to discriminate between them only by length.</p><p>We compared ParamixerNet with several transformer-based models on this task. Because both data sets are imbalanced, so we report ROC AUC values for this experiment. As shown in <ref type="table" target="#tab_4">Table 3</ref>, Paramixer achieves 100% ROC AUC on HFDNA, outperforming the runner-up result from Transformer (94.32%). Notably, our design holds the best score at 84.86% ROC AUC on MTcDNA, which is higher than Transformer by 5.48 percentage points. The experimental results show that Paramixer can successfully handle long DNA sequences and has the potential to assist in the biological applications that require modeling long-distant gene interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a scalable and effective building block called Paramixer for attention neural networks. Our method replaced the dot-products and softmax with parameterization of the mixing links in full-rank sparse factors of the attention matrix and thus got rid of the low-rank bottleneck in most existing attention models. Our method has complexity as low as O(N log N ) and can efficiently deal with sequences up to tens of thousands. Besides scalability, Paramixer has also demonstrated strong performance in terms of accuracy. Neural networks, by stacking the proposed Paramixer blocks, have defeated Transformer and many of its variants in a variety of tasks, including synthetic data inference, the public Long Rang Arena benchmark, and classification of very long text documents and genome sequences.</p><p>In the future, we could study other applications beyond classification, for example, gene expression prediction from sequence and pretraining with unsupervised data. The basic Paramixer block could be extended using other existing attention techniques such as multiple heads and relative positional encoding. Later, in addition to the CHORD and CDIL protocols, we could consider the other predefined protocols or even adaptively learned protocols for the sparse structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Sparse Factorization of Large Square Matrices 1 Synthetic Data Experiments</head><p>For both problems in the scalability test, we generated sequences using the setup described in the main paper. We ran the experiments on sequences with variable lengths: from 128 to 32k. The longer sequences, the more complex the retrieval process. There is a slight difference in the pre-processing part. For the Adding problem, the input data was only two-dimensional. To avoid using such a low-dimensional embedding space, we augmented the dimensionality with an additional linear layer to assure sufficient freedom for dot-product attention architectures. The training configuration and hyperparameters are the same for both the Adding problem and the Temporal Order problem. Their summary is in <ref type="table" target="#tab_2">Table A1</ref> 2 Long Range Arena</p><p>The data set for the LRA benchmark is publicly available. The information about data and the download link can be found in the official GitHub repository: https://github.com/google-research/long-range-arena.</p><p>? ListOps The raw data for this problem is organized as three separate files basic_train.tsv, basic_test.tsv, basic_val.tsv for training, testing, and validation data, respectively. The split is fixed. In addition to the tokens described in the main paper, each sequence has "(" and ")" symbols, which should be removed. To equalize the lengths of the sequences, we used the built-in PyTorch padding functional. After the sequences are prepared, the embedding layer processes each unique value, thus mapping elements to the embedding space. The rest of the training process is straightforward. ? Text Classification We downloaded IMDB data set using the tensorflow-dataset package, and got 25000 instances for training and another 25000 for testing. We went through the whole corpus and extracted the character vocabulary. Then we mapped each sequence to a vector of indices using this vocabulary. Finally, we truncated or padded each sequence to a fixed length of 4096. For every review, we add ["CLS"] token to each sequence and use the embedding of ["CLS"] token for final classification. We used three blocks Paramixer for this task. ? Image Classification CIFAR10 is a well-known dataset, which can be downloaded from the torchvision package. The train/test splitting is fixed. To make images grayscaled, we used standard transformation transforms-grayscale from the same package. An image is flattened to a sequence of length 1024. Then each element is mapped to a dictionary of size 256 (all possible intensity values) and given to the embedding layer. ? Pathfinder The problem data consists of two types of files: images and metafiles. Metafiles store information about all the images and their corresponding labels (positive or negative). There are three classes of images: curv_baseline (easy), curv_contour_length_9 (medium), curv_contour_length_14 (hard). An image class corresponds to the distance between its endpoints (curve length), thus positively correlates with the difficulty level. The exact data split is not provided. To separate the data into three parts, we iterated over all metafiles from the catalogs and constructed the training/val/test (90%/5%/5%) sets such that all three types of images are present equally. The rest of the processing is similar to the Image Classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Document Classification</head><p>The task is a four-class classification problem. The class of a paper is defined by its arxiv categorization, namely, cs.AI, cs.NE, math.AC, and math.GR. Each class in the data set is almost equally presented, with a slight class imbalance: 2995, 3012, 2885, and 3065 documents, respectively. To transform the raw articles into sequences, we first went through the whole corpus and extracted the character vocabulary. Then we mapped each character sequence to a vector of indices using this vocabulary. We fine-tuned Paramixer and X-formers to get the best results. The hyperparameters of Paramixer were selected using a similar process. Final configurations are shown in <ref type="table" target="#tab_2">Table A1</ref>. For every document, we add ["CLS"] token to each sequence and use the result embedding of ["CLS"] token for the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Genome Classification</head><p>When building MTcDNA we downloaded cDNA sequences of Chelonoidis abingdonii and Gopherus agassizii, and merged them as a turtle data set. Following the same strategy, we built the mouse data set using Mus musculus and  <ref type="table" target="#tab_2">Table A1</ref>.</p><p>5 Proof of Proposition 3.1 in the main paper Definition 1. An N ? N circulant matrix C takes the form is called the associated polynomial of circulant matrix C.</p><formula xml:id="formula_13">C = ? ? ? ? ? ? ? ? c 0 c N ?1 ? ? ? c 2 c 1 c 1 c 0 c N ?1 ? ? ? c 2 . . . c 1 c 0 . . . . . . c N ?2 ? ? ? . . . . . . c N ?1 c N ?1 c N ?2 . . .</formula><p>We have the following theorem in the literature <ref type="bibr" target="#b34">[35]</ref>: Theorem 2. The rank of a circulant matrix C is equal to N ? d, where d is the degree of the polynomial GCD(f (x), x N ?1 ). Now we can prove Preposition 3.1 for the CHORD protocol. The proof for CDIL follows similarly.</p><p>Proof. The associated polynomial of W (m) is</p><formula xml:id="formula_14">f (x) = log 2 N ?1 k=0 x k</formula><p>Because GCD(f (x), x N ? 1) = 1 = x 0 , the rank of W (m) is N .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of (a &amp; b) the CHORD and (c &amp; d) the CDIL protocols for N = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of an example Paramixer neural network (M = 4). After adding the positional embedding, it applies L Paramixer blocks and obtains the transformed tensor X new .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 1 .</head><label>1</label><figDesc>W<ref type="bibr" target="#b0">(1)</ref> , . . . , W (M ) constructed in Eq. 5 or Eq. 6 are in general full-rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Error percentage of Paramixer and the X-formers for both the Adding problem and the Temporal Order problem with increasing sequence lengths.Image, automobileImage, horse Pathfinder, Negative Pathfinder, Positive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example data from the Image Classification (left two) and Pathfinder tasks (right two).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Sequence length distributions for HFDNA (top) and MTcDNA (bottom). X-axis is the range of genome sequence length, and y-axis is the percentage of total instance numbers for each bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition 2 .</head><label>2</label><figDesc>The polynomialf (x) = c 0 + c 1 x + ? ? ? + c N ?1 x N ?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy by Paramixer and X-formers on the four LRA tasks. Methods that are absent in the corresponding paper are signed by a dash ("-"). For Paramixer, we present the mean (?) and standard deviation (?) across multiple runs in the ? ? ? format.</figDesc><table><row><cell>Model</cell><cell>ListOps</cell><cell>Text</cell><cell>Image</cell><cell>Pathfinder</cell></row><row><cell></cell><cell>N = 2000</cell><cell>N = 4000</cell><cell>N = 1024</cell><cell>N = 1024</cell></row><row><cell>Transformer [24]</cell><cell>36.37</cell><cell>64.27</cell><cell>42.44</cell><cell>71.40</cell></row><row><cell>Transformer [25]</cell><cell>37.13</cell><cell>65.35</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformer [15]</cell><cell>37.10</cell><cell>65.02</cell><cell>38.20</cell><cell>74.16</cell></row><row><cell>Sparse Transformer [24]</cell><cell>17.07</cell><cell>63.58</cell><cell>44.24</cell><cell>71.71</cell></row><row><cell>Longformer [24]</cell><cell>35.63</cell><cell>62.58</cell><cell>42.22</cell><cell>69.71</cell></row><row><cell>Linformer [24]</cell><cell>37.70</cell><cell>53.94</cell><cell>38.56</cell><cell>76.34</cell></row><row><cell>Linformer [25]</cell><cell>37.38</cell><cell>56.12</cell><cell>-</cell><cell>-</cell></row><row><cell>Linformer [15]</cell><cell>37.25</cell><cell>55.91</cell><cell>37.84</cell><cell>67.60</cell></row><row><cell>Reformer [24]</cell><cell>37.27</cell><cell>56.10</cell><cell>38.07</cell><cell>68.50</cell></row><row><cell>Reformer [25]</cell><cell>36.44</cell><cell>64.88</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer [15]</cell><cell>19.05</cell><cell>64.88</cell><cell>43.29</cell><cell>69.36</cell></row><row><cell>Performer [24]</cell><cell>18.01</cell><cell>65.40</cell><cell>42.77</cell><cell>77.05</cell></row><row><cell>Performer [25]</cell><cell>32.78</cell><cell>65.21</cell><cell>-</cell><cell>-</cell></row><row><cell>Performer [15]</cell><cell>18.80</cell><cell>63.81</cell><cell>37.07</cell><cell>69.87</cell></row><row><cell>BigBird [24]</cell><cell>36.06</cell><cell>64.02</cell><cell>40.83</cell><cell>74.87</cell></row><row><cell>Linear Transformer [24]</cell><cell>16.13</cell><cell>65.90</cell><cell>42.34</cell><cell>75.30</cell></row><row><cell>Transformer-LS [25]</cell><cell>38.36</cell><cell>68.40</cell><cell>-</cell><cell>-</cell></row><row><cell>RFA-Gaussian [18]</cell><cell>36.80</cell><cell>66.00</cell><cell>-</cell><cell>-</cell></row><row><cell>Nystr?mformer [25]</cell><cell>37.34</cell><cell>65.75</cell><cell>-</cell><cell>-</cell></row><row><cell>Nystr?mformer [15]</cell><cell>37.15</cell><cell>65.52</cell><cell>41.58</cell><cell>70.94</cell></row><row><cell>Paramixer (CHORD)</cell><cell>39.</cell><cell></cell><cell></cell><cell></cell></row></table><note>57?0.32 83.12?0.33 45.01?0.21 80.49?0.13 Paramixer (CDIL) 37.78?0.28 83.32?0.19 46.58?0.05 67.13?0.42</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies on long document classification.</figDesc><table><row><cell>Model</cell><cell cols="2">N = 16k N = 32k</cell></row><row><cell>Transformer</cell><cell>25.62</cell><cell>25.62</cell></row><row><cell>Linformer</cell><cell>64.69</cell><cell>65.36</cell></row><row><cell>Performer</cell><cell>25.62</cell><cell>25.20</cell></row><row><cell>Reformer</cell><cell>25.04</cell><cell>25.04</cell></row><row><cell cols="2">Transformer-LS 70.25</cell><cell>60.93</cell></row><row><cell>Nystr?mformer</cell><cell>71.70</cell><cell>67.69</cell></row><row><cell>Paramixer</cell><cell>83.89</cell><cell>84.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The performance of Paramixer in comparison to Xformers on the Genomic classification task. The metric is Area Under the Receiver Operating Characteristic Curve (ROC AUC) ?100%</figDesc><table><row><cell>Model</cell><cell cols="2">HFDNA MTcDNA</cell></row><row><cell>Transformer</cell><cell>94.32</cell><cell>79.38</cell></row><row><cell>Linformer</cell><cell>91.65</cell><cell>77.06</cell></row><row><cell>Performer</cell><cell>93.46</cell><cell>78.92</cell></row><row><cell>Reformer</cell><cell>92.32</cell><cell>74.94</cell></row><row><cell cols="2">Transformer-LS 93.19</cell><cell>75.81</cell></row><row><cell cols="2">Nystr?mformer 92.26</cell><cell>71.98</cell></row><row><cell>Paramixer</cell><cell>100.00</cell><cell>84.86</cell></row><row><cell cols="3">threshold. We split the data set to 60% training, 20% validation, and 20% test. Each sequence is either padded or</cell></row><row><cell>truncated to a fixed length of 16 384.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A1 :</head><label>A1</label><figDesc>Hyperparameters details for every task. N , B, V , E, H, lr refer to max sequence length, batch size, vocabulary size, embedding size, hidden states size, and learning rate, respectively. The vocabulary size includes padding index and ["CLS"]. More details can be found in the main paper. For the HFDNA classification task, one Paramixer block is enough to get 100% accuracy. However, ParamixerNn with two blocks result in the best test accuracy for MTcDNA. The selected hyperparameters are listed in</figDesc><table><row><cell>Task</cell><cell>N</cell><cell cols="2">Protocol n_links</cell><cell>lr</cell><cell>B V</cell><cell cols="2">E H pos_embed Pooling Type</cell></row><row><cell>Adding</cell><cell cols="2">32768 CHORD</cell><cell>15</cell><cell cols="2">0.001 40 -</cell><cell>32 32</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>Temporal Order</cell><cell cols="2">16384 CHORD</cell><cell>14</cell><cell cols="2">0.001 40 6</cell><cell>32 32</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>ListOps</cell><cell cols="2">2000 CHORD</cell><cell>12</cell><cell cols="3">0.001 48 16 32 32</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>CIFAR10</cell><cell cols="2">1024 CDIL</cell><cell>3</cell><cell cols="3">0.001 64 256 32 32</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>Text</cell><cell cols="2">4096 CDIL</cell><cell>9</cell><cell cols="3">0.0001 32 97 32 128</cell><cell>False</cell><cell>CLS</cell></row><row><cell>Pathfinder</cell><cell cols="2">1024 CHORD</cell><cell>11</cell><cell cols="3">0.001 64 256 32 32</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>Long Document</cell><cell cols="2">16384 CHORD</cell><cell>15</cell><cell cols="3">0.0001 16 4290 100 128</cell><cell>False</cell><cell>CLS</cell></row><row><cell>Long Document</cell><cell cols="2">32768 CHORD</cell><cell>16</cell><cell cols="3">0.0001 16 4290 100 128</cell><cell>False</cell><cell>CLS</cell></row><row><cell cols="3">Genome Classification 16384 CHORD</cell><cell>15</cell><cell cols="2">0.0001 16 5</cell><cell>32 128</cell><cell>True</cell><cell>FLAT</cell></row><row><cell>Mus spretus.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Non-zero entries are those stored, including both non-zeros and explicit zeros.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">available at https://github.com/lucidrains</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.noncode.org/download.php</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://www.ensembl.org/info/data/ftp/index.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective gene expression prediction from sequence by integrating long-range interactions</title>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08483</idno>
		<title level="m">Etc: Encoding long and structured inputs in transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lightweight and efficient neural natural language processing with quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04393</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-rank bottleneck in multi-head attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">omformer: A nystr\&quot; om-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nystr\</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02143</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Random feature attention</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chord: A scalable peer-topeer lookup service for internet applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02192</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Listops: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06028</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Veerabadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Windolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03953</idno>
		<title level="m">Breaking the softmax bottleneck: A high-rank rnn language model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long document classification from local word glimpses via recurrent attention learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="40707" to="40718" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a comprehensive catalogue of validated and target-linked human enhancers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Molly</forename><surname>Gasperini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shendure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="292" to="310" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Noncodev6: an updated database dedicated to long non-coding rna annotation in both animals and plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingrui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangsang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dechao</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="165" to="171" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premanand</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Achuthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez-Jarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><forename type="middle">M</forename><surname>M Ridwan Amode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><forename type="middle">G</forename><surname>Armean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Azov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothish</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="884" to="891" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<ptr target="http://www.ensembl.org/index.html" />
	</analytic>
	<monogr>
		<title level="j">Ensembl Release</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="2021" to="2026" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The rank of circulant matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Ingleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="460" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
