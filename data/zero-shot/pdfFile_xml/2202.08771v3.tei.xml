<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Realistic Blur Synthesis for Learning Image Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Rim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungeon</forename><surname>Kim</surname></persName>
							<email>jungeonkim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
							<email>junyonglee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
							<email>s.cho@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">POSTECH</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Realistic Blur Synthesis for Learning Image Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Realistic Blur Synthesis</term>
					<term>Dataset and Analysis</term>
					<term>Deblurring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training learning-based deblurring methods demands a tremendous amount of blurred and sharp image pairs. Unfortunately, existing synthetic datasets are not realistic enough, and deblurring models trained on them cannot handle real blurred images effectively. While real datasets have recently been proposed, they provide limited diversity of scenes and camera settings, and capturing real datasets for diverse settings is still challenging. To resolve this, this paper analyzes various factors that introduce differences between real and synthetic blurred images. To this end, we present RSBlur, a novel dataset with real blurred images and the corresponding sharp image sequences to enable a detailed analysis of the difference between real and synthetic blur. With the dataset, we reveal the effects of different factors in the blur generation process. Based on the analysis, we also present a novel blur synthesis pipeline to synthesize more realistic blur. We show that our synthesis pipeline can improve the deblurring performance on real blurred images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motion blur is caused by camera shake or object motion during exposure, especially in a low-light environment that requires long exposure time. Image deblurring is the task of enhancing image quality by removing blur. For the past several years, numerous learning-based deblurring methods have been introduced and significantly improved the performance <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b48">39,</ref><ref type="bibr" target="#b28">19,</ref><ref type="bibr" target="#b29">20,</ref><ref type="bibr" target="#b54">45,</ref><ref type="bibr" target="#b53">44,</ref><ref type="bibr" target="#b19">10,</ref><ref type="bibr" target="#b49">40,</ref><ref type="bibr" target="#b52">43]</ref>.</p><p>Training learning-based deblurring methods demands a significant amount of blurred and sharp image pairs. Since it is hard to obtain real-world blurred and sharp image pairs, a number of synthetically generated datasets have been proposed, whose blurred images are generated by blending sharp video frames captured by high-speed cameras <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b46">37,</ref><ref type="bibr" target="#b61">52,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr" target="#b33">24]</ref>. Unfortunately, such synthetic images are not realistic enough, so deblurring methods trained on them often fail to deblur real blurred images <ref type="bibr" target="#b42">[33]</ref>.</p><p>To overcome such a limitation, Rim et al. <ref type="bibr" target="#b42">[33]</ref> and Zhong et al. <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> recently presented the RealBlur and BSD datasets, respectively. These datasets consist of real blurred and sharp ground truth images captured using specially arXiv:2202.08771v3 [cs.CV] 21 Jul 2022 designed dual-camera systems. Nevertheless, coverage of such real datasets are still limited. Specifically, both RealBlur and BSD datasets are captured using a single camera model, Sony A7R3, and a machine vision camera, respectively <ref type="bibr" target="#b42">[33,</ref><ref type="bibr" target="#b58">49,</ref><ref type="bibr" target="#b59">50]</ref>. As a result, deblurring models trained on each of them show significantly low performance on the other dataset, as shown in Sec. <ref type="bibr" target="#b15">6</ref>. Moreover, it is not easy to expand the coverage of real datasets as collecting such datasets require specially designed cameras and a tremendous amount of time.</p><p>In this paper, we explore ways to synthesize more realistic blurred images for training deblurring models so that we can improve deblurring quality on real blurred images without the burden of collecting a broad range of real datasets. To this end, we first present RSBlur, a novel dataset of real and synthetic blurred images. Then, using the dataset, we analyze the difference between the generation process of real and synthetic blurred images and present a realistic blur synthesis method based on the analysis.</p><p>Precise analysis of the difference between real and synthetic blurred images requires pairs of synthetic and real blurred images to facilitate isolating factors that cause the difference. However, there exist no datasets that provide both synthetic and real blurred images of the same scenes so far. Thus, to facilitate the analysis, the RSBlur dataset provides pairs of a real blurred image and a sequence of sharp images captured by a specially-designed high-speed dualcamera system. With the dataset, we can produce a synthetic blurred image by averaging a sequence of sharp images and compare it with its corresponding real blurred image. This allows us to analyze the difference between real and synthetic blurred images focusing on their generation processes. In particular, we investigate several factors that may degrade deblurring performance of synthetic datasets on real blurred images, such as noise, saturated pixels, and camera ISP. Based on the analysis, we present a method to synthesize more realistic blurred images. Our experiments show that our method can synthesize more realistic blurred images, and our synthesized training set can greatly improve the deblurring performance on real blurred images compared to existing synthetic datasets.</p><p>Our contributions are summarized as follows:</p><p>-We propose RSBlur, the first dataset that provides pairs of a real blurred image and a sequence of sharp images, which enables accurate analysis of the difference between real and synthetic blur.</p><p>-We provide a thorough analysis of the difference between the generation processes of real and synthetic blurred images.</p><p>-We present a novel synthesis method to synthesize realistic blurred images for learning image deblurring. While collecting large-scale real datasets for different cameras is challenging, our method offers a convenient alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deblurring Methods Traditional deblurring methods rely on restrictive blur models, thus they often fail to deblur real-world images <ref type="bibr" target="#b44">[35,</ref><ref type="bibr" target="#b20">11,</ref><ref type="bibr" target="#b51">42,</ref><ref type="bibr" target="#b39">30,</ref><ref type="bibr" target="#b47">38,</ref><ref type="bibr" target="#b18">9,</ref><ref type="bibr" target="#b31">22,</ref><ref type="bibr" target="#b32">23]</ref>. To overcome such limitations, learning-based approaches that restore a sharp image from a blurred image by learning from a large dataset have recently been proposed <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b48">39,</ref><ref type="bibr" target="#b28">19,</ref><ref type="bibr" target="#b29">20,</ref><ref type="bibr" target="#b54">45,</ref><ref type="bibr" target="#b53">44,</ref><ref type="bibr" target="#b19">10,</ref><ref type="bibr" target="#b49">40,</ref><ref type="bibr" target="#b52">43]</ref>. However, they require a large amount of training data.</p><p>Deblurring Datasets For evaluation of deblurring methods, Levin et al. <ref type="bibr" target="#b31">[22]</ref> and K?hler et al. <ref type="bibr" target="#b27">[18]</ref> collected real blurred images by capturing images on the wall while shaking the cameras. Sun et al. <ref type="bibr" target="#b47">[38]</ref> generate 640 synthetic blurred images by convolving 80 sharp images with eight blur kernels. Lai et al. <ref type="bibr" target="#b30">[21]</ref> generate spatially varying blurred images from 6D camera trajectories and construct a dataset including 100 real blurred images. However, due to the small number of images, these datasets cannot be used for learning-based methods. Several synthetic datasets using high-speed videos have been proposed for training learning-based methods. They capture high-speed videos and generate synthetic blurred images by averaging sharp frames. GoPro <ref type="bibr" target="#b37">[28]</ref> is the most widely used dataset for learning-based deblurring methods. REDS <ref type="bibr" target="#b36">[27]</ref> and DVD <ref type="bibr" target="#b46">[37]</ref> provide synthetically blurred videos for learning video deblurring. Stereo Blur <ref type="bibr" target="#b61">[52]</ref> consists of stereo blurred videos generated by averaging high-speed stereo video frames. HIDE <ref type="bibr" target="#b45">[36]</ref> provides synthetic blurred images with bounding box labels of humans. To expand deblurring into high-resolution images, 4KRD <ref type="bibr" target="#b22">[13]</ref> is presented, which consists of synthetically blurred UHD video frames. All the datasets discussed above, except for GoPro, use frame interpolation before averaging sharp images to synthesize more realistic blur <ref type="bibr" target="#b36">[27,</ref><ref type="bibr" target="#b46">37,</ref><ref type="bibr" target="#b61">52,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr" target="#b22">13]</ref>. HFR-DVD <ref type="bibr" target="#b33">[24]</ref> uses high-speed video frames captured at 1000 FPS to synthesize blur without frame interpolation. However, all the aforementioned datasets are not realistic enough, thus deblurring networks trained with them often fail to deblur real-world blurred images.</p><p>Recently, real-world blur datasets <ref type="bibr" target="#b42">[33,</ref><ref type="bibr" target="#b58">49,</ref><ref type="bibr" target="#b59">50]</ref> have been proposed. They simultaneously capture a real blurred image and its corresponding sharp image using a dual-camera system. Rim et al. <ref type="bibr" target="#b42">[33]</ref> collected a real-world blur dataset in low-light environments. Zhong et al. <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> proposed the BSD dataset containing pairs of real blurred and sharp videos. However, their performances degrade on other real images captured in different settings due to their limited coverage. Synthesis of Realistic Degraded Images In the denoising field, synthesizing more realistic noise for learning real-world denoising has been actively studied <ref type="bibr" target="#b10">[1,</ref><ref type="bibr" target="#b15">6,</ref><ref type="bibr" target="#b26">17,</ref><ref type="bibr" target="#b50">41,</ref><ref type="bibr" target="#b24">15,</ref><ref type="bibr" target="#b13">4]</ref>. Abdelhamed et al. <ref type="bibr" target="#b10">[1]</ref>, Chang et al. <ref type="bibr" target="#b15">[6]</ref>, and Jang et al. <ref type="bibr" target="#b26">[17]</ref> use generative models to learn a mapping from a latent distribution to a real noise distribution. Zhang et al. <ref type="bibr" target="#b57">[48]</ref> and Wei et al. <ref type="bibr" target="#b50">[41]</ref> propose realistic noise generation methods based on the physical properties of digital sensors. Guo et al. <ref type="bibr" target="#b24">[15]</ref> and Brooks et al. <ref type="bibr" target="#b13">[4]</ref> generate realistic noise by unprocessing arbitrary clean sRGB images, adding Poisson noise, and processing them back to produce noisy sRGB images. These methods show that more realistically synthesized noise datasets greatly improve the denoising performance of real-world noisy images. Synthesis of Blurred Images A few methods have been proposed to synthesize blur without using high-speed videos <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b56">47]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RSBlur Dataset</head><p>Our proposed RSBlur dataset provides real blurred images of various outdoor scenes, each of which is paired with a sequence of nine sharp images to enable the analysis of the difference between real and synthetic blur. The dataset includes a total of 13,358 real blurred images of 697 scenes. For our analysis, we split the dataset into training, validation, and test sets with 8,878, 1,120, and 3,360 blurred images of 465, 58, and 174 scenes, respectively. Below, we explain the acquisition process and other details of the RSBlur dataset.</p><p>To collect the dataset, we built a dual camera system ( <ref type="figure" target="#fig_5">Fig. 1(a)</ref>) as done in <ref type="bibr" target="#b42">[33,</ref><ref type="bibr" target="#b58">49,</ref><ref type="bibr" target="#b59">50,</ref><ref type="bibr" target="#b60">51]</ref>. The system consists of one lens, one beam splitter, and two camera modules with imaging sensors so that the camera modules can capture the same scene while sharing one lens. The shutters of the camera modules are carefully synchronized in order that the modules can capture images simultaneously ( <ref type="figure" target="#fig_5">Fig. 1(b)</ref>). Specifically, one camera module captures a blurred image with a long exposure time. During the exposure time of a blurred image, the other module captures nine sharp images consecutively with a short exposure time. The shutter for the first sharp image opens when the shutter for the blurred image opens, and the shutter for the last sharp image closes when the shutter for the blurred image closes. The exposure time of the fifth sharp image matches with the center of the exposure time of the blurred image so that the fifth sharp image can be used as a ground-truth sharp image for the blurred one.</p><p>The blurred images are captured with a 5% neutral density filter installed in front of a camera module to secure a long exposure time as done in <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>. The exposure times for the blurred and sharp images are 0.1 and 0.005 seconds, respectively. We capture images holding our system in hand so that blurred images can be produced by hand shakes. The captured images are geometrically and photometrically aligned to remove misalignment between the camera modules as done in <ref type="bibr" target="#b42">[33]</ref>. We capture all images in the camera RAW format, and convert them into the nonlinear sRGB space using a simple image signal processing (ISP) pipeline similar to <ref type="bibr" target="#b11">[2]</ref> consisting of four steps: 1) white balance, 2) demosaicing, 3) color correction, and 4) conversion to the sRGB space using a gamma correction of sRGB space as a camera response function (CRF). More details on our dual-camera system and ISP are in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Real vs Synthetic Blur</head><p>Using the RSBlur dataset, we analyze the difference between the generation process of real and synthetic blur. Specifically, we first compare the overall generation process of real and synthetic blur, and discover factors that can introduce the dominant difference between them. Then, we analyze each factor one by one and discuss how to address them by building our blur synthesis pipeline.</p><p>In the case of real blur, camera sensors accumulate incoming light during the exposure time to capture an image. During this process, blur and photon shot noise are introduced due to camera and object motion, and due to the fluctuation of photons, respectively. The limited dynamic range of sensors introduces saturated pixels. The captured light is converted to analog electrical signals and then to digital signals. During this conversion, additional noise such as dark current noise and quantization noise is added. The image is then processed by a camera ISP, which performs white balance, demosaicing, color space conversion, and other nonlinear operation that distort the blur pattern and noise distribution.</p><p>During this process, an image is converted through multiple color spaces. Before the camera ISP, an image is in the camera RAW space, which is devicedependent. The image is then converted to the linear sRGB space, and then to the nonlinear sRGB space. In the rest of the paper, we refer to the linear sRGB space as the linear space, and the nonlinear sRGB space as the sRGB space.</p><p>On the other hand, the blurred image generation processes of the widely used datasets, e.g., GoPro <ref type="bibr" target="#b37">[28]</ref>, DVD <ref type="bibr" target="#b46">[37]</ref>, and REDS <ref type="bibr" target="#b36">[27]</ref>, are much simpler. They use sharp images in the sRGB space consecutively captured by a high-speed camera. The sharp images are optionally interpolated to increase the frame rate <ref type="bibr" target="#b46">[37,</ref><ref type="bibr" target="#b36">27]</ref>. Then, they are converted to the linear space, and averaged together to produce a blurred image. The blurred image is converted to the sRGB space. For conversion between the linear to sRGB spaces, GoPro uses a gamma curve with ? = 2.2 while REDS uses a CRF estimated from a GOPRO6 camera. Between the two processes described above, the main factors that cause the gap between synthetic and real blur include 1) discontinuous blur trajectories in synthetic blur, 2) saturated pixels, 3) noise, and 4) the camera ISP. In this paper, we analyze the effect of these factors one by one. Below, we discuss these factors in more detail. Discontinuous Blur Trajectories The blur generation process of the GoPro dataset <ref type="bibr" target="#b37">[28]</ref>, which is the most popular dataset, captures sharp video frames at a high frame rate and averages them to synthesize blur. However, temporal gaps  between the exposure of consecutive frames cause unnatural discontinuous blur ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). While DVD <ref type="bibr" target="#b46">[37]</ref> and REDS <ref type="bibr" target="#b36">[27]</ref> use frame interpolation to fill such gaps, the effects of discontinuous blur and frame interpolation on the deblurring performance have not been analyzed yet. Saturated Pixels While real-world blurred images may have saturated pixels ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>) due to the limited dynamic range, previous synthetic datasets do not have such saturated pixels as they simply average sharp images. As saturated pixels in real blurred images form distinctive blur patterns from other pixels, it is essential to reflect them to achieve high-quality deblurring results <ref type="bibr" target="#b21">[12]</ref>. Noise Noise is inevitable in real-world images including blurred images, especially captured by a low-end camera at night ( <ref type="figure" target="#fig_1">Fig. 2(d)</ref>). Even for high-end sensors, noise cannot be avoided due to the statistical property of photons and the circuit readout process. In the denoising field, it has been proven important to model the realistic noise for high-quality denoising of real-world images <ref type="bibr" target="#b57">[48,</ref><ref type="bibr" target="#b50">41,</ref><ref type="bibr" target="#b10">1,</ref><ref type="bibr" target="#b15">6,</ref><ref type="bibr" target="#b26">17,</ref><ref type="bibr" target="#b13">4]</ref>. On the other hand, noise is ignored by the blur generation processes of the previous synthetic datasets <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b46">37,</ref><ref type="bibr" target="#b61">52,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr" target="#b33">24]</ref>, and its effect on deblurring has not been investigated. Our experiments in Sec. 6 show that accurate modeling of noise is essential even for the RealBlur dataset, which consists of images mostly captured from a high-end camera with the lowest ISO. Camera ISP ISPs perform various operations, including white balancing, color correction, demosaicing, and nonlinear mapping using CRFs, which affect the noise distribution and introduce distortions <ref type="bibr" target="#b13">[4,</ref><ref type="bibr" target="#b14">5]</ref>. However, they are ignored by the previous synthetic datasets <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b46">37,</ref><ref type="bibr" target="#b61">52,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr" target="#b33">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Realistic Blur Synthesis</head><p>To synthesize more realistic blur while addressing the factors discussed earlier, we propose a novel blur synthesis pipeline. The proposed pipeline will also serve as a basis for the experiments in Sec. 6 that study effect of each factor that degrades the quality of synthetic blur. <ref type="figure" target="#fig_2">Fig. 3</ref> shows an overview of our blur  synthesis pipeline. Our pipeline takes sharp video frames captured by a highspeed camera as done in <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b46">37]</ref>, and produces a synthetic blurred image. Both input and output of our pipeline are in the sRGB space. Below, we explain each step in more detail.</p><p>Frame Interpolation To resolve the discontinuity of blur trajectory, our pipeline adopts frame interpolation as done in <ref type="bibr" target="#b46">[37,</ref><ref type="bibr" target="#b36">27]</ref>. We increase nine sharp images to 65 images using ABME <ref type="bibr" target="#b40">[31]</ref>, a state-of-the-art frame interpolation method. In this step, we perform frame interpolation in the sRGB space to use an off-the-shelf frame interpolation method without modification or fine-tuning.</p><p>sRGB2Lin &amp; Averaging To synthesize blur using the interpolated frames, we convert the images into the linear space, and average them to precisely mimic the real blur generation process. While the actual accumulation of incoming light happens in the camera RAW space, averaging in the camera RAW space and in the linear space are equivalent to each other as the two spaces can be converted using a linear transformation. <ref type="figure" target="#fig_3">Fig. 4</ref>(a) shows an example of the averaging of interpolated frames.</p><p>Saturation Synthesis In this step, we synthesize saturated pixels. To this end, we propose a simple approach. For a given synthetic blurred image B syn from the previous step, our approach first calculates a mask M i of the saturated pixels in the i-th sharp source image S i of B syn as follows:</p><formula xml:id="formula_0">M i (x, y, c) = 1, if S i (x, y, c) = 1 0, otherwise,<label>(1)</label></formula><p>where (x, y) is a pixel position, and c ? {R, G, B} is a channel index. S i has a normalized intensity range [0, 1]. Then, we compute a mask M sat of potential saturated pixels in B syn by averaging M i 's. <ref type="figure" target="#fig_3">Fig. 4</ref>(b) shows an example of M sat . Using M sat , we generate a blurred image B sat with saturated pixels as:</p><formula xml:id="formula_1">B sat = clip(B syn + ?M sat )<label>(2)</label></formula><p>where clip(?) is a clipping function that clips input values into [0, 1], and ? is a scaling factor randomly sampled from a uniform distribution U(0.25, 1.75). For the sake of analysis, we also generate blurred images with oracle saturated pixels. An oracle image B oracle is generated as:</p><formula xml:id="formula_2">(d) (e) Poisson (f) Real (a) Averaging (b) ( 3) (c)</formula><formula xml:id="formula_3">B oracle (x, y, c) = B real (x, y, c), if M sat (x, y, c) &gt; 0 B syn (x, y, c), otherwise.<label>(3)</label></formula><p>Our approach is simple and heuristic, and cannot reproduce the saturated pixels in real images due to missing information in sharp images. Specifically, while we resort to a randomly-sampled uniform scaling factor ?, for accurate reconstruction of saturated pixels, we need pixel-wise scaling factors, which are impossible to estimate. show that our approach still noticeably improves the deblurring performance on real blurred images. Conversion to RAW In the next step, we convert the blurred image from the previous step, which is in the linear space, into the camera RAW space to reflect the distortion introduced by the camera ISP. In this step, we apply the inverse of each step of our ISP except for the CRF step in the reverse order. Specifically, we apply the inverse color correction transformation, mosaicing, and inverse white balance sequentially. As the color correction and white balance operations are invertible linear operations, they can be easily inverted. More details are provided in the supplement. Noise Synthesis After the conversion to the camera RAW space, we add noise to the image. Motivated by <ref type="bibr" target="#b50">[41,</ref><ref type="bibr" target="#b57">48]</ref>, we model noise in the camera RAW space as a mixture of Gaussian and Poisson noise as:</p><formula xml:id="formula_4">B noisy = ? 1 (I + N shot ) + N read<label>(4)</label></formula><p>where B noisy is a noisy image, and I is the number of incident photons. ? 1 is the overall system gain determined by digital and analog gains. N shot and N read are photon shot and read noise, respectively. We model (I + N shot ) as a Poisson distribution, and N read as a Gaussian distribution with standard deviation ? 2 . Mathematically, (I + N shot ) and N read are modeled as:</p><formula xml:id="formula_5">(I + N shot ) ? P B raw ? 1 ? 1 , and<label>(5)</label></formula><formula xml:id="formula_6">N read ? N (0, ? 2 )<label>(6)</label></formula><p>where P and N denote Poisson and Gaussian distributions, respectively. B raw is a blurred image in the camera RAW space from the previous step.</p><p>To reflect the noise distribution in the blurred images in the RSBlur dataset, we estimate the parameters ? 1 and ? 2 of our camera system as done in <ref type="bibr" target="#b57">[48]</ref>, where ? 1 and ? 2 are estimated using flat-field and dark-frame images, respectively. Refer to <ref type="bibr" target="#b57">[48]</ref> for more details. The estimated values of ? 1 and ? 2 are 0.0001 and 0.0009, respectively. To cover a wider range of noise in our synthetic blurred images, we sample random parameter values ? ? 1 and ? ? 2 from U(0.5? 1 , 1.5? 1 ) and U(0.5? 2 , 1.5? 2 ), respectively. Then, using Eq. (5) and Eq. (6) with ? ? 1 and ? ? 2 , we generate a noisy blurred image in the camera RAW space.</p><p>For the analysis in Sec. 6, we also consider Gaussian noise, which is the most widely used noise model. We obtain a noisy image with Gaussian noise as:</p><formula xml:id="formula_7">B noisy = B + N gauss (7)</formula><p>where B is an input blurred image and N gauss is Gaussian noise sampled from N (0, ?), and ? is the standard deviation. As we include Gaussian noise in our analysis to represent the conventional noise synthesis, we skip the ISP-related steps (conversion to RAW, and applying camera ISP), but directly add noise to a blurred image in the sRGB space, i.e., we apply gamma correction to B sat from the previous step, and add Gaussian noise to produce the final results. In our experiments, we randomly sample standard deviations of Gaussian noise from U(0.5? ? , 1.5? ? ) where ? ? = 0.0112 is estimated using a color chart image.</p><p>Applying Camera ISP Finally, after adding noise, we apply the camera ISP to the noisy image to obtain a blurred image in the sRGB space. We apply the same ISP described in Sec. 3, which consists of white balance, demosaicing, color correction, and CRF steps. <ref type="figure" target="#fig_3">Fig. 4</ref>(e) shows our synthesis result with Poisson noise and ISP distortions. As the example shows, our synthesis pipeline can synthesize a realistic-looking blurred image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we evaluate the performance of our blur synthesis pipeline, and the effect of its components on the RSBlur and other datasets. To this end, we synthesize blurred images using variants of our pipeline, and train a learningbased deblurring method using them. We then evaluate its performance on real blur datasets. In our analysis, we use SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> as it is a strong baseline <ref type="bibr" target="#b42">[33]</ref>, and requires a relatively short training time. We train the model for 262,000 iterations, which is half the iterations suggested in <ref type="bibr" target="#b48">[39]</ref>, with additional augmentations including random horizontal and vertical flip, and random rotation, which we found improve the performance. We also provide additional analysis results using another state-of-the-art deblurring method, MIMO-UNet <ref type="bibr" target="#b19">[10]</ref>, in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis using the RSBlur Dataset</head><p>We first evaluate the performance of the blur synthesis pipeline, and analyze the effect of our pipeline using the RSBlur dataset. <ref type="table" target="#tab_2">Table 1</ref> compares different variants of our blur synthesis pipeline. The method 1 uses real blurred images for training SRN-DeblurNet model <ref type="bibr" target="#b48">[39]</ref>, while the others use synthetic images for training. To study the effect of saturated pixels, we divide the RSBlur test set into two sets, one of which consists of images with saturated pixels, and the other does not, based on whether a blurred image has more than 1,000 nonzero pixels in M sat computed from its corresponding sharp image sequence. The numbers of images in the sets with and without saturated pixels are 1,626 and 1,734, respectively. Below, we analyze the effects of different methods and components based on <ref type="table" target="#tab_2">Table 1</ref>. As the table includes a large number of combinations of different components, we include the indices of methods that each analysis compares in the title of each paragraph. Na?ve Averaging <ref type="formula" target="#formula_1">(2 &amp; 3)</ref> We first evaluate the performance of the na?ve averaging approach, which is used in the GoPro dataset <ref type="bibr" target="#b37">[28]</ref>. The GoPro dataset provides two sub-datasets: one of which applies gamma-decoding and encoding before and after averaging, and the other performs averaging without gammadecoding and encoding. Thus, in this analysis, we also include two versions of na?ve averaging. The method 2 in <ref type="table" target="#tab_2">Table 1</ref> is the most na?ve approach, which uses na?ve averaging and ignores CRFs. The method 3 also uses na?ve averaging, but it uses a gamma correction of sRGB space as a CRF. The table shows that both methods perform significantly worse than the real dataset. This proves that there is a significant gap between real blur and synthetic blur generated by the na?ve averaging approach of the previous synthetic dataset. The table also shows that considering CRF is important for the deblurring performance of real blurred images. Frame Interpolation <ref type="bibr">(3, 4, 5 &amp; 6)</ref> We then study the effect of frame interpolation, which is used to fill the temporal gap between consecutive sharp frames by the REDS <ref type="bibr" target="#b36">[27]</ref> and DVD <ref type="bibr" target="#b46">[37]</ref> datasets. Methods 5 and 6 in <ref type="table" target="#tab_2">Table  1</ref> use frame interpolation. The method 6 adds synthetic Gaussian noise to its images as described in Sec. 5. Interestingly, the table shows that the method 5 performs worse than the method 3 without frame interpolation. This is because of the different amounts of noise in blurred images of methods 3 and 5. As frame interpolation increases the number of frames, more frames are averaged to produce a blurred image. Thus, a resulting blurred image has much less noise. The results of methods 6 and 4, both of which add Gaussian noise, verify this.</p><p>The results show that frame interpolation performs better than na?ve averaging when Gaussian noise is added. Saturation <ref type="bibr">(6, 7, 8, 9 &amp; 10)</ref> To analyze the effect of saturated pixels, we first compare the method 6, which does not include saturated pixels whose values are clipped, and the method 7, which uses oracle saturated pixels. As shown by the results, including saturated pixels improves the deblurring quality by 0.12 dB. Especially, the improvement is large for the test images with saturated pixels (0.30 dB). Methods 8 and 10 use our saturation synthesis approach. The result of the method 8 shows that, while it is worse than the method 7 (the oracle method), it still performs better the method 6, which does not perform saturation synthesis, especially for the test images with saturated pixels. Also, our final method (method 10) performs comparably to the oracle method (method 9). Both methods 9 and 10 achieve 32.06 dB for all the test images. This confirms the effectiveness of our saturation synthesis approach despite its simplicity.</p><p>Noise &amp; ISP <ref type="bibr">(5, 6, 7, 8, 9 &amp; 10)</ref> We study the effect of noise and the ISP.</p><p>To this end, we compare three different approaches: 1) ignoring noise, 2) adding Gaussian noise, and 3) adding Gaussian and Poisson noise with an ISP. The first approach corresponds to previous synthetic datasets that do not consider noise, such as GoPro <ref type="bibr" target="#b37">[28]</ref>, REDS <ref type="bibr" target="#b36">[27]</ref> and DVD <ref type="bibr" target="#b46">[37]</ref>. The second is the most widely used approach for generating synthetic noise in many image restoration tasks <ref type="bibr" target="#b55">[46]</ref>. The third one reflects real noise and distortion caused by an ISP. The table shows that, compared to the method 5 (No noise), the method 6 (Gaussian noise) performs significantly better by 1.57 dB. Moreover, a comparison between methods 7 and 8 (Gaussian noise) and methods 9 and 10 (Gaus-sian+Poisson noise with an ISP) shows that adding more realistic noise and distortion further improves the deblurring performance consistently.</p><p>Finally, our final method (method 10) achieves 32.06 dB, which is more than 1 dB higher than those of the na?ve methods 2, 3, and 5. In terms of SSIM, our final method outperforms the na?ve methods by more than 0.05. Compared to all the other methods, our final method achieves the smallest difference against the method 1 which uses real blurred images. In terms of SSIM, our final method achieves 0.8332, which is only 0.0076 lower than that of the method 1. This proves the effectiveness of our method, and the importance of realistic blur synthesis. Qualitative Examples <ref type="figure" target="#fig_8">Fig. 5(b)</ref>-(e) show qualitative deblurring results produced by models trained with different methods in <ref type="table" target="#tab_2">Table 1</ref>. As <ref type="figure" target="#fig_8">Fig. 5(c)</ref> shows, a deblurring model trained with images synthesized using frame interpolation without noise synthesis fails to remove blur in the input blurred image. Adding Gaussian noise improves the quality ( <ref type="figure" target="#fig_8">Fig. 5(d)</ref>), but blur still remains around the lights. Meanwhile, the method trained with our full pipeline ( <ref type="figure" target="#fig_8">Fig. 5(e)</ref>) produces a comparable result to the method trained with real blurred images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Application to Other Datasets</head><p>We evaluate the proposed pipeline on other datasets. Specifically, we apply several variants of our pipeline to the sharp source images of the GoPro dataset <ref type="bibr" target="#b37">[28]</ref> to synthesize more realistic blurred images. Then, we train a deblurring model on synthesized images, and evaluate its performance on the RealBlur J <ref type="bibr" target="#b42">[33]</ref> and BSD <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> datasets. The BSD dataset consists of three subsets with different shutter speeds. We use all of them as a single set, which we denote by BSD All. Limited Coverage of Real Datasets We examine the performance of real datasets on other real datasets to study the coverage of real datasets. To this end, we compare methods 1 and 2 in <ref type="table" target="#tab_5">Table 2</ref>. The comparison shows that the performance of a deblurring model on one dataset significantly drops when trained on the other dataset. This proves the limitation of the existing real datasets and the need for a blur synthesis approach that can generate realistic datasets for different camera settings. Improving GoPro The method 3 in <ref type="table" target="#tab_5">Table 2</ref> performs na?ve averaging to the sharp source images in the GoPro dataset <ref type="bibr" target="#b37">[28]</ref> without gamma correction. The method 4 performs gamma decoding, na?ve averaging, and then gamma encoding. These two methods correspond to the original generation processes of GoPro. As the images in both RealBlur J <ref type="bibr" target="#b42">[33]</ref> and BSD <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> datasets have blur distorted by the CRFs, the method 4 performs better. However, both of them perform much worse than the real-world blur training sets for both RealBlur J and BSD All.</p><p>The method 5 performs 0.01 dB worse than the method 4 on RealBlur J. This again shows that frame interpolation without considering noise may degrade the deblurring performance as it reduces noise as discussed in Sec. 6.1. Adding Gaussian noise (method 6), and saturated pixels (method 7) further improves the deblurring performance on both test sets. For Gaussian noise, we simply add Gaussian noise with standard deviation ? = 0.0112.</p><p>The method 8 uses the noise and ISP parameters estimated for the RealBlur J dataset <ref type="bibr" target="#b42">[33]</ref>. The RealBlur J dataset was captured using a Sony A7R3 camera, of which we can estimate the noise distribution, color correction matrix, and CRF. We use the method described in Sec. 5 for noise estimation. For the color correction matrix and CRF estimation, we refer the readers to our supplementary material. As the CRFs of the training set and RealBlur J are different, the <ref type="table" target="#tab_5">Table 2</ref>. Performance comparison of different blur synthesis methods on the Real-Blur J <ref type="bibr" target="#b42">[33]</ref> and BSD All <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>  method 8 uses different CRFs in different steps. Specifically, it uses gamma decoding with sharp source images of the GoPro dataset into the linear space, and in the last step of our pipeline, it applies the estimated CRF of Sony A7R3. The method 8 achieves 30.32 dB for the RealBlur J dataset, which is much higher than 28.93 dB of the method 4. This proves that our blur synthesis pipeline reflecting the noise distribution and distortion caused by the ISP improves the quality of synthetic blur. It is also worth mentioning that the method 8 performs worse than the method 7 on the BSD All dataset because the camera ISP of BSD All is different from that of RealBlur J. This also shows the importance of correct camera ISP parameters including CRFs, and explains why real datasets perform poorly on other real datasets. <ref type="figure">Fig. 6</ref>(c)-(e) show results of deblurring methods trained on the GoPro dataset with different methods in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Convolution-Based Blur Synthesis Our pipeline also applies to convolutionbased blur synthesis and improves its performance as well. To verify this, we build a dataset with synthetic blur kernels as follows. For each sharp image in the GoPro dataset, we randomly generate ten synthetic blur kernels following <ref type="bibr" target="#b42">[33]</ref> in order that we can convolve them with sharp images to synthesize blurred images instead of frame interpolation and averaging. We also compute saturation masks M sat by convolving the masks of saturated pixels in each sharp image with the synthetic blur kernels. We denote this dataset as GoPro U. Methods 9 to 13 in <ref type="table" target="#tab_5">Table 2</ref> show different variants of our pipeline using GoPro U. For these methods, except for the frame interpolation and averaging, all the other steps in our pipeline are applied in the same manner.</p><p>In <ref type="table" target="#tab_5">Table 2</ref>, methods 9 and 10 perform much worse than methods 1 and 2, which use real blurred images. Methods 11 and 12 show that considering Gaussian noise (? = 0.0112) and saturated pixels significantly improves the performance. Finally, the method 13 that uses our full pipeline, achieves 30.75 dB in PSNR, which is only 0.04 dB lower than that of the method 1, and achieves 0.9019 in SSIM, which is 0.003 higher than that of the method 1. This shows that our pipeline is also effective for the convolution-based blur model. <ref type="figure">Fig.  6</ref>(h)-(j) show results of deblurring methods trained on the GoPro U dataset with different methods in <ref type="table" target="#tab_5">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented the RSBlur dataset, which is the first dataset that provides pairs of a real-blurred image and a sequence of sharp images. Our dataset enables accurate analysis of the difference between real and synthetic blur. We analyzed several factors that introduce the difference between them with the dataset and presented a novel blur synthesis pipeline, which is simple but effective. Using our pipeline, we quantitatively and qualitatively analyzed the effect of each factor that degrades the deblurring performance on real-world blurred images. We also showed that our blur synthesis pipeline could greatly improve the deblurring performance on real-world blurred images. Limitations and Future Work Our method consists of simple and heuristic steps including a simple ISP and a mask-based saturation synthesis. While they improve the deblurring performance, further gains could be obtained by adopting sophisticated methods for each step. Also, there is still the performance gap between real blur datasets and our synthesized datasets, and some other factors may exist that cause the gap. Investigating that is an interesting future direction. <ref type="table" target="#tab_2">Table 1</ref> shows a statistical comparison with the RSBlur and other real-world blur datasets. The proposed RSBlur dataset consists of 13,358 real blurred images, which make the dataset the second largest real-world dataset. While the Real-Blur <ref type="bibr" target="#b42">[33]</ref> and BSD <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> datasets consist of real blurred images and groundtruth sharp images, we provide real blurred images and sequences of nine sharp images to enable analysis on the blur generation process between real blurred and synthetic blurred images. In terms of image resolution, the RSBlur dataset provides the largest resolution. We also report the estimated noise levels using a single image noise estimation method <ref type="bibr" target="#b16">[7]</ref> for comparing the amounts of noise in the real-world blurred datasets. While there exist a couple of real-world blur datasets such as RealBlur <ref type="bibr" target="#b42">[33]</ref> and BSD <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>, their coverage is limited. The real-world blurred images in the RSBlur dataset can also serve as an additional benchmark dataset that complements the existing benchmark datasets in terms of coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Statistics of the RSBlur</head><p>In this section, we provide a benchmark on recent state-of-the-art deblurring methods using the RSBlur dataset to provide a basis for future deblurring research. Using real-world blurred images of the RSBlur dataset, we train state-of-the-art deblurring methods <ref type="bibr" target="#b48">[39,</ref><ref type="bibr" target="#b19">10,</ref><ref type="bibr" target="#b53">44,</ref><ref type="bibr" target="#b52">43,</ref><ref type="bibr" target="#b49">40]</ref>. We use the source codes provided by the authors for training, and evaluate their performance using the real-blur test set of the RSBlur dataset. Here, we briefly report the qualitative and quantitative results of the state-of-the-art methods in <ref type="table" target="#tab_5">Table 2</ref> and in <ref type="figure" target="#fig_1">Fig.  2</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Details of Dual-Camera System</head><p>In this section, we describe the details of our dual-camera system. <ref type="figure" target="#fig_1">Fig. 2</ref> shows our dual-camera system and a diagram of the system. The system consists of a mount for the lens, one beam splitter, and two camera modules with imaging sensors (Basler daA1920-160uc) so that the camera modules can capture the same scene while sharing one lens. For the lens, we used a Samyang 10mm F2.8 ED AS NCS CS. We installed a 5% neutral density filter (OD 1.3 VIS, 12.5mm Dia. Non-Reflective ND Filter) in front of a camera module. For compensation of beam-splitter tolerance, a 63% neutral density filter (0.2 OD, 25mm Dia., Precision Absorptive ND Filter) is installed in front of the other camera module. Two camera modules are installed on adjustable plates, so we can physically align the modules by adjusting the plates. One camera module with a 5% neutral density filter captures a blurred image with a long exposure time (0.1 seconds). The other module captures nine sharp images with a short exposure time (0.005 seconds) during the exposure time of a blurred image. The gains of the two modules are set to 0 for both. To increase the number of images and diversity of blur, we capture 20 pairs of a blurred image and a sequence of nine sharp images of the same scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Camera ISP</head><p>To collect our dataset, we captured all images in the camera RAW format, and converted them into the nonlinear sRGB space using a simple image signal processing (ISP) pipeline. Our ISP consists of four steps: 1) white balance, 2) demosaicing, 3) color correction, and 4) conversion to the sRGB space using a camera response function. We use the demosaicing method of Malvar et al. <ref type="bibr">[</ref>  for the second step and a gamma correction of standard RGB space for the fourth step. For the white balance and color correction steps in our ISP, we utilize a color chart. Specifically, when collecting our dataset, we captured reference images of a color chart for different scenes. Using the reference images, we estimate the gain g c for the color channel c ? {R, G, B} for the white balance as:</p><formula xml:id="formula_8">g c = P n (G) P n (c)<label>(1)</label></formula><p>where P n (c) is the mean intensity of the color channel c of neutral patches in the color chart. Then, in the first step of our ISP, each color channel of a RAW image is multiplied by the corresponding gain. For the color correction in the third step of our ISP, we estimate a color correction matrix of the XYZ color space as:</p><formula xml:id="formula_9">? ? X ref 1 ? ? ? X ref 24 Y ref 1 ? ? ? Y ref 24 Z ref 1 ? ? ? Z ref 24 ? ? = ? ? T ? ? X 1 ? ? ? X 24 Y 1 ? ? ? Y 24 Z 1 ? ? ? Z 24 ? ? (2) where (X ref i , Y ref i , Z ref i</formula><p>) is the reference XYZ color of the i-th color chart patch, and (X i , Y i , Z i ) is the measured XYZ color after white balancing and demosaicing. ? is a single scalar value for matching the brightness levels of the color chart patches and the captured patches. T is a 3 ? 3 color correction matrix. We first estimate ? by minimizing the mean-squared error between the color chart patches and captured patches. Then, we estimate T by finding the least-squares solution of Eq. (2) with fixed ?.</p><p>Once a color correction matrix T is obtained, we apply T in the third step of our ISP as follows:</p><formula xml:id="formula_10">? ? R Lin G Lin B Lin ? ? = M XY Z2Lin ? T ? M Lin2XY Z ? ? ? R Dem G Dem B Dem ? ? (3)</formula><p>where (R Dem , G Dem , B Dem ) is an RGB color of an image after demosaicing, and (R Lin , G Lin , B Lin ) is a resulting RGB color in the linear sRGB space. M Lin2XY Z and M XY Z2Lin are matrices for color conversion between the linear sRGB and XYZ color spaces. <ref type="figure" target="#fig_2">Fig. 3</ref> shows intermediate results of our camera ISP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Photometric Alignment between Camera Modules</head><p>Due to the optical spectrum difference caused by the beam splitter and ND filters, captured images may have slight photometric misalignments. To mitigate this, we conduct photometric alignment using a color chart image after the color correction step of our camera ISP. Specifically, we formulate the relationship between the colors of images from the two camera modules as: </p><formula xml:id="formula_11">(a) RAW (b) Demosaic (c) WB (d) Color Corrected</formula><formula xml:id="formula_12">? ? X C1 1 ? ? ? X C1 24 Y C1 1 ? ? ? Y C1 24 Z C1 1 ? ? ? Z C1 24 ? ? = T p ? ? X C2 1 ? ? ? X C2 24 Y C2 1 ? ? ? Y C2 24 Z C2 1 ? ? ? Z C2 24 ? ? (4) where (X C1 i , Y C1 i , Z C1 i ) and (X C2 i , Y C2 i , Z C2 i )</formula><p>are the XYZ color values of the i-th color chart patch after the color correction of one camera module (C1) and the other camera module (C2), respectively. T p is a 3 ? 3 matrix for the photometric alignment. We estimate T p by finding the least-squares solution of Eq. (4). As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>(a)-(b), images captured by the two camera modules have color differences. After photometric alignment, the color difference is significantly reduced, as shown in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Geometric Alignment between Camera Modules</head><p>Although the two camera modules are physically aligned as much as possible, there may exist a small amount of geometric misalignment between images from them ( <ref type="figure" target="#fig_8">Fig. 5(c)</ref>). Thus, after capturing images, we conduct geometric alignment to compensate for this. The geometric alignment is performed for each pair of a blurred image and its corresponding sharp image sequence as the degree of misalignment can vary with respect to the distance between the camera system and scene. Specifically, for a given sharp image sequence, we first increase the frame rate 8? using a frame interpolation method <ref type="bibr" target="#b40">[31]</ref>, and synthesize a blurred image by averaging them. Then, we estimate a homography between a real blurred image and the synthesized one using the enhanced correlation coefficient method <ref type="bibr" target="#b23">[14]</ref>. Finally, we warp the sharp images according to the estimated homography. We perform geometric alignment to the images processed by the ISP. <ref type="figure" target="#fig_8">Fig. 5(d)</ref> shows a result of our geometric alignment, where the red and cyan lights are better aligned after geometric alignment. Also, it shows that the real blurred image and nine sharp images are well synchronized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Camera ISP for the RealBlur Dataset</head><p>In the main paper, we improve the performance of SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> by mimicking the ISP of the Sony A7R3 camera, which is used for the RealBlur dataset <ref type="bibr" target="#b42">[33]</ref>. Our camera ISP for the Sony A7R3 also consists of the white balance, demosaicing, color correction, and camera response function (CRF) steps.</p><p>As the ISP affects the noise distribution and non-linearity of the blur, we match the white balance gains, color correction matrix, and CRF as much as possible to those of the RealBlur dataset. For white balance, we extract the white balance gains from the RAW images of the RealBlur training set. Similar to <ref type="bibr" target="#b13">[4]</ref>, we randomly sample the gains from the RealBlur training set and multiply a RAW image by the gains. After white balancing, we apply the demosaicing method of Malvar et al. <ref type="bibr" target="#b34">[25]</ref>.</p><p>For the color correction, we extract a characterization matrix of A7R3 from the Libraw library and convert it into a color correction matrix following <ref type="bibr" target="#b43">[34]</ref>. The extracted color correction matrix directly maps from the RAW space to the XYZ color space. We convert a demosaiced image into the linear sRGB space as: where (R Dem , G Dem , B Dem ) is an RGB color of an image after demosaicing, and (R Lin , G Lin , B Lin ) is a resulting RGB color in the linear sRGB space.</p><formula xml:id="formula_13">? ? R Lin G Lin B Lin ? ? = M XY Z2Lin ? T A7R3 ? ? ? R Dem G Dem B Dem ? ? (5)</formula><p>The final step applies a CRF. Motivated by <ref type="bibr" target="#b35">[26]</ref>, we model the CRF as a high-order polynomial as follows:</p><formula xml:id="formula_14">I srgb = K k=0 c k I k Lin<label>(6)</label></formula><p>where I srgb , I Lin , and c k are a non-linear and a linear sRGB image, and polynomial coefficients, respectively. K is the polynomial order, which is set to 7. We capture 11 color chart images with different shutter speeds using a Sony A7R3 camera. Then, we measure the RGB values of the color patches in JPEG images from the camera and in linear sRGB images converted from RAW images. Using the pairs of the measured RGB values, we estimate the coefficients of the CRF c k by solving a least-squares problem. <ref type="figure">Fig. 6(a)</ref> shows the estimated CRF using color chart images. <ref type="figure">Fig. 7</ref> shows an overview of our realistic blur synthesis pipeline. As described in the main paper, we convert the image from the saturation synthesis step into the mosaiced camera RAW space. To this end, we apply inverse color correction, mosaicing, and inverse white balance sequentially. Then, we apply the camera ISP to reflect distortions introduced by the camera ISP. In the case of Sony A7R3, we apply the ISP described in Sec. 7. In the following, we describe each step of the conversion to RAW in more detail. Lin2Cam This step performs inverse color correction. As color correction is a simple linear operation, we can apply inverse color correction as follows:</p><formula xml:id="formula_15">? ? R Cam G Cam B Cam ? ? = M XY Z2Lin ? T ?1 ? M Lin2XY Z ? ? ? R Lin G Lin B Lin ? ?<label>(7)</label></formula><p>where (R Lin , G Lin , B Lin ) and (R Cam , G Cam , B Cam ) are RGB colors in the linear sRGB and RAW RGB spaces, respectively. M Lin2XY Z and M XY Z2Lin are matrices for color conversion between the linear sRGB and XYZ color spaces. T ?1 is the inverse of a color correction matrix T . In the case of the Sony A7R3, the color correction matrix T A7R3 directly maps colors in the RAW color space into the XYZ color space as mentioned in Sec. 7. Thus, we perform inverse color correction as:</p><formula xml:id="formula_16">? ? R Cam G Cam B Cam ? ? = T ?1 A7R3 ? M Lin2XY Z ? ? ? R Lin G Lin B Lin ? ?<label>(8)</label></formula><p>where (R Lin , G Lin , B Lin ) and (R Cam , G Cam , B Cam ) are RGB colors in the linear sRGB and RAW RGB spaces, respectively. Mosaic Following <ref type="bibr" target="#b24">[15]</ref>, we randomly sample a Bayer pattern from RGGB, BGGR, GRBG, and GBRG to reflect distortions caused by various Bayer patterns. Then, we perform mosaicing using the sampled pattern. Inverse WB As we already know white balance gains for each image, in this step, we simply apply their inverse g ?1 c to each color channel of a mosaiced image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Additional Analysis Results</head><p>In this section, we also provide additional analysis results using MIMO-Unet <ref type="bibr" target="#b19">[10]</ref>.</p><p>For the experiments, we train MIMO-Unet model for 790K iterations, which is half the number of iterations suggested in <ref type="bibr" target="#b19">[10]</ref>, with variants of our pipeline. <ref type="table" target="#tab_7">Table 3</ref> shows that the method 2 performs worse than method 1, which uses real blurred images for training. Methods 3 and 4 show that adding Gaussian noise (? = 0.0112) and our saturation synthesis significantly improves the deblurring performance. The method 5, which corresponds to our full pipeline, achieves   <ref type="table" target="#tab_7">Table 3</ref>. Best viewed in zoom in.</p><p>32.08 dB. The analysis shows that MIMO-Unet <ref type="bibr" target="#b19">[10]</ref> also has significant performance improvement with the proposed synthesis pipeline. <ref type="figure" target="#fig_10">Fig. 8</ref> shows results of MIMO-Unet <ref type="bibr" target="#b19">[10]</ref> trained on the RSBlur dataset with different methods in <ref type="table" target="#tab_7">Table  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Experiments with Rough Camera Parameters</head><p>Estimating accurate camera-specific parameters can be easily done by taking a few shots of images. Even if the camera is not available, rough parameters can be easily obtained using images available on the internet in the case of most consumer cameras. To show this, assuming that an A7R3 camera is not available, we conducted additional experiments on the RealBlur J <ref type="bibr" target="#b42">[33]</ref> dataset where we estimated camera-specific parameters from images from the internet. In the case of most consumer cameras, the characterization matrix is easily obtained from the Libraw library 1 . As described in Sec. 7, we extract the characterization matrix of the A7R3 camera and convert it into a color correction matrix. The SIDD dataset <ref type="bibr" target="#b11">[2]</ref> provides the noise parameters of four cameras on different ISO settings. As the RealBlur J dataset is mostly captured with ISO   <ref type="table" target="#tab_8">Table 4</ref>. (g)-(j) Methods 2, 6, 7 and 8 in <ref type="table" target="#tab_8">Table 4</ref>. Best viewed in zoom in.</p><p>100, we sample the noise parameters of Google Pixel on the ISO 100 setting. Following <ref type="bibr" target="#b13">[4]</ref>, we randomly sample gains for the red and blue channels from U(1.9, 2.4) and U(1.5, 1.9) for the white balance.</p><p>In the case of the CRF, there is no dataset including the CRFs of the latest consumer cameras, and it is difficult to estimate CRFs without cameras. Instead, we utilize the camera profile of Sony A7R3 available on Adobe Lightroom. Specifically, we first download a RAW image captured by an A7R3 from the internet. Then, we convert the RAW image into an sRGB image using the camera profile of Adobe Lightroom. We also convert the RAW RGB image into a linear sRGB image using the color correction matrix from the Libraw library and white balance gains of the RAW RGB image. Then, using the pixel values of the converted linear sRGB image and sRGB image, we estimate the coefficients of Eq. (6) by solving a least-squares problem. <ref type="figure">Fig. 6(b)</ref> shows the estimated <ref type="table">Table 5</ref>. Performance comparison of SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> trained on the Real-Blur J <ref type="bibr" target="#b42">[33]</ref>, BSD All <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>  CRF using the RAW image resembles the estimated CRF using a color chart very closely.</p><p>To verify the effectiveness of the rough parameters estimated as described above, we train SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> using the proposed synthesis pipeline with the estimated parameters, and evaluate its performance. In <ref type="table" target="#tab_8">Table 4</ref>, methods 5 and 8 that use the rough parameters perform worse than methods 4 and 7 that use accurate camera parameters. Neverthelss, compared to the methods 3 and 6, the methods 5 and 8 still perform significantly better, validating the effectiveness of the rough parameters. <ref type="figure" target="#fig_11">Fig. 9</ref> shows qualitative examples of using the proposed pipeline with rough and accurate camera parameters and other na?ve methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Limited Coverage of Real Datasets</head><p>In the main paper, we show the limited coverage of the existing real datasets including RealBlur <ref type="bibr" target="#b42">[33]</ref> and BSD All <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>. Specifically, we train SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> using one dataset, and evaluate its performance on the other dataset. In this supplementary material, we report a full comparison result among the real datasets including the RSBlur real dataset in <ref type="table">Table 5</ref>. As the table shows, the deblurring performance of one dataset significantly drops on the other datasets. This result again verifies the limited coverage of the existing real datasets and the usefulness of our synthesis pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Saturation Synthesis: Local vs. Global</head><p>One important component in our blur synthesis pipeline is the saturation synthesis step, which locally scales intensity values of a blurred image before clipping as done in <ref type="bibr" target="#b25">[16]</ref>. Another option that has been used in several previous works is global scaling <ref type="bibr" target="#b38">[29,</ref><ref type="bibr" target="#b17">8,</ref><ref type="bibr" target="#b41">32]</ref>. In this section, we discuss the global and local scaling approaches and compare their performance.</p><p>Without considering noise and camera ISP for the brevity of the discussion, we can model a blurred image B with clipped intensity values as B = clip[I * K], where clip[?] is a clipping function, K is a convolution kernel, and I is a sharp image. To obtain clipped intensity values in B, I should contain unclipped intensity values larger than the upper limit of the dynamic range. However, as sharp images also have the limited dynamic range, it is inevitable to synthesize sharp images with large intensity values, which can be done by either global scaling <ref type="bibr" target="#b38">[29,</ref><ref type="bibr" target="#b17">8,</ref><ref type="bibr" target="#b41">32]</ref> or local scaling <ref type="bibr" target="#b25">[16]</ref>.</p><p>The global scaling approach generates saturated pixels by scaling all the intensity values as clip[? 1 ? I * K], where ? 1 is a scaling factor. The local scaling approach, on the other hand, scales only some intensity values as clip[(I n sat + ? 1 ? I sat ) * K], where I n sat and I sat are images that have non-zero pixels on nonsaturated and saturated region of I, respectively. Due to the information loss at clipped pixels, both methods randomly sample ? 1 to generate non-clipped pixels. By replacing the convolution operation with K by averaging operation over consecutive video frames, we can also model the blur synthesis based on averaging video frames, on which our analysis in the main paper is based. <ref type="figure" target="#fig_5">Fig. 10</ref> shows real saturated images and synthesized images using global scaling and local scaling. Both methods cannot exactly reproduce the real saturated pixels <ref type="figure" target="#fig_5">(Fig. 10(c)</ref>,(e), and (f)) due to the missing information in sharp images caused by clipping. Nevertheless, the local scaling approach has a couple of advantages over the global scaling approach. First, global scaling affects all the pixels, thus introduces a larger domain gap (brighter images) as shown in <ref type="figure" target="#fig_5">Fig.  10(e)</ref>, and severe distortion of the distribution of signal-dependent noise. Second, as global scaling increases the intensities of larger areas, it is usually more difficult to mimic sharp light streaks often observed in real blurred images. This difference can also affect the deblurring performance. We conducted an experiment using the GoPro U training set with global scaling, and found that global scaling performs worse than our saturation synthesis by 0.35 dB on the RealBlur test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Direct Measurement of the Quality of Blur Synthesis</head><p>To evaluate the synthesis methods, we measure the deblurring performance trained with them in the main paper. Another possible option would be to directly compare synthetic blurred images against real blurred images to quantify the quality of the synthetic methods using the RSBlur dataset. In this section, we discuss about the direct measurement of the quality of blur synthesis and report additional evaluation results.</p><p>To measure the quality of different blur synthesis approaches, we measure the PSNR and SSIM values of synthetic blurred images against real-blurred images. However, the PSNR or SSIM values of synthetic images are not 100% reliable due to noise. One possible option is to measure the KL-divergence of the distributions of real and synthesized images as done in noise-synthesis approaches <ref type="bibr" target="#b10">[1,</ref><ref type="bibr" target="#b15">6,</ref><ref type="bibr" target="#b26">17]</ref>. Specifically, we compute the KL-divergence between pixel-wise marginal distributions (B real ? B interp ) and (B syn ? B interp ) where B real and B interp is a real blurred image and an averaging of interpolated images, respectively. B syn is a synthesized blurred image using our pipeline. We use B interp as rough noise-free counterparts of real blurred images for computing the KL-divergence. <ref type="table" target="#tab_10">Table 6</ref> shows PSNR, SSIM, and the KL-divergence of synthesized images using variants of our synthesis methods. The table shows that frame interpolation and saturation synthesis are effective in terms of PSNR (methods 2 and 3). The method 7 shows noise and saturation synthesis methods improve the KL-divergence. We omit the KD-divergence values of methods 2 and 3 as the synthetic images have no noise thus their noise distributions are not properly defined. Note that, measuring the KL-divergence also has its own flaws as we don't have accurate noise-free counterparts of real blurred images. So, 100% accurate estimation of real distribution is not feasible. Also, as our ultimate goal is to improve the deblurring quality, the deblurring performance of the trained network is a most reasonable measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">Training with Both Real and Synthetic Datasets</head><p>Even if a real-world blur training set is available, an additional synthetic dataset generated by our method could further improve the deblurring performance on real blurred images. One important question when training with two datasets is how to mix them. To verify the effect of using both real and synthetic datasets and the mixing strategy, in this section, we compare the deblurring performance obtained using one of real and synthetic datasets and using both of them. Specifically, we compare four different training strategies: training 1) with only Real-Blur J, 2) with only GoPro U, 3) with both RealBlur J and GoPro U together half and half at each iteration, and 4) with RealBlur J and GoPro U one by one at each iteration. We train SRN-DeblurNet using the four strategies and evaluate their performances on the RealBlur test set. As the table shows, using both RealBlur J and GoPro U clearly improves the deblurring performance regardless of the training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15">Additional Results on Other Datasets</head><p>In the main manuscript, we evaluate the proposed pipeline on the RealBlur <ref type="bibr" target="#b42">[33]</ref> and BSD All <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> datasets. Additionally, in this supplementary material, we also report evaluation results on K?hler et al.'s dataset <ref type="bibr" target="#b27">[18]</ref>. <ref type="table" target="#tab_12">Table 8</ref> shows the performance of SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> trained with variants of the proposed convolution-based synthesis pipeline. Note that the images in K?hler et al.'s dataset are in the linear RGB space, and do not have saturated pixels. Thus, the method 5 performs the best, while the other methods with wrong CRFs show significant performance drops. Again, the poor performances of the existing real datasets on K?hler et al.'s dataset prove their limited coverage, as discussed in the main manuscript. We also compare variants of the convolution-based synthesis pipeline in Table 8 on Lai et al.'s dataset <ref type="bibr" target="#b30">[21]</ref>, which provides 100 real blurred images without ground-truth sharp images for qualitative comparison. In <ref type="figure" target="#fig_1">Figs. 11 and 12</ref>, methods 1, 2, and 3 show that the models trained on real datasets do not successfully deblur real blurred images of Lai et al.'s dataset. On the other hand, the method 9 shows that the deblurring results produced by the model trained with our pipeline performs better. This again shows the limited coverage of the real datasets and the practicality of the proposed pipeline. The proposed pipeline can generate a huge number of images with various contents, and blur shapes and sizes effortlessly compared to collecting real datasets, leading to better deblurring performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16">Additional Qualitative Examples</head><p>Figs. 13, 14 and 15 show additional qualitative examples on the RSBlur, Re-alBlur J <ref type="bibr" target="#b42">[33]</ref>, and BSD All <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> datasets, respectively. The figures show deblurring results of SRN-DeblurNet <ref type="bibr" target="#b48">[39]</ref> trained with training images synthesized by different methods in order to compare different blur synthesis methods.  <ref type="bibr" target="#b30">[21]</ref> produced by models trained with different synthesis methods in <ref type="table" target="#tab_12">Table 8</ref>. Best viewed in zoom in. <ref type="figure" target="#fig_2">Fig. 13</ref>. Qualitative comparison on the RSBlur dataset. Green: Trained on real blurred images. Red: Trained on synthetic blurred images. Avg.: Na?ve averaging-based blur synthesis. Interp.: Averaging-based blur synthesis using frame interpolation. G: Gaussian noise. Oracle: Using oracle saturated images. Sat: Our saturation synthesis. All of synthesis methods consider gamma decoding and encoding. <ref type="figure" target="#fig_5">Fig. 15</ref>. Qualitative comparison on the BSD All dataset <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>. Green: Trained on real blurred images. Red: Trained on synthetic blurred images. Linear: Na?ve averagingbased blur synthesis with linear CRF. Avg.: Na?ve averaging-based blur synthesis. Interp.: Averaging-based blur synthesis using frame interpolation. G: Gaussian noise. Sat: Our saturation synthesis. All of synthesis methods except Linear consider gamma decoding and encoding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The top row shows real blurred images and the bottom row shows the corresponding synthetic blurred images. Best viewed in zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of our realistic blur synthesis pipeline. Lin2Cam: Inverse color correction, i.e., color space conversion from the linear space to the camera RAW space. WB: White balance. Cam2Lin: Color correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Generated images from our synthesis pipeline. (a) Averaging image of interpolated frames. (b) Msat scaled by three times. (c)-(d) Examples of saturated images. (e)-(f) Synthetic noisy image and real image. The images except for (b) are converted into the sRGB space for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 (</head><label>4</label><figDesc>c) and (d) show examples of B sat and B oracle where the image in (c) looks different from the one in (d). Nevertheless, our experiments in Sec. 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Qualitative comparison of state-of-the-art deblurring methods on real-world blurred images of the RSBlur test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>The dual-camera system and detailed diagram of the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Intermediate images of our camera ISP. All images are gamma corrected for visualization. Result of photometric alignment. (a) &amp; (b) show captured images from a one camera module and the other camera module, respectively. (c) shows the photometric alignment result of (b). (d)-(f) show magnified views of (a)-(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Result of geometric alignment. (a) &amp; (b) show stereo-anaglyph images, where a real blurred and the averaging of nine sharp images are visualized in red and cyan, respectively. (c) &amp; (d) show magnified views of (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>The blue and red lines show the estimated CRFs of the Sony A7R3 using color chart images and a raw-RGB image from the internet, respectively. The orange dots show measured values. The green lines show gamma correction of sRGB space. Overview of our realistic blur synthesis pipeline. Lin2Cam: Inverse color correction, i.e., color space conversion from the linear sRGB space to the camera RAW space. WB: White balance. Cam2Lin: Color correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative comparison of deblurring results on the RSBlur test set produced by MIMO-Unet [10] trained with different synthesis methods. (b)-(e) &amp; (h)-(k) Methods 1, 2, 3 and 5 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative comparison of deblurring results on the RealBlur J test set produced by models trained with different synthesis methods. (b)-(e) Methods 1, 3, 4 and 5 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Generated images from saturation synthesis methods using global scaling and local scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Brooks et al.<ref type="bibr" target="#b12">[3]</ref> generate a blurred image from two sharp images using a line prediction layer, which esti-</figDesc><table><row><cell>Sensors</cell><cell>Averaging</cell><cell></cell></row><row><cell>Irradiance</cell><cell>GT Sharp Image</cell><cell>Synthetic</cell></row><row><cell>Beam Splitter</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Time</cell><cell>Real</cell></row><row><cell>(a) Our dual-camera system</cell><cell>(b) Image acquisition process</cell><cell></cell></row><row><cell cols="3">Fig. 1. The dual-camera system and the acquisition process for simultaneously cap-</cell></row><row><cell cols="2">turing a real blurred image and sharp images.</cell><cell></cell></row></table><note>mates spatially-varying linear blur kernels. However, linear blur kernels cannot express a wide variety of real-world blur. Zhang et al. [47] use real-world blurred images without their ground-truth sharp images to train a GAN-based model to generate a blurred image from a single sharp image. However, their results are not realistic enough as their generative model cannot accurately reflect the physical properties of real-world blur.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Conversion to Raw Camera ISP</head><label></label><figDesc></figDesc><table><row><cell>Sharp Images</cell><cell>Frame Interpolation</cell><cell>sRGB2Lin &amp; Averaging</cell><cell>Saturation Synthesis</cell><cell>Lin2Cam</cell><cell>Mosaic</cell><cell>Inverse WB</cell></row><row><cell>Realistic Blurred</cell><cell>CRF</cell><cell>Cam2Lin</cell><cell>Demosaic</cell><cell>WB</cell><cell>Noise Synthesis</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison among different blur synthesis methods on the RS-Blur test set. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise.</figDesc><table><row><cell></cell><cell cols="5">Blur Synthesis Methods</cell><cell>PSNR / SSIM</cell></row><row><cell cols="6">No. Real CRF Interp. Sat. Noise ISP</cell><cell>All</cell><cell>Saturated</cell><cell>No Saturated</cell></row><row><cell>1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32.53 / 0.8398 31.20 / 0.8313 33.78 / 0.8478</cell></row><row><cell>2</cell><cell cols="2">Linear</cell><cell></cell><cell></cell><cell></cell><cell>30.12 / 0.7727 28.67 / 0.7657 31.47 / 0.7793</cell></row><row><cell>3</cell><cell cols="2">sRGB</cell><cell></cell><cell></cell><cell></cell><cell>30.90 / 0.7805 29.60 / 0.7745 32.13 / 0.7861</cell></row><row><cell>4</cell><cell cols="2">sRGB</cell><cell></cell><cell></cell><cell>G</cell><cell>31.69 / 0.8258 30.18 / 0.8174 33.11 / 0.8336</cell></row><row><cell>5</cell><cell cols="2">sRGB</cell><cell>?</cell><cell></cell><cell></cell><cell>30.20 / 0.7468 29.06 / 0.7423 31.27 / 0.7511</cell></row><row><cell>6</cell><cell cols="2">sRGB</cell><cell>?</cell><cell></cell><cell>G</cell><cell>31.77 / 0.8275 30.28 / 0.8194 33.17 / 0.8352</cell></row><row><cell>7</cell><cell cols="2">sRGB</cell><cell>?</cell><cell cols="2">Oracle G</cell><cell>31.89 / 0.8267 30.58 / 0.8191 33.12 / 0.8338</cell></row><row><cell>8</cell><cell cols="2">sRGB</cell><cell>?</cell><cell>Ours</cell><cell>G</cell><cell>31.83 / 0.8265 30.47 / 0.8187 33.12 / 0.8339</cell></row><row><cell>9</cell><cell cols="2">sRGB</cell><cell>?</cell><cell cols="3">Oracle G+P ? 32.06 / 0.8315 30.79 / 0.8243 33.25 / 0.8384</cell></row><row><cell>10</cell><cell cols="2">sRGB</cell><cell>?</cell><cell cols="3">Ours G+P ? 32.06 / 0.8322 30.74 / 0.8248 33.30 / 0.8391</cell></row><row><cell cols="2">(a) Blurred image</cell><cell cols="3">(b) Method 1</cell><cell>(c) Method 5</cell><cell>(d) Method 6</cell><cell>(e) Method 10</cell></row><row><cell cols="2">PSNR/SSIM</cell><cell cols="3">29.81/0.8066</cell><cell>26.87/0.7129</cell><cell>27.45/0.7938</cell><cell>29.38/0.8027</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row></table><note>RSBlur (f) Ground truth Fig. 5. Qualitative comparison of deblurring results on the RSBlur test set produced by models trained with different synthesis methods. (b)-(e) Methods 1, 5, 6 and 10 in Table 1. Best viewed in zoom in.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>test sets. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise. A7R3: Using camera ISP parameters estimated from a Sony A7R3 camera, which was used for collecting the RealBlur dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Blur Synthesis Methods</cell><cell></cell><cell></cell><cell>PSNR / SSIM</cell></row><row><cell cols="2">No. Training set</cell><cell>CRF</cell><cell cols="4">Interp. Sat. Noise ISP</cell><cell>RealBlur J</cell><cell>BSD All</cell></row><row><cell>1</cell><cell>RealBlur J</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30.79 / 0.8985 29.67 / 0.8922</cell></row><row><cell>2</cell><cell>BSD All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.66 / 0.8589 33.35 / 0.9348</cell></row><row><cell>3</cell><cell>GoPro</cell><cell>Linear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.79 / 0.8741 29.17 / 0.8824</cell></row><row><cell>4</cell><cell>GoPro</cell><cell>sRGB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.93 / 0.8738 29.65 / 0.8862</cell></row><row><cell>5</cell><cell>GoPro</cell><cell>sRGB</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>28.92 / 0.8711 30.09 / 0.8858</cell></row><row><cell>6</cell><cell>GoPro</cell><cell>sRGB</cell><cell>?</cell><cell></cell><cell>G</cell><cell></cell><cell>29.17 / 0.8795 31.19 / 0.9147</cell></row><row><cell>7</cell><cell>GoPro</cell><cell>sRGB</cell><cell>?</cell><cell>Ours</cell><cell>G</cell><cell></cell><cell>29.95 / 0.8865 31.41 / 0.9154</cell></row><row><cell>8</cell><cell>GoPro</cell><cell>sRGB, A7R3</cell><cell>?</cell><cell cols="4">Ours G+P A7R3 30.32 / 0.8899 30.48 / 0.9060</cell></row><row><cell>9</cell><cell>GoPro U</cell><cell>Linear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.09 / 0.8810 29.22 / 0.8729</cell></row><row><cell>10</cell><cell>GoPro U</cell><cell>sRGB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.28 / 0.8766 29.72 / 0.8773</cell></row><row><cell>11</cell><cell>GoPro U</cell><cell>sRGB</cell><cell></cell><cell></cell><cell>G</cell><cell></cell><cell>29.50 / 0.8865 30.22 / 0.8973</cell></row><row><cell>12</cell><cell>GoPro U</cell><cell>sRGB</cell><cell></cell><cell>Ours</cell><cell>G</cell><cell></cell><cell>30.40 / 0.8970 30.31 / 0.8995</cell></row><row><cell>13</cell><cell>GoPro U</cell><cell>sRGB, A7R3</cell><cell></cell><cell cols="4">Ours G+P A7R3 30.75 / 0.9019 29.72 / 0.8925</cell></row><row><cell cols="2">(a) Blurred image</cell><cell>(b) Method 1</cell><cell></cell><cell>(c) Method 5</cell><cell></cell><cell cols="2">(d) Method 6</cell><cell>(e) Method 8</cell></row><row><cell cols="2">PSNR/SSIM</cell><cell>28.27/0.8562</cell><cell></cell><cell>26.00/0.8218</cell><cell></cell><cell cols="2">26.69/0.8430</cell><cell>28.16/0.8562</cell></row><row><cell></cell><cell></cell><cell>RealBlur_J</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours (GoPro)</cell></row><row><cell cols="2">(f) Ground truth</cell><cell>(g) Method 2</cell><cell></cell><cell cols="2">(h) Method 10</cell><cell cols="2">(i) Method 11</cell><cell>(j) Method 13</cell></row><row><cell></cell><cell></cell><cell>24.68/0.7471</cell><cell></cell><cell>26.27/0.8191</cell><cell></cell><cell cols="2">26.50/0.8534</cell><cell>28.67/0.8870</cell></row><row><cell></cell><cell></cell><cell>BSD_All</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Ours (GoPro_U) Fig. 6. Qualitative comparison of deblurring results on the RealBlur J test set pro- duced by models trained with different synthesis methods. (b)-(e) Methods 1, 5, 6 and 8 in Table 2. (g)-(j) Methods 2, 10, 11 and 13 in Table 2. Best viewed in zoom in.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Statistical comparison of real-world blur datasets. The average noise levels are estimated using a single image noise estimation method<ref type="bibr" target="#b16">[7]</ref>.</figDesc><table><row><cell></cell><cell cols="5">Frames Real/Synth. Resolution Shutter (ms) Noise</cell></row><row><cell cols="2">RealBlur [33] 4,738</cell><cell>Real</cell><cell>680 ? 773</cell><cell>500</cell><cell>0.4378</cell></row><row><cell cols="2">BSD [49, 50] 33,000</cell><cell>Real</cell><cell>640 ? 480</cell><cell>8, 16, 24</cell><cell>0.3404</cell></row><row><cell>RSBlur</cell><cell cols="3">13,358 Real &amp; Synth 1920 ? 1200</cell><cell>100</cell><cell>0.7736</cell></row><row><cell cols="6">2 Real-world Deblurring Benchmark on the RSBlur</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Benchmark of state-of-the-art deblurring methods on the real blurred test set of the RSBlur dataset. We trained all methods using real blurred training set of the RSBlur dataset.</figDesc><table><row><cell>Methods</cell><cell>PSNR / SSIM</cell></row><row><cell cols="2">SRN-Deblur [39] 32.53 / 0.8398</cell></row><row><cell cols="2">MiMO-UNet [10] 32.73 / 0.8457</cell></row><row><cell cols="2">MiMO-UNet+ [10] 33.37 / 0.8560</cell></row><row><cell>MPRNet [44]</cell><cell>33.61 / 0.8614</cell></row><row><cell>Restormer [43]</cell><cell>33.69 / 0.8628</cell></row><row><cell>Uformer-B [40]</cell><cell>33.98 / 0.8660</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Additional analysis using MIMO-Unet [10] on the RSBlur dataset. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise.</figDesc><table><row><cell></cell><cell cols="5">Blur Synthesis Methods</cell><cell>PSNR / SSIM</cell></row><row><cell cols="6">No. Real CRF Interp. Sat. Noise ISP</cell><cell>All</cell><cell>Saturated</cell><cell>No Saturated</cell></row><row><cell>1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32.73 / 0.8457 31.44 / 0.8385 33.93 / 0.8524</cell></row><row><cell>2</cell><cell cols="2">sRGB</cell><cell>?</cell><cell></cell><cell></cell><cell>28.83 / 0.7164 27.42 / 0.7052 30.16 / 0.7270</cell></row><row><cell>3</cell><cell cols="2">sRGB</cell><cell>?</cell><cell></cell><cell>G</cell><cell>29.63 / 0.7552 28.28 / 0.7486 30.90 / 0.7614</cell></row><row><cell>4</cell><cell cols="2">sRGB</cell><cell>?</cell><cell cols="2">Ours G</cell><cell>29.84 / 0.7658 28.49 / 0.7590 31.12 / 0.7723</cell></row><row><cell>5</cell><cell cols="2">sRGB</cell><cell>?</cell><cell cols="3">Ours G+P ? 32.08 / 0.8362 30.68 / 0.8290 33.39 / 0.8429</cell></row><row><cell cols="2">(a) Blurred image</cell><cell cols="3">(b) Method 1</cell><cell>(c) Method 2</cell><cell>(d) Method 3</cell><cell>(e) Method 5</cell><cell>(f) Ground truth</cell></row><row><cell cols="2">PSNR/SSIM</cell><cell cols="3">32.41/0.8665</cell><cell>26.85/0.7209</cell><cell>28.11/0.7836</cell><cell>31.45/0.8600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RSBlur</cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell cols="2">(g) Blurred image</cell><cell cols="3">(h) Method 1</cell><cell>(i) Method 2</cell><cell>(j) Method 3</cell><cell>(k) Method 5</cell><cell>(l) Ground truth</cell></row><row><cell cols="2">PSNR/SSIM</cell><cell cols="3">34.09/0.8714</cell><cell>28.95/0.7362</cell><cell>30.52/0.7839</cell><cell>33.89/0.8691</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RSBlur</cell><cell></cell><cell></cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of different blur synthesis methods on the Real-Blur J<ref type="bibr" target="#b42">[33]</ref> test sets. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise. A7R3: Using camera ISP with accurate parameters estimated from a Sony A7R3 camera. A7R3*: Using camera ISP with rough parameters.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Blur Synthesis Methods</cell><cell>PSNR / SSIM</cell></row><row><cell cols="3">No. Training set</cell><cell>CRF</cell><cell cols="2">Interp. Sat. Noise</cell><cell>ISP</cell><cell>RealBlur J</cell></row><row><cell>1</cell><cell cols="2">RealBlur J</cell><cell></cell><cell></cell><cell>30.79 / 0.8985</cell></row><row><cell>2</cell><cell>BSD All</cell><cell></cell><cell></cell><cell></cell><cell>28.66 / 0.8589</cell></row><row><cell>3</cell><cell>GoPro</cell><cell></cell><cell>sRGB</cell><cell>?</cell><cell>28.92 / 0.8711</cell></row><row><cell>4</cell><cell>GoPro</cell><cell></cell><cell>sRGB, A7R3</cell><cell>?</cell><cell>Ours G+P A7R3 30.32 / 0.8899</cell></row><row><cell>5</cell><cell>GoPro</cell><cell></cell><cell>sRGB, A7R3*</cell><cell>?</cell><cell>Ours G+P A7R3* 30.23 / 0.8864</cell></row><row><cell>6</cell><cell>GoPro U</cell><cell></cell><cell>sRGB</cell><cell></cell><cell>29.28 / 0.8766</cell></row><row><cell>7</cell><cell>GoPro U</cell><cell></cell><cell>sRGB, A7R3</cell><cell></cell><cell>Ours G+P A7R3 30.75 / 0.9019</cell></row><row><cell>8</cell><cell>GoPro U</cell><cell></cell><cell>sRGB, A7R3*</cell><cell></cell><cell>Ours G+P A7R3* 30.55 / 0.8956</cell></row><row><cell cols="2">(a) Blurred image</cell><cell cols="2">(b) Method 1</cell><cell cols="2">(c) Method 3</cell><cell>(d) Method 4</cell><cell>(e) Method 5</cell></row><row><cell cols="2">PSNR/SSIM</cell><cell cols="2">30.46/0.8756</cell><cell cols="2">26.43/0.8094</cell><cell>29.95/0.8515</cell><cell>29.82/0.8471</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RealBlur_J</cell><cell></cell><cell>A7R3 (GoPro)</cell><cell>A7R3* (GoPro)</cell></row><row><cell cols="2">(f) Ground truth</cell><cell cols="2">(g) Method 2</cell><cell cols="2">(h) Method 6</cell><cell>(i) Method 7</cell><cell>(j) Method 8</cell></row><row><cell></cell><cell></cell><cell cols="2">26.96/0.8034</cell><cell cols="2">26.66/0.8156</cell><cell>30.33/0.8604</cell><cell>30.27/0.8611</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BSD_All</cell><cell></cell><cell>A7R3 (GoPro_U)</cell><cell>A7R3* (GoPro_U)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, and RSBlur datasets.</figDesc><table><row><cell></cell><cell>Test</cell><cell></cell><cell>PSNR / SSIM</cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell></cell><cell>RealBlur J</cell><cell>BSD All</cell><cell>RSBlur</cell><cell></cell></row><row><cell cols="5">RealBlur J 30.79 / 0.8985 29.67 / 0.8922 29.86 / 0.7895</cell><cell></cell></row><row><cell cols="2">BSD All</cell><cell cols="3">28.66 / 0.8589 33.35 / 0.9348 30.89 / 0.8049</cell><cell></cell></row><row><cell cols="2">RSBlur</cell><cell cols="3">29.86 / 0.8855 30.85 / 0.9069 32.53 / 0.8398</cell><cell></cell></row><row><cell>(a) Averaging</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f) Real</cell></row><row><cell></cell><cell>1.5</cell><cell>2.5</cell><cell>1.5</cell><cell>2.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Comparison among different blur synthesis methods. We compute PSNR, SSIM and KL-divergence (KLD) from synthesized and real blurred images.</figDesc><table><row><cell></cell><cell cols="3">Blur Synthesis Methods</cell><cell></cell><cell></cell></row><row><cell>No.</cell><cell>Interp.</cell><cell>Sat.</cell><cell>Noise</cell><cell>ISP</cell><cell cols="2">PSNR / SSIM / KLD</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">35.74 / 0.8802 / 1.0064</cell></row><row><cell>2</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>35.97 / 0.8888 /</cell><cell>-</cell></row><row><cell>3</cell><cell>?</cell><cell>Ours</cell><cell></cell><cell></cell><cell>36.03 / 0.8888 /</cell><cell>-</cell></row><row><cell>4</cell><cell>?</cell><cell></cell><cell>G</cell><cell></cell><cell cols="2">34.16 / 0.8075 / 0.4312</cell></row><row><cell>5</cell><cell>?</cell><cell></cell><cell>G+P</cell><cell>?</cell><cell cols="2">33.14 / 0.7928 / 0.3319</cell></row><row><cell>6</cell><cell>?</cell><cell>Ours</cell><cell>G</cell><cell></cell><cell cols="2">34.21 / 0.8077 / 0.4313</cell></row><row><cell>7</cell><cell>?</cell><cell>Ours</cell><cell>G+P</cell><cell>?</cell><cell cols="2">33.18 / 0.7928 / 0.3263</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .</head><label>7</label><figDesc>Comparison of different training strategies using additional synthetic datasets for further performance improvements.</figDesc><table><row><cell>No.</cell><cell>Train dataset</cell><cell>Strategy</cell><cell>PSNR / SSIM</cell></row><row><cell>1</cell><cell>RealBlur J</cell><cell></cell><cell>30.79 / 0.8985</cell></row><row><cell>2</cell><cell>GoPro U</cell><cell></cell><cell>30.75 / 0.9019</cell></row><row><cell>3</cell><cell cols="3">RealBlur J + GoPro U Half &amp; Half 31.17 / 0.9065</cell></row><row><cell>4</cell><cell cols="3">RealBlur J + GoPro U One ? One 31.15 / 0.9059</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Performance comparison of different blur synthesis methods on the K?hler et al.'s [18] dataset. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise. A7R3: Using camera ISP parameters estimated from a Sony A7R3 camera, which was used for collecting the RealBlur dataset.</figDesc><table><row><cell></cell><cell cols="3">Blur Synthesis Methods</cell><cell></cell><cell>PSNR / MSSIM</cell></row><row><cell cols="2">No. Training set</cell><cell>CRF</cell><cell cols="2">Sat. Noise ISP</cell><cell>K?hler et al.</cell></row><row><cell>1</cell><cell>RealBlur J</cell><cell></cell><cell></cell><cell></cell><cell>26.79 / 0.8401</cell></row><row><cell>2</cell><cell>BSD All</cell><cell></cell><cell></cell><cell></cell><cell>25.24 / 0.7920</cell></row><row><cell>3</cell><cell>RSBlur</cell><cell></cell><cell></cell><cell></cell><cell>26.29 / 0.8285</cell></row><row><cell>4</cell><cell>GoPro U</cell><cell>Linear</cell><cell></cell><cell></cell><cell>28.28 / 0.8599</cell></row><row><cell>5</cell><cell>GoPro U</cell><cell>Linear</cell><cell></cell><cell>G</cell><cell>28.41 / 0.8624</cell></row><row><cell>6</cell><cell>GoPro U</cell><cell>sRGB</cell><cell></cell><cell></cell><cell>26.86 / 0.8430</cell></row><row><cell>7</cell><cell>GoPro U</cell><cell>sRGB</cell><cell></cell><cell>G</cell><cell>27.23 / 0.8529</cell></row><row><cell>8</cell><cell>GoPro U</cell><cell>sRGB</cell><cell>Ours</cell><cell>G</cell><cell>27.10 / 0.8500</cell></row><row><cell>9</cell><cell>GoPro U</cell><cell cols="4">sRGB, A7R3 Ours G+P A7R3 27.41 / 0.8516</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Fig. 11. Qualitative comparison of deblurring results on the Lai et al.'s dataset<ref type="bibr" target="#b30">[21]</ref> produced by models trained with different synthesis methods inTable 8. Best viewed in zoom in.Fig. 12. Qualitative comparison of deblurring results on the Lai et al.'s dataset</figDesc><table><row><cell>Blurred image Blurred image</cell><cell>Method 1 Method 1</cell><cell>Method 2 Method 2</cell><cell>Method 3 Method 3</cell></row><row><cell></cell><cell>RealBlur_J RealBlur_J</cell><cell>BSD_All BSD_All</cell><cell>RSBlur RSBlur</cell></row><row><cell>Method 6 Method 6</cell><cell>Method 7 Method 7</cell><cell>Method 8 Method 8</cell><cell>Method 9 Ours (GoPro_U) Method 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours (GoPro_U)</cell></row><row><cell>Blurred image</cell><cell>Method 1</cell><cell>Method 2</cell><cell>Method 3</cell></row><row><cell>Blurred image</cell><cell>RealBlur_J Method 1</cell><cell>BSD_All Method 2</cell><cell>RSBlur Method 3</cell></row><row><cell></cell><cell>RealBlur_J</cell><cell>BSD_All</cell><cell>RSBlur</cell></row><row><cell>Method 6</cell><cell>Method 7</cell><cell>Method 8</cell><cell>Method 9</cell></row><row><cell>Method 6</cell><cell>Method 7</cell><cell>Method 8</cell><cell>Ours (GoPro_U) Method 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours (GoPro_U)</cell></row><row><cell>Blurred image</cell><cell>Method 1</cell><cell>Method 2</cell><cell>Method 3</cell></row><row><cell>Blurred image</cell><cell>RealBlur_J Method 1</cell><cell>BSD_All Method 2</cell><cell>RSBlur Method 3</cell></row><row><cell></cell><cell>RealBlur_J</cell><cell>BSD_All</cell><cell>RSBlur</cell></row><row><cell>Method 6</cell><cell>Method 7</cell><cell>Method 8</cell><cell>Method 9</cell></row><row><cell>Method 6</cell><cell>Method 7</cell><cell>Method 8</cell><cell>Ours (GoPro_U) Method 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours (GoPro_U)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/LibRaw/src/tables/colordata.cpp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro_U</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro_U</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro_U</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">+</forename><surname>Gp + Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Green: Trained on real blurred images. Red: Trained on synthetic blurred images. Interp.: Averagingbased blur synthesis using frame interpolation. G: Gaussian noise. GP: Gaussian and Poisson noise with a camera ISP of Sony A7R3. Sat: Our saturation synthesis. Conv.: Convolution-based blur synthesis. All of synthesis methods consider gamma decoding and encoding</title>
		<imprint/>
	</monogr>
	<note>Fig. 14. Qualitative comparison on the RealBlur J dataset [33</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. + G +</forename><surname>Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. + G +</forename><surname>Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopro</forename><surname>Interp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. + G +</forename><surname>Sat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noise flow: Noise modeling with conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to synthesize motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pseudo-isp: Learning pseudo in-camera signal processing pipeline from A color image denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10234</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning camera-aware noise models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An efficient statistical method for image noise level estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning a non-blind deblurring network for night blurry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convergence analysis of map based blur kernel estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Handling outliers in non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-scale separable network for ultra-high-definition video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deblurring low-light images with light streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">C2n: Practical generative noise modeling for real-world denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (ordersof-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient marginal likelihood optimization in blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arvo: Learning all-range volumetric correspondence for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality linear interpolation for demosaicing of bayer-patterned color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Malvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Radiometric self calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitsunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR. vol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07-01" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">l0 -regularized intensity and gradient prior for deblurring text images and beyond. IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Asymmetric bilateral motion estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep non-blind deconvolution via generalized low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Color conversion matrices in digital cameras: a tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Rowlands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2018-06-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A physics-based noise formation model for extreme low-light raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking noise synthesis and modeling in raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.16028</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Towards rolling shutter correction and deblurring in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Davanet: Stereo deblurring with view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
