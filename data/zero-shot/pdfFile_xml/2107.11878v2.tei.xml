<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Representation Factorization for Video-based Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Aich</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
							<email>amitrc@ece.ucr.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Representation Factorization for Video-based Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as appearance similarity among various people, occlusions, and frame misalignment. To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Further, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features particularly useful in cases of occlusion or spatial misalignment. These two factorization operations taken together result in a modular architecture for our parameter-wise light STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves performance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the problem of video-based person re-IDentification (re-ID). Given a video tracklet of a person of interest, the task is to retrieve the closest match (which ideally should be the true match) among a gallery set of video This work was done during Abhishek Aich's internship with United Imaging Intelligence. Corresponding author: Srikrishna Karanam. <ref type="figure">Figure 1</ref>: Illustration of proposed concept and its efficacy. We present the intuition behind our proposed Spatio-Temporal Representation Factorization (STRF) module designed to conquer common real-world re-ID system issues, e.g., similar-appearance identities, occlusions and misaligned frame. By capturing temporally static/dynamic and spatially coarse/fine information at different layers of a 3D-CNN, STRF produces robust discriminative representation to tackle these challenges as demonstrated here through attention maps of penultimate layer of feature extractor. <ref type="bibr">tracklets</ref>. With numerous applications in security, surveillance, and forensics <ref type="bibr" target="#b1">[2]</ref>, this problem has seen a dramatic increase in interest and various methodologies in the vision community <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b61">60]</ref>.</p><p>While there has been admirable progress in image-based re-ID as evidenced by recent quantitative results <ref type="bibr" target="#b6">[7]</ref>, there are many challenges that still preclude the ubiquitous use of re-ID algorithms in real-world systems. One such issue is appearance similarity, where multiple people wear similar looking clothes (e.g., large conferences or public events with a strict dress code). Other challenging issues include occlusions and frame misalignment that are a direct consequence of large crowd flow densities (e.g., in airports just after flight arrival) and inter-camera viewpoint disparities.</p><p>Having access to additional data, e.g., an extra temporal dimension like videos instead of 2D images, can help alleviate some of these issues by leveraging spatio-temporal data.</p><p>Video-based re-ID has seen much recent work <ref type="bibr">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">55]</ref> in part due to the availability of relevant large-scale video datasets <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b59">58]</ref>. However, learning a spatio-temporal representation that can alleviate the issues noted above still remains a challenge. While advances in general 3D convolutional networks (3D-CNNs) provide reasonable baseline spatio-temporal features, existing re-ID techniques typically rely on specialized architectures <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">55]</ref> that are inflexible to be used with these baseline models. Other lines of work are focused entirely on learning either temporal or spatial representations separately <ref type="bibr">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">22]</ref>, overlooking the complementarity that both streams of information provide in challenging scenarios, e.g., distinguishing people wearing similar clothes.</p><p>To address the aforementioned issues, we present a flexible new computational unit called Spatio-Temporal Representation Factorization (STRF) module. Given a feature volume from a certain 3D convolutional layer in a baseline 3D-CNN model, STRF extracts complementary information along both spatial (h ? w) and temporal (time, t) dimensions. By design, the proposed STRF module can be inserted in an existing 3D-CNN model after any convolutional layer, introducing only ?0.15 million learnable parameters per unit (for instance, this results in only a ?1.73% overall parameter increase with I3D <ref type="bibr" target="#b2">[3]</ref>), resulting in a flexible and parameter-wise economic framework that is end-to-end trainable. STRF comprises two modules, called temporal feature factorization module (FFM) and spatial feature factorization module, to process feature tensors. The design principles of these modules are motivated by certain observations in video tracklets, which we discuss next.</p><p>The intuition behind STRF is demonstrated in <ref type="figure">Figure 1</ref>. We begin with the factorization module in the temporal dimension. First, the overall or "global" appearance of the person (e.g., color of clothes, skin, hair, etc) in a tracklet does not change (static) substantially over time. While one can argue these can change with illumination variations, we assume these variations are limited in a given camera view over a short period of time. Next, the walking patterns of a person may change over time, e.g., walking on a level surface vs. climbing stairs (dynamic). Consequently, there are two possible information factorization strategies when processing feature maps: low-frequency (static) sampling and high-frequency (dynamic) sampling. Low-frequency sampling of feature maps results in capturing the "slowlymoving" or approximately constant features, i.e., the appearance information. On the other hand, high-frequency sampling of feature maps results in capturing information that is more dynamically varying, i.e., walking patterns <ref type="bibr" target="#b35">[35]</ref>. The temporal factorization module results in capturing static and dynamic features across time, which is especially helpful in identifying different individuals with similar appearance (see last row video tracklet in <ref type="figure">Figure 1</ref>).</p><p>The spatial factorization module, on the other hand, does the same low-frequency (which we call "coarse") and highfrequency (which we call "fine") sampling and processing as above, but along the spatial h ? w dimensions. This is motivated by commonly occurring real-world issues such as occlusions and frame misalignment. Under these scenarios, the spatial FFM's high-frequency sampling and processing unit is able to capture more "details" of the person of interest as opposed to the other entities that are the causes of occlusion, or other background information in the case of misalignment. To understand this better, observe the attention maps for top row video tracklet in <ref type="figure">Figure 1</ref>. The baseline model, without our proposed module, highlights mostly the bicycle regions in the feature maps, whereas by adding our module, the model is able to capture the person regions in the frames more comprehensively. Similarly, to cover cases where there are no occlusions or misalignment, the spatial FFM's low-frequency sampling and processing unit become responsible for capturing more slowly-varying or spatially global appearance information. This results in the spatial factorization module to capture two separate streams of spatial information for robust representations.</p><p>To summarize, when multiple people in the gallery "look alike" (e.g., same clothes), features from our temporal factorization branch help disambiguate (i.e., people may look alike but walk differently). On the other hand, with occlusion/clutter, our idea is to rely on "local" features, which can be learned using our spatial branch. Our main contributions are as follows.</p><p>? We present a novel framework in video-based re-ID to learn discriminative 3D features by factorizing both temporal and spatial dimension of features into low-frequency (static/coarse) and high-frequency (dynamic/fine) components to tackle misalignment, occlusion, and similar appearance problems.</p><p>? To realize these factorization, we propose a flexible trainable unit with negligible computational overhead, called Spatio-Temporal Representation Factorization (STRF) module, that can be used in conjunction with any baseline 3D-CNN based re-ID architecture (see <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>? We conduct extensive experiments on multiple datasets to demonstrate how the proposed STRF module improves the performance of baseline architectures and also achieves state-of-the-art performance obtained by standard re-ID evaluation protocols (see <ref type="table" target="#tab_1">Table 2</ref> and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review some recent methods pertaining to video-based person re-ID, and later discuss 3D-CNNs as feature extractors for video re-ID tasks.</p><p>Video-based re-ID. Following the success in image-based re-ID <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b61">60]</ref>, there has been much recent progress in video-based re-ID as well <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b63">62]</ref>. For instance, <ref type="bibr" target="#b51">[51]</ref> proposed multi-granular hypergraph learning framework which leveraged hierarchically divided feature maps at last layer of feature encoder with different levels of granularities to capture spatial and temporal cues, treating both the spatial and temporal dimensions the same. Additionally, there have also been a class of methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b28">28]</ref> that perform feature modulation by expanding the feature extractor with additional learning modules instead of processing just the last layer's output as in <ref type="bibr" target="#b51">[51]</ref>. Different from all the above works, we focus on learning factorized (dynamic/static and coarse/fine) information in both spatial and temporal dimensions (see <ref type="figure" target="#fig_1">Figure  2</ref>). This leads to a flexible feature processing module that can be used anywhere in any 3D-CNN based re-ID architecture, leading to improved performance of various baseline 3D-CNN models (see <ref type="table" target="#tab_1">Table 2</ref>). We provide a characteristic comparison of recent works in <ref type="table" target="#tab_6">Table 1</ref>.</p><p>3D-CNN based Feature Extractor. 3D-CNNs <ref type="bibr" target="#b24">[24]</ref> naturally process input videos to output spatio-temporal features, whereas 2D-CNNs need additional modules such as recurrent networks to extract temporal information. Given this advantage, 3D-CNNs are more suitable for videorelated applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref>, including videobased re-ID tasks <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b34">34]</ref>. For example, <ref type="bibr" target="#b14">[14]</ref> introduced a two-stream model with the first branch comprised of 3D-CNNs and the other comprised of 2D-CNNs to extract temporal and spatial cues. In <ref type="bibr" target="#b17">[17]</ref>, appearance-preserving 3D convolution (AP3D) was proposed to leverage the idea of image registration <ref type="bibr" target="#b64">[63]</ref> to perform feature-level image alignment. While these methods demonstrated good results, they either required both 3D and 2D CNNs <ref type="bibr" target="#b29">[29]</ref>, or additional operations, e.g., non-local convolutions, to achieve best performances <ref type="bibr" target="#b17">[17]</ref>, leading to parameter-wise bulky models. Furthermore, these methods do no explicitly exploit spatial cues of video tracklets. On the other hand, our proposed STRF method modifies the backbone feature encoder by means of a modular computational unit, does not require specialized modules such as recurrent networks or non-local operations, leading to only a minimal increase in learnable parameters while also demonstrating state-of-theart performance on benchmark datasets. <ref type="table" target="#tab_6">Table 1</ref>: Characteristic comparison with state-of-the-art works. We compare our STRF with few current state-of-the-art works. Unlike these methods, STRF uses factorized information from both spatial (S) and temporal (T) dimensions, does not require non-local operations, and is adaptable to multiple baselines. AP3D <ref type="bibr" target="#b17">[17]</ref> MGH <ref type="bibr" target="#b51">[51]</ref> AFA <ref type="bibr" target="#b4">[5]</ref> STRF (Ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatio-Temporal Factorization</head><p>As noted in Section 1, existing re-ID methods for learning video representations do not focus on the complementarity that is provided by the spatial and temporal dimensions. Specifically, we conjecture that the temporal dimension contains both static (e.g., appearance across time) as well as dynamic (e.g., walking patterns) content, whereas the spatial dimension comprises both fine (e.g., focus on details such as a person's legs that may be missed under occlusions) as well as coarse (e.g., overall global appearance) details. Consequently, we argue that all these features should be learned jointly in order to deal with unavoidable challenges such as appearance similarity, occlusions, and frame misalignment.</p><p>To address these issues, we introduce Spatio-Temporal Representation Factorization (STRF), a generic parameterwise lightweight computational unit that can be inserted between convolutional layers in any 3D-CNN architecture for re-ID (note that by the term factorization, we refer to the joint sampling and processing operations for discussion below). This modularity makes STRF particularly appealing for practical applications that may require customized architectures based on data distribution. Along with the performance improvements in baseline architectures (see <ref type="table" target="#tab_1">Table 2</ref>), STRF also demonstrates superior utility of the proposed module over existing specialized architectures for learning spatio-temporal re-ID representations (see <ref type="table" target="#tab_2">Table 3</ref>) <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>.</p><formula xml:id="formula_0">Notations. Let V = v 1 , v 2 , ? ? ?</formula><p>, v t ? R t?h?w denote an input video tracklet comprising t frames each of height h and width w. Let F ? (?) denote the feature encoder of any baseline 3D-CNN (e.g., I3D ResNet-50 <ref type="bibr" target="#b2">[3]</ref>). Let f ? R c ?t ?h ?w be the feature tensor at the th layer of F ? (?), where c , t , h , and w indicate number of channels, number of frames, height, and width, respectively. Let the input and output feature volumes of our STRF module at the th layer be f (i) and f (o) , respectively. Finally, let the static/coarse and dynamic/fine components be denoted with ? and ? , respectively and subscript t and s denote the temporal and spatial dimension, respectively. We use d ? {t, s} and k ? {?, ?} for compact notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Factorization Module (FFM)</head><p>Given f (i) , we propose to factorize this feature volume into four parts: static and dynamic content from temporal t dimension, and coarse and fine detail from spatial h ? w dimension. The intuition here is that the static content in the temporal dimension will capture "what does not change over time", e.g., appearance such as color of clothes, and the dynamic content will capture "what may change over time", e.g., walking patterns <ref type="bibr" target="#b35">[35]</ref>. Similarly, coarse details in the spatial dimension will capture overall global information in the current feature map (e.g., "where is the person?") whereas fine detail helps address situations where the per- son of interest may be occluded by other entities, by capturing local context at different locations of the feature map.</p><p>Our motivations above are particularly relevant given existing 3D-CNN architectures for re-ID do not have explicit mechanisms to focus on features corresponding to the person of interest in cases such as occlusions, image misalignment, or people with similar clothing appearing together in same tracklet. Furthermore, such a factorization enables a 3D-CNN to weight features that are important for downstream matching and re-ID, e.g., the dynamic content along the temporal dimension is more important in cases where people wear similar clothes and can be distinguished only by their walking patterns.</p><p>To realize this proposed factorization and feature reweighting, STRF proposes to use four FFM modules, in which each FFM learns a different type of attention mask from Factorized Attention Mask (FAM) block (we discuss detailed architecture of FAM in next section) for either static/dynamic or coarse/fine content along the temporal and spatial dimensions respectively, and output refined feature volumes. Specifically, given f (i) , we first reshape it into the feature volume f (i) with size c t ? h w and then, use the FAM block to generate a factorized attention mask M dk . This mask is then used to compute a new feature volume as:</p><formula xml:id="formula_1">f (dk) = f (i) M dk d ? {t, s}, k ? {?, ?}<label>(1)</label></formula><p>STRF then integrates the four attention-weighted feature</p><formula xml:id="formula_2">volumes { f (t? ) , f (t?) , f (s? ) , f (s?) }</formula><p>to output a new feature volume which is then passed on to the subsequent convolutional layer. The output of this subsequent layer is then processed by the next instantiation of the STRF. This way, STRF provides a flexible computational unit that can be easily integrated with existing 3D-CNN architectures. Our proposed methodology is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> where one can note that the four individual factorization modules, FFM t, ? , FFM t, ? , FFM s, ? , and FFM s, ? , combine to produce an enhanced feature representation f (o) using their respective FAM blocks. We next discuss the fac-torization attention masks and each of these proposed individual FFM modules in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Factorized Attention Masks (FAM) Block</head><p>To realize the four-way factorization for the feature volume f (i) discussed above, we define four functions below:</p><formula xml:id="formula_3">T k d f (i) = G dk H dk f (i) ,<label>(2)</label></formula><formula xml:id="formula_4">with, d ? {t, s}, k ? {?, ?}</formula><p>where G dk (?) are the factorizing functions. Different for each FFM block, G dk (?) is designed using pooling functions to extract specific information after the input feature volume is passed through a channel reduction layer H dk (?) : c ? c /n, where H dk (?) is a convolutional layer with c /n kernels of size 1. Following <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b25">25]</ref>, we set n = <ref type="bibr" target="#b16">16</ref>. With the output composite function T k d f (i) of size c /nt ? h w from (2), we summarize input features by computing their variance matrix C dk to obtain a representation of each point of T k d f (i) as:</p><formula xml:id="formula_5">C dk = ?T k d f (i) T k d f (i) (3)</formula><p>where represents transpose operation. We set the temperature hyper-parameter ? as 4 following <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b22">22]</ref>. Then, the factorizing mask is computed using the unnormalized sample covariance matrix as M dk (q) = ?(C dk ), where ?(?) is the softmax function. This factorized mask is employed in (1) to obtain the specific factorized representation of f (i) . Next, each factorization module is discussed in more detail.</p><p>Temporal Factorization Module, FFM t, ?, ? . While methods for learning static and dynamic information have been presented in prior work <ref type="bibr">[1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b44">44]</ref>, we take a modular approach to this problem, proposing computational units that can be applied at multiple layers of the base feature encoder. Instead of skipping frames as in <ref type="bibr" target="#b11">[11]</ref>, we define the following temporal factorizing functions: where r t? &gt; r t? . These degenerate pool functions can be implemented using the max pooling (denoted as m) and average pooling (denoted as a) operations with their corresponding static temporal resolutions r t? and dynamic temporal resolutions r t? . We also use suitable padding on H t? (f (i) ) and H t? (f (i) ) to maintain the same size between the input and output feature volumes. The intuition behind setting r t? &gt; r t? is to factorize features in time dimension to capture the information that does not vary much, whereas r t? helps in summarizing information that shows more variations. Capturing such static information with G t? will aid in learning the global appearance features of the person that does not change much along the time dimension. On the other hand, G t? captures dynamic information in the input feature volume, e.g., walking patterns of the person. Finally, the output of FFM t, ?, ? is defined as:</p><formula xml:id="formula_6">G t? = pool r t? , 1, 1 , G t? = pool r t? , 1, 1<label>(4)</label></formula><formula xml:id="formula_7">f (to) = f (t? ) + f (t?)<label>(5)</label></formula><p>where f (t? ) and f (t?) are computed using <ref type="formula" target="#formula_1">(1)</ref>.</p><p>Spatial Factorization Modules, FFM s, ?, ? . Similar to the temporal dimension above, we factorize the feature volume along the spatial dimension as well, extracting coarselevel and fine-level information. The intuition here is that coarse-level information in the spatial dimension comprise global features of the person in the input frames that do not have much occlusion. For frames where the person is occluded or there is spatial misalignment, fine-level features capture the "person-part" of the frame. To realize this, we define the following spatial factorizing functions:</p><formula xml:id="formula_8">G s? = pool 1, r s? , r s? , G s? = pool 1, r s? , r s? (6)</formula><p>where r s? &gt; r s? are the spatially coarse and fine resolution, respectively. As in FFM t, ?, ? , we use appropriate padding on H s? (f (i) ) and H s? (f (i) ) to maintain the same size between the input and output feature volumes. Finally, the output of FFM s, ?, ? is defined as:</p><formula xml:id="formula_9">f (so) = f (s? ) + f (s?)<label>(7)</label></formula><p>where f (s? ) and f (s?) are computed using <ref type="bibr">(1)</ref>. Note that when the resolutions are set as 1 in <ref type="formula">(6)</ref> and <ref type="formula" target="#formula_6">(4)</ref>, the factorizing functions behave as identity mapping. In our experiments, we set r s? = r t? and r s? = r t? for simplicity.</p><p>Integration and overall STRF output. After computing f (to) and f (so) as discussed above, we provide two schemes to integrate them and generate the final feature volume output of our proposed STRF computational unit:</p><formula xml:id="formula_10">f (o) = ? f (to) , f (so) where, ? ? ? {?, } (8)</formula><p>Here, ? denotes using the temporal and spatial factorization modules in cascade, and denotes using them in parallel. When in cascade, the input f (i) is fed to both modules in sequence, i.e. FFM s, ?, ? followed by FFM t, ?, ? , or vice-versa. When in parallel, the outputs of FFM s, ?, ? and FFM t, ?, ? are simply added. In our experiments, we noticed only minor performance differences across these operations (see <ref type="figure" target="#fig_3">Figure 4</ref>(c)).</p><p>Learning Objective. Any STRF-aided network can be trained in an end-to-end manner with following objective:</p><formula xml:id="formula_11">L = L ce + L triplet<label>(9)</label></formula><p>where L ce is the standard cross-entropy classification, L triplet is the cosine distance based triplet loss with batchhard mining <ref type="bibr" target="#b21">[21]</ref>, and L is the overall loss function. Note that our method demonstrates state-of-the-art results (see <ref type="table" target="#tab_2">Table 3</ref>) without any re-ID tricks <ref type="bibr" target="#b38">[38]</ref>, e.g. label smoothing <ref type="bibr" target="#b43">[43]</ref>, in our learning objective.</p><p>How do we employ STRF? The problem of person re-ID benefited tremendously with introduction of residual blocks <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20]</ref>. With the backbone feature extractor as inflated C2D (time dimension of kernel set to 1) residual network, we propose to enhance its feature representation learning paradigm by simply replacing residual blocks at different stages with different STRF-aided I3D or STRF-aided Pseudo-3D (P3D) <ref type="bibr" target="#b39">[39]</ref> residual blocks (see <ref type="figure" target="#fig_1">Figure 2</ref>(B)). To convert P3D residual blocks to their STRF-P3D forms, we add the STRF module with the convolutional layer of kernel size 3 ? 1 ? 1 demonstrating the generic ability of the proposed unit. We have empirically analyzed and discussed this choice of location in the supplementary material. Moreover, a single STRF module introduces only minimal extraparameters which makes it parameter-wise lightweight but performance-wise beneficial (see <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>FAM vs Channel Attention (CA). We note that there are substantial differences between FAM and the popular CA strategy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b54">54]</ref>. Unlike CA that has one global feature pooling layer, i.e., no separate spatial and temporal operations, FAM has four pooling functions G dk (?), defined in <ref type="formula" target="#formula_6">(4)</ref> and <ref type="formula">(6)</ref>. This captures both spatial and temporal feature dependencies without any new learning parameters. In fact, with r ? and r ? set to same size of the input feature maps, CA can be considered to be a special case of FAM.</p><p>FFM vs Non-Local (NL). Unlike the popular NL module <ref type="bibr" target="#b46">[46]</ref> where there is no factorization, FFM factorizes f (i) into its constituent spatio/temporal factors. Please see supplementary material for additional insights and discussions on our proposed STRF module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimentation</head><p>Datasets, implementation details, and evaluation metrics. We conduct extensive experiments on standard publicly available video-based person re-ID datasets, includ-ing MARS <ref type="bibr" target="#b59">[58]</ref>, DukeMTMC-VideoReID <ref type="bibr" target="#b47">[47]</ref>, and iLIDS-VID <ref type="bibr" target="#b45">[45]</ref>. For evaluation, we use the value of the cumulative matching characteristic curve at rank-1 (R@1), and mean average precision (mAP) <ref type="bibr" target="#b60">[59]</ref>. See supplementary material for full implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improvement over Baselines</head><p>Quantitative analysis. We build a model with inflated 2D convolutions in ResNet50 (temporal kernel size set to 1) architecture. We then replace stage 2 and stage 3 (See <ref type="table" target="#tab_4">Table  5</ref>) with four residual blocks I3D (temporal kernel size set to 3) and three pseudo-3D residual blocks P3D-A, P3D-B and P3D-C to create four baselines. For comparative evaluation, we replace these I3D and P3D residual blocks with STRF-I3D, STRF-P3DA, STRF-P3DB and STRF-P3DC residual blocks respectively and summarize the results in <ref type="table" target="#tab_1">Table 2</ref>. One can clearly note that the STRF-aided models give improved performance (at least 2.5% mAP increment for P3D baselines and about 0.5% mAP increment for I3D baseline on MARS), with the best performance achieved with STRF-P3DC. Similar trends can be observed on the DukeMTMC-VideoReID as well. Furthermore, when compared to the number of baseline model parameters (denoted in <ref type="table" target="#tab_1">Table 2</ref> as P(M) on MARS in the millions of parameters), the number of new parameters introduced by our proposed module is only 0.05 million more compared with I3D or P3D models, suggesting it does not add any substantial computational overhead. This also demonstrates that STRF can improve performance of diverse architectures. For all subsequent experiments, we report results with STRF-P3DC following its best performance from <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Qualitative Analysis. To qualitatively demonstrate STRF's impact, we visualize feature maps of challenging videos (e.g., occlusions, misalignment) using off-the-shelf techniques <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b53">53]</ref> in <ref type="figure" target="#fig_4">Figure 5</ref>. Note that STRF helps focus more clearly on the person of interest (e.g., under "occlusion", unlike the baseline, STRF is able to more clearly distinguish between person's foreground and occlusion regions). Please see supplementary material for more qualitative and attention map results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Utility of FAM block. Our temporal and spatial factorization modules are realized with the proposed factorized attention masks M dk . These self-attention masks are utilized to re-weight the input feature volume f (i) in order to produce a richer representation of the video tracklet. Specific information captured via M dk (due to different G dk for both low-frequency (static/coarse) and high-frequency (dynamic/fine) information) enhance input feature volume to represent robust features by re-weighting them as in (1). Consequently, FAM is an important component of our proposed STRF module. To validate this, we present an analysis of STRF with and without the FAM in <ref type="figure" target="#fig_5">Figure 6</ref>(a) on MARS <ref type="bibr" target="#b59">[58]</ref>. It can be observed that without FAM, the proposed module weakens the feature representations (non-weighted multiplication (?) of f (i) with itself) resulting in a comparatively lower performance. More concretely, without FAM, we do not have "coarse/fine" and "static/dynamic" factors, and hence FFM does not receive appropriate factors to re-weight f (i) . Note that using FAM alone (without FFM) is not possible by design.</p><p>Analysis of different components. The proposed static and dynamic factorizing functions differ essentially in their temporal and spatial resolutions, and in <ref type="figure" target="#fig_3">Figure 4(a)</ref>, we analyze various combinations of these resolution parameters while keeping the static resolution (or coarser) r ? larger than the dynamic (or finer) resolution r ? . Note that we keep the low-and high-frequency (static and coarse) resolutions of both these modules the same for simplicity and reduced parameter search space. The maximum resolution is dependent on output size of the last conv layer STRF is applied to. In our case, this last layer output is f (i) ? R 2048?8?14?7 , giving only possible choices of 1, 3, 5, and 7. A (1, 7, 7) filter will give f (o) ? R 2048?8?7?1 , i.e., 7 ? 1 spatial dimension, unsuitable for computing M dk . As coarse resolution should be larger than fine resolution, only 4 plausible pairs (including (r ? , r ? ) = (1, 1) for reference) results are presented. One can note from the graph that STRF performs the best with the resolution pair (r ? , r ? ) <ref type="figure" target="#fig_2">= (1, 3)</ref>. The graph also shows that STRF is not very sensitive to the different resolution pairs, with a difference of 0.4% mAP when (r ? , r ? ) = <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5)</ref>, and difference of 0.2% mAP when (r ? , r ? ) = (1, 5). Next, we analyze various combinations of factorizing functions defined as part of STRF modules in <ref type="figure" target="#fig_3">Figure 4</ref>(b). Our framework performs the best with both temporal and spatial G dk set to the max pool (m) operation. This is likely because factorization based on max pooling helps focus on information that represents the discriminative portion of the feature volume. Finally, we analyze the different integration functions described in <ref type="bibr" target="#b7">(8)</ref>, where we note that the best performance is obtained when we first factorize f (i) by the temporal factorization module FFM(t, ?, ?) and then feed this output to spatial factorization module FFM(s, ?, ?), i.e., when ?(?) = ?. Further, when ?(?) = , a comparable performance is observed with a difference of about 0.4% in mAP.</p><p>Which stage to add? <ref type="table" target="#tab_4">Table 5</ref> presents results of adding STRF at various stages of a baseline model. Using STRF module in Stage 2 and Stage 3 gives the best performance, but reduces (in mAP) when added to Stage 1. This is likely because with Stage 1, low-level features do not contain enough descriptive semantic information for detailed factorization. Additionally, Stage 4 (last two rows in <ref type="table" target="#tab_4">Table 5</ref>) exhibits differing behavior, likely due to the feature pooling operation performed at this layer (for subsequent classification), which provides spatio-temporally-entangled gradients which may not be useful for our STRF module. Please see supplementary material for more results.</p><p>Influence of each factorization module. To study the efficacy of each module, we perform an ablation analysis (see <ref type="table" target="#tab_3">Table 4</ref>). Each individual module FFM(t, ? ), FFM(t, ?), FFM(s, ? ), and FFM(s, ?) improves the baseline with at least 2% in mAP and 1.2% in R@1. Further, temporal and spatial factorization modules perform better when used together. The temporal/spatial similarity (in margins) in Ta- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art approaches</head><p>Despite being parameter-wise lightweight and agnostic to baseline architectures, STRF gives competitive results when compared to sophisticated 3D-CNN methods. As can be observed in <ref type="figure" target="#fig_5">Figure 6</ref>(b), STRF outperforms both AP3D and M3D with ?6 million (w.r.t. AP3D <ref type="bibr" target="#b17">[17]</ref>) and ?75 mil-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel Spatio-Temporal Representation Factorization (STRF) computational unit that learns complementary spatio-temporal feature representations to deal with real-world re-ID challenges such as occlusions, imperfect detection, and appearance similarity. Our STRF module factorizes temporal dynamic/static, and spatial coarse/fine components from input 3D-CNN feature maps, helping baseline models discover more complementary and discriminative spatio-temporal representations for robust video re-ID. Extensive evaluations of our STRF module with various baseline architectures on benchmark videobased re-ID datasets show its efficacy and generality. As part of future work, we would like to extend it to general video understanding problems like semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-Temporal Representation Factorization</head><p>for Video-based Person Re-Identification (Supplementary Material) CONTENTS A. Simplified Demonstration of STRF  <ref type="bibr">. 15</ref> List of Tables </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Hyperparameters Details. We build our feature extractors by first inflating 2D-ResNet50 <ref type="bibr" target="#b20">[20]</ref>, pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>, with time dimension of all kernels set to 1 (See <ref type="figure" target="#fig_1">Figure 2(A)</ref> in main manuscript). The last stride of the model is set to 1 following <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b52">52]</ref>. Then, we replace stage 2 and 3 with the proposed STRF-P3D residual blocks. We train our model with the Adam <ref type="bibr" target="#b27">[27]</ref> optimizer with a weight decay of 0.0005 for 250 epochs. The initial learning rate is set to 0.0003, and is reduced by a factor of 10 times after every 50 epochs. For data augmentation, we use random erasing <ref type="bibr" target="#b62">[61]</ref> and random horizontal flip following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref>. As part of each training batch, we randomly sample 4 frames with a stride of 8 frames to form a clip for each tracklet. Each batch contains 8 persons with 4 video clips each. All the frames are resized to 256 ? 128.</p><p>The feature dimension is set to 2048 which is obtained after temporal pooling for both training and testing. We use PyTorch <ref type="bibr" target="#b37">[37]</ref> for all our experiments. Training time is ?10 hrs on 3 NVIDIA Tesla-V100 GPUs.</p><p>Testing Protocol. For fair comparisons, we follow exact testing protocols as in prior works <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b22">22]</ref>. We split each video tracklet into several four-frame clips and extract their feature representations. The final feature representation is computed by averaging across all the clips. Finally, for retrieval, cosine distances are computed between query and gallery video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Discussions on STRF</head><p>Location of STRF in Pseudo-3D <ref type="bibr" target="#b39">[39]</ref> residual blocks. We observe in our preliminary experiments that STRF is more effective with the 3 ? 1 ? 1 convolutional layer rather than the 1 ? 3 ? 3 convolutional layer (see <ref type="figure">Figure 1</ref>). Hence, we place the STRF module with the 3 ? 1 ? 1 convolutional layer as indicated in <ref type="figure" target="#fig_1">Figure 2</ref>(B) of the main manuscript. One explanation for this behavior of STRF can be attributed to the fact that time-degenerate convolutions are more effective in extracting rich information of temporal dimension which has shown to be more important for recognition in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">11]</ref>. Moreover, the temporal integrity is diminished with 1 ? 3 ? 3 as each feature map in the volume is treated individually. Hence, after the proposed enhancement of the feature volume, the 3 ? 1 ? 1 convolutional layer performs comparatively well.</p><p>Additional analysis of STRF on different stages of feature extractor. We present additional analysis of the impact of adding the proposed STRF module at various stages to the baseline model in <ref type="table" target="#tab_1">Table 2</ref>. We can observe that the STRF module is effective at every stage to enhance the performance of the baseline model. Additional analysis of STRF's four factorization components. We present additional analysis of the different combinations of factorization modules of STRF in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Attention Maps</head><p>In this section, we present the efficacy of the proposed STRF module in challenging real-world scenarios of occlusion, frame misalignment, and different identities with similar appearance. From <ref type="figure" target="#fig_2">Figure 3</ref>(a) (DukeMTMC-VideoReID) and <ref type="figure" target="#fig_2">Figure 3</ref>(b) (MARS), it can be observed that STRF is able to locate the person of interest more precisely when employed with the baseline model. Note that these attention maps are obtained from stage 3 of the feature extractor as we add our proposed module here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>In this section, we present some cases where the baseline model was unable to find the right match of the query in the gallery (see <ref type="figure" target="#fig_3">Figure 4</ref>(a) for DukeMTMC-VideoReID and <ref type="figure" target="#fig_3">Figure 4</ref>(b) for MARS) in Rank-1 retrieval. It can be observed that our proposed module helps to enhance the ability of the baseline model to identify the person of interest in difficult examples. <ref type="figure" target="#fig_3">Figure 4</ref>: Rank-1 (R@1) retrieval results in challenging scenarios on DukeMTMC-VideoReID <ref type="bibr" target="#b47">[47]</ref>, MARS <ref type="bibr" target="#b59">[58]</ref>. We present R@1 retrieval cases where our proposed module STRF enabled the baseline (P3DB for DukeMTMC-VideoReID <ref type="bibr" target="#b47">[47]</ref>, P3DC for MARS <ref type="bibr" target="#b59">[58]</ref>) to correctly identify the query in the gallery. Red bounding boxes indicates incorrect retrieval. Green bounding boxes indicates correct retrieval. Blue indicates labels. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of STRF module. Our proposed module contains four factorization units being applied on input feature volume at th layer to extract static/coarse and dynamic/fine information and generate richer feature representation. Each unit is composed of a Feature Factorization Module aided by the proposed Factorized Attention Mask block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of our 3D-CNN model training and examples of proposed blocks. In our framework, we employ a 3D-CNN based model to learn discriminative features for input video tracklets (see Figure (A)). This model is built with inflated 2D-residual blocks with Stage 2 and Stage 3 replaced by our proposed STRF aided residual blocks (see Figure (B)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of different components of STRF. (a) Each (r? , r? ) refers to spatially coarse resolutions : (1, r? , r? ), spatially fine resolutions : (1, r? , r? ), temporally static resolutions : (r? , 1, 1), temporally dynamic resolutions : (r? , 1, 1). Best results are obtained with (r? , r? ) = (1, 3). (b) Performance of different combinations of factorizing functions G dk (?): Best results are obtained when all G dk (?) are set as maxpooling function. (c) Performance of different integration operations ?(?): Best results are obtained when the spatial module is followed by the temporal module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Attention map visualizations. STRF helps baseline models extract more discriminative features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Advantages of STRF. (a) Without FAM block, FFM cannot factorize features leading to poor performance, signifying FAM's importance in STRF. (b) STRF is comparatively parameterwise most light-weight and best performing 3D-CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>. 14 D</head><label>14</label><figDesc>. Additional Discussions on STRF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The appropriate weighting of f (i) with these factors to obtain f (o) is automatically learned with FAM, making the proposed design different from NL and more suitable for re-ID. For additional empirical substantiation, using the P3DC architecture on the MARS dataset<ref type="bibr" target="#b59">[58]</ref>, the NL module gives 84.8% mAP and 89.9% R@1, whereas STRF gives 86.1% mAP and 90.3% R@1. Further, STRF only adds an additional ?0.5 million parameters (w.r.t. the baseline) as opposed to NL's ?5 million additional parameters, demonstrating better compute efficiency.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Baseline improvement. STRF consistently improves the performance of baseline models. P(M) is model size in millions.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">DATASETS</cell><cell></cell></row><row><cell>MODEL</cell><cell>P(M)</cell><cell cols="2">MARS [58]</cell><cell cols="2">DukeMTMC [47]</cell></row><row><cell></cell><cell></cell><cell>mAP %</cell><cell>R@1 %</cell><cell>mAP %</cell><cell>R@1 %</cell></row><row><cell>I3D</cell><cell>28.92</cell><cell>82.70</cell><cell>88.50</cell><cell>95.20</cell><cell>95.40</cell></row><row><cell>+ STRF</cell><cell>28.97</cell><cell>83.10</cell><cell>88.70</cell><cell>95.20</cell><cell>95.90</cell></row><row><cell>P3DA</cell><cell>25.48</cell><cell>83.20</cell><cell>88.90</cell><cell>95.00</cell><cell>95.00</cell></row><row><cell>+ STRF</cell><cell>25.53</cell><cell>85.40</cell><cell>89.80</cell><cell>95.60</cell><cell>96.00</cell></row><row><cell>P3DB</cell><cell>25.48</cell><cell>83.00</cell><cell>88.80</cell><cell>95.40</cell><cell>95.30</cell></row><row><cell>+ STRF</cell><cell>25.53</cell><cell>85.60</cell><cell>90.30</cell><cell>96.40</cell><cell>97.40</cell></row><row><cell>P3DC</cell><cell>25.48</cell><cell>83.10</cell><cell>88.50</cell><cell>95.30</cell><cell>95.30</cell></row><row><cell>+ STRF</cell><cell>25.53</cell><cell>86.10</cell><cell>90.30</cell><cell>96.20</cell><cell>97.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art. STRF gives state-of-the-art performance on all datasets (best results in red, second best in blue, and third best results in green.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DATASETS</cell><cell></cell><cell></cell></row><row><cell>METHODS</cell><cell>VENUE</cell><cell cols="2">MARS [58]</cell><cell cols="2">DukeMTMC [47]</cell><cell>iLiDS-VID [45]</cell></row><row><cell></cell><cell></cell><cell>mAP %</cell><cell>R@1 %</cell><cell>mAP %</cell><cell>R@1 %</cell><cell>R@1 %</cell></row><row><cell>ADFD [57]</cell><cell>CVPR 2019</cell><cell>78.20</cell><cell>87.00</cell><cell>-</cell><cell>-</cell><cell>86.30</cell></row><row><cell>VRSTC [23]</cell><cell>CVPR 2019</cell><cell>82.30</cell><cell>88.50</cell><cell>93.50</cell><cell>95.00</cell><cell>86.30</cell></row><row><cell>COSAM [41]</cell><cell>ICCV 2019</cell><cell>79.90</cell><cell>84.90</cell><cell>94.10</cell><cell>95.40</cell><cell>79.60</cell></row><row><cell>GLTR [28]</cell><cell>ICCV 2019</cell><cell>78.50</cell><cell>87.00</cell><cell>93.74</cell><cell>96.29</cell><cell>86.00</cell></row><row><cell>MGH [51]</cell><cell>CVPR 2020</cell><cell>85.80</cell><cell>90.00</cell><cell>-</cell><cell>-</cell><cell>85.60</cell></row><row><cell>STGCN [52]</cell><cell>CVPR 2020</cell><cell>83.70</cell><cell>89.95</cell><cell>95.70</cell><cell>97.29</cell><cell>-</cell></row><row><cell>MG-RAFA [55]</cell><cell>CVPR 2020</cell><cell>85.90</cell><cell>88.80</cell><cell>-</cell><cell>-</cell><cell>88.60</cell></row><row><cell>TACAN [30]</cell><cell>WACV 2020</cell><cell>84.00</cell><cell>89.10</cell><cell>95.40</cell><cell>96.20</cell><cell>88.90</cell></row><row><cell>M3D [29]</cell><cell>TPAMI 2020</cell><cell>79.46</cell><cell>88.63</cell><cell>93.67</cell><cell>95.49</cell><cell>86.67</cell></row><row><cell>AFA [5]</cell><cell>ECCV 2020</cell><cell>82.90</cell><cell>90.20</cell><cell>95.40</cell><cell>97.20</cell><cell>88.50</cell></row><row><cell>AP3D [17]</cell><cell>ECCV 2020</cell><cell>85.60</cell><cell>90.70</cell><cell>96.10</cell><cell>97.20</cell><cell>88.70</cell></row><row><cell>TCLNet [22]</cell><cell>ECCV 2020</cell><cell>85.10</cell><cell>89.80</cell><cell>96.20</cell><cell>96.90</cell><cell>86.60</cell></row><row><cell>STRF</cell><cell>Ours</cell><cell>86.10</cell><cell>90.30</cell><cell>96.40</cell><cell>97.40</cell><cell>89.30</cell></row><row><cell cols="3">ble 4 suggests each module is equally effective in identify-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ing unique features w.r.t. baseline. Finally, the best per-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">formance is obtained when all modules are put together,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">demonstrating their focus on complementary information.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Contribution of each factorization module. All four STRF modules FFM(t, ? ), FFM(t, ?) FFM(s, ? ), and FFM(s, ?) show improvement individually and collectively with the P3DC baseline on MARS [58].</figDesc><table><row><cell>MODEL</cell><cell>MODULES (s, ? ) (s, ?) (t, ? ) (t, ?)</cell><cell cols="2">MARS [58] mAP % R@1 %</cell></row><row><cell>Baseline</cell><cell></cell><cell>83.10</cell><cell>88.50</cell></row><row><cell></cell><cell></cell><cell>85.20</cell><cell>89.70</cell></row><row><cell></cell><cell></cell><cell>85.10</cell><cell>89.90</cell></row><row><cell>Baseline + STRF</cell><cell></cell><cell>85.20 85.10 85.50 85.30 85.40</cell><cell>89.90 90.00 90.10 89.70 90.00</cell></row><row><cell></cell><cell></cell><cell>85.70</cell><cell>90.10</cell></row><row><cell></cell><cell></cell><cell>86.10</cell><cell>90.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Per-stage influence of STRF. All four STRF modules are effective at various stages, with best results at Stage 2 and 3 of STRF-P3DC on MARS<ref type="bibr" target="#b59">[58]</ref>.</figDesc><table><row><cell>MODEL</cell><cell>STAGE</cell><cell>mAP %</cell><cell>R@1 %</cell></row><row><cell>Baseline</cell><cell></cell><cell>83.10</cell><cell>88.50</cell></row><row><cell></cell><cell>1</cell><cell>83.40</cell><cell>88.80</cell></row><row><cell>Baseline + STRF</cell><cell>1, 2 2, 3 1, 2, 3 2, 3, 4 1, 2, 3, 4</cell><cell>83.60 86.10 84.70 85.50 83.70</cell><cell>89.00 90.30 89.30 90.00 88.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>1</head><label></label><figDesc>Additional experiments on per-stage influence of STRF. . . . . . . . . 14 2Additional analysis of STRF's four factorization components . . . .. . 14    </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Per-stage influence of STRF. STRF is effective at various stages of STRF-P3DC on MARS [58]. Location of STRF. Our STRF module performs the best with 3 ? 1 ? 1 compared to 1?3?3 as demonstrated here on MARS [58].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w. (1 ? 3 ? 3)</cell><cell>baseline</cell><cell>w. (3 ? 1 ? 1)</cell></row><row><cell>Baseline Baseline + STRF</cell><cell>STAGE 3, 4 2 3</cell><cell>mAP % 83.10 84.00 85.40 85.20</cell><cell>R@1 % 88.50 89.40 89.80 89.70</cell><cell>80 83 86 89 92</cell><cell>mAP % 80.6 83.1 86.1</cell><cell cols="2">R@1 % 90.3 88.5 87.4</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell>85.30</cell><cell>90.10</cell><cell>Figure 2:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Contribution of each factorization module. Additional analysis of STRF's four factorization components with the P3DC baseline on MARS [58].</figDesc><table><row><cell cols="2">(s, ? ) (s, ?) (t, ? ) (t, ?) mAP(%) R@1(%)</cell></row><row><cell>85.40</cell><cell>89.50</cell></row><row><cell>85.30</cell><cell>89.60</cell></row><row><cell>85.60</cell><cell>90.10</cell></row><row><cell>85.70</cell><cell>90.20</cell></row><row><cell>85.40</cell><cell>89.80</cell></row><row><cell>85.60</cell><cell>90.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) DukeMTMC-VideoReID<ref type="bibr" target="#b47">[47]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work was partially supported by ONR grants N00014-19-1-2264 and N00014-18-1-2252.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simplified Demonstration of STRF</head><p>We present a simplified demonstration of our proposed framework STRF. STRF is designed to extract four types of information from input feature maps. Intuitively, STRF learns: I. What is static temporally or changing slowly in time (e.g., how people look, TS) II. What is changing temporally or dynamic in time (e.g., how people move, TD) III. What is coarsely observable spatially (e.g., global appearance/outline, SC) IV. What is finely observable spatially (e.g., fine appearance details, SF) Each "factor" above has its own contribution. For instance, TD can provide robust features when people can only be distinguished based on motion/dynamics (e.g., same dress code). Under frame misalignment, TS (with SF/SC) can provide person-specific features while suppressing background/occlusions. The questions, then, are Q1. How are they learned? Q2. How to "weight" the input feature map using them? Factor-specific pooling functions G dk (?) help answer Q1 above. Given input feature map f (p) ? R c?4?h?w , i.e., more temporal feature maps (i.e. 4) since one needs more data points to capture what is changing dynamically (compared to TS above) in time. Similar argument holds for SF/SC spatially. Finally, the factor-specific attention map M dk helps weight feature volumes appropriately using matrix multiplication in eq. (1) (main paper) towards our objective function (helping answer question Q2 above). This shows why 4 pooling functions are necessary. As each FFM has unique G dk (?), they are different and help in focusing different aspects of information available in input feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets Details</head><p>In this section, we provide more details for each of the three datasets, MARS <ref type="bibr" target="#b59">[58]</ref>, DukeMTMC-VideoReID <ref type="bibr" target="#b47">[47]</ref>, and iLIDS-VID <ref type="bibr" target="#b45">[45]</ref>, used in the paper. Sample video tracklets for each are shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARS [58]</head><p>: MARS is a large-scale multi-camera (six views) dataset, comprising 17503 tracklets corresponding to 1261 identities, with an average number of 59 frames per tracklet. Of the 1261 identities, 625 identities are used for training and the rest for testing. Additionally, it has 3248 distractor tracklets to be used as part of the gallery. Each bounding box is detected and subsequently tracked using the DPM detection <ref type="bibr" target="#b12">[12]</ref> and GMCP tracking <ref type="bibr" target="#b8">[9]</ref> algorithms, respectively.</p><p>DukeMTMC-VideoReID <ref type="bibr" target="#b47">[47]</ref>: DukeMTMC-VideoReID is part of the DukeMTMC tracking dataset <ref type="bibr" target="#b40">[40]</ref>, comprising 1812 identities of which 702 are used for training, 702 for testing, and the rest 408 as distractors. In total, there are 2196 video tracklets for training and 2636 video tracklets for testing. Each frame in the video tracklet is sampled at an interval of 12 frames and has manually annotated bounding boxes.</p><p>iLIDS-VID <ref type="bibr" target="#b45">[45]</ref>: iLIDS-VID is a two-camera-view dataset comprising 600 video tracklets with 300 identities with an average of 73 frames per identity and manually annotated bounding boxes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-Adversarial Video Synthesis with Learned Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakib</forename><surname>Hyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6090" to="6099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From the Lab to the Real World: Re-Identification in an Airport Camera Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hebble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Circuit Systems and Video Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="540" to="553" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1169" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of Computer Vision</title>
		<meeting>the European Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">S3D-UNet: Separable 3D U-Net for Brain Tumor Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suting</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="358" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Salience-Guided Cascaded Suppression Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Second-Order Attention Network for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GMMCP tracker: Globally Optimal Generalized Maximum Multi-Clique Problem for Multiple Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shayan Modiri Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4091" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SlowFast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting Temporal Modeling for Video-based Person Re-Id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02104</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person Reidentification Using Spatio-temporal Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angels</forename><surname>Rates-Borras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating Appearance Models for Recognition, Re-acquisition, and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Performance Evaluation for Tracking and Surveillance</title>
		<meeting>the IEEE International Workshop on Performance Evaluation for Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Appearance-Preserving 3D Convolution for Video-based Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of Computer Vision</title>
		<meeting>the European Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ALANET: Adaptive Latent Attention Network for Joint Video Deblurring and Interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmaja</forename><surname>Jonnalagedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal Complementary Learning for Video Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of Computer Vision</title>
		<meeting>the European Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VRSTC: Occlusion-Free Video Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7176" to="7185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Style Normalization and Restitution for Generalizable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Srikrishna Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard J Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Angels Rates-Borras, Octavia Camps</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global-Local Temporal Representations for Video Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3958" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale temporal cues learning for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4461" to="4473" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal Aggregation with Clip-level Attention for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengliu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongli</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3376" to="3384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diversity Regularized Spatiotemporal Attention for Videobased Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harmonious Attention Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Local Maximal Occurrence Representation and Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video-based Person Re-Identification via 3D Convolutional Networks and Non-Local Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouwang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="620" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gait Recognition by Deformable Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Adachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual Attention Networks for Multimodal Reasoning and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Video Person Re-ID: Fantastic Techniques and Where to Find Them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Amir Erfan Eshratifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gormish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05295</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance Measures and a Data set for Multi-Target, Multi-Camera Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Athira Nambiar, and Anurag Mittal. Co-Segmentation Inspired Attention Networks for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arulkumar</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of Computer Vision</title>
		<meeting>the European Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing Motion and Content for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Video Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploit the Unknown Gradually: One-Shot Video-based Person Re-Identification by Stepwise Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking Spatio-Temporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Multi-Granular Hypergraphs for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2899" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial-Temporal Graph Convolutional Network for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<pubPlace>Lichen Wang, Bineng</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image Super-Resolution using Very Deep Residual Channel Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-based Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10407" to="10416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Relation-Aware Global Attention for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attribute-Driven Feature Disentangling and Temporal Aggregation for Video Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MARS: A Video Benchmark for Large-Scale Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scalable Person Re-Identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Re-Identification with Consistent Attentive Siamese Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4747" to="4756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image Registration Methods: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Zitova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="977" to="1000" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">13 2 Design choice for STRF: Location of STRF in residual modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . . . . . . . . . . . . .</forename><surname>Datasets</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17</orgName>
			</affiliation>
		</author>
		<idno>Sample tracklets from DukeMTMC-VideoReID [47], MARS [58], iLIDS-VID [45</idno>
		<imprint/>
	</monogr>
	<note>14 3 More attention maps visualization on DukeMTMC-VideoReID [47], MARS [58] . . .</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Rank-1 (R@1) retrieval results in challenging scenarios on DukeMTMC-VideoReID [47], MARS [58]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . .</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">As seen from the tracklets, video-based person re-identification is a challenging problem due to occlusions, similar appearances in different identities, and misaligned frames</title>
		<imprint/>
	</monogr>
	<note>DukeMTMC-VideoReID, (b) MARS, (c) iLIDS-VID datasets. Rows correspond to different persons. Best viewed in color. (b) MARS [58</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">We present attention maps corresponding to different real-world challenges where our proposed module STRF enabled the baseline (P3DB for DukeMTMC-VideoReID [47</title>
	</analytic>
	<monogr>
		<title level="m">More attention maps visualization on DukeMTMC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>P3DC for MARS [58]) to correctly locate the person of interest in the video tracklet. Best viewed in color. (a) DukeMTMC-VideoReID [47] (b) MARS [58</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
