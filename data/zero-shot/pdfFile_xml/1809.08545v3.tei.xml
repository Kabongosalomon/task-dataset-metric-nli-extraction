<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bounding Box Regression with Uncertainty for Accurate Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
							<email>chenchez@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
							<email>jianrenw@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<email>marioss@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bounding Box Regression with Uncertainty for Accurate Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(a) (d) (c) (b)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: In object detection datasets, the ground-truth bounding boxes have inherent ambiguities in some cases. The bounding box regressor is expected to get smaller loss from ambiguous bounding boxes with our KL Loss. (a)(c) The ambiguities introduced by inaccurate labeling. (b) The ambiguities introduced by occlusion. (d) The object boundary itself is ambiguous. It is unclear where the left boundary of the train is because the tree partially occludes it. (better viewed in color)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Large-scale object detection datasets (e.g., MS-COCO) try to define the ground truth bounding boxes as clear as possible. However, we observe that ambiguities are still introduced when labeling the bounding boxes. In this paper, we propose a novel bounding box regression loss for learning bounding box transformation and localization variance together. Our loss greatly improves the localization accuracies of various architectures with nearly no additional computation. The learned localization variance allows us to merge neighboring bounding boxes during non-maximum suppression (NMS), which further improves the localization performance. On MS-COCO, we boost the Average Precision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly, for ResNet-50-FPN Mask R-CNN, our method improves the AP and AP 90 by 1.8% and 6.2% respectively, which significantly outperforms previous stateof-the-art bounding box refinement methods. Our code and models are available at github.com/yihui-he/KL-Loss arXiv:1809.08545v3 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large scale object detection datasets like ImageNet <ref type="bibr" target="#b5">[6]</ref>, MS-COCO <ref type="bibr" target="#b34">[35]</ref> and CrowdHuman <ref type="bibr" target="#b46">[47]</ref> try to define the ground truth bounding boxes as clear as possible.</p><p>However, we observe that the ground-truth bounding boxes are inherently ambiguous in some cases. The ambiguities makes it hard to label and hard to learn the bounding box regression function. Some inaccurately labeled bounding boxes from MS-COCO are shown in <ref type="figure">Figure 1</ref> (a)(c). When the object is partially occluded, the bounding box boundaries are even more unclear, shown in <ref type="figure">Figure 1 (d)</ref> from YouTube-BoundingBoxes <ref type="bibr" target="#b39">[40]</ref>.</p><p>Object detection is a multi-task learning problem consisting of object localization and object classification. Current state-of-the-art object detectors (e.g., Faster R-CNN <ref type="bibr" target="#b41">[42]</ref>, Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref> and Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>) rely on bounding box regression to localize objects.</p><p>However, the traditional bounding box regression loss (i.e., the smooth L1 loss <ref type="bibr" target="#b12">[13]</ref>) does not take such the ambiguities of the ground truth bounding boxes into account. Besides, bounding box regression is assumed to be accurate when the classification score is high, which is not always the case, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>To address these problems, we propose a novel bounding box regression loss, namely KL Loss, for learning bounding box regression and localization uncertainty at the same time. Specifically, to capture the uncertainties of bounding box prediction, we first model the bounding box pre- diction and ground-truth bounding box as Gaussian distribution and Dirac delta function respectively. Then the new bounding box regression loss is defined as the KL divergence of the predicted distribution and ground-truth distribution. Learning with KL Loss has three benefits: <ref type="bibr" target="#b0">(1)</ref> The ambiguities in a dataset can be successfully captured. The bounding box regressor gets smaller loss from ambiguous bounding boxes. (2) The learned variance is useful during post-processing. We propose var voting (variance voting) to vote the location of a candidate box using its neighbors' locations weighted by the predicted variances during nonmaximum suppression (NMS). <ref type="formula" target="#formula_2">(3)</ref> The learned probability distribution is interpretable. Since it reflects the level of uncertainty of the bounding box prediction, it can potentially be helpful in down-stream applications like self-driving cars and robotics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>To demonstrate the generality of KL Loss and var voting, we evaluate various CNN-based object detectors on both PASCAL VOC 2007 and MS-COCO including VGG-CNN-M-1024, VGG-16, ResNet-50-FPN, and Mask R-CNN. Our experiments suggest that our approach offers better object localization accuracy for CNN-based object detectors. For VGG-16 Faster R-CNN on MS-COCO, we improve the AP from 23.6% to 29.1%, with only 2ms increased inference latency on the GPU (GTX 1080 Ti). Furthermore, we apply this pipeline to ResNet-50-FPN Mask R-CNN and improve the AP and AP 90 by 1.8% and 6.2% respectively, which outperforms the previous state-of-the-art bounding box refinement algorithm <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage Detectors: Although one-stage detection algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52]</ref> are efficient, state-of-the-art object detectors are based on two-stage, proposal-driven mechanism <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref>. Two-stage detectors generate cluttered object proposals, which result in a large number of duplicate bounding boxes. However, during the standard NMS procedure, bounding boxes with lower classification scores will be discarded even if their locations are accurate. Our var voting tries to utilize neighboring bounding boxes based on localization confidence for better localization of the selected boxes.</p><p>Object Detection Loss: To better learn object detection, different kind of losses have been proposed. UnitBox <ref type="bibr" target="#b49">[50]</ref> introduced an Intersection over Union (IoU) loss function for bounding box prediction. Focal Loss <ref type="bibr" target="#b33">[34]</ref> deals with the class imbalance by changing the standard cross entropy loss such that well-classified examples are assigned lower weights. <ref type="bibr" target="#b38">[39]</ref> optimizes for the mAP via policy gradient for learning globally optimized object detector. <ref type="bibr" target="#b27">[28]</ref> introduces uncertainties for depth estimation. The idea is further extended to the 3D object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">9]</ref>. <ref type="bibr" target="#b28">[29]</ref> proposes to weight multi-task loss for scene understanding by considering the uncertainty of each task. With KL Loss, our model can adaptively adjust variances for the boundaries of every object during training, which can help to learn more discriminative features.</p><p>Non-Maximum Suppression: NMS has been an essential part of computer vision for many decades. It is widely used in edge detection <ref type="bibr" target="#b43">[44]</ref>, feature point detection <ref type="bibr" target="#b36">[37]</ref> and objection detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. Recently, soft NMS and learning NMS <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> are proposed to improve NMS results. Instead of eliminating all lower scored surrounding bounding boxes, soft-NMS <ref type="bibr" target="#b0">[1]</ref> decays the detection scores of all other neighbors as a continuous function of their overlap with the higher scored bounding box. Learning NMS <ref type="bibr" target="#b23">[24]</ref> proposed to learn a new neural network to perform NMS using only boxes and their classification scores.</p><p>Bounding Box Refinement: MR-CNN <ref type="bibr" target="#b10">[11]</ref> is first proposed to merge boxes during iterative localization. Relation network <ref type="bibr" target="#b24">[25]</ref> proposes to learn the relation between bounding boxes. Recently, IoU-Net <ref type="bibr" target="#b26">[27]</ref> proposes to learn the IoU between the predicted bounding box and the ground-truth bounding box. IoU-NMS is then applied to the detection boxes, guided by the learned IoU. Different from IoU-Net, we propose to learn the localization variance from a probabilistic perspective. It enables us to learn the variances for the four coordinates of a predicted bounding box separately instead of only IoU. Our var voting determine the new location of a selected box based on the variances of neighboring bounding boxes learned by KL Loss, which can work together with soft-NMS <ref type="table" target="#tab_10">(Table 1 and Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first introduce our bounding box parameterization. Then we propose KL Loss for training detection network with localization confidence. Finally, a new NMS approach is introduced for improving localization accuracy with our confidence estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding Box Parameterization</head><p>Based on a two-stage object detector Faster R-CNN or Mask R-CNN <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17]</ref> shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we propose to regress the boundaries of a bounding box separately. Let (x 1 , y 1 , x 2 , y 2 ) ? R 4 be the bounding box representation as a 4-dimensional vector, where each dimension is the box boundary location. We adopt the parameterizations of the (x 1 , y 1 , x 2 , y 2 ) coordinates instead of the (x, y, w, h) coordinates used by R-CNN <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_0">t x1 = x 1 ? x 1a w a , t x2 = x 2 ? x 2a w a t y1 = y 1 ? y 1a h a , t y2 = y 2 ? y 2a h a t * x1 = x * 1 ? x 1a w a , t * x2 = x * 2 ? x 2a w a t * y1 = y * 1 ? y 1a h a , t * y2 = y * 2 ? y 2a h a (1) where t x1 , t y1 , t x2 , t y2 are the predicted offsets. t * x1 , t * y1 , t * x2 , t *</formula><p>y2 are the ground-truth offsets. x 1a , x 2a , y 1a , y 2a , w a , h a are from the anchor box. x 1 , y 1 , x 2 , y 2 are from the predicted box. In the following discussions, a bounding box coordinate is denoted as x for simplicity because we can optimize each coordinate independently.</p><p>We aim to estimate the localization confidence along with the location. Formally, our network predicts a probability distribution instead of only bounding box location. Though the distribution could be more complex ones like multivariate Gaussian or a mixture of Gaussians, in this paper we assume the coordinates are independent and use single variate gaussian for simplicity:</p><formula xml:id="formula_1">P ? (x) = 1 ? 2?? 2 e ? (x?xe) 2 2? 2 (2)</formula><p>where ? is the set of learnable parameters. x e is the estimated bounding box location. Standard deviation ? measures uncertainty of the estimation. When ? ? 0, it means our network is extremely confident about estimated bounding box location. It is produced by a fully-connected layer on top of the fast R-CNN head (fc7). <ref type="figure" target="#fig_1">Figure 3</ref> illustrates The ground-truth bounding box can also be formulated as a Gaussian distribution, with ? ? 0, which is a Dirac delta function:</p><formula xml:id="formula_2">P D (x) = ?(x ? x g )<label>(3)</label></formula><p>where x g is the ground-truth bounding box location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bounding Box Regression with KL Loss</head><p>The goal of object localization in our context is to esti-mate? that minimize the KL-Divergence between P ? (x) and P D (x) <ref type="bibr" target="#b42">[43]</ref> over N samples:</p><formula xml:id="formula_3">? = arg min ? 1 N D KL (P D (x)||P ? (x))<label>(4)</label></formula><p>We use the KL-Divergence as the loss function L reg for bounding box regression. The classification loss L cls remains the same. For a single sample: <ref type="figure">Figure 4</ref>, when the location x e is estimated inaccurately, we expect the network to be able to predict larger variance ? 2 so that L reg will be lower. log(2?)/2 and H(P D (x)) do not depend on the estimated parameters ?, hence: <ref type="figure">Figure 4</ref>: The Gaussian distributions in blue and gray are our estimations. The Dirac delta function in orange is the distribution of the ground-truth bounding box. When the location x e is estimated inaccurately, we expect the network to be able to predict larger variance ? 2 so that L reg will be lower (blue)</p><formula xml:id="formula_4">L reg = D KL (P D (x)||P ? (x)) = P D (x) log P D (x)dx ? P D (x) log P ? (x)dx = (x g ? x e ) 2 2? 2 + log(? 2 ) 2 + log(2?) 2 ? H(P D (x)) (5) Shown in</formula><formula xml:id="formula_5">L reg ? (x g ? x e ) 2 2? 2 + 1 2 log(? 2 )<label>(6)</label></formula><formula xml:id="formula_6">( $ , 2 ) ( ? + )</formula><p>When ? = 1, KL Loss degenerates to the standard Euclidean loss:</p><formula xml:id="formula_7">L reg ? (x g ? x e ) 2 2<label>(7)</label></formula><p>The loss is differentiable w.r.t location estimation x e and localization standard deviation ?:</p><formula xml:id="formula_8">d dx e L reg = x e ? x g ? 2 d d? L reg = ? (x e ? x g ) 2 ? 3 + 1 ?<label>(8)</label></formula><p>However, since ? is in the denominators, the gradient sometimes can explode at the beginning of training. To avoid gradient exploding, our network predicts ? = log(? 2 ) instead of ? in practice:</p><formula xml:id="formula_9">L reg ? e ?? 2 (x g ? x e ) 2 + 1 2 ? (9)</formula><p>We convert ? back to ? during testing.</p><p>For |x g ? x e | &gt; 1, we adopt a loss similar to the smooth L 1 loss defined in Fast R-CNN <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_10">L reg = e ?? (|x g ? x e | ? 1 2 ) + 1 2 ?<label>(10)</label></formula><p>We initialize the weights of the FC layer for ? prediction with random Gaussian initialization. The standard deviation and mean are set to 0.0001 and 0 respectively, so that KL Loss will be similar to the standard smooth L1 loss at the beginning of training. <ref type="figure">(Equation 9</ref> and Equation 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variance Voting</head><p>After we obtain the variance of predicted location, it is intuitive to vote candidate bounding box location according to the learned variances of neighboring bounding boxes.</p><p>Shown in Algorithm 1, we change NMS with three lines of code:</p><p>Algorithm 1 var voting B is N ? 4 matrix of initial detection boxes. S contains corresponding detection scores. C is N ? 4 matrix of corresponding variances. D is the final set of detections. ? t is a tunable parameter of var voting. The lines in blue and in green are soft-NMS and var voting respectively.</p><formula xml:id="formula_11">B = {b 1 , .., b N }, S = {s 1 , .., s N }, C = {? 2 1 , .., ? 2 N } D ? {} T ? B while T = empty do m ? argmax S T ? T ? b m S ? Sf (IoU (b m , T )) soft-NMS idx ? IoU (b m , B) &gt; 0 var voting p ? exp(?(1 ? IoU (b m , B[idx])) 2 /? t ) b m ? p(B[idx]/C[idx])/p(1/C[idx]) D ? D b m end while return D, S</formula><p>We vote the location of selected boxes within the loop of standard NMS or soft-NMS <ref type="bibr" target="#b0">[1]</ref>.</p><p>After selecting the detection with maximum score b, {x 1 , y 1 , x 2 , y 2 , s, ? x1 , ? y1 , ? x2 , ? y2 }, its new location is computed according to itself and its neighboring bounding boxes. Inspired by soft-NMS, we assign higher weights for boxes that are closer and have lower uncertainties. Formally, let x be a coordinate (e.g., x 1 ) and x i be the coordinate of ith box. The new coordinate is computed as follow:</p><formula xml:id="formula_12">p i = e ?(1?IoU (bi,b)) 2 /?t x = i p i x i /? 2 x,i i p i /? 2 x,i subject to IoU (b i , b) &gt; 0<label>(11)</label></formula><p>? t is a tunable parameter of var voting. Two types of neighboring bounding boxes will get lower weights during voting: (1) Boxes with high variances.</p><p>(2) Boxes that have small IoU with the selected box. Classification score is not involved in the voting, since lower scored boxes may have higher localization confidence. In <ref type="figure" target="#fig_2">Figure 5</ref>, we provide a visual illustration of var voting. With var voting, the two situations as mentioned earlier in <ref type="figure" target="#fig_0">Figure 2</ref> that lead to detection failure can sometimes be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate our method for accurate object detection, we use two datasets: MS-COCO <ref type="bibr" target="#b34">[35]</ref>   2007 <ref type="bibr" target="#b7">[8]</ref>. We use four GPUs for our experiments. The training schedule and batch size are adjusted according to the linear scaling rule <ref type="bibr" target="#b14">[15]</ref>. For VGG-CNN-M-1024 and VGG-16 Net <ref type="bibr" target="#b47">[48]</ref>, our implementation is based on Caffe <ref type="bibr" target="#b25">[26]</ref>. For ResNet-50 FPN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> and Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>, our implementation is based on Detectron <ref type="bibr" target="#b13">[14]</ref>. For VGG-16 <ref type="bibr" target="#b47">[48]</ref> Faster R-CNN, following py-faster-rcnn 1 , we train on train2014 and test on val2014. For other object detection networks, we train and test on the newly defined train2017 and val2017 respectively. We set ? t to 0.02. Unless specified, all hyper-parameters are set to default 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We evaluate the contribution of each element in our detection pipeline: KL Loss, soft-NMS and var voting with VGG-16 Faster R-CNN. The detailed results are shown in 1 github.com/rbgirshick/py-faster-rcnn 2 github.com/facebookresearch/Detectron <ref type="table">Table 1.</ref> KL Loss: Surprisingly, simply training with KL Loss greatly improves the AP by 2.8%, which is also observed on ResNet-50 Faster R-CNN and Mask R-CNN (1.5% and 0.9% improvement respectively, shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref>). First, by learning to predict high variances for samples with high uncertainties during training, the network can learn more from useful samples. Second, the gradient for localization can be adaptively controlled by the network during training <ref type="figure">(Equation 8</ref>), which encourages the network to learn more accurate object localization. Third, KL Loss incorporates learning localization confidence which can potentially help the network to learn more discriminative features.</p><p>The learned variances through our KL Loss are interpretable. Our network will output higher variances for challenging object boundaries, which can be useful in vision applications like self-driving cars and robotics. The first row    <ref type="figure" target="#fig_2">Figure 5</ref> shows some qualitative examples of the standard deviation learned through our KL Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-NMS:</head><p>As expected, soft-NMS performs consistently on both baseline and our network trained with KL Loss. It improves the AP by 1.2% and 1.4% on the baseline and our network respectively, shown in <ref type="table">Table 1</ref>.</p><p>Variance Voting: Finally, with var voting, the AP is further improved to 29.1%. We made the observation that improvement mainly comes from the more accurate localization. Notice that the AP 50 is only improved by 0.1%. However, AP 75 , AP M and AP L are improved by 1.8%, 1.8%, and 1.6% respectively, shown in <ref type="table">Table 1</ref>. This indicates that classification confidence is not always associated with localization confidence. Therefore, learning localization confidence apart from classification confidence is important for more accurate object localization.  We also found that var voting and soft-NMS can work well together. Applying var voting with the standard NMS improves the AP by 1.4%. Applying var voting after soft-NMS still can improve the AP by 1.3%. We argue that soft-NMS is good at scoring candidate bounding boxes which improve overall performance, whereas var voting is good at refining those selected bounding boxes for more accurate object localization. The second row of <ref type="figure" target="#fig_2">Figure 5</ref> shows some qualitative examples of our var voting.</p><p>Shown in <ref type="figure" target="#fig_4">Figure 6</ref>, we test the sensitivity of the tunable parameter ? t for var voting. When ? t = 0, var voting is not activated. We observe that the AP 75 , AP 80 and AP 90 can be significantly affected by ? t , while AP 50 is less sensitive to ? t . Acceptable values of ? t varies from around 0.005 ? 0.05. We use ? t = 0.02 in all experiments.</p><p>Inference Latency: We also evaluate the inference time of our improved VGG-16 Faster R-CNN on a single GTX 1080 Ti GPU with CUDA 8 and CUDNN 6, as it is crucial for resource-limited applications <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>. AP AP 50 AP 60 AP 70 AP 80 AP 90 baseline <ref type="bibr" target="#b13">[14]</ref> 38   <ref type="table" target="#tab_2">Table 2</ref>, our approach only increases 2ms latency on GPU. Different from IoUNet <ref type="bibr" target="#b26">[27]</ref> which uses 2mlp head for IoU prediction, our approach only requires a 4096 ? 324 fully-connected layer for the localization confidence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI Box Head:</head><p>We test the effectiveness of KL Loss with different RoI box heads on a deeper backbone: ResNet-50. res5/conv5 head consists of 9 convolutional layers which can be applied to each RoI as fast R-CNN head. 2mlp head consists of two fully connected layers. res5 head can learn more complex representation than the commonly used 2mlp head. Shown in <ref type="table" target="#tab_3">Table 3</ref>, KL Loss can improve the AP by 0.9% with mask. KL Loss can further improve the AP by 1.5% with conv5 head. We hypothesize that the localization variance is much more challenging to learn than localization, therefore KL Loss can benefit more from the expressiveness of conv5 head. Since conv5 head is not commonly used in recent state-of-theart detectors, we still adopt the 2mlp head in the following experiments.  <ref type="bibr" target="#b45">[46]</ref> 60.6 QUBO (greedy) <ref type="bibr" target="#b45">[46]</ref> 61.9 soft-NMS <ref type="bibr" target="#b0">[1]</ref> 70  head for learning localization confidence, which introduces nearly no additional computation. (3) var voting does not require iterative refinement, which is much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Accurate Object Detection</head><p>We further evaluate our approach on the feature pyramid network (ResNet-50 FPN) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref>, shown in <ref type="table" target="#tab_10">Table 6</ref>.</p><p>For fast R-CNN version, training with KL Loss increases the baseline by 0.4%. After applying var voting along with soft-NMS, our model achieves 38.0% in AP, which outperforms both IoU-NMS and soft-NMS baselines. Training end-to-end with KL Loss can help the network learn more discriminative features, which improves the baseline AP by 0.6%. The final model achieves 39.2% in AP, which improves the baseline by 1.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on PASCAL VOC 2007</head><p>Even though our approach is designed for large scale object detection, it could also generalize well on small datasets. We perform experiments with Faster R-CNN <ref type="bibr" target="#b41">[42]</ref>   Net <ref type="bibr" target="#b47">[48]</ref> are tested. Shown in <ref type="table" target="#tab_8">Table 5</ref>, we compare our approach with soft-NMS and quadratic unconstrained binary optimization (QUBO <ref type="bibr" target="#b45">[46]</ref>). For QUBO, we test both greedy and classical tabu solver (we manually tuned the penalty term for both solvers to get better performance). We observe that it is much worse than the standard NMS, though it was reported to be better for pedestrian detection. We hypothesize that QUBO is better at pedestrian detection since there are more occluded bounding boxes <ref type="bibr" target="#b46">[47]</ref>. For VGG-CNN-M-1024, training with var voting improves the mAP by 1.6%. var voting further improves the mAP by 0.8%. For VGG-16, our approach improves the mAP by 2.9%, combined with soft-NMS. We notice that var voting could still improve performance even after soft-NMS is applied to the initial detection boxes. This observation is consistent with our experiments on MS-COCO <ref type="table">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To conclude, the uncertainties in large-scale object detection datasets can hinder the performance of state-of-theart object detectors. Classification confidence is not always strongly related to localization confidence. In this paper, a novel bounding box regression loss with uncertainty is proposed for learning more accurate object localization. By training with KL Loss, the network learns to predict localization variance for each coordinate. The resulting variances empower var voting, which can refine the selected bounding boxes via voting. Compelling results are demonstrated for VGG-16 Faster R-CNN, ResNet-50 FPN and Mask R-CNN on both MS-COCO and PASCAL VOC 2007.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of failure cases of VGG-16 Faster R-CNN on MS-COCO. (a) both candidate boxes are inaccurate in a certain coordinate. (b) the left boundary of the bounding box which has the higher classification score is inaccurate. (better viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our network architecture for estimating localization confidence. Different from standard fast R-CNN head of a two stage detection network, our network esitmates standard deviations along with bounding box locations, which are taken into account in our regression loss KL Loss the fast R-CNN head of our network architecture for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Results of var voting with VGG-16 Faster R-CNN on MS-COCO. The green textbox in the middle of each boundary is the corresponding standard deviation ? we predicted (Equation 2). Two failure situations corresponding to Figure 2 that can be improved by var voting: (a) When each candidate bounding box is inaccurate in some coordinates (women on the right), our var voting can incorporate their localization confidence and produce better boxes. (b) The bounding box with a higher classification score (train 0.99) actually has lower localization confidence than the bounding box with a lower classification score (train 0.35). After var voting, the box scored 0.99 moves towards the correct location. (better viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Varying ? t for var voting with ResNet-50 Faster R-CNN. (better viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>KL Loss soft-NMS var voting AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100</figDesc><table><row><cell></cell><cell>23.6 44.6</cell><cell>22.8</cell><cell>6.7</cell><cell>25.9</cell><cell>36.3 23.3</cell><cell>33.6</cell><cell>34.3</cell></row><row><cell></cell><cell>24.8 45.6</cell><cell>24.6</cell><cell>7.6</cell><cell>27.2</cell><cell>37.6 23.4</cell><cell>39.2</cell><cell>42.2</cell></row><row><cell></cell><cell>26.4 47.9</cell><cell>26.4</cell><cell>7.4</cell><cell>29.3</cell><cell>41.2 25.2</cell><cell>36.1</cell><cell>36.9</cell></row><row><cell></cell><cell>27.8 48.0</cell><cell>28.9</cell><cell>8.1</cell><cell>31.4</cell><cell>42.6 26.2</cell><cell>37.5</cell><cell>38.3</cell></row><row><cell></cell><cell>27.8 49.0</cell><cell>28.5</cell><cell>8.4</cell><cell>30.9</cell><cell>42.7 25.3</cell><cell>41.7</cell><cell>44.9</cell></row><row><cell></cell><cell>29.1 49.1</cell><cell>30.4</cell><cell>8.7</cell><cell>32.7</cell><cell>44.3 26.2</cell><cell>42.5</cell><cell>45.5</cell></row><row><cell cols="8">Table 1: The contribution of each element in our detection pipeline on MS-COCO. The baseline model is VGG-16 Faster</cell></row><row><cell>R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">method latency (ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ours</cell><cell>101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Inference time comparison on MS-COCO with</cell></row><row><cell cols="3">VGG-16 Faster R-CNN on a GTX 1080 Ti GPU, CUDA</cell></row><row><cell cols="2">8 [38] and CUDNN 6 [3]</cell><cell></cell></row><row><cell>fast R-CNN head</cell><cell>backbone KL Loss</cell><cell>AP</cell></row><row><cell>2mlp head</cell><cell>FPN</cell><cell>37.9 38.5 +0.6</cell></row><row><cell>2mlp head + mask</cell><cell>FPN</cell><cell>38.6 39.5 +0.9</cell></row><row><cell>conv5 head</cell><cell>RPN</cell><cell>36.5 38.0 +1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different fast R-CNN heads. The model is ResNet-50 Faster R-CNN of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of different methods for accurate object detection on MS-COCO. The baseline model is ResNet-50-FPN Mask R-CNN. We improve the baseline by ? 2% in AP Shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>summarizes the performance of different methods for accurate object detection on ResNet-50-FPN Mask R-CNN. With KL Loss, the network can learn to adjust the gradient for ambiguous bounding boxes during training. As a result, Mask R-CNN trained with KL Loss performs significantly better than the baseline for high overlap metrics like AP 90 . Variance Voting improves the localization results by voting the location according to the localization confidences of neighboring bounding boxes. AP 80 and AP 90 are further improved by 0.4% and 1.2% respectively. Variance Voting is also compatible with soft-NMS. Variance Voting combined with soft-NMS improves the AP 90 and the overall AP of the final model by 6.2% and 1.8% respectively.</figDesc><table><row><cell>Compared with IoUNet [27]: (1) our variance and localiza-</cell></row><row><cell>tion are learned together with KL Loss, which improves the</cell></row><row><cell>performance. (2) KL Loss does not require a separate 2mlp</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different approaches on PASCAL VOC 2007 with Faster R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>on PASCAL VOC 2007, which consists of about 5k voc_2007_trainval images and 5k voc_2007_test images over 20 object categories. Backbone networks: VGG-CNN-M-1024 and VGG-16 type method AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell>baseline (1x schedule) [14]</cell><cell>36.4 58.4</cell><cell cols="4">39.3 20.3 39.8 48.1</cell></row><row><cell></cell><cell>baseline (2x schedule) [14]</cell><cell>36.8 58.4</cell><cell cols="4">39.5 19.8 39.5 49.5</cell></row><row><cell></cell><cell>IoU-NMS [27]</cell><cell>37.3 56.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>fast R-CNN</cell><cell>soft-NMS [1]</cell><cell>37.4 58.2</cell><cell cols="4">41.0 20.3 40.2 50.1</cell></row><row><cell></cell><cell>KL Loss</cell><cell>37.2 57.2</cell><cell cols="4">39.9 19.8 39.7 50.1</cell></row><row><cell></cell><cell>KL Loss+var voting</cell><cell>37.5 56.5</cell><cell cols="4">40.1 19.4 40.2 51.6</cell></row><row><cell></cell><cell>KL Loss+var voting+soft-NMS</cell><cell>38.0 56.4</cell><cell cols="4">41.2 19.8 40.6 52.3</cell></row><row><cell></cell><cell>baseline (1x schedule) [14]</cell><cell>36.7 58.4</cell><cell cols="4">39.6 21.1 39.8 48.1</cell></row><row><cell></cell><cell>IoU-Net [27]</cell><cell>37.0 58.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>IoU-Net+IoU-NMS [27]</cell><cell>37.6 56.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>baseline (2x schedule) [14]</cell><cell>37.9 59.2</cell><cell cols="4">41.1 21.5 41.1 49.9</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">IoU-Net+IoU-NMS+Refine [27] 38.1 56.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>soft-NMS[1]</cell><cell>38.6 59.3</cell><cell cols="4">42.4 21.9 41.9 50.7</cell></row><row><cell></cell><cell>KL Loss</cell><cell>38.5 57.8</cell><cell cols="4">41.2 20.9 41.2 51.5</cell></row><row><cell></cell><cell>KL Loss+var voting</cell><cell>38.8 57.8</cell><cell cols="4">41.6 21.0 41.5 52.0</cell></row><row><cell></cell><cell>KL Loss+var voting+soft-NMS</cell><cell>39.2 57.6</cell><cell cols="4">42.5 21.2 41.8 52.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison with FPN ResNet-50 on MS-COCO</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was partially supported by National Key R&amp;D Program of China (No. 2017YFA0700800).</p><p>We would love to express our appreciation to Prof. Kris Kitani and Dr. Jian Sun for the useful discussions during this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00726</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1703.06211</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Motion prediction of traffic actors for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3266" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Leveraging heteroscedastic aleatoric uncertainties for robust real-time lidar 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05590</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He. Detectron. github.com/facebookresearch/detectron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning 6-dof grasping and pick-place using attention focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Gualtieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06134</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressnet: Shift-based primitives for efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huasong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vehicle traffic driven camera placement for better metropolis security surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08508</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Softer-nms: Rethinking bounding box regression for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08545</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6469" to="6477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11575</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07115</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Light-head r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single image super-resolution via a lightweight residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable parallel programming with cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning globally optimized object detector via policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6190" to="6198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Machine learning, a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Robert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge and curve detection for visual scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azriel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-maximum suppression for object detection by passing messages between windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimized pedestrian detection for multiple and occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitapa</forename><surname>Rujikietgumjorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3690" to="3697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03280</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Prediction-tracking-segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
