<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ehsan</forename><surname>Mirsadeghi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Royat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2021 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning Methods</term>
					<term>Object Detection</term>
					<term>Seg- mentation and Categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is one of the basic, yet essential scene understanding tasks for an autonomous agent. The recent developments in supervised machine learning and neural networks have enjoyed great success in enhancing the performance of the state-of-the-art techniques for this task. However, their superior performance is highly reliant on the availability of a large-scale annotated dataset. In this paper, we propose a novel fully unsupervised semantic segmentation method, the socalled Information Maximization and Adversarial Regularization Segmentation (InMARS). Inspired by human perception which parses a scene into perceptual groups, rather than analyzing each pixel individually, our proposed approach first partitions an input image into meaningful regions (also known as superpixels). Next, it utilizes Mutual-Information-Maximization followed by an adversarial training strategy to cluster these regions into semantically meaningful classes. To customize an adversarial training scheme for the problem, we incorporate adversarial pixel noise along with spatial perturbations to impose photometrical and geometrical invariance on the deep neural network. Our experiments demonstrate that our method achieves the stateof-the-art performance on two commonly used unsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.</p><p>Digital Object Identifier (DOI): see top of this page. Fig. 1: Segmentation Result. (first row) Some examples from COCO-Stuff datasets [16], (second row) their truth, and (third row) the predicted segmentation masks using our proposed method (InMARS + ). Recently unsupervised learning techniques such as deep clustering methods show promises in dealing with image-level tasks such as unsupervised image classification problems [9], [10], [11], [12], [13].</p><p>More specifically, maximizing mutual information plays a remarkable role in the core of recent works, and many algorithms have successfully utilized this concept for unsupervised image classification [9], [10], [11],  [12], <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. However, these techniques cannot be naively applied to semantic segmentation tasks, which require spatially smooth and consistent label predictions at the pixel-levels.</p><p>Recently, there have been few attempts to tackle this challenging problem by conducting various strategies and domain knowledge <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. However, these techniques are more susceptible to degenerate/trivial solutions, and their superior performance are highly dependent on their incorporated data augmentation strategy.</p><p>This paper introduces a novel pixel-wise and fully unsupervised learning method for semantic segmentation. Our proposed approach uses an adversarially trained information maximization technique over the features extracted from a set of segmented regions, also known as superpixels. By extracting features from these grouped pixels, we, indeed, reformulate unsupervised image segmentation as a sub-region unsupervised classification task and transform decision space from pixel-level into sub-region domain. Next, we utilize Mutual Information Maximization for maintaining maximum arXiv:2107.00691v1 [cs.CV] 1 Jul 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ANY autonomous agents, e.g. mobile robots and driverless vehicles, rely on scene understanding tasks for navigation and interaction with the environment and semantic segmentation is one of the basic, yet most important visual perception tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In recent years, the significant developments in machine learning and neural networks have been conducive to supervised semantic segmentation regime <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, the superior performance of these supervised techniques is highly subject to the availability of a large-scale and high-quality annotated dataset, and providing such a pixel-level annotation for a dataset from a novel domain, e.g. satellite or underwater images, can be a very challenging and laborious task <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>information between input and output, which enforces semantically similar and different data to be assigned to the same and distinct clusters, respectively. We also incorporate an adversarial training strategy to act as a regularizer for our model, helping it learn a robust and generalizable representation for clustering the extracted features. To customize our learning regime for the segmentation task, we also propose a novel adversarial strategy by bundling a learnable adversarial pixel intensity noise with a set of random geometric perturbations on the superpixels. We test and evaluate the proposed method on four subsets of two commonly used unsupervised image segmentation benchmark datasets: Potsdam <ref type="bibr" target="#b20">[21]</ref>, and COCOstuff <ref type="bibr" target="#b15">[16]</ref>. We demonstrate the superior performance of our proposed method, outperforming the existing state-of-the-art methods on these datasets. Moreover, we perform an ablation study to show each component's contribution to the proposed method. In summary, the main contributions of this work can be summarized as follows:</p><p>1) We propose a new unsupervised image segmentation approach based on adversarial training and mutual information, 2) We introduce a novel sub-region-level training strategy for image segmentation and incorporate customized adversarial pixel noise followed by spatial perturbations to make the model robust to the photometrical and geometrical variations, 3) Our approach achieves the state-of-the-art results on commonly used unsupervised semantic segmentation datasets.</p><p>II. RELATED WORKS Supervised Semantic Segmentation: The recent advances in deep learning and convolutional neural architectures, along with the availability of large-scale annotated datasets, have been helpful to the impressive performance of the recent semantic segmentation algorithms <ref type="bibr" target="#b21">[22]</ref>. Fully Convolutional Network (FCNs) <ref type="bibr" target="#b4">[5]</ref> is a successful generation of convolutional neural network (CNN) architectures that achieved a significant improvement over classical approaches. Several notable architectural variants of FCNs are later proposed by exploiting various strategies. For instance, <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b3">[4]</ref> proposed different multi-scale and pyramid FCN architectures to learn multi-scale feature representation resulting in a better segmentation performance for various region sizes' resolution. Chen et al. <ref type="bibr" target="#b0">[1]</ref> also proposed atrous spatial pyramid pooling to segment objects at multiple scales robustly while applying Conditional Random Field (CRF) additionally at the end of the network to smooth out the pixel-wise label predictions. Jun et al. <ref type="bibr" target="#b22">[23]</ref> suggested an attention mechanism to integrate local features with their global dependencies. Recently, <ref type="bibr" target="#b2">[3]</ref> introduced a computational tractable multi-stage architecture to extract a fast but high-resolution feature representation. Despite the progressive success of supervised techniques, their great performance depends on the availability of large-scale annotated data.</p><p>Deep Clustering: Data clustering is a classical pattern recognition problem, and several model-based approaches, e.g. K-means <ref type="bibr" target="#b23">[24]</ref>, Gaussian Mixture <ref type="bibr" target="#b24">[25]</ref>, Kernel-based <ref type="bibr" target="#b25">[26]</ref> and spectral clustering <ref type="bibr" target="#b26">[27]</ref>, have been developed to tackle this fundamental problem. However, their performance considerably degrades on highly non-linear data and/or in a high dimensional space, e.g. raw and unprocessed images. Deep learning has provided an opportunity for these clustering techniques to overcome their drawbacks by clustering on a proper embed (and possibly low-dimensional) feature space, encoded by these neural models. Deep Embedded Clustering (DEC) <ref type="bibr" target="#b8">[9]</ref> is one of the pioneering works, which combines one of the classical clustering methods with deep learning. This method combines a reconstruction loss on an Auto-Encoder (AE) with a clustering objective on the encoded features to construct and cluster the rich low-dimensional encoded features, simultaneously. Other similar approaches also use AE reconstruction loss but followed by a K-means <ref type="bibr" target="#b27">[28]</ref>, or a crossentropy objective between multiple network predictions <ref type="bibr" target="#b12">[13]</ref> to perform the clustering task on the deep embed space. The other generative models such as Variational AE (VAE) <ref type="bibr" target="#b9">[10]</ref> and InfoGAN <ref type="bibr" target="#b10">[11]</ref> are also used in combination with clustering objectives to learn a clustering-friendly latent space.</p><p>Unlike the earlier works, IMSAT <ref type="bibr" target="#b11">[12]</ref> considers the clustering task as a discrete representation problem, where each discrete representation would be a code for the assignment of input data to a specific cluster. Then an objective is defined to maximize mutual information between inputs and outputs while regularizing this representation by an adversarial perturbation. In spite of their success in the data clustering problems, these deep clustering methods cannot be naively used for the unsupervised semantic segmentation problem where the task is to cluster semantically same pixels with high spatial/local dependencies using fine-grained deep features.</p><p>Unsupervised Semantic Segmentation: Being a very challenging problem, there have been a few recent attempts to tackle unsupervised image segmentation. Hwang et al. <ref type="bibr" target="#b18">[19]</ref> proposed a method by combining two sequential clustering modules for both pixel-level and segment-level to perform the task. Notwithstanding its satisfactory performance, this approach relies on mean-shift clustering, making it susceptible to the degeneracy issue, i.e. assigning random labels to different candidates. Generative model-based approaches such as <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> generate a semantic mask for an image by perturbing <ref type="bibr" target="#b28">[29]</ref> or redrawing <ref type="bibr" target="#b29">[30]</ref> the generated foreground and background masks. Despite their great performance, their application is limited to the cases where each image only contains a single foreground object.</p><p>Perhaps the most relevant and competitive approaches to ours are the recent works of IIC <ref type="bibr" target="#b17">[18]</ref> and AC <ref type="bibr" target="#b19">[20]</ref>, which similarly uses the mutual information maximization concept to tackle unsupervised segmentation task. Different from these frameworks, our proposed method, inspired by <ref type="bibr" target="#b11">[12]</ref>, models the unsupervised segmentation task as maximizing mutual information between the inputs and a discrete representation, i.e. the cluster assignments. Moreover, while these methods rely on heuristic data augmentation strategies for their unsupervised model, our proposed framework incorporates a wisely customized adversarial training strategy as a learning-based data augmentation, which is jointly optimized with mutual information maximization objectives. Additionally, we suggest a new overlook into the problem by suggesting to directly cluster spatially connected groups of pixels, i.e. superpixels, instead of the pixels. This strategy facilitates training, reduces the computational cost, and increases statistics' reliability during training by increasing batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The semantic segmentation task can be considered as a mapping function F : X ? Y from input image X ? R (w?h?c) to the output map Y ? B (w?h?k) where w and h define the input image's width and height and c and k represent the color and the semantic label channels of the input and the output, respectively. Note that in this representation, Y is a discrete binary tensor, and the label of each pixel in the image X is encoded as a one-hot binary vector in Y . Therefore, a group of pixels with an identical one-hot binary code represents the same semantic object class. Without loss of generality, we can assume there always exists a proper mapping function between the inputs and outputs, which can be estimated using a deep neural network <ref type="bibr" target="#b0">1</ref> .</p><p>In an unsupervised setting, the main task is to find a proper function F using a deep model, which learns to perform this mapping without any annotation. To tackle this, we build a framework ( <ref type="figure" target="#fig_0">Fig. 2</ref>) based on three major components, including Region-Wise Embedding (RWE), Mutual Information Maximization (MIM), and Adversarial Regularization, which are separately elaborated in the followings.</p><p>From Pixel to Region: At the pixel-level, extracting semantically meaningful features is more complicated than the image-level, as we need to ensure maintaining the spatial smoothness and connectivity between the semantic label of neighboring pixels while generating an embedding for each pixel. To this end, we propose to exploit segmentation region proposals, e.g. superpixels, and to assign the same label to all the pixels in each region to ensure holding their spatial label smoothness while reducing the problem's complexity. These superpixels can be directly extracted using any unsupervised, model-based, or deep learning-based frameworks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In fact, each superpixel provides a hypothetically good initial guess for the spatial connectivity of the semantically meaningful pixels in proximity without any supervision. Therefore, from each of these superpixel regions, one feature embedding can be extracted as the representative. The module extracting these features is called Region-Wise-Embedding (RWE) in our framework. RWE receives an embedding map from the backbone's output and produces a fixed-length embedding for each superpixel using a pooling strategy. The effect of the superpixel parameter selection and pooling strategy for this module will be discussed in the ablation study. Note that the RWE module might resemble the Region Proposal Network (RPN) in Faster R-CNN detector <ref type="bibr" target="#b34">[35]</ref>.</p><p>Mutual Information Maximization and Adversarial Regularization: Training a deep network requires optimizing a loss function. In unsupervised settings, a loss function should optimize the network parameters without any annotated data. Hence, the design of the loss function capturing a proper source of information for the task plays a remarkable role in tackling the problem. In the absence of an annotated data, the input measurements and the model predictions are the only available source of information for training the model. In addition, data augmentation, as a very popular strategy, can be conducive to the unsupervised training scheme, e.g. by generating additional pseudo labeled data. This is because the augmented version of input can hypothetically have the same label as the original input. Using this pair match, the model is encouraged to generate a similar output for this pair, i.e. the input data, and its augmented version.</p><p>Selecting a proper data augmentation strategy with a reasonable perturbation/augmentation magnitude has a crucial impact on the quality of optimization convergence, e.g. simple data augmentation or a minor perturbation may not help the optimization to converge to a good solution, and a high perturbation or incorrect augmentation strategy may result in optimization divergence, <ref type="bibr" target="#b35">[36]</ref>. To this end, the previous works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[37]</ref> have also attempted to adopt a proper augmentation strategy for their methods to generate reasonable results. Despite this popularity, designing proper augmentation is still challenging since the perturbation should not alter the meaning of data.</p><p>To address this challenge, we utilize adversarial training in our data augmentation module in order to generate adversarial examples while bound the amount of perturbation in a safe region <ref type="bibr" target="#b11">[12]</ref>. A safe region is an area where adding perturbation does not alter the meaning of input data, and it is a hyperparameter in adversarial training. As depicted in <ref type="figure">Figure.</ref> 2, the adversarial augmentation module gets an image X and generate its augmented version X . This secondary input passes through the network and result in Y . The adversarial objective is then computed as a deviation between Y and Y . Mutual information is an information-theoretic criterion to evaluate the correlation between two variables, so it is a valuable measure to train a deep network in the lack of annotation. We exploit this concept to maximize the dependency between input and output and encourage the model to hold maximum information between them. We show this criterion in the <ref type="figure" target="#fig_0">Figure 2</ref> between X and Y . The final loss function is a weighted average of the adversarial and mutual information terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Method</head><p>This paper proposes a novel approach to find the proper mapping function called Information Maximization and Adversarial Regularization Segmentation (InMARS) to segment input image into a predefined number of clusters without any label or supervision. To explain the pipeline, we start by providing further details about RWE and the variant of the extracted features. Then, as our clustering technique, we elaborate more on the Mutual Information Maximization objective. Finally, we present our customized adversarial training as a regularizer .</p><p>Region Wise Embedding (RWE): As mentioned earlier, in this stage, our goal is to produce candidate region/sub-regions to preserve spatial information to some extent and to generate smooth label predictions for each sub-region while generating a reliable feature from this candidate for the next stage. Moreover, we need to ensure that this RWE module can extract a fixed-length feature representation from each superpixel with arbitrary sizes before inputting them into the clustering objective. To this end, we evaluate three different strategies to generate the fixed-length embedding from variable-size inputs as follow: (1) Sub-region Pooling, (2) Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b37">[38]</ref>, and (3) Interpolation. In the sub-region pooling strategy, we produce a single embedding by applying max/average-pooling to each superpixel separately. SPP is a fixed-length map that tunes 2D maxpooling parameters based on input size in multiple scales to generate a fixed size output for any input size's region. We utilize a zero mask in the SPP to zero out pixels' embedding that fall into SPP's rectangular region but do not belong to the selected superpixel. Then, we passed SPP's output from a series of fully connected (FC) layers to generate the final embedding. Interpolation is another strategy to build an equal size embedding for all candidates. For this, we resize superpixel regions into a predefined size. Then, similar to SPP, we apply a zero mask and feed a flattened version of these equal-sized regions to the fully connected layers. Their effect on the performance will be studied in the ablation part.</p><p>Unsupervised Segmentation by Maximizing Mutual Information and Adversarial Training: Let Y i ? B (w?h) be i th channel of Y as a discrete representation in the network's output where 1 ? i ? k and k is the maximum number of the semantic labels/clusters. Also, let x ? X and y i ? Y i be random variables for the input and output, respectively. Given x and considering each label as a conditionally independent variable, we can formulate the joint distribution of the output by p ? (y 1 , . . . ,</p><formula xml:id="formula_0">y k | x) = ? k i=1 p ? (y i | x),</formula><p>where ? is the parameters of the model, i.e. deep neural network. During training, mutual information between input X and output Y is maximized using marginal entropy and conditional entropy, while the training is regularized via adversarial training. To this end, we have the following objective to minimize:</p><formula xml:id="formula_1">R(? ) ? ? I(X;Y )<label>(1)</label></formula><p>Where R(? ) is the regularization term that enforces the deep network to produce informative and reliable features. I(X;Y ) is the mutual information between input and embedding of a network, and ? is a trade-off between two terms.</p><p>Mutual Information Objective: There are two ways to deal with the data clustering problem using mutual information maximization (MIM), 1) MIM(X,Y ) between the inputs X and the outputs Y and 2) MIM(Y,Y ) between the output Y and the prediction of an augmented version of the input? . In both approaches, the goal is to train a network that maps similar inputs into similar representations <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The MIM(Y,Y ) requires a pair of the original input and augmented version, yet the MIM(X,Y ) does not depend on any additional pairs and can be readily computed from the network's output. Following <ref type="bibr" target="#b11">[12]</ref>, we selected MIM(X,Y ) strategy as our strategy. In terms of entropy, mutual information can be described as</p><formula xml:id="formula_2">I(X;Y ) = H(Y ) ? H(Y | X), where H(Y )</formula><p>and H(Y | X) are entropy and conditional entropy, respectively, defined by:</p><formula xml:id="formula_3">H(Y ) ? h(p ? (y)) = h 1 N N ? j=1 p ? (y | x j ) (2) H(Y | X) = 1 N N ? j=1 h p ? (y | x j )<label>(3)</label></formula><p>where h(.) is the entropy function and x j is each training data input. According to <ref type="bibr" target="#b38">[39]</ref>, we can incorporate prior knowledge, e.g. a distribution over the sample size in each class in the case of a class imbalance in the data, by rewriting H(Y ) as log(K) ? KL[p ? (y)||q(y)] where K is the number of clusters, q(y) is the prior distribution and KL[.||.] is the Kullback-Leibler divergence <ref type="bibr" target="#b39">[40]</ref>. In our experiments for simplicity, we assume the prior q(y) is a uniform distribution.</p><p>To better understand MIM as an objective function, we could evaluate marginal entropy H(Y ) and conditional entropy H(Y | X). The marginal entropy pushes cluster assignment to be uniform (distributing data to all clusters) while the conditional entropy forces cluster assignment to be explicit (push the same data to a single cluster).</p><p>Adversarial Training: To augment each data sample with adversarial training, an additional regularization term R(? ) is incorporated into the loss function:</p><formula xml:id="formula_4">R(? ; T ) = KL (p ? (y i | x) , p ? (y i | T (x)))<label>(4)</label></formula><p>where KL, p ? (y i | x) and p ? (y i | T (x)) are KL divergence, the network's embedding for the original data x, and the transformed (augmented) data T (x), respectively.</p><p>This regularization objective penalizes embedding dissimilarity between data and augmented versions and enforces the consistency of embedding on the network's output. We select pixel noise to increase the invariance of the extracted features against perturbations r as T (x) = x+r. Then, we regularize the network against adversarial samples while keeping the decision boundary in the low-density region of data distribution by confining adversarial perturbations. The confined adversarial samples are mined according to the following criteria. r = argmax r R adv (? ; x, T (x)); r 2 ? ?</p><p>Where r is the additive noise and ? is a hyper-parameter that controls the range of the local perturbation. We can approximate adversarial term r within the training process by an additional forward/backward pass. To this end, we called R adv as adversarial form of R when we compute T (x) in adversarial fashion. In addition, we also add random affine perturbations to include geometrical invariance to the network as in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Training Objective: Combining adversarial and mutual information terms result in the final objective function. Recall that our main objective (Equation 1) consist of two terms R adv (? ) and I(X;Y ). Therefore, we can write L R and L MI for these terms as follow:</p><formula xml:id="formula_6">L R = 1 2 R adv + 1 2 R geo L MI = H(Y | X) ? ?H(Y ) L total = L R + ? L MI<label>(6)</label></formula><p>where L R and L MI are regularization loss and mutual information loss, respectively. ? and ? are the hyper-parameters. We fix the ? during training but decrease the ? gradually (see Sec. IV -Hyper-parameters, for more details). The higher value of ? at initial stages helps to uniformly generate class in the output while the decay of this coefficient gradually assigns more importance on the explicit class generation in the final stages. R adv and R geo are adversarial additive pixel noise and random affine perturbation, respectively. For R geo we use random scaling, translation, rotation and shearing similar to <ref type="bibr" target="#b11">[12]</ref>. All hyper-parameters are tuned in a non-overlapping subset with the train and validation sets, and the computed parameters are used for the final training and evaluation. Minimizing loss function L total is equivalent to minimizing R and maximizing I(X;Y ), simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Dataset: Ji et al. <ref type="bibr" target="#b17">[18]</ref> has released two benchmark from COCO-Stuff <ref type="bibr" target="#b15">[16]</ref> and Potsdam <ref type="bibr" target="#b20">[21]</ref> datasets, which has become the most popular datasets for evaluating the existing unsupervised image segmentation methods. Potsdam dataset consists of 8550 RGBIR 200 ? 200 pixel satellite images with 3150 unlabelled samples. The first subset (Potsdam) has six classes (roads and cars, vegetation and trees, buildings, and clutter), while the second one (Potsdam-3) has three classes. Potsdam-3 is created by merging six classes of the first subset into three coarser classes. COCO-Stuff <ref type="bibr" target="#b15">[16]</ref> is another challenging dataset that contains stuff classes. The first subset of COCO-Stuff has 15-coarse classes with 52K images. These images at least contain 75% of stuff pixels. The second subset (COCO-Stuff-3) is generated by taking 36K images from the first subset with the sky, ground, and plant classes.</p><p>Implementation Details: We conducted all experiments with randomly initialized networks. We used Hourglass <ref type="bibr" target="#b40">[41]</ref> as the main backbone of our segmentation network; however, this backbone can be replaced by any other fully convolutional networks, e.g. the UNet <ref type="bibr" target="#b41">[42]</ref>, as shown in our ablation study <ref type="table" target="#tab_0">(Table II)</ref>. Hourglass or "stacked hourglass" network is built on the successive steps of pooling and upsampling done to produce a final set of predictions. Each hourglass module is closely connected to fully convolutional networks, and the connection of modules could process spatial information at multiple scales for dense prediction <ref type="bibr" target="#b40">[41]</ref>. We used two stacks of the hourglass with two residual blocks in each hourglass as our backbone. Then, the RWE head is employed on the top of the backbone to map pixels' embedding to regions. In RWE, we picked SLIC Superpixel <ref type="bibr" target="#b30">[31]</ref> with parameters so that each candidate region conveys meaningful spatial information and has self-determining features. In fact, the form and spatial size of the ground truth classes can be important to choose proper superpixel parameters. Therefore, the parameters are set such that (i) the number of superpixels is not less than the number of semantic classes; (ii) Since under a general superpixel parameters' setup, there is no guarantee that each generated superpixel region carries the pixels with the same semantic labels, over-segmentation is essential in our framework. By over-segmentation, our framework can merge the superpixels with the same semantic classes. We implemented the RWE module with three different approaches: (1) subregion pooling, (2) SPP, and <ref type="formula" target="#formula_3">(3)</ref> interpolation. In the ablation part, we studied the impact of each approach on the network's performance <ref type="table" target="#tab_0">(Table III)</ref>. In our final result, we chose SPP as the best module for the RWE module.</p><p>Our proposed method (InMARS) has three variants, namely InMARS b , InMARS and InMARS + . InMARS b is the argmaxed outputs from the backbone component of our model during inference <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Note that in our implementations, we ensured that the number of the backbone's output channels is equal to the number of ground-truth classes. InMARS is the inference output of our proposed model <ref type="figure" target="#fig_0">(Fig. 2)</ref> including RWE module, trained with 2 * N gt superpixels. InMARS + is another variant (multi-scale) of InMARS, incorporating all three superpixel resolutions <ref type="table" target="#tab_4">(Table V)</ref> during inference to generate multiple outputs for each pixel. Then, the final prediction is formed by pixel-wise averaging of the three embeddings. We compared the performance of our method against <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. We also reported the results for a supervised baseline, trained using the UNet <ref type="bibr" target="#b41">[42]</ref>, as the best performing backbone from the possible backbones we used in our experiment.</p><p>Hyper-parameters: All the hyper-parameters in our experiments were tuned in a non-overlapping subset of the dataset (%10 randomly selected), and they were fixed during optimization and evaluation on the dataset on which we evaluated and reported the final results. We selected Adam optimizer <ref type="bibr" target="#b47">[48]</ref> with the initial learning rate lr init = 10 ?5 and exponential decays for the learning rate by ? = 0.96 in every epoch to do all the experiments. The learning rate in every epoch is derived by lr i = ? i * lr init . In this equation lr init is the initial learning rate, i is the numerator of epoch and lr i is the learning rate in the epoch i. We trained the network for 100 epochs and picked the last model for evaluation. We selected (5, 2 * N gt ) for compactness and number of SLIC superpixels during training. To speed up the training process, we extracted all superpixels once offline. In training, invalid pixels (pixels that do not belong to any of the classes) were discarded, and the loss function was calculated just on valid pixels. We followed the prior works for image size (200 ? 200 and 128 ? 128 pixel images for Potsdam and COCO-Stuff, respectively. For hourglass, we resized the network's inputs from 200 ? 200 to 192 ? 192 pixels, as with depth equal to 4, the input should be divisible by 16 <ref type="bibr">(2 depth</ref> ). We used PyTorch library <ref type="bibr" target="#b48">[49]</ref> for implementation and NVidia Titan X and GTX 1080ti GPUs for training. The hyper-parameters ? , ? and ? are set to 4, 1 and .25, respectively. We fixed the ? and ? during training but decreased the ? from 1 to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation</head><p>As in IIC <ref type="bibr" target="#b17">[18]</ref> and AC <ref type="bibr" target="#b19">[20]</ref>, we used the same evaluation measure (pixel accuracy) to report the performance of our method, i.e. ACC = max m ? N n=1 1{l n =m(c n )} N where l n and c n are the ground-truth label and cluster assignment for input x n , respectively. m ranges over all possible one-to-one mapping between cluster assignments and ground-truth classes. The maximum operator is effectively computed using linear assignment <ref type="bibr" target="#b49">[50]</ref> to find the best one-to-one mapping between the predicted classes and ground-truth labels. In both the assignment and evaluation step, we discarded invalid pixels as in visualization <ref type="figure" target="#fig_1">(Figure 3)</ref>. <ref type="table" target="#tab_0">Table I</ref>. demonstrates the superior performance of our InMARS + compared to the state-of-the-art methods in the three datasets, validating the efficacy of our suggested modules for this task, i.e. unsupervised segmentation.</p><p>Qualitative Result: <ref type="figure" target="#fig_1">Figure 3</ref>. illustrates the output of In-MARS (output of the backbone) and InMARS + (output of the RWE) on the COCO-Stuff-3 dataset. The RWE module has spatial smoothness ability and performs well on the top of the backbone to produce a continuous segmentation mask while preserving semantic information. The superpixels with the finest resolution (N 2 gt ) are displayed in <ref type="figure" target="#fig_1">Fig. 3-(c)</ref>. In addition, we included the output of the fully supervised UNet <ref type="bibr" target="#b41">[42]</ref> and IIC <ref type="bibr" target="#b17">[18]</ref> as two baselines for visual comparison. Note that similar to <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref> in all the results, invalid pixels (pixels that do not belong to any of the classes) were discarded and are shown with dark colors. In <ref type="figure">Figure 4</ref> some failure cases presented. The first row is an ambiguous sample of the ground class (gray color in the ground-truth). Our method failed to distinguish this sample (artificial turf) from natural class. In fact, this level of perception requires a higher level of knowledge which is only available in the supervised objective. The second row shows a sample with a cluttered background. Our method failed to understand the natural class since it cannot understand this level of clutter.</p><p>Cluster Visualization: <ref type="figure">Figure 5</ref> illustrated the TSNE visualization for superpixels' embedding in the output of the network. It is worth to mention that TSNE is a 2D visualization that map high-dimensional data to lower dimension. This mapping cannot reflect the high dimensional separation capability of the method solely. However, we tried to highlight the network's performance by reducing the image with low average Intersection-over-Union (IoU) with ground-truth. <ref type="figure">Figure 5 (a)</ref> is the network's output at the initial stage, before training. As we expected, samples fall in a random pattern. <ref type="figure">Figure 5 (b)</ref> is the network's output after training.     separate classes in feature space properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>In this part, we assessed the role of each component in the proposed method. <ref type="table" target="#tab_0">Table II -Table V</ref> present the ablation study of different components conducted on 10% of randomly selected images of datasets.</p><p>Backbone: The backbone is a fully convolutional deep network, like FCN <ref type="bibr" target="#b4">[5]</ref> which gets the input image and generates an embedding map of equal size to the input image. In Table II, we compared the effect of choosing different backbone architectures on the accuracy of the proposed method in the Potsdam-3 and COCO-Stuff-3 dataset. We chose UNet <ref type="bibr" target="#b41">[42]</ref> and Hourglass <ref type="bibr" target="#b40">[41]</ref> backbone in our experiments, while other semantic segmentation backbones can be used instead. The results depicted that the hourglass backbone is performed best among others. RWE: We designed three different ways to generate fixedlength embedding for superpixels: (1) Sub-region pooling, (2) SPP, and (3) interpolation. In <ref type="table" target="#tab_0">Table III</ref>, we reported the result of experiments with different RWE modules. In the InMARS, we chose SPP+FC for our final model.</p><p>Adversarial Regularization: The regularization term plays a remarkable role in the performance of the proposed method. <ref type="table" target="#tab_0">Table IV</ref>. compares two different strategies for regularization: adversarial additive noise and random geometrical affine perturbation (similar to <ref type="bibr" target="#b11">[12]</ref>). The best accuracy achieved by a combination of two terms according to the Equation <ref type="bibr" target="#b0">1</ref> 2 R adv + 1 2 R geo . Superpixel: <ref type="table" target="#tab_4">Table V</ref>. shows various parameter selection for superpixel candidates. The extent and size of target classes define the approximate number of superpixels. The number of superpixels is selected regarding the number of ground-truth classes (N gt ). Since superpixel is not an ideal proposal, oversegmentation performs better. Finally, we selected combination of N gt , 2N gt and N 2 gt in InMARS + .</p><p>V. CONCLUSION We have proposed a fully unsupervised semantic segmentation network, which utilizes the mutual information as the supervision and adversarial training as a regularization. The proposed method leads to an unsupervised semantic segmentation model that can partition the scene into semantic classes while generating good boundaries in the output by exploiting the off-the-shelf superpixel method. We further demonstrate the state-of-the-art results on benchmarks comparable with supervised methods and wish to reduce the gap with these methods. In the future, improvement of the method to handle more semantic classes in complex datasets would be fruitful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Building blocks of our proposed method. The yellow and red colors indicate original data and augmented version of original data, respectively. The input image X and its adversarial augmented version X are passed from the network and generate response Y and Y , respectively. The network is combined of three modules, Backbone, Superpixel, and RWE. Backbone is a fully convolutional network. Superpixel module extract sub-region from the input image. RWE map output of Backbone to the sub-regions to generate fixed-length feature. The clustering loss I(X;Y ) is computed between input X and its corresponding embedding Y . The regularization loss R(? ) controls the network to generate rich embedding via Adversarial Augmentation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Segmentation Results from COCO-Stuff-3 [16]. (a) Few input images examples, (b) The IIC [18] outputs. The figures (c)-(e) represent the component of our model, i.e. (c) Superpixels in the finest resolution (i.e. N 2 gt ), (d) The argmaxed version of the our backbone outputs ( InMARS b ) (e) The outputs of the multi-scale variant of our model (InMARS + ). The outputs of the supervised baseline and the Ground-truths are shown in (f) and (g), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Failure Cases. Some failure cases of the proposed method from COCO-Stuff-3<ref type="bibr" target="#b15">[16]</ref>. The proposed algorithm fails to recognize some classes due to the similarity of classes (first row) and clutter in the background (second row) Sky Plants Ground TSNE Visualization. TSNE plot for the superpixel embedding on a subset of COCO-Stuff dataset using our proposed method (InMARS + ). From left to right, before training with random weights, after training, and trained model for images with IoU &gt; .25 and IoU &gt; .5, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5(c) and (d) are obtained by including image with IoU more than .25% and .50%, respectively. The TSNE of the trained model inFigure 5(c) and (d) confirm that InMARS + can</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Accuracy of Unsupervised Image Segmentation InMARS with multi-scale test by different number of superpixels.</figDesc><table><row><cell></cell><cell>COCO</cell><cell>COCO</cell><cell>Potsdam-3</cell><cell>Potsdam</cell></row><row><cell></cell><cell>Stuff-3</cell><cell>Stuff</cell><cell></cell><cell></cell></row><row><cell>Supervised Baseline</cell><cell>84.7</cell><cell>89.7</cell><cell>80.2</cell><cell>82.4</cell></row><row><cell>K-means [43]</cell><cell>52.2</cell><cell>14.1</cell><cell>45.7</cell><cell>35.3</cell></row><row><cell>SIFT [44]</cell><cell>38.1</cell><cell>20.2</cell><cell>38.2</cell><cell>28.5</cell></row><row><cell>Doersch 2015 [45]</cell><cell>47.5</cell><cell>23.1</cell><cell>49.6</cell><cell>37.2</cell></row><row><cell>Isola 2016 [46]</cell><cell>54.0</cell><cell>24.3</cell><cell>63.9</cell><cell>44.9</cell></row><row><cell>Deep Cluster 2018 [47]</cell><cell>41.6</cell><cell>19.9</cell><cell>41.7</cell><cell>29.2</cell></row><row><cell>IIC 2019 [18]</cell><cell>72.3</cell><cell>27.7</cell><cell>65.1</cell><cell>45.4</cell></row><row><cell>AC 2020 [20]</cell><cell>72.9</cell><cell>30.8</cell><cell>66.5</cell><cell>49.3</cell></row><row><cell>InMARS b (Ours)</cell><cell>72.2</cell><cell>29.2</cell><cell>68.5</cell><cell>46.2</cell></row><row><cell>InMARS (Ours)</cell><cell>72.8</cell><cell>30.3</cell><cell>68.8</cell><cell>46.9</cell></row><row><cell>InMARS + (Ours)</cell><cell>73.1</cell><cell>31.0</cell><cell>70.1</cell><cell>47.3</cell></row></table><note>b InMARS backbone with N gt output classes.+</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Backbone.We trained different backbone architectures with the same loss function and RWE parameters in the reduced version of COCO-Stuff-3 and Potsdam-3 datasets to compare their effects on the proposed method.</figDesc><table><row><cell></cell><cell>COCO-Stuff-3</cell><cell>Potsdam-3</cell></row><row><cell>UNet [42]</cell><cell>52.5</cell><cell>50.2</cell></row><row><cell>Hourglass [41]</cell><cell>54.5</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Region-Wise Embedding (RWE). We tested three configurations for the RWE head on the top of the backbone to build a fixed-length embedding from superpixels. Max and average pooling are two kinds of sub-region pooling. SPP is spatial pyramid pooling, and FC stands for fully connected layers. The heads with the FC layer perform better since they can model complex tasks.</figDesc><table><row><cell></cell><cell>COCO-Stuff-3</cell><cell>Potsdam-3</cell></row><row><cell>max-pooling</cell><cell>54.5</cell><cell>51.4</cell></row><row><cell>mean-pooling</cell><cell>50.1</cell><cell>51.0</cell></row><row><cell>SPP [38]+FC</cell><cell>57.3</cell><cell>55.5</cell></row><row><cell>Interpolation+FC</cell><cell>59.8</cell><cell>55.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Adversarial Regularization. we illustrated the accuracy of the trained network without any regularization (no ? adversarial), with adversarial additive noise (R adv ), random geometrical affine perturbation (R geo ) and, their combination. The backbone is hourglass and max-pooling module selected for RWE part.</figDesc><table><row><cell></cell><cell>COCO-Stuff-3</cell><cell>Potsdam-3</cell></row><row><cell>no ? adversarial</cell><cell>44.2</cell><cell>42.3</cell></row><row><cell>R adv</cell><cell>54.5</cell><cell>51.4</cell></row><row><cell>R geo</cell><cell>61.2</cell><cell>59.1</cell></row><row><cell>1 2 R adv + 1 2 R geo</cell><cell>61.6</cell><cell>60.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Superpixel. we inspected the effect of number of superpixel. (N gt ) is the number of ground-truth classes.</figDesc><table><row><cell>(number of superpixels)</cell><cell>COCO-Stuff-3</cell><cell>Potsdam-3</cell></row><row><cell>N gt</cell><cell>54.5</cell><cell>51.4</cell></row><row><cell>2  *  N gt</cell><cell>55.3</cell><cell>52.1</cell></row><row><cell>N 2 gt</cell><cell>55.2</cell><cell>52.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This claim can be supported by this fact that deep learning has favorably applied to the supervised semantic segmentation task, and many algorithms successfully tackled this problem<ref type="bibr" target="#b0">[1]</ref>,<ref type="bibr" target="#b1">[2]</ref>,<ref type="bibr" target="#b3">[4]</ref>,<ref type="bibr" target="#b4">[5]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">W-net: A deep model for fully unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potsdam</surname></persName>
		</author>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html" />
		<title level="m">ISPRS. ISPRS 2D Semantic Labeling Contest</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<idno>ArXiv:2001.05566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">K-means hashing: An affinity-preserving quantization method for learning binary compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Basford</surname></persName>
		</author>
		<title level="m">Mixture models: Inference and applications to clustering. M. Dekker</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards k-meansfriendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superpixel sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning superpixels with segmentation-aware affinity loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Superpixel segmentation with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<idno>ArXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pytorch: An open source machine learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Soumith Chintala</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
