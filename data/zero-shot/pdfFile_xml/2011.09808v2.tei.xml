<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unmixing Convolutional Features for Crisp Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Unmixing Convolutional Features for Crisp Edge Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Edge detection</term>
					<term>deep learning</term>
					<term>convolutional feature unmixing !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a context-aware tracing strategy (CATS) for crisp edge detection with deep edge detectors, based on an observation that the localization ambiguity of deep edge detectors is mainly caused by the mixing phenomenon of convolutional neural networks: feature mixing in edge classification and side mixing during fusing side predictions. The CATS consists of two modules: a novel tracing loss that performs feature unmixing by tracing boundaries for better side edge learning, and a context-aware fusion block that tackles the side mixing by aggregating the complementary merits of learned side edges. Experiments demonstrate that the proposed CATS can be integrated into modern deep edge detectors to improve localization accuracy. With the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves the F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6% respectively when evaluating without using the morphological non-maximal suppression scheme for edge detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A S a fundamental computer vision task of localizing boundaries of perceptually salient objects in natural images, edge detection has been studied with a long history <ref type="bibr" target="#b0">[1]</ref>. In the early stage, the low-level features were extensively studied for detecting edges <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These approaches can obtain crisp edge maps but leave a challenging problem of suppressing high-frequency texture regions. Later, learning techniques were introduced to classify image patches as edge and non-edge classes <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. End-to-end deep edge detectors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have been recently proposed to learn multi-level side edges with deep supervision, and weight the side predictions according to the learned side importance to obtain the final predictions.</p><p>Deep learning solutions dramatically improved the performance of edge detection as the classification on hierarchical deep features generated by large receptive fields can robustly suppress false alarms in texture regions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These methods, however, have a challenging issue of detecting crisp edge maps that are free of localization ambiguity. As shown in <ref type="figure">Fig. 1</ref>, both the network outputs of HED <ref type="bibr" target="#b7">[8]</ref> and RCF <ref type="bibr" target="#b8">[9]</ref> suffer from localization ambiguity in the regions that contain true positive edge pixels across convolutional stages. To obtain crisp edge maps, it is required to use a morphological non-maximal suppression (NMS) scheme for the network outputs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>There have been some works that focused on the crispness of deep edge detection <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> without using the morphological NMS scheme. Wang et al. <ref type="bibr" target="#b15">[16]</ref> proposed to ? L. <ref type="bibr">Huan</ref>  use a new refinement architecture to tackle the problem of crispness. In the same period, Deng et al. <ref type="bibr" target="#b16">[17]</ref> found that the commonly-used weighted cross entropy loss is a key factor of posing the issue of localization ambiguity and adopted a Dice loss <ref type="bibr" target="#b17">[18]</ref> for edge detection. Limited by the numerical stability issue, the Dice loss tends to yield weaker edge response than the weighted cross entropy, which is disadvantageous for distinguishing edge and nonedge points, and <ref type="bibr" target="#b16">[17]</ref> thus has to add the ResNeXt block <ref type="bibr" target="#b18">[19]</ref> to obtain better features than the commonly-used vanilla VGG16 <ref type="bibr" target="#b19">[20]</ref> network.</p><p>In this paper, we focus on the crispness of deep edge detection. That is to say, we expect the raw edge predictions by the convolution layers are with less localization ambiguity around the true positive predictions while maintaining strong capability of texture suppression. This is important for separating the adjacent edges that are glued together, as such glued edges in the raw predictions can be hard for the post-processing operation to handle. In contrast to the previous works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we are going to study if it is possible to learn crisp edge maps with the commonly-used backbone architecture VGG16 <ref type="bibr" target="#b19">[20]</ref> for edge detection.</p><p>We found that the issue of localization ambiguity for deep edge detectors is mainly caused by the mixing phenomenon in convolutional neural networks (CNNs).</p><p>-Feature Mixing. Limited by the existence of maxpooling layers (for larger receptive field) and upsampling operators (for pixel-wise prediction), the higherlevel convolutional features are spatially mixed. When training a network to obtain a (higher-level) side edge map at a convolutional stage using the commonlyused weighted cross entropy loss or its variants, it is challenging to correctly classify multiple pixels sourced with the severe class-imbalance issue from the same mixed feature vectors. As a result, the higher convolutional stage is, the blurrier side edge maps are obtained by existing deep edge detectors .  <ref type="bibr" target="#b8">[9]</ref>) and our proposed CATS. The top row displays an example image and its edge annotations from the BSDS500 dataset <ref type="bibr" target="#b20">[21]</ref>. The final edge predictions and the multi-level side edge maps estimated by different edge detection approaches are listed in rows. In the bottom line, we leverage our proposed CATS on RCF <ref type="bibr" target="#b8">[9]</ref> to obtain the most precisely-located side edge maps and final edge prediction.</p><p>required to fuse them into a final edge prediction. The phenomenon of side mixing is easily to be observed in existing detectors as they simply weight the side edge predictions according to the learned side importance. On the one hand, because the final edge prediction should be able to suppress the complicated textures in an image, the high-level side edges will have greater importance than low-level ones. On the other hand, such a fusion strategy equally treats all pixels in the same side output, thus making it hard to selectively preserve the complementary merits of different side edges. As shown in <ref type="figure">Fig. 1</ref>, the final edge predictions of both HED and RCF are dominated by the high-level side edge maps while ignoring the low-level ones that have less localization ambiguities.</p><p>Based on the above discussion, we are going to unmix the convolutional features to learn crisp edge maps. First, we address the feature mixing by tracing the true positive boundaries during training. We term the false positives caused by the feature mixing as the confusing pixels (of edges). To this end, we present a novel tracing loss with a specific focus on confusing pixels. We use a boundary tracing function to trace the actual position of an edge by enlarging the response difference between an edge pixel and the neighboring confusing pixels. Once the traced pixels are delineated, all the remained pixels belong to the background category and we only need to suppress the textures. Different from the existing loss functions for edge detection, we propose a texture suppression function to robustly handle texture regions by holistically suppressing the non-edge pixels that lie in the same image patch rather than treating these pixels as independent individuals. For further alleviation of localization ambiguity, a pixel-wise fusion mechanism is required to handle the side mixing caused by image-level weighted average. We present a simple context-aware fusion (CoFusion) block that dynamically learns filters <ref type="bibr" target="#b21">[22]</ref> with contextual information across side predictions under the general self-attention framework <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. A similar design of leveraging dynamic filters for edge perception task was previously presented in DFF <ref type="bibr" target="#b25">[26]</ref>, it, however, focuses on fusing side predictions with high-level information for semantic edge detection. By contrast, our CoFusion block aims at addressing side mixing with the clues provided by the side edges themselves. The CoFusion block handles the side mixing that is hard for the tracing loss to alleviate with image-level weighted average; while the tracing loss offers necessary guidance to exert the CoFusion block.</p><p>The tracing loss and the CoFusion block together form a context-aware tracing strategy (CATS) that is specifically designed for crisp edge detection. As displayed at the bottom of <ref type="figure">Fig. 1</ref>, with the proposed tracing function available, the side edge maps are reliably learned with less localization ambiguity, and the CoFusion fusion block makes the final edge predictions better in the aspect of crispness.</p><p>In experiments, we leverage our proposed CATS on the existing state-of-the-art deep edge detectors HED <ref type="bibr" target="#b7">[8]</ref>, RCF <ref type="bibr" target="#b8">[9]</ref> and BDCN <ref type="bibr" target="#b9">[10]</ref> to demonstrate that the CATS can fully exploit the multi-level side outputs in terms of supervision and fusion to obtain precisely-located edge predictions. In the standard evaluation of edge detectors, our proposed CATS consistently improves the accuracy of HED, RCF and BDCN on three common edge detection datasets. We achieve the state-of-the-art performance with a F-measure (ODS) of 0.812, 0.752, 0.897 on BSDS500 <ref type="bibr" target="#b20">[21]</ref>, NYUDv2 <ref type="bibr" target="#b26">[27]</ref> and MultiCue <ref type="bibr" target="#b27">[28]</ref> for RGB data, respectively. Furthermore, we quantitatively evaluate the aforementioned edge detectors with CATS on BSDS500 and NYUDv2 datasets without using the standard morphological nonmaximal suppression scheme. The results validate that the CATS improves the F-measure (ODS) performance by large margins on BSDS500 and NYUDv2 with at most 12% and 7.6% respectively with robust edge localization ability.</p><p>Our main contributions are summarized as follow: (1) We explicitly treat the false positive pixels as the confusing pixels, and propose a tracing loss to address the localization ambiguity for deep edge detectors. (2) We present a context-aware tracing strategy to learn precisely-located edge maps from natural images in an end-to-end manner. <ref type="formula" target="#formula_2">(3)</ref> The proposed CATS consistently improves the performance of HED, RCF and BDCN on the BSDS500, the NYUDv2 and the Multicue datasets, and dramatically improves the edge localization accuracy of these detectors with no postprocessing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>In this section, we elaborate the context-aware tracing strategy (CATS) that alleviates edge localization ambiguity by a tracing loss and a context-aware fusion (CoFusion) block for crisp edge learning. For feature unmixing with the weighted cross entropy, the tracing loss introduces a boundary tracing function to specifically separate the confusing pixels from edges, and a texture suppression function to perform holistic texture-region smoothing. Supervised by the tracing loss, the CoFusion block learns to selectively aggregate the different delineation merits of side edges in a pixel-wise fashion, which tackles the side mixing during the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The tracing loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Weighted cross entropy</head><formula xml:id="formula_0">Given an edge prediction? = {? i } N i=1 and the correspond- ing edge label Y = {y i } N i=1 , the weighted cross entropy is formulated as Lce(? , Y ) = ?? ? i?Y + log?i ? (1 ? ?) i?Y ? log (1 ??i), (1) where Y + = {i|y i ? Y, y i &gt; ?}, Y ? = {i|y i ? Y, y i = 0}</formula><p>denote the edge and non-edge sample sets respectively, and ? is the proportion of the negative pixels in the set Y + ? Y ? . The hyper-parameter ? serves for importance balancing between edge and non-edge samples, and ? is a threshold to remove semantically controversial pixels when there are several annotators <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>The weighted cross entropy can effectively supervise the network to learn reasonable edge maps, but it has highly imbalanced attention between edge and non-edge samples, making it hard to distinguish the confusing pixels that share features with edges and consistently smooth high-frequency regions. Consequently, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the detected edge maps usually contain misclassified confusing pixels that incur thick edges around true positive edge pixels, and get false positives that form dark shadows in texture regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Feature unmixing by tracing boundaries</head><p>Different from the non-edge pixels that lie beyond edges in a complete texture regions, the confusing pixels are intertwined with edges by the shared features. For a clear separation from edges, the confusing pixels therefore require an edge-aware suppression that is capable of feature unmixing in the high-level side edges, rather than being simply smoothed as texture points.</p><p>Based on the above discussion, we devised a boundary tracing function to weaken the interaction between edges and their confusing neighbors by enlarging the response difference for crisp edge delineation. The boundary tracing function is formulated as</p><formula xml:id="formula_1">L bdry (? , Y ) = ? p?E log ? ? i?Lp? i/( i?R e p \Lp? i + i?Lp? i) ? ? ,<label>(2)</label></formula><p>In Eq. <ref type="formula" target="#formula_1">(2)</ref>, E is the set of all the edge points in the edge label Y . R e p denotes a small image patch (e.g., a 7 ? 7 rectangle patch) that contains edge fragments, and the patch centers at an edge point p. The set of the edge points in R e p is represented as L p .</p><p>Minimizing L bdry (? , Y ) will force i?R e p \Lp? i to be 0 while increasing i?Lp? i , which enlarges the response difference between edge fragments and accompanying confusing pixels. The boundary tracing function L bdry therefore smooths confusing pixels with awareness to the predicted response strength of neighboring edge fragments, and traces the actual edge position with less of the ambiguity caused by confusing pixels. In practice, L bdry is computed via three convolution layers that are applied on the prediction and the label. The time complexity is about O((2 + 3k 2 ) ? M ? N ) with an M ? N image and a k ? k patch size.</p><p>With edges and confusing pixels handled by the boundary tracing function, the remained texture regions can be consistently suppressed by a texture suppression function defined as</p><formula xml:id="formula_2">L tex (? , Y ) = ? p?Y \? log(1 ? i?R t p? i /|R t p |),<label>(3)</label></formula><p>where R t p represents an image patch that centers at a nonedge point p (e.g., a 3 ? 3 rectangle area), and? is a set that includes all edges and their confusing pixels that have been used in the boundary tracing function.? serves as a buffer zone to weaken the negative interaction between edge fragments and texture areas that require a stronger suppression than confusing pixels.</p><p>As revealed by Eq. <ref type="formula" target="#formula_2">(3)</ref>, the texture suppression function groups the non-edge points in the same texture region patch, and holistically suppresses these pixels rather than treating them as independent individuals. Focusing on complete texture regions beyond edges, the texture suppression function actually works in a complementary way with the boundary tracing function. <ref type="figure">Fig. 3</ref>. The mechanism of the CoFusion Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hadamard Product</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3?3 Conv+ReLu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3?3 Conv+SoftMax</head><p>With the boundary tracing function and the texture suppression function, the tracing loss follows as</p><formula xml:id="formula_3">TracingLoss(? , Y ) = L ce + ? 1 L bdry + ? 2 L tex ,<label>(4)</label></formula><p>where? and Y respectively denote the edge prediction and the edge label, ? 1 and ? 2 are hyper-parameters to balance each element in the tracing loss. During model training, L ce performs a rough edge learning, L bdry tackles edge localization refinement by feature unmixing, and L tex imposes a strong holistic suppression on texture regions. With L bdry and L tex , the tracing loss processes the non-edge points that are gathered according to their surrounding context with target-specific suppression, and achieves crisp edge generation with less localization ambiguity than single weighted cross entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context-aware fusion block</head><p>To leverage the edge details in low-level edge maps and the global context with robust texture suppression in the highlevel counterparts, weighted average is a common operation adopted in previous works for obtaining a unified edge prediction from multi-level side edges <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Albeit effective, this average operation incurs the issue of side mixing as all the pixels in a side edge map share the same fixed weight and have equal importance during fusion.</p><p>For harnessing the multi-level side maps without side mixing, as shown in <ref type="figure">Fig. 3</ref>, we designed a context-aware fusion (CoFusion) block within a self-attention framework to absorb the merits and bypass the limitations of different side maps by a position-wise aggregation of side edges.</p><p>Let Z = [Z 1 , ..., Z L ] ? R H?W ?L denote L side edge heatmaps. The CoFusion block first learns a weight map W context ? R H?W ?L from Z by an attention block. To derive a score map A score = [a ijl ] ? R H?W ?L from Z, the attention block adopts three 3 ? 3 convolution layers to capture contextual information for inferring W context . A score is subsequently normalized through softmax activation to get a weight map W context = [w ijl ], where</p><formula xml:id="formula_4">w ijl = e a ijl / L k=1 e a ijk .<label>(5)</label></formula><p>A vector v ij ? R 1?1?L in W context determines how much each side map contributes to the pixel p ij of the final edge prediction P final . Because v ij is calculated with the features around p ij across side maps, W context changes according to the context information in the multi-level side maps and leverages their different delineation preferences for the final prediction. With the context-aware weights W context , P final can be calculated as follows.</p><formula xml:id="formula_5">CoFusion(Z) = L l=1 W context ? Z; (6) P final = sigmoid(CoFusion(Z)),<label>(7)</label></formula><p>where ? denotes Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND ANALYSIS</head><p>To validate the effectiveness and generality of the CATS, we implement it in Pytorch framework <ref type="bibr" target="#b28">[29]</ref> with three VGG16based <ref type="bibr" target="#b19">[20]</ref> edge detectors, including HED <ref type="bibr" target="#b7">[8]</ref>, RCF <ref type="bibr" target="#b8">[9]</ref> and BDCN <ref type="bibr" target="#b9">[10]</ref>. In this section, we evaluate the CATS on three challenging edge detection benchmarks, including NYUDv2 <ref type="bibr" target="#b26">[27]</ref>, BSDS500 <ref type="bibr" target="#b20">[21]</ref> and Multicue <ref type="bibr" target="#b27">[28]</ref> datasets. An ablation study is subsequently presented for a more comprehensive performance analysis of the CATS. <ref type="figure" target="#fig_0">BSDS500 [21]</ref>. This dataset is a challenging edge detection benchmark that is composed of 200 training, 100 validation and 200 test images. Each image in this dataset is annotated by several annotators. The training and validation data are jointly augmented for model training as in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>NYUDv2 <ref type="bibr" target="#b26">[27]</ref>. This is a challenging dataset for indoor scene parsing and is also a commonly used benchmark for edge detection evaluation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>. It contains 1449 densely annotated RGB-D images, and is divided into 381 training, 414 validation and 654 testing images. We use this dataset to compare our proposed CATS with the state-ofthe-art edge detectors, and also perform a comprehensive analysis on the NYUDv2 dataset in the ablation study.</p><p>Multicue dataset <ref type="bibr" target="#b27">[28]</ref>. This dataset strictly distinguishes the definitions of boundary and edge, and it thus consists of two sub datasets: Multicue Boundary and Multicue Edge. This dataset regards semantically meaningful pixels as object boundary points, and treats pixels with abrupt perceptional changes as low-level edge points. We testify the CATS on both sub-datasets. Same with <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we randomly split the Multicue dataset into 80 training and 20 testing samples, and conducted three independent trials on both sub-datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>Experiments in this paper are conducted on a single GeForce RTX 2080 Ti GPU. The model parameters are updated by stochastic gradient descent (SGD) optimizer with momentum 0.9 and weight decay 0.0002, and the batch size is set to 10. The initial learning rate is 1e-6 and is multiplied with 0.1 after every given period shown in Tab. 1. The hyperparameters in the tracing loss are also specified in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Label HED <ref type="bibr" target="#b7">[8]</ref> RCF <ref type="bibr" target="#b8">[9]</ref> BDCN <ref type="bibr" target="#b9">[10]</ref> CATS-RCF <ref type="figure">Fig. 4</ref>. Qualitative comparison between the prior arts of deep edge detection and our proposed CATS on the BSDS500 dataset <ref type="bibr" target="#b20">[21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation protocols</head><p>The core step of evaluating an edge detector is to pixelwisely match the ground truth of edges to the binarized edge prediction under a specified maximum allowed distance tolerance. Following the previous works, a parametric curve of the precision and recall will be drawn to evaluate the overall performance of an edge detector with different thresholds of binarization. The best performance of an edge detector will be reported by the F-measure scores with the optimal thresholds at both dataset scale (ODS) and image scale (OIS). The maximum allowed distance tolerance for correct matches between the edge predictions and the annotations is conventionally set to 0.0075 for the BSDS500 and Multicue datasets, and 0.011 for the NYUDv2 dataset <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Standard evaluation protocol. Before binarizing a given edge prediction, the general evaluation procedure in previous researches will first apply a standard post-processing scheme. The post-processing scheme includes a nonmaximum suppression (NMS) step and a mathematical morphology operation to obtain a thinned edge map, and the standard evaluation protocol then uses the thinned edge prediction for matching with the ground truth of edge maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crispness-emphasized evaluation protocol.</head><p>Although the post-processing scheme can partially remove the falselyalarmed edge pixels around the true positive ones, it would be interesting to explore whether the raw outputs of the deep edge detectors have a better localization ability of edges. Accordingly, we remove the standard postprocessing scheme to evaluate the performance of a trained edge detector in the aspect of crispness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with state-of-the-art methods</head><p>In this section, we report the statistic comparison of our method with existing state-of-the-art models on BSDS500, NYUDv2 and Multicue datasets. We tested and validated the proposed CATS on the VGG16-based HED <ref type="bibr" target="#b7">[8]</ref>, RCF <ref type="bibr" target="#b8">[9]</ref> and BDCN <ref type="bibr" target="#b9">[10]</ref>. Models in experiments were fine-tuned with a VGG16 <ref type="bibr" target="#b19">[20]</ref> model pre-trained on ImageNet <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">BSDS500 Dataset</head><p>The numerical results on BSDS500 dataset are listed in Tab. 2, showing that the application of CATS achieves consistent performance boosting with HED, RCF and BDCN, by at most 0.7% in both ODS and OIS under the standard evaluation protocol.  <ref type="bibr" target="#b30">[31]</ref> 0.746 0.770 --N 4 -Fields <ref type="bibr" target="#b14">[15]</ref> 0.753 0.769 --DeepContour <ref type="bibr" target="#b5">[6]</ref> 0.757 0.776 --HFL <ref type="bibr" target="#b31">[32]</ref> 0.767 0.788 --CEDN <ref type="bibr" target="#b10">[11]</ref> 0.788 0.804 --DeepBoundary <ref type="bibr" target="#b13">[14]</ref> 0.789 0.811 --COB <ref type="bibr" target="#b11">[12]</ref> 0.793 0.820 --CED <ref type="bibr" target="#b15">[16]</ref> 0.794 0.811 0.642 0.656 AMH-Net <ref type="bibr" target="#b32">[33]</ref> 0.798 0.829 --DCD <ref type="bibr" target="#b33">[34]</ref> 0.799 0.817 --LPCB <ref type="bibr" target="#b16">[17]</ref> 0.800 0.816 0.693 0.700 HED <ref type="bibr" target="#b7">[8]</ref> 0 In Tab. 2, the CATS-applied models gain over the original counterparts by a dramatic margin in terms of the ODS and OIS scores of crispness-emphasized evaluation. The significant improvement indicates that the CATS can facilitate accurate edge localization with a robust removal of false positives along edges (i.e., the confusing pixels). One may notice that the standard evaluation scores do not always synchronize with the crispness-emphasized evaluation counterparts, although they show simultaneous performance improvement when using the CATS on the same model. The post-processing operation will help mitigate the impact of the false positives around true positive edges, and the standard evaluation is therefore more influenced by the misclassified texture points than the confusing pixels. Despite of the different focuses of the two evaluation protocols, the results still indicate that the standard evaluation can benefit from less localization ambiguity.</p><p>Visualization results on BSDS500 dataset can be found in <ref type="figure">Fig. 4</ref>. In the given examples, the CATS successfully tackles the confusing pixels caused by the issue of feature mixing and side mixing during fusion to generate crisper edges than other networks, which in turn makes adjacent boundaries easier to distinguish. More detailed comparison is supplied in <ref type="figure" target="#fig_1">Fig. 5</ref>, which gives the precision-recall curves of the CATS-based models and the corresponding originals on BSDS500 regarding the two evaluation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">NYUDv2 Dataset</head><p>We independently conducted experiments on the RGB and HHA data provided in the NYUDv2 dataset, and averaged the RGB-and HHA-based predictions to generate merged edge predictions like previous work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The quantitative comparison with several existing methods is reported in Tab. 3.</p><p>Compared to the original HED, RCF and BDCN, their CATS versions all show steady accuracy growth regarding the standard evaluation, where the gain in ODS and OIS can respectively reach as much as 1.0% and 0.9% with respect to the RGB data. The crispness-emphasized evaluation also reveals the remarkable improvement in edge localization accuracy by the proposed CATS, which brings 7.6% and 7.5% progress at most in the ODS and OIS scores respectively for the RGB data. In <ref type="figure">Fig. 6</ref>, we present the qualitative results on the NYUDv2 dataset, where the proposed CATS generates crisper edges than other models, illustrating the capability of CATS for robust texture suppression in complex circumstances. For a comprehensive comparison, <ref type="figure" target="#fig_1">Fig. 5</ref> plots the precision-recall curves of the CATS-based models and the original networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Multicue Dataset</head><p>The CATS-applied models are further compared with HED, RCF and BDCN on the edge and boundary data of Multicue via the standard evaluation procedure. In evaluation, the performance of a model is measured by the average score of three independent trials, and the performance fluctuation is appraised by the mean standard deviation. The quantitative results with only single-scale input are presented in Tab. 4. In Tab. 4, the CATS-applied approaches obtain an apparent detection accuracy increase against the corresponding original models on both the Multicue Boundary and Multicue Edge datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>The ablation study was conducted on NYUDv2 dataset to avoid the impact of the label threshold, and the NYUDv2 dataset was only augmented by flipping. RCF <ref type="bibr" target="#b8">[9]</ref> with weighted cross entropy was chosen as the baseline for comparison. We verified the functionality of each component in the tracing loss, and investigated the side unmixing effect of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Label HED <ref type="bibr" target="#b7">[8]</ref> RCF <ref type="bibr" target="#b8">[9]</ref> BDCN <ref type="bibr" target="#b9">[10]</ref> CATS-RCF <ref type="figure">Fig. 6</ref>. Qualitative comparison between the prior arts of deep edge detection and our proposed CATS on the NYUDv2 dataset <ref type="bibr" target="#b26">[27]</ref>. Human <ref type="bibr" target="#b27">[28]</ref> 0.760(0.017) -Multicue <ref type="bibr" target="#b27">[28]</ref> 0.720(0.014) -HED <ref type="bibr" target="#b7">[8]</ref> 0.814(0.011) 0.822(0.008) CATS-HED 0.827(0.007) 0.830(0.008) RCF <ref type="bibr" target="#b8">[9]</ref> 0.817(0.004) 0.825(0.005) CATS-RCF 0.841(0.001) 0.846(0.002) BDCN <ref type="bibr" target="#b9">[10]</ref> 0.836(0.001) 0.846(0.003) CATS-BDCN 0.842(0.001) 0.847(0.001)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge</head><p>Human <ref type="bibr" target="#b27">[28]</ref> 0.750(0.024) -Multicue <ref type="bibr" target="#b27">[28]</ref> 0.830(0.002) -HED <ref type="bibr" target="#b7">[8]</ref> 0.851(0.014) 0.864(0.011) CATS-HED 0.885(0.004) 0.893(0.003) RCF <ref type="bibr" target="#b8">[9]</ref> 0.857(0.004) 0.862(0.004) CATS-RCF 0.892(0.001) 0.895(0.001) BDCN <ref type="bibr" target="#b9">[10]</ref> 0.891(0.001) 0.898(0.002) CATS-BDCN 0.897(0.001) 0.904(0.001) the CoFusion block by substituting it for the weighted average layer in RCF. All models were conventionally trained from scratch, where the weights in convolution layers were initialized by zero-mean Gaussian distributions with a standard deviation 0.01 and the biases were set to 0. Tab. 5 presents the details of quantitative analysis. In terms of the crispness-emphasized evaluation displayed in Tab. 5, the proposed CATS gains over the RCF by 7% in ODS and 7.1% in OIS. The results in the first three rows show that, although L bdry or L tex alone can promote the performance of RCF with post-processing, they still have similar crispness scores with the weighted cross entropy. This is because that L bdry and L tex focus on different types of non-edge points. Only applying one of them is insufficient for a comprehensive refinement of side edges. In contrast, with the effective feature unmixing and texture suppression offered by combining L bdry and L tex , the tracing loss brings a remarkable crispness enhancement to RCF. Similarly, the CoFusion block guided by the weighted cross entropy also increases the scores of the standard evaluation measures, but with little crispness improvement. When working with L bdry or L tex , however, the CoFusion block shows its power in addressing side mixing for crisp edge detection. With the complete CATS where the CoFusion is guided by the tracing loss, CATS-RCF achieves the most crispness improvement, which demonstrates that the tracing loss and the CoFusion block work complementarily for crisp edge detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Deeper into the CoFusion block</head><p>For a perceptual understanding of the side unmixing effect in the CoFusion block, we visualized the weight maps generated by the CoFusion block in <ref type="figure">Fig. 7</ref>.</p><p>It can be found that the CoFusion block gives more attention to the two highest-level outputs on the finely smoothed non-edge regions and higher weights to the lower-level side predictions at boundaries. By concentrating on the different merits of side maps for edge delineation, the CoFusion block solves the issue of side mixing in the fusion procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Side Edge 1</head><p>Side Edge 2 Side Edge 3 Side Edge 4 Side Edge 5 <ref type="figure">Fig. 7</ref>. The weight maps generated by the CoFusion block in CATS-RCF for multi-level side edges on the NYUDv2 dataset <ref type="bibr" target="#b26">[27]</ref>. The results show the different attention preference of multi-level side edges for non-edge region smoothing and edge details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we introduce a simple yet effective contextaware tracing strategy (CATS) to tackle the problem of localization ambiguity for modern deep edge detectors. The CATS addresses this issue by unmixing the edge features with a tracing loss and the side predictions with a CoFusion block during the fusion procedure. The effectiveness of CATS was validated in three VGG16-based edge detectors, i.e., HED, RCF, and BDCN. Extensive experiments demonstrate that the CATS can bring consistent performance improvement to the three detectors. Especially, the qualitative and the crispness-emphasized evaluation results show that the CATS facilitates modern edge detectors to obtain crisp edges from raw edge predictions, with the localization accuracy dramatically improved. In the future, it would be interesting to explore the effectiveness of CATS for other down-stream vision tasks. Source code and more results are available at https://github.com/WHUHLX/CATS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of confusing pixels. The left image displays edges predicted by RCF<ref type="bibr" target="#b8">[9]</ref> and the right image is the enlarged rectangle region (red box) in the left image. Pixels in different colors in the right image indicate the ground truth edge pixels (purple), the confusing pixels of edges (blue), and the dark shadows in texture regions (light green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>The precision-recall curves of the CATS-based and the compared original models on BSDS500 and NYUDv2 datasets. (a) and (c) depict the results under the standard evaluation, while (b) and (d) are the results of the crispness-emphasized evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and X. Zheng are with the LIESMARS, Wuhan University, Wuhan 430079, China. E-mail: {whu hlx, zhengxw}@whu.edu.cn.</figDesc><table /><note>? N. Xue and G.-S. Xia are with the School of Computer Science, Wuhan University, Wuhan 430079, China.? W. He is with the RIKEN Center, Tokyo 1030027, Japan.? J. Gong is with the LIESMARS and the School of Remote Sensing and Information Engineering, Wuhan University, Wuhan 430079, China.? Linxi Huan and Nan Xue contributed equally to this work.? Corresponding author: X. Zheng.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Parameter setting of the tracing loss in experiments.</figDesc><table><row><cell>Data</cell><cell>Param.</cell><cell>period/epoch</cell><cell>?</cell><cell>?</cell><cell>L1-L3</cell><cell>?1/?2 L4-L5</cell><cell>Final</cell></row><row><cell cols="2">NYUDv2</cell><cell>20/60</cell><cell>-</cell><cell>1.2</cell><cell>4/0.05</cell><cell>2/0.1</cell><cell>6/0.05</cell></row><row><cell cols="2">BSDS500</cell><cell>10/40</cell><cell>0.3</cell><cell>1.1</cell><cell>2/0.05</cell><cell>1/0.1</cell><cell>4/0.05</cell></row><row><cell cols="2">Multicue bdr</cell><cell>20/60</cell><cell>0.3</cell><cell>1.2</cell><cell>2/0.05</cell><cell>1/0.1</cell><cell>4/0.03</cell></row><row><cell cols="2">Multicue edge</cell><cell>20/60</cell><cell>0.2</cell><cell>1.1</cell><cell>4/0.01</cell><cell>2/0.01</cell><cell>6/0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Quantitative analysis on the BSDS500 dataset. SEval denotes the standard evaluation, and CEval is the crispness-emphasized evaluation. For fair comparison, we only list the single-scale results generated by models trained with only BSDS500 data.</figDesc><table><row><cell>Methods</cell><cell cols="2">SEval ODS OIS</cell><cell cols="2">CEval ODS OIS</cell></row><row><cell>Human</cell><cell>0.803</cell><cell>0.803</cell><cell>-</cell><cell>-</cell></row><row><cell>OEF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 Quantitative</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">analysis on the NYUDv2 dataset. SEval and CEval</cell></row><row><cell cols="5">respectively refer to the standard evaluation and the</cell><cell></cell></row><row><cell></cell><cell cols="3">crispness-emphasized evaluation.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Data</cell><cell cols="2">SEval ODS OIS</cell><cell cols="2">CEval ODS OIS</cell></row><row><cell></cell><cell>RGB</cell><cell>0.739</cell><cell>0.754</cell><cell>-</cell><cell>-</cell></row><row><cell>LPCB [17]</cell><cell>HHA</cell><cell>0.707</cell><cell>0.719</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.762</cell><cell>0.778</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RGB</cell><cell>0.722</cell><cell>0.737</cell><cell>0.387</cell><cell>0.404</cell></row><row><cell>HED [8]</cell><cell>HHA</cell><cell>0.691</cell><cell>0.704</cell><cell>0.335</cell><cell>0.350</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.746</cell><cell>0.764</cell><cell>0.368</cell><cell>0.384</cell></row><row><cell></cell><cell>RGB</cell><cell>0.732</cell><cell>0.746</cell><cell>0.405</cell><cell>0.418</cell></row><row><cell>CATS-HED</cell><cell>HHA</cell><cell>0.693</cell><cell>0.703</cell><cell>0.349</cell><cell>0.362</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.755</cell><cell>0.770</cell><cell>0.382</cell><cell>0.397</cell></row><row><cell></cell><cell>RGB</cell><cell>0.745</cell><cell>0.759</cell><cell>0.398</cell><cell>0.413</cell></row><row><cell>RCF [9]</cell><cell>HHA</cell><cell>0.701</cell><cell>0.712</cell><cell>0.333</cell><cell>0.348</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.764</cell><cell>0.778</cell><cell>0.374</cell><cell>0.385</cell></row><row><cell></cell><cell>RGB</cell><cell>0.752</cell><cell>0.765</cell><cell>0.474</cell><cell>0.488</cell></row><row><cell>CATS-RCF</cell><cell>HHA</cell><cell>0.710</cell><cell>0.721</cell><cell>0.433</cell><cell>0.445</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.768</cell><cell>0.782</cell><cell>0.439</cell><cell>0.452</cell></row><row><cell></cell><cell>RGB</cell><cell>0.748</cell><cell>0.762</cell><cell>0.426</cell><cell>0.450</cell></row><row><cell>BDCN [10]</cell><cell>HHA</cell><cell>0.704</cell><cell>0.716</cell><cell>0.347</cell><cell>0.367</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.766</cell><cell>0.779</cell><cell>0.375</cell><cell>0.392</cell></row><row><cell></cell><cell>RGB</cell><cell>0.752</cell><cell>0.765</cell><cell>0.442</cell><cell>0.462</cell></row><row><cell>CATS-BDCN</cell><cell>HHA</cell><cell>0.712</cell><cell>0.724</cell><cell>0.422</cell><cell>0.439</cell></row><row><cell></cell><cell>RGB-HHA</cell><cell>0.770</cell><cell>0.783</cell><cell>0.418</cell><cell>0.435</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Evaluation results on Multicue dataset.The mean standard deviations are given in the parentheses.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>ODS</cell><cell>OIS</cell></row><row><cell>Boundary</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>The numerical results for ablation study. L bdry and Ltex denote the boundary tracing function and texture suppression function. SEval and CEval respectively denote the standard evaluation and the crispness-emphasized evaluation.In Tab. 5, the complete CATS improves the performance of RCF by 1.1% in ODS and 0.8% in OIS, while using the tracing loss or the CoFusion block alone gains over the original RCF by at most 0.9% in ODS and 0.8% in OIS.</figDesc><table><row><cell></cell><cell>CATS</cell><cell></cell><cell cols="2">SEval</cell><cell cols="2">CEval</cell></row><row><cell>L bdry</cell><cell>Ltex</cell><cell>CoFusion</cell><cell>ODS</cell><cell>OIS</cell><cell>ODS</cell><cell>OIS</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.720 0.736</cell><cell cols="2">0.381 0.397</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.722 0.738</cell><cell cols="2">0.382 0.399</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.726 0.743</cell><cell cols="2">0.383 0.396</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.729 0.744</cell><cell cols="2">0.411 0.424</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.725 0.739</cell><cell cols="2">0.381 0.394</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.727 0.740</cell><cell cols="2">0.427 0.441</cell></row><row><cell>RCF</cell><cell></cell><cell></cell><cell cols="2">0.728 0.743</cell><cell cols="2">0.409 0.427</cell></row><row><cell>CATS-RCF</cell><cell></cell><cell></cell><cell cols="2">0.731 0.744</cell><cell cols="2">0.451 0.468</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine perception of three-dimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the accuracy of the sobel edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="584" to="592" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bi-directional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to highlevel tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning relaxed deep supervision for better edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">N 4 -fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries: From boundaries to higher-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to predict crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="570" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic feature fusion for semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="782" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A systematic comparison between visual cues for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>M?ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-for-low and low-forhigh: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep structured multi-scale features using attention-gated crfs for contour prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3961" to="3970" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep-learning-based object-level contour detection with ccg and crf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="859" to="864" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
