<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Styleformer: Transformer based Generative Adversarial Networks with Style Vector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeeseung</forename><surname>Park</surname></persName>
							<email>jspark@may-i.io</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younggeun</forename><surname>Kim</surname></persName>
							<email>younggeun@mindslab.ai</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">mAy-I Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">MINDsLab Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Styleformer: Transformer based Generative Adversarial Networks with Style Vector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Styleformer, a generator that synthesizes image using style vectors based on the Transformer structure. In this paper, we effectively apply the modified Transformer structure (e.g., Increased multi-head attention and Prelayer normalization) and introduce novel Attention Style Injection module which is style modulation and demodulation method for self-attention operation. The new generator components have strengths in CNN's shortcomings, handling long-range dependency and understanding global structure of objects. We present two methods to generate high-resolution images using Styleformer. First, we apply Linformer in the field of visual synthesis (Styleformer-L), enabling Styleformer to generate higher resolution images and result in improvements in terms of computation cost and performance. This is the first case using Linformer to image generation. Second, we combine Styleformer and Style-GAN2 (Styleformer-C) to generate high-resolution compositional scene efficiently, which Styleformer captures longrange dependencies between components. With these adaptations, Styleformer achieves comparable performances to state-of-the-art in both single and multi-object datasets. Furthermore, groundbreaking results from style mixing and attention map visualization demonstrate the advantages and efficiency of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b20">[21]</ref> is one of the widely used generative model. Since the appear of DC-GAN <ref type="bibr" target="#b42">[43]</ref>, convolution operations have been considered essential for high-resolution image generation and stable training. Convolution operations are created under the assumption of the locality and stationarity of the image (i.e., inductive bias), which is advantageous for image processing <ref type="bibr" target="#b36">[37]</ref>. Through convolution neural networks (CNNs) with this strong inductive bias, GAN have efficiently gen-* Equal contribution. The order was determined randomly. erated realistic, high-fidelity images.</p><p>However, drawbacks of CNNs clearly exist. Local receptive field of CNNs makes model difficult to capture longrange dependency and understanding global structure of object. Stacking multiple layers can solve this problem, but this leads to another problem of losing spatial information and fine details <ref type="bibr" target="#b54">[55]</ref>. Moreover, sharing kernel weights across locations leads to unstable training when the pattern or styles differ by location in the image <ref type="bibr" target="#b55">[56]</ref>. This is also related to the poor quality of generated structured images or compositional scenes (e.g., outdoor scenes), unlike the generation of a single object (e.g., faces)</p><p>In this paper, we propose Styleformer, a generator that uses style vectors based on the Transformer structure. Unlike CNNs, Styleformer utilizes self-attention operation to capture long-range dependency and understand global structure of objects efficiently. Furthermore, we overcome computation problem of Transformer and show superior performance not only in low-resolution but also in high resolution images. Specifically, we introduce the following three models:</p><p>1) Styleformer -The basic block of Styleformer is based on Transformer encoder, so we introduce components that need to be changed for stable learning. Inspired by Mo-bileStyleGAN <ref type="bibr" target="#b2">[3]</ref>, we enhance the multi-head attention in original Transformer by increasing the number of heads, allowing model to generate image efficiently. We also modify layer normalization, residual connection, and feed-forward network (Section 3.2). Moreover, we introduce novel attention style injection module, suitable style modulation, and demodulation method for self-attention operation (Section 3.3). This design allows Styleformer to generate image stably, and enables model to handle long-range dependency and understand global structures.</p><p>2) Styleformer-L -We sidestep scalability limitation arising from the quadratic mode of attention operation by applying Linformer <ref type="bibr" target="#b49">[50]</ref> (Styleformer-L). As such, Styleformer-L can generate high-resolution images with linear computational costs. This paper is the first case to apply Linformer in the field of visual synthesis (Section 3.4).</p><p>3) Styleformer-C -We further combine Styleformer and StyleGAN2, applying Styleformer at low resolution and style block of StyleGAN2 at high resolution (Styleformer-C). As can be seen from our experiments and analysis (e.g., style mixing and visualizing attention map), we show that Styleformer-C with the structure above can generate compositional scenes efficiently, and showing flexibility of our model. In detail, we prove that Styleformer in low resolution help model to capture long-range dependency between components, and style block in high resolution help model to refine the details of each components such as color or texture. This novel blending structure enables fast training, which is the advantage of StyleGAN2, while maintaining the advantages of Styleformer that can generate structured images.(Section 4).</p><p>Styleformer achieves comparable performances to stateof-the-art in both single and multi-object datasets. We record FID 2.82 and IS 10.00 at the unconditional setting on CIFAR-10. These results outperform all GAN-based models including StyleGAN2-ADA <ref type="bibr" target="#b31">[32]</ref> which recently recorded state-of-the-art. As can be expected, Styleformer show strength especially in multi-object images or compositional scenes generation (e.g., CLEVR, Cityscapes). Styleformer-C records FID 11.67, IS 2.27 in CLEVR, and FID 5.99, IS 2.56 in Cityscapes, showing better performance than pure StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>After origion of GAN <ref type="bibr" target="#b20">[21]</ref>, various methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to enhance its training stability and performance. As a result, fidelity and diversity of the generated images have dramatically improved. In addition to image synthesis task, GAN has been widely adopted in various tasks, such as image-to-image translation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58]</ref>, super resolution <ref type="bibr" target="#b37">[38]</ref>, image editing <ref type="bibr" target="#b54">[55]</ref>, and style transfer <ref type="bibr" target="#b6">[7]</ref>. In particular, StyleGAN-based architectures have been applied for various applications <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. However, since all of these models are based on convolution backbones, they have met with only limited success on generating complex or compositinal scenes <ref type="bibr" target="#b28">[29]</ref>.</p><p>Transformer <ref type="bibr" target="#b48">[49]</ref> was first introduced to the natural language processing(NLP) domain, achieving a significant advance in NLP. Recently, there were efforts to utilize Transformer in the computer vision field <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">57]</ref>. Using huge amounts of data and a transformer module, ViT <ref type="bibr" target="#b11">[12]</ref> obtains comparable result with state-of-the-art model in the existing CNN based image classification model <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref>. Inspired by <ref type="bibr" target="#b11">[12]</ref>, various models such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref> emerges based on this structure. There have also been attempts to utilize transformer for tasks such as video understanding <ref type="bibr" target="#b3">[4]</ref> and segmentation <ref type="bibr" target="#b56">[57]</ref> as well as image classification. Even in GAN, there have been attempts to utilize Transformer: GANformer <ref type="bibr" target="#b25">[26]</ref> proposes a bipartite Transformer struc- ture and applies it to StyleGAN <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. With this structure, GANformer successfully advance the generative modeling of structured images and scenes, which have been challenging in existing GAN. However, they use a bipartite attention, differ from the self-attention operation. Trans-GAN <ref type="bibr" target="#b27">[28]</ref> demonstrates a convolution-free generator based on the structure of vanilla GAN, which doesn't show good performance compared to state-of-the-art model.</p><p>Unlike these studies, Styleformer generate images with self-attention operation using style vector and showing comparable performance state-of-the-art models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Previous methods (TransGAN) mainly use pre-defined sparse attention patterns for efficient attention mechanism, but we explore the low-rank property in self-attention. Our model can generate high resolution images (512 ? 512) with reduced computation complexity, while GANformer and TransGAN show a maximum of 256 ? 256 image synthesis. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the overall architecture of Styleformer, and in <ref type="figure" target="#fig_1">Figure 2b</ref> we show Styleformer encoder network, the basic block of Styleformer. Like existing synthesis network of StyleGAN, our generator is conditioned on a learnable constant input. The difference is that the constant input (8 ? 8) is flattened (64) to enter the Transformer-based encoder. Then the input which is combined with learnable positional encoding passes through the Styleformer encoder. Styleformer encoder is based on Transformer encoder, but there are several changes to generate an image efficiently, which will be discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Styleformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Styleformer Architecture</head><p>After passing several encoder blocks in each resolution, we proceed bilinear upsample operation by reshaping encoder output to the form of a square feature map. After upsampling, flatten process is carried out again to match the input form of the Styleformer encoder. This process is repeated until the feature map resolution reaches the target  image resolution. For each resolution, the number of the Styleformer encoder and hidden dimension size can be chosen as hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Styleformer Components from Transformer</head><p>Increased Multi-Head Attention Modern vision architectures allow communications between different channels and different spatial locations (i.e., pixels) <ref type="bibr" target="#b47">[48]</ref>. Conventional CNNs perform the above two communications at once, but these communications can be clearly separated like depthwise separable convolutions <ref type="bibr" target="#b23">[24]</ref>. We also separate the pixel-communication (self-attention), channelcommunication operations (multi-head integration) in the Transformer encoder. However, in depthwise separable convolutions, distinct convolution kernels are applied to each channel, unlike the self-attention operation share only one huge kernel A (i.e., attention map). With same kernel applied to each channel, diversity in generated image can be decreased. We overcome this problem by increasing the number of heads of multi-head attention (Increased multi-head attention). Then, the created attention map will be different for each head, and so the kernel applying operation. Then the attention maps will be created for each head, making the channels in each head meet different kernels. However, increasing the number of heads too much may cause attention map to not be properly created, resulting in poor performance. We demonstrate experimentally that increasing the number of heads improves performance only when the depth is at least 32, as shown in <ref type="figure">Figure 3</ref>. Therefore, we fix the depth to 32 for all future experiment. More details about increased multi-head attention can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Layer Normalization</head><p>We change the position of layer normalization in Transformer encoder. The layer normalization of the existing Transformer comes after a linear layer that integrates multi-heads (Post-Layer normalization). We hypothesis that the role of layer normalization in a Transformer is the preparation of generating an attention map. If we perform layer normalization at the end of Styleformer encoder (Layernorm B in <ref type="figure">Figure 4</ref>), style modulation is applied before making query and key, which can disturb learning attention map. This is supported by ablation study and attention map analysis in <ref type="table" target="#tab_1">Table 1</ref> and Appendix B, respectively. Therefore, to solve this problem, we proceed layer normalization before operation making query, key and value (Pre-Layernorm in <ref type="figure" target="#fig_1">Figure 2b</ref>) Modified Residual Connection Unlike the Transformer encoder, input feature map is scaled by style vector (Mod input in <ref type="figure" target="#fig_1">Figure 2b</ref>) in Styleformer encoder. We hence find the residual connection suitable for scaled input. After ablation study, we apply residual connection like Modified Residual in <ref type="figure" target="#fig_1">Figure 2b</ref>. Demodulation operation is additionally performed in residual connection, which will be described in Section 3.3. <ref type="table" target="#tab_1">Table 1</ref> presents ablation details of residual connection.</p><p>Eliminating Feed-Forward Network As can be seen Table 1, we remove the feed-forward structure because eliminating feed forward structure makes the model perform better and more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention Style Injection</head><p>Unlike vanilla GAN, StyleGAN generates an image with layer-wise style vectors as inputs, enabling controllable generation via style vectors, i.e., scale-specific control. Specifically, style vector scales the input feature map for each layer, i.e., style modulation, amplifying certain feature maps. For scale-specific control, this amplified effect must be removed before entering the next layer. StyleGAN allows scale-specific control through a normalization operation called AdaIN operation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, which normalizes each feature map separately, then scale and bias each feature map with style vector. StyleGAN2 is an advanced form of StyleGAN and addresses the artifact problem caused by the AdaIN operation, solving it by demodulation operation. While the AdaIN operation normalize the output feature map directly, demodulation operation is based on statistical assumptions about the input feature map. For details, similar to the goal of normalization operation, demodulation operation aims to have an output feature map with a unit standard deviation while assuming that the input feature maps have a unit standard deviation, i.e., statistical assumption. Our goal is to design a Transformerbased generator that generates images through style vector while enabling scale specific control. Therefore, we propose style modulation, demodulation method for the selfattention operation, i.e., Attention style injection.</p><p>Modulation for Self-Attention Just as the input feature map is scaled by style vector in the style block of Style-GAN2, the input feature map in the Styleformer encoder is also scaled by style vector (Mod Input in <ref type="figure" target="#fig_1">Figure 2b</ref>). But unlike convolution operation in StyleGAN2, there are two steps in self-attention operation: dot product of query and key to create an attention map (i.e. kernel), weighted sum of the value with calculated attention map. We hypothesis that the style vector applied to the operation in each step should be different. Therefore, we perform style modulation twice as in <ref type="figure" target="#fig_1">Figure 2b</ref> (Mod Input, Mod Value). This hypothesis is supported in <ref type="table" target="#tab_1">Table 1</ref>. In <ref type="figure" target="#fig_1">Figure 2b</ref>, Style Input is a style vector for input, and Style Value is a style vector only for value. Two style vectors are created through common mapping networks as in StyleGAN but different learned affine transformations.</p><p>Demodulation for Query, Key, Value As shown in <ref type="figure">Figure</ref> 2b, Styleformer encoder creates query (Q), key (K), and value (V ) through linear operation to the input feature map scaled with Style Input vector. After that, V will be modulated with Style Value vector additionally, so the demodulation operation for removing scaled effect of Style Input is clearly required. Also, we observe that when an attention map is created with Q, K from input scaled by Style In- put, specific value in the attention map becomes very large, demonstrated in Appendix B. This prevents the attention operation from working properly. We sidestep this problem with demodulation operation to Q, K, before creating attention map. Eventually, demodulation operation is all required for Q, K, and V . Let's first look at the style modulation to the input, i.e., Mod Input. Each flattened input feature map is scaled through a style vector, which is equivalent to scaling the linear weight:</p><formula xml:id="formula_0">Method Style1 Style2 Style1=Style2 Residual A Residual B Residual C Layernorm A Layernorm B Layernorm C Feed-Forward FID Baseline O O X X X O X X O X 8.56 Attention Style Injection O X - X X O X X O X 11.01 X O - X X O X X O X 11.40 O O O X X O X X O X 10.27 Residual Connection O O X X X X X X O X 19.09 O O X O X X X X O X 14.70 O O X X O X X X O X 9.94 Layer Normalization O O X X X O O X X X 9.00 O O X X X O X O X X 10.96 Feed-Forward O O X X X O X X O O 14.75</formula><formula xml:id="formula_1">w ? ij = s i ? w ij ,<label>(1)</label></formula><p>where w is original linear weight to make (Q, K, V ) from flattened input feature map, and w ? is modulated linear weight. s i is ith component of style vector, which scales ith flattened input feature map, and j means the dimension of (Q, K, V ). Assuming that flattened input feature maps have unit standard deviation (i.e., statistical assumption of demodulation), after passing style modulation and linear operation, a standard deviation of output is as follows:</p><formula xml:id="formula_2">? j = i w ? ij 2 .<label>(2)</label></formula><p>We scale output activations for each dimension of Q, K, and V by 1/? j (i.e., demodulation), making Q, K, and V back to unit standard deviation.</p><p>Demodulation for Encoder Output After demodulation operation to Q, K, and V , Styleformer encoder performs style modulation to V (Mod Value), weighted sum of V with attention map (Increased Multi-head Self-attention), and then performs linear operation (Multi-Head Integration), as shown in <ref type="figure" target="#fig_1">Figure 2b</ref>. Encoder output will be input for next encoder, so demodulation operation is necessary. We show in Appendix D that, assuming V has a unit standard deviation (This can be assumed because of demodulation for V ), the standard deviation of Styleformer encoder output can be derived as follows:</p><formula xml:id="formula_3">? ? lk = ? A l? 2 ? j w ? jk 2 ,<label>(3)</label></formula><p>where w ? jk = s j ? w jk , i.e., modulated linear weight. s j scales jth feature map of V , and k enumerates the flattened output feature map. Attention map A is computed same as existing Transformer: dot products of Q and K, divide each by square root of depth, and softmax function. A l? denotes attention score vector for lth pixel.</p><p>However, there are two problems with demodulation by simply scaling each flattened output feature map k with 1/? ? lk (Equation 3). First, scaling output feature map k with 1/? ? lk will normalize each pixel as a unit, different from AdaIN operation which normalizes each feature map as a unit. Second, the attention map, which is a matrix derived from Q and K, is dependent on the input. With input dependent variables, demodulation operations based on statistical assumptions can not be applied as in <ref type="bibr" target="#b18">[19]</ref>. Therefore we scale the flattened output feature map k with 1/? ?? k where ? ?? k = j w ? jk 2 , normalizing each feature map as a unit, and excluding input dependent variables A l . Then the standard deviation of output activations will be</p><formula xml:id="formula_4">? lk = ? ? lk ? ?? k = ? A l? 2 .<label>(4)</label></formula><p>However in this way, standard deviation of output is not unit, rather approaching to zero when the numbers of pixels increase, as detailed in Appendix D. To prevent this effect, we have applied modified residual connection like Modified Residual in <ref type="figure" target="#fig_1">Figure 2b</ref>. More specifically, we perform linear operation to Mod Value, then perform demodulation operation (same as demodulation for query, key, value). With these modulation and demodulation operations in residual connection, variables with unit standard deviation are added to the output. Therefore it helps to keep the final output ac-tivation having unit standard deviation, when ? jk is close to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">High Resolution Synthesis with Styleformer</head><p>The main problem in applying Transformer to image generation is the efficiency problem with image resolution. In this section, we introduce two different techniques in Styleformer that can generate high resolution images. We show a method of applying Linformer, making computation complexity to linear. Then, we introduce a method of combining Styleformer and StyleGAN2, which can obtain the advantages of both models.</p><p>Applying Linformer to Styleformer For high-resolution images, input sequence length of the Styleformer encoder increases quadratically, and the standard self-attention mechanism requires a complexity of O(n 2 ) with respect to the sequence length. It means attending to all pixels for each layer is almost impossible for high-resolution image generation. Therefore, we apply Linformer <ref type="bibr" target="#b49">[50]</ref> to our model, which projects key and value to the k dimension when applying self-attention, reducing the time and space complexity from O(n 2 ) to O(nk). We fix k to 256 and apply Linformer to the encoder block above 32 ? 32 resolution, only when n is 1024 or higher. We call this model as Styleformer-L.</p><p>[50] explains that this new self-attention mechanism succeeds because the attention map matrix is low-rank. We observe this can be applied equally to the attention map matrix in the image: in the case of images, the pixel that needs to attend is often in a particular location, not all pixels(e.g. where objects are located in the image), which results in low-rank attention map matrix. Applying Linformer creates a more dense attention map, and also reduces computation. This is proved by spectrum analysis of attention map in Section 4.2. See Appendix E for more details about Styleformer-L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining Styleformer and StyleGAN2</head><p>Even with applying Linformer, it is difficult to generate an image for extremely high resolution like 512 ? 512 using only Transformer. We solve this problem by combining Styleformer and StyleGAN2 to generate a high-resolution image, and we call this model Styleformer-C. Styleformer-C is composed of Styleformer at low resolution, and style block of StyleGAN2 at high resolution. As demonstrated in 4.1, Styleformer encoder in low resolution help model to capture long-range dependency between components or global shape of object, and style block in high resolution help model to refine the details of each components or objects. In other words, model can capture global interactions efficiently using Styleformer only at low resolution, which leads to fast training speed. The overall architecture and details of Styleformer-C are described in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We only change the architecture of the generator in StyleGAN2-ADA, i.e., synthesis network, while maintaining the discriminator architecture and loss function. We use Fr?chet Inception Distance (FID) <ref type="bibr" target="#b22">[23]</ref> and Inception Score (IS) <ref type="bibr" target="#b43">[44]</ref>, evaluation metrics mainly used in the field of image generation. We compare our model with top GAN models such as StyleGAN2-ADA <ref type="bibr" target="#b31">[32]</ref>, and model related to our research such as TransGAN. In Section 4.1, we show performance results of Styleformer in low-resolution dataset. Section 4.2 provide evidence for a successful application of Linformer, including performance of Styleformer-L. In Section 4.3, we show high performance of Styleformer-C and prove the advantage and efficiency of our model by style mixing, and attention map visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Low-Resolution Synthesis with Styleformer</head><p>Styleformer achieves comparable performance to stateof-the-art in various low-resolution single-object datasets, including CIFAR-10 (32 ? 32) <ref type="bibr" target="#b35">[36]</ref>, STL-10 (48 ? 48) <ref type="bibr" target="#b8">[9]</ref>, and CelebA (64 ? 64) <ref type="bibr" target="#b39">[40]</ref>.</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, Styleformer outperforms prior GAN-based models, in terms of FID and IS. Especially in CIFAR-10, Styleformer records FID 2.82, and IS 10.00, which is comparable to current state-of-the-art and outperforming StyleGAN2-ADA-tuning. These results indicates that the Styleformer encoder has been modified to generate image successfully. Implementation details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Applying Linformer to Styleformer</head><p>We experiment our method at Section 3.4 which applies Linformer to Styleformer (Styleformer-L) on CelebA, 64 ? 64 resolution, and LSUN-Church <ref type="bibr" target="#b53">[54]</ref> dataset resized to 128?128 resolution. As shown in <ref type="table">Table 3</ref>, we find significant improvements in speed and memory and better performance than conventional Styleformer on CelebA. Memory performance is approximately three times more effective and speed performance is 1.3 times better in Styleformer-L. We also succeed in generating images of 128 ? 128 resolution with the LSUN-Church dataset, which is difficult with pure Styleformer due to expensive memory.</p><p>In addition, in the CelebA dataset, Styleformer-L shows higher performance in terms of FID than Styleformer, improving FID scores from 3.92 to 3.36. To analyze this phenomenon, we extract an attention map from Styleformer for generated CelebA images. As in <ref type="bibr" target="#b49">[50]</ref>, we apply singular value decomposition into attention map matrix, and plot the normalized cumulative singular value averaged over 1k generated images. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, most of the informa- Cifar-10 STL-10  <ref type="table">Table 3</ref>. Results on Styleformer-L which applies Linformer. "Memory" is measured on 4 Titan-RTX with 16 batch size per GPU and "Speed" means seconds for processing 1k images (sec/1kimg). We use the same hidden dimension and the number of layers in Styleformer and Styleformer-L. tion in the attention map matrix can be recovered from the few large singular value, which means that the rank of attention map matrix is low. With low rank attention map,</p><formula xml:id="formula_5">CelebA Method FID ? IS ? Method FID ? IS ? Method FID ?</formula><formula xml:id="formula_6">Method CLEVR Cityscapes FID ? IS ? FID ? IS ? GAN [21]</formula><p>25.02 2.17 11.57 1.63 k-GAN <ref type="bibr" target="#b44">[45]</ref> 28.09 2. <ref type="bibr" target="#b20">21</ref>   <ref type="table">Table 4</ref>. Comparison between popular CNN based GAN models and Styleformer-C on CLEVR and Cityscapes. We use the results in <ref type="bibr" target="#b25">[26]</ref> for the performance of other models.</p><p>Linformer can be applied more efficiently <ref type="bibr" target="#b49">[50]</ref>. Therefore, we show the possibility that when applying a self-attention operation for high-resolution images, it is not necessary to apply attention to all pixels and provide scalability to generate high-resolution images using Styleformer-L. See Appendix E for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Styleformer can Capture Global Interaction</head><p>We experiment our method at Section 3.4 which combines Styleformer and StyleGAN2 (Styleformer-C) on CLEVR(256?256) <ref type="bibr" target="#b29">[30]</ref> and Cityscapes (256?256) <ref type="bibr" target="#b9">[10]</ref> for multi-object images and compositional scenes, AFHQ CAT (512 ? 512) <ref type="bibr" target="#b7">[8]</ref> for high-resolution single-object images. <ref type="figure">Figure 7</ref>. Style mixing experiment with Styleformer-C on CLEVR dataset. The images on the x-axis and y-axis were generated from their respective latent codes (StyleGAN2 source and Styleformer source, respectively); the rest of the images were generated by applying styles from Styleformer source to Styleformer at low resolution and applying styles from StyleGAN2 source to StyleGAN2 at high resolution <ref type="bibr" target="#b32">[33]</ref>. As shown in <ref type="table">Table 4</ref>, Styleformer-C records FID 11.67, IS 2.27 in CLEVR, and FID 5.99, IS 2.56 in Cityscapes which is comparable performance to current state-of-theart, and showing better performance than StyleGAN2 in multi-object images and compositional scenes. This indirectly shows that Styleformer helps model to handle longrange dependency between components.</p><p>To show more solid evidence that Styleformer captures global interaction, We conduct style mixing <ref type="bibr" target="#b32">[33]</ref> in Styleformer-C. In detail, when generating new image from CLEVR dataset, we use two different latent codes z 1 , z 2 and applying z 1 to Styleformer at low resolution and z 2 to Style-GAN2 at high resolution. As shown in <ref type="figure">Figure 7</ref>, style corresponding to Styleformer (low-resolution) brings the basis for structural generation such as the location and structure of objects, while all colors or textures remain same. On the contrary, style corresponding to StyleGAN2 (high-resolution) brings the color and texture change, while maintaining location and shape of objects. This results directly prove that Styleformer controls global structure between objects, and handles long-range dependency.</p><p>In addition, we visualize the attention map to provide more insight into the model's generating process. <ref type="figure" target="#fig_5">Figure 8</ref> shows the concentration of attention to the position where the object exists. These visualizations show that the selfattention operation worked efficiently, enabling the model to perform long-range interaction, overcome the shortcoming of convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose Styleformer, a Transformer-based generative network that is novel and effective. We propose a method to efficiently generate images with self attention operation and achieve SOTA performance on various datasets. Furthermore, we propose Styleformer-L, which reduces the complex computation to linear, enabling to generate high-resolution images. We also present a method of efficiently generating a compositional scene while capturing with long-range dependency through Styleformer-C. There still seems to be room for improvement, such as reducing computation cost, but we hope that our work will speed up the application of Transformers to the field of computer vision, helping the development of the computer vision field. However, development of the generative model can create fake media data using synthesized face images (e.g. deepfake), so particular attention should be paid in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Styleformer</head><p>We implemented our Styleformer on top of the StyleGAN2-ADA Pytorch implementation 1 . Most of the details have not changed except generator architecture. We used CIFAR-10 tuning version of StyleGAN2-ADA, which means disabling style mixing regularization, path length regularization, and residual connections in D when training. We also fixed mapping network's depth to 2. We used bilinear filtering in all upsampling layer used in Styleformer. We use data augmentation pipeline suggested in StyleGAN2-ADA, and did not use mixed-precision training for all experiments.</p><p>For Styleformer encoder, we add bias and noise at the end of the encoder block, then performing leaky RELU with ? = 0.2. After passing several encoder blocks, we reshaped it to the form of square feature map, i.e., Unflatten. We then proceed bilinear upsample operation as we said in the Section 3.1, but also convert these reshaped output for each resolution into an RGB channel, using ToRGB layer. We upsample each of RGB output and add to each other, creating an output-skip connection generator, similar to StyleGAN2 generator. Originally ToRGB layer converts high-dimensional per pixel data into RGB per pixel data via 1?1 convolution operation, which we replace it to same operation, linear operation. We initialize all weights in Styleformer encoder using same method used in Pytorch linear layer. Unlike StyleGAN2-ADA, we employ weight demodulation also in ToRGB layer. We perform all experiments with 4 Titan-RTX using Pytorch 1.7.1. All of our experiments presented in the paper including failure spent about four months.</p><p>As in Section 4.1, the number of the Styleformer encoder and hidden dimension size for each resolution can be chosen as hyperparameters. We call these two hyperparameters as "Layers" and "Hidden size", respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Resolution Synthesis with Styleformer</head><p>For lowresolution synthesis experiment in Section 4.1, we use pure Styleformer. Each Layers and Hidden size used for CIFAR-10, STL-10, and CelebA are shown in <ref type="table" target="#tab_4">Table 5</ref>. We trained Styleformer for 65M, 92M, 25M at CIFAR-10, STL-10 and CelebA, respectively.</p><p>For CIFAR-10 experiment, we use 50K images (32 ? 32) at the training set, without using the label. For STL-10 experiment, we resize 96?96 image datasets to 48?48, and using 5k training images, 100k unlabeled images together as in <ref type="bibr" target="#b27">[28]</ref>. We change the size of the constant input from 8 ? 8 to 12 ? 12 to generate an image with a size of 48 ? 48.</p><p>For CelebA dataset, we use 200k unlabeled face images of the Align and Cropped version, which we resize to 64 ? 64 resolution as in <ref type="bibr" target="#b27">[28]</ref>. We start at 8 ? 8 constant, as we train CIFAR-10 dataset.</p><p>Ablation study details As we said in paper, we use small version of Styleformer with CIFAR-10 dataset for ablation study. In more detail, we use 1,2,2 for Layers, 256, 64, 16 for Hidden size, trained for 20M images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of head experiment</head><p>We conduct experiment about number of heads effect using one layer Styleformer, as said in <ref type="figure">Figure 3</ref>. One Layer Styleformer is a model that starts with 32?32 learned constant and only have one Styleformer encoder with hidden dimension size 256. We trained the model for 20M images, same as ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention map analysis</head><p>Post-Layer normalization We analyze the results of the attention map experiment on layer normalization. We propose in Section 3.2 that if we perform layer normalization at the end of Styleformer encoder, it breaks the attention map. <ref type="figure" target="#fig_6">Figure 9</ref> shows that if layer normalization is located at the end of the encoder, the attention map has not been properly learned. Therefore, we position layer normalization to the front of the encoder (i.e. after applying style modulation) so that the attention map can effectively learn the relationship between pixels. The experiment is all conducted on CIFAR-10, using small version of Styleformer, same as ablation study.</p><p>Demodulation for Query and Key As in Section 3.3, we demonstrate that when an attention map is created with Q, K from input scaled by style vector, specific value in attention map becomes very large. We show the attention map without demodulation operation to Q and K in Styleformer at <ref type="figure" target="#fig_6">Figure 9</ref>. We can see that without demodulation operation, attention is heavily concentrated on a particular pixel, preventing the attention operation from working properly. We overcome this problem with demodulation operation to Q, K, before creating attention map.  <ref type="figure" target="#fig_0">Figure 10</ref>. Attention map without demodulation operation to query and key. Comparing with <ref type="figure" target="#fig_6">Figure 9</ref> upper row, we can see specific large value in attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intuition of Increased Multi-Head Attention</head><p>Transformer is designed for natural language processing (NLP). It is difficult to use original Transformer in the field of image generation. A convolution network can efficiently generate images, unlike a linear layer, because it proceeds both operations between pixels using kernels and between channels. Although not a convolution network structure, <ref type="bibr" target="#b47">[48]</ref> has shown that architecture with pixel-topixel and channel-to-channel operations can be efficient in learning information about images.</p><p>Unlike the original convolution operation, in Mo-bileStyleGAN <ref type="bibr" target="#b2">[3]</ref>, they show good performance in image generation even by separating the interpixel and interchannel operations with depthwise separable convolution.</p><p>Main operation of Transformer can be divided into a interpixel operation (i.e. pixel section) and a interchannel operation (i.e. channel section) and it can be expressed by the following formula.</p><formula xml:id="formula_7">A i = sof tmax( Q i K T i ? d k ),<label>(5)</label></formula><formula xml:id="formula_8">head i = A i V i ,<label>(6)</label></formula><p>M ultihead(Q, K, V ) =Concat(head 1 , . . . , head k )W O ,</p><p>In above formulation, Q i , K i , V i is query, key, value for each head. d is the dimension of (query, key, value), k is the number of heads, and d k is d/k. A i is attention map and W O is the parameters of a linear layer that integrates multi-heads.</p><p>Pixel section corresponds for the self-attention operation between pixels (i.e., Equation 6), and channel section corresponds for integration of multi-head with linear layer, which operates between channels (i.e., <ref type="bibr">Equation 7</ref>). In a Transformer, the pixel section is slightly different from depthwise convolution. In depthwise convolution, kernel weights exist for each channel, but in Transformer, attention map A acts like one huge kernel, which means applying equal kernel weight to all different channels in V . It is difficult to create a powerful generator using a Transformer because the same attention kernel is applied for each channel, unlike the generator using depthwise separable convolution.</p><p>Using increased multi-head attention, this problem could be overcome. We can generate various attention maps (i.e., kernels) by increasing the number of heads. However, increasing the number of heads inevitably leads to smaller depth, where depth is hidden channel dimension divided by the number of heads. Since depth is a dimension used to create the attention map, there exists a minimum depth required. However, due to differences in the properties of pixels and tokens, the required depth size in computer vision is smaller than NLP.</p><p>In the field of NLP, a single token has a lot of information, so the required depth dimension, which represents the token, must be large, but one pixel in an image has less information than a token, which means that the required depth is smaller than a traditional Transformer. As described in the caption of <ref type="figure">Figure 3</ref>, the left graph shows a hidden dimension of 256, and the right graph shows 32. In the left graph, until the number of heads is 8 (depth is 32), per-formance increases when the number of heads grows up, but after that, even if the number of heads is increased, the performance decreases because the depth is less than 32. Similarly, in the right graph, performance is best when the number of heads is 1 (depth is 32), and after that, performance degrades because the depth is less than 32. From these results, we demonstrate that increasing the number of heads too much results in poor performance, and fixing the depth to 32.</p><p>Meanwhile, the channel section is a layer to integrate the multi-head together, where the linear layer is exactly the same operation as the 1 ? 1 convolution, i.e., pointwise convolution. Unlike convolution, the attention kernel is a kernel generated by the input itself, so it can create a more dense kernel, and it is advantageous to capture global features because it considers the relationship between all pixels. Therefore, by enhancing multi-head attention, the Styleformer can play a more powerful role as depthwise separable convolution, enabling generate high-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Demodulation for Encoder Output</head><p>As described in Section 3.3, self-attention conducts more operations than the convolution operation, making the demodulation process more complex. We show how standard deviation of encoder output can be derived (Demodulation for Encoder Output at Section 3.3), and the effect of the number of pixel in output standard deviation.</p><p>Derivation of encoder output standard deviation After demodulation operation to Q, K and V , Styleformer encoder performs style modulation to input V , weighted sum of V with attention map, and then performs linear operation. Let's consider the attention map matrix as A, and the weight matrix of linear operation as w. Since the matrix multiplication is associative, statistics of the output are the same even if the linear operation is calculated before multiplication with attention map :</p><formula xml:id="formula_10">output = [A(s j ? V )]w = A[(s j ? V )w].<label>(8)</label></formula><p>From now on, we think the linear operation is conducted before the multiplication with the attention map. Therefore, same as demodulation for query, key, and value, style modulation to V can be replaced to scaling linear weights:</p><formula xml:id="formula_11">w ? jk = s j ? w jk ,<label>(9)</label></formula><p>where s j scales jth feature map of V , and k enumerates the flattened output feature map. Assuming that input V have a unit standard deviation, the standard deviation of output after linear operation can be derived as follows:</p><formula xml:id="formula_12">? k = j w ? jk 2 .<label>(10)</label></formula><p>Finally, this output is multiplied with the attention map matrix A. When the attention score vector for lth pixel is expressed as A l? , standard deviation of encoder output is as follows:</p><formula xml:id="formula_13">? ? lk = ? A l? 2 ? j w ? jk 2 .<label>(11)</label></formula><p>Effect of the number of pixel Scaling the encoder out- </p><p>Since the attention map A is matrix after softmax operation, ? A l? = 1. Assuming A l? are i.i.d random variables from normal distribution with mean 1 n and variance 1 n 2 , and n denotes the number of pixel, ? lk 2 can be derived as follows:</p><formula xml:id="formula_15">? lk 2 = ? A l? 2 = ? (A l? ? 1 n ) 2 + 1 n ,<label>(13)</label></formula><p>using ? A l? = 1. Then (A l? ? 1 n ) become random variables from normal distribution with zero mean and variance 1/n 2 .</p><p>Based on the property of normal distribution, ? (A l? ? 1 n ) follows gamma distribution with shape parameter n 2 and scale parameter 2 n 2 . Therefore, using Chebyshev inequality, we have</p><formula xml:id="formula_16">Pr(| ? (A l? ? 1 n ) 2 ? 1 n | ? 1 n ) ? 1 ? 2 n ,<label>(14)</label></formula><p>meaning ? lk approaches zero when the number of pixels(i.e., n) increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation details of Styleformer-L</head><p>We introduce Styleformer-L, a model with Linformer applied to self-attention operation of Styleformer. When applying Linformer, we fix k (i.e., projection dimension for key, value) to 256, and apply to the encoder block above 32 ? 32 resolution. For projection parameter sharing effect, we used Key-value sharing. It means we create single E projection matrix for each layer that applies equally to the key, value of each head. We project key and value to k dimension after demodulation for key and value, i.e., before creating attention map and weight sum of value. When using Linformer, to prevent augmentation leaking, we clamp augmentation probability to 0.7.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>High-resolution compositional scenes generated by Styleformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Overall Architecture of Styleformer. (b) Styleformer encoder structure, which is the basic block of Styleformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>It shows FID on CIFAR-10 with one layer Styleformer, which hidden dimension size is fixed as 256 and 32, respectively. Both experiments show the best result when the depth is 32. Styleformer encoder structure for ablation study, including residual connection, layer normalization, attention style injection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>From the left, results generated by Styleformer on CIFAR-10 and STL-10, results generated by Styleformer-L on CelebA and LSUN-church and results generated by Styleformer-C on AFHQ-Cat. For more generated samples, please see Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Spectrum analysis of attention map matrix at 32, 64 resolution.We use pretrained Styleformer with CelebA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualizing attention map in generated CLEVR images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of attention map experimented based on Layer Normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>put feature map k with 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>High-resolution samples generated by Styleformer-C on CLEVR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>High-resolution samples generated by Styleformer-C on Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .</head><label>14</label><figDesc>High-resolution samples generated by Styleformer-L on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 .</head><label>15</label><figDesc>High-resolution samples generated by Styleformer-L on LSUN-church.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 .</head><label>16</label><figDesc>Samples generated by Styleformer on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation details of Styleformer components. Ablation study was conducted using small version of Styleformer with CIFAR-10 dataset, trained for 20M images. See Appendix A for further implementation details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison results between Styleformer and other GAN models on low-resolution datasets. Results of other GAN models are collected from papers that reports their best results. We compute FID, IS in the same way as StyleGAN2-ADA, generating 50k images and compare their statistics against the 50k images from the training set for FID, computing the mean over 10 dependent trials using 5k generated images per trial for IS.</figDesc><table><row><cell cols="2">Progressive-GAN [31]</cell><cell>15.52</cell><cell>8.80 ? 0.05</cell><cell>SN-GAN [42]</cell><cell>40.1</cell><cell>9.16 ? 0.12</cell><cell>PAE [5]</cell><cell>49.2</cell></row><row><cell cols="2">AutoGAN [20]</cell><cell>12.42</cell><cell cols="3">8.55 ? 0.10 Improving MMD-GAN [51] 37.64</cell><cell>9.23 ? 0.08</cell><cell>BEGAN-CS [6]</cell><cell>34.14</cell></row><row><cell cols="2">StyleGAN V2 [34]</cell><cell>11.07</cell><cell>9.18</cell><cell>AutoGAN [20]</cell><cell>31.01</cell><cell>9.16 ? 0.12</cell><cell>PeerGAN [52]</cell><cell>13.95</cell></row><row><cell cols="3">Adversarial NAS-GAN [20] 10.87</cell><cell>8.74 ? 0.07</cell><cell cols="2">Adversarial NAS-GAN [17] 26.98</cell><cell cols="3">9.63 ? 0.19 TransGAN-XL [28] 12.23</cell></row><row><cell cols="2">TransGAN-XL [28]</cell><cell>9.26</cell><cell>9.02 ? 0.11</cell><cell>TransGAN-XL [28]</cell><cell cols="2">18.28 10.43 ? 0.17</cell><cell>HDCGAN [11]</cell><cell>8.77</cell></row><row><cell cols="2">StyleGAN2-ADA [32]</cell><cell>2.92</cell><cell>9.83 ? 0.04</cell><cell>SNGAN-DCD [46]</cell><cell>17.68</cell><cell>9.33</cell><cell>NCP-VAE [1]</cell><cell>5.25</cell></row><row><cell cols="2">Styleformer</cell><cell>2.82</cell><cell>10.00 ? 0.12</cell><cell>Styleformer</cell><cell cols="2">15.17 11.01 ? 0.15</cell><cell>Styleformer</cell><cell>3.92</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">FID ? Memory per GPU ? Speed ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CelebA</cell><cell>Styleformer Styleformer-L</cell><cell>3.92 3.36</cell><cell>14668MiB 5316MiB</cell><cell>6.46 4.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSUN church</cell><cell>Styleformer Styleformer-L</cell><cell>-7.99</cell><cell>OOM 8118MiB</cell><cell>-9.81</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Details of Styleformer hyperparameters at low resolution synthesis. This model setting match with performance result at Table 2 in paper.</figDesc><table><row><cell>Datasets</cell><cell>Layers</cell><cell>Hidden size</cell><cell>FID</cell></row><row><cell>CIFAR-10</cell><cell>{1,3,3}</cell><cell>{1024,512,512}</cell><cell>2.82</cell></row><row><cell>STL-10</cell><cell>{1,2,2}</cell><cell>{1024,256,64}</cell><cell>15.17</cell></row><row><cell>CelebA</cell><cell cols="3">{1,2,1,1} {1024,256,64,64} 3.92</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https : / / github . com / NVlabs / stylegan2 -adapytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 11. High-resolution samples generated by Styleformer-C on AFHQ-Cat.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Kautz, and Arash Vahdat. Ncp-vae: Variational autoencoders with noise contrastive priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<title level="m">Soumith Chintala, and L?on Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mobilestylegan: A lightweight convolutional neural network for high-fidelity image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Belousov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Probabilistic auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uro?</forename><surname>Seljak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Escaping from collapsing modes in a constrained space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che-Rung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">High-resolution deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Curt?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Zarza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Feature-wise transformations. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://distill.pub/2018/feature-wise-transformations.4" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Style generator inversion for image enhancement and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Yee Whye Teh and Mike Titterington</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generative adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>De- cember 2015. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Top-k training of gans: Improving gan performance by throwing away bad samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Discriminator contrastive divergence: Semi-amortized generative modeling by exploring energy of the discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improving mmd-gan training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saman</forename><surname>Halgamuge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Peergan: Generative adversarial networks with a competing peer discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiutong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Semantically multi-modal image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">CelebA We conduct CelebA experiment using Styleformer-L in the same setting as CelebA using pure Styleformer for fair comparision</title>
		<imprint/>
	</monogr>
	<note>Table 3 in paper</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">We use {1, 2, 1, 1} for Layers, {1024, 256, 64, 64} for Hidden size, and training for 25M images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">We use {1, 2, 1, 1, 1} for Layers, {1024, 256, 64, 64, 64} for Hidden size, training for 40M images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun-Church</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">We generate low-resolution parts (up to 32?32) of image using Styleformer Encoder, and the rest of the resolutions parts of image are generated by applying StyleGAN2 block. Fundamentally, in Styleformer-C, the detail of Styleformer part is the same as that of Appendix A, and the StyleGAN2 part is the same as that of StyleGAN2 [34] implementation. We experiment with CLEVR and Cityscapes which is high-resolution multiobject or compositional scene datasets with a image size of 256 ? 256, and we also experiment AFHQ-Cat which is a high-resolution single-object dataset with a image size of 512 ? 512</title>
	</analytic>
	<monogr>
		<title level="m">F. Implementation Details of Styleformer-C We introduce Styleformer-C, which combines Styleformer and StyleGAN2</title>
		<imprint/>
	</monogr>
	<note>We performed all training runs on NVIDIA 2 Tesla V100 GPUs</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">we start at 8 ? 8 learned constant input just like the default setting and used {1, 2, 1, 1, 1, 1} for Layers, {1024, 256, 256, 256, 256, 128} for Hidden size, training for 10M images. Here, what Layers means in StyleGAN2 is a block which has two convolution operations and what Hidden size means in StyleGAN2 is the number of channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cityscapes</forename><surname>Clevr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clevr</forename><surname>In</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cityscapes, we apply Style-GAN2 in the same way as CLEVR. We also set Layers and Hidden size same as CLEVR experiment. Furthermore, we train Styleformer-C for 36M images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">AFHQ-Cat Likewise, we start with 8 ? 8 learned constant input and generate an image of 512 ? 512 size</title>
		<imprint/>
	</monogr>
	<note>We set Layers to {1, 2, 1, 1, 1, 1} and Hidden size to {1024, 256, 256, 256, 256, 64} to train Styleformer-C for 9M images</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Visual Samples We show various samples generated with Styleformer, Styleformer-L and Styleformer-C in this section</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
