<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For human pose estimation in monocular images, joint occlusions and overlapping upon human bodies often result in deviated pose predictions. Under these circumstances, biologically implausible pose predictions may be produced. In contrast, human vision is able to predict poses by exploiting geometric constraints of joint inter-connectivity. To address the problem by incorporating priors about the structure of human bodies, we propose a novel structure-aware convolutional network to implicitly take such priors into account during training of the deep network. Explicit learning of such constraints is typically challenging. Instead, we design discriminators to distinguish the real poses from the fake ones (such as biologically implausible ones). If the pose generator (G) generates results that the discriminator fails to distinguish from real ones, the network successfully learns the priors.</p><p>To better capture the structure dependency of human body joints, the generator G is designed in a stacked multi-task manner to predict poses as well as occlusion heatmaps. Then, the pose and occlusion heatmaps are sent to the discriminators to predict the likelihood of the pose being real. Training of the network follows the strategy of conditional Generative Adversarial Networks (GANs). The effectiveness of the proposed network is evaluated on two widely used human pose estimation benchmark datasets. Our approach significantly outperforms the state-of-the-art methods and almost always generates plausible human pose predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a key step in understanding the actions of people in images and videos. Understanding of a person's limb articulation location is very helpful for highlevel vision tasks like human tracking, action recognition, and also serves as a fundamental tool in fields such as humancomputer interaction applications. It is a challenging task in computer vision due to high flexibility of body limbs, self and outer occlusion, various camera angles, etc.</p><p>Recently, significant improvements have been achieved on this topic by using Deep Convolutional Neural Networks (DCNNs) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>. These approaches mainly follow the strategy of regressing heatmaps of each body part using DCNNs. These regression models have shown great ability of learning better feature representations. However, for body parts with heavy occlusions (especially from body parts of surrounding people) and background which seems similar to body parts, DCNNs may have difficulty in regressing accurate heatmaps.</p><p>Human vision is capable of learning the variety and limitless of human body shape structures from observations. Even under extreme occlusions, we can infer the potential poses and remove the implausible ones. It is, however, very challenging to incorporate the priors about human body structures into DCNNs, because, as pointed out in <ref type="bibr" target="#b29">[30]</ref>, the lowlevel mechanics of DCNNs is typically difficult to interpret, and DCNNs are most capable of learning features.</p><p>As a consequence, an unreasonable human pose may be produced by a standard DCNN. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, in challenging test cases with heavy occlusions, standard DCNNs tend to perform poorly. To solve this problem, priors about the structure of the body joints must be considered. The key point of this problem is to learn the real body joints distribution from a large amount of training data. However, explicit learning of such a distribution can be very difficult.</p><p>To address this problem, we attempt to learn the distribution of the human body structures implicitly. We suppose that we have a "discriminator" which can tell whether the predicted pose is geometrically reasonable. If the DCNN regressor is able to "deceive" the "discriminator" that its predictions are all reasonable, the network would have successfully learned the priors of the human body structure. Inspired by the recent success in Generative Adversarial Networks (GAN) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref>, we propose to design the "discriminator" as the discriminator network while the regression network functions as a generative network. Training the generator in the adversarial manner against the discriminator exactly meets our intention.</p><p>To accomplish the above goals, the discriminator should be fed with sufficient information to perform classification, while the generator should have the ability in modeling the complicated features in pose estimation. Thus, a multitask learning network G is designed, which simultaneously regresses the pose heatmaps and the occlusion heatmaps. Based on the pose and occlusion heatmaps, the pose discriminator (P) is used to tell whether the body configuration is plausible.</p><p>In addition, our preliminary results show that correct locations often correspond to highly confident heatmaps. Therefore, we design another discriminator to make a decision on the confidence of the predicted pose heatmaps. The generator is asked to "fool" both the pose and confidence discriminators by training G and {P, C} in the generative adversarial manner. Thus, the human body structure is implied in the P net by guiding G to the direction that is close to ground-truth heatmaps and satisfies joint-connectivity constraints of the human body. The learned G net is expected to be more robust to occlusions and cluttered backgrounds where the precise description for different body parts are required.</p><p>The main contributions of this work are three folds.</p><p>? We design a novel network framework for human pose estimation which takes the geometric constraints of human joints connectivity into consideration. By incorporating the priors of the human body, prediction mistakes caused by occlusions and cluttered backgrounds are considerably reduced. Even when the network fails, the outputs of the network appear more like "human" predictions instead of "machine" predictions. ? To our best knowledge, we are the first to use Generative Adversarial Networks to exploit the constrained human-pose distribution for improving human pose estimation. We also design a stacked multi-task network for predicting both the pose heatmaps and the occlusion heatmaps to achieve better results. ? We evaluate our method on two public human pose estimation datasets. Our approach significantly outperforms state-of-the-art methods, and is able to consistently produce more plausible pose predictions compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Our work is closely related to work using heatmap based DCNN methods for human pose estimation and Generative Adversarial Networks.</p><p>Human Pose Estimation. Traditional human pose estimation methods often follow the framework of tree structured graphical model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. With the introduction of "DeepPose" by Toshev et al. <ref type="bibr" target="#b30">[31]</ref>, deep network based methods become more popular in this area. This work is more related to the methods generating pose heatmaps from images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. For example, Tompson et al. <ref type="bibr" target="#b29">[30]</ref> generated heatmaps by running an image through multiple resolution banks in parallel to simultaneously capture features at a variety of scales. Tompson et al. <ref type="bibr" target="#b28">[29]</ref> used multiple branches of convolutional networks to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose predictions without Structure</head><p>Structure-aware pose predictions fuse the features from an image pyramid, and used Markov Random Field (MRF) for post-processing. In the following, Convolutional Pose Machine <ref type="bibr" target="#b32">[33]</ref> incorporated the inference of the spatial correlations among body parts within convolutional networks. Hourglass Network <ref type="bibr" target="#b18">[19]</ref> proposed a state-of-the-art architecture for bottom-up and top-down inference with residual blocks. The structure of our G net is also a fully convolutional network with "conv-deconv" architecture. However, our network is designed in a multi-task manner with features of both tasks connected to the feature for the second stacked network.</p><p>Generative Adversarial Network. Generative Adversarial Networks have been widely studied in previous work for discrete labels <ref type="bibr" target="#b17">[18]</ref>, text <ref type="bibr" target="#b24">[25]</ref> and also images. The imageconditional models have tackled inpainting <ref type="bibr" target="#b19">[20]</ref>, image prediction from a normal map <ref type="bibr" target="#b31">[32]</ref>, future frame prediction <ref type="bibr" target="#b16">[17]</ref>, future state prediction <ref type="bibr" target="#b38">[39]</ref>, product photo generation <ref type="bibr" target="#b36">[37]</ref>, and style transfer <ref type="bibr" target="#b15">[16]</ref>. Human pose estimation can been considered as a translation from a RGB image to a multichannel heatmap. The designed bottom-up and top-down G net can well accomplish this translation. Different from previous work, the goal of the discrimination network is not only to distinguish the fake from real, but also to incorporate geometric constrain to the model. This is the reason for the different training strategy for fake samples compared with traditional GANs which will be explained in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Proposed Adversarial PoseNet</head><p>As mentioned in <ref type="figure" target="#fig_2">Fig. 2</ref>, our Adversarial PoseNet model consists of three parts, i.e., the pose generator network G, the pose discriminator network P and the confidence discriminator C. The generative network is a bottom-up and top-down network, where the inputs are the RGB images and the outputs are 32 heatmaps for each input image in our case. Half of the returned heatmaps are pose estimations for 16 pose key points, and the other half are for the corresponding occlusion predictions. The values in each heatmap are confidence scores in the range of [0, 1] where a Gaussian blur is done around the ground truth position.</p><p>Without discriminators, G will be updated simply by forward and backward propagations of itself (cf., the lines with 1 in <ref type="figure" target="#fig_2">Fig. 2</ref>). That might generate low confidence and even incorrect location pose estimations. It is necessary to leverage the power of discriminators to correct these poor estimations. Therefore, two discriminator networks C and P are introduced into the framework.  The sub-network in purple is the stacked multi-task network (G) for pose generation. The networks in blue (P) and green (C) are used to discriminate whether the generated pose is "real" (reasonable as a body shape) and whether the generator has strong confidence in locating the body parts, respectively. Dashed lines into G indicate backward gradients to update G. 1 shows the forward and backward of the G net. 2 shows the process of G updated by the gradient from the C net. Then, G is updated by the gradients from P as shown in lines with 3 .</p><p>After updating G by training with C in the adversarial manner (cf. the lines with 2 ), more confident results are produced. Furthermore, after training G with both P and C (cf. the lines with 3 ), the human body priors are implicitly exploited, and the prediction confidences are accordingly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Task Generative Network</head><p>In this section, we introduce the generative network G in our framework. <ref type="figure" target="#fig_4">Fig. 3</ref> presents the architecture of G. Knowledge of whether a body part being occluded clearly offers important information for inferring the geometric information of a human pose. Here, in order to effectively incorporate both pose estimation and occlusion predictions, we propose to tackle the problem with a multi-task generative network.</p><p>The goal of the multi-task generative network is to learn a function G which attempts to project an image x to both the corresponding pose heatmaps y and occlusion heatmaps z, i.e., G(x) = {?,?} where? and? are the predicted heatmaps.</p><p>In addition, as reported in <ref type="bibr" target="#b32">[33]</ref>, large contextual regions are important for locating body parts. So the contextual region of a neuron, which is its receptive field, should be large. To achieve this goal, an "encoder-decoder" architecture is used.</p><p>Besides, for the problem of human pose estimation, local evidence is essential for identifying features for faces and hands. Meanwhile, the final pose estimation requires a coherent understanding of the full body image. To capture this information at each scale, skip connections between mirrored layers in the encoder and decoder are added. Inspired by <ref type="bibr" target="#b18">[19]</ref>, our network is also stacked to provide the network with a mechanism for re-evaluation of initial estimates and features across the entire image. In each module of the G net, a residual block <ref type="bibr" target="#b11">[12]</ref> is used for the convolution operator. Given the original image x, a basic block of the stacked multi-task generator network can be expressed as follows:</p><formula xml:id="formula_0">{Y n , Z n , X} = G n (Y n?1 , Z n?1 , X) if n 2 {Y n , Z n , X} = G n (X) if n = 1 ,</formula><p>where Y n and Z n are the output activation tensors of the n-th stacked generative network for pose estimations and occlusion predictions, respectively. X is the image feature tensor, obtained after pre-processing on the original image through two residual blocks. Suppose that there are N times stacking of the basic block, then the multi-task generative network can be formulated as:</p><formula xml:id="formula_1">{Y N , Z N , X} = G N (G N ?1 (? ? ? (G 1 (X), Y 1 , Z 1 ))) .</formula><p>In each basic block, the final heatmap outputs? n ,? n are obtained from Y n and Z n by two 1 ? 1 convolution layers with the step size of 1 and without padding. Specifically, the first convolution layer reduces the number of feature maps from the number of feature maps to the number of body parts. The second convolution layer acts as a linear classifier to obtain the final predicted heatmaps. Therefore, given a training set</p><formula xml:id="formula_2">{x i , y i , z i } M i=1</formula><p>where M is the number of training images, the loss function of our multi-task generative network is presented as:</p><formula xml:id="formula_3">L G (?) = 1 2M N N n=1 M i=1 y i n ?? i n 2 + z i n ?? i n 2 .</formula><p>(1) where ? denotes the parameter set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pose Discriminator</head><p>To enable the training of the network to exploit priors about the human body joints configurations, we design the pose discriminator P. The role of the discriminator P is to distinguish the fake poses (poses do not satisfy the constraints of human body joints) from the real poses.</p><p>It is intuitive that we need local image regions to identify the body parts and the large image patches (or the whole image) to understand the relationships between body parts. However, when some parts are seriously occluded, it can be very difficult to locate the body parts. Human can achieve that by using prior knowledge and observing both the local image patches around the body parts and relationships among different body parts. Inspired by this, both low-level and high-level information can be important to infer whether the predicted poses are biologically plausible. In contrast to previous work, we use an encoder-decoder architecture to implement the discriminator P. Skip connections between parallel layers are used to incorporate both the local and global information.</p><p>Additionally, even when the generative network fails to predict the correct pose locations for a particular image, the predicted pose may be a plausible one, only for a different human body shape. Thus, simply using the pose and occlusion features may still face difficulty in training an accurate P.</p><p>Such inference should be made by taking the original image into consideration at the same time. Occlusion information can also be useful in inferring the pose rationality. We use the input RGB image with pose and occlusion maps generated by the G net as the input to P for predicting whether a pose is reasonable or not for a particular image. The network structure of P is shown in <ref type="figure">Fig. 4</ref>. To achieve this goal, GAN is set in the conditional manner for P in our framework. As GANs learn a generative model of data, conditional GANs (cGANs) learn a conditional generative model <ref type="bibr" target="#b10">[11]</ref>. The objective of a conditional adversarial P network is expressed as follows:</p><formula xml:id="formula_4">L P (G, P ) = E[logP (y, z, x)]+E[log(1?(P (G(x), x)?p fake ))] .</formula><p>(2) where p fake is the ground truth pose discriminator label. In traditional GAN, p fake is simply set as 0. The choice of p fake here will be discussed in detail in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Confidence Discriminator</head><p>By observing the differences between ground truth heatmaps and predicted heatmaps by previous methods, we find that the predicted ones are often not Gaussian centered because of occlusions and body overlapping. Recalling the mechanism of human vision, even when the body parts are occluded, we can still confidently locate the body parts. That is mainly because we already acquire the geometric prior of human body joints. Motivated by this, we design a second auxiliary discriminator, which is termed Confidence Discriminator (i.e., C) to discriminate the high-confidence predictions from the low-confidence predictions. The inputs for C are the pose and occlusion heatmaps. The objective of a traditional adversarial C network can be expressed as:</p><formula xml:id="formula_5">L C (G, C) = E[logC(y, z)]+E[log(1?(C(G(x))?c fake ))] .</formula><p>(3) where c fake is the ground truth confidence label. In traditional GAN, c fake is simply set as 0. The choice of c fake here will also be discussed in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training of the Adversarial Networks</head><p>In this section, we describe the detailed inputs and outputs of the discriminators, and then explain in detail how P and C contribute to the accurate pose predictions with structure constraints.</p><p>As mentioned in Section 2.2 and Section 2.3, the previous pose estimation networks usually have less confidences in locating the occluded body parts as the local information are neglected. However, if the G network can learn to see through the occlusions like the inferences made by human in this situation, it should achieve high confidences in locating such body parts.</p><p>If G generates low-confidence heatmaps, C will classify the result as "fake". As G is optimized to deceive C that the fakes being real, this process would help G to generate high confidence heatmaps even with occlusions presented. The outputs are the confidence scores c which in fact corresponds to whether the network is confident in locating body parts.</p><p>During training C, the real heatmaps are labelled with a 16 ? 1 (16 is the number of body parts) unit vector c real . The confidence of the fake (predicted) heatmap should be high when it is close to ground truth and low otherwise. So the fake (predicted) heatmaps are labelled with a 16 ? 1 vector c fake where the elements of c fake are the corresponding  confidence scores.</p><formula xml:id="formula_6">c i fake = 1 if y i ?? i &lt; ? 0 if y i ?? i ? ,</formula><p>where ? is the threshold parameter, and i is the i-th body part. The range of the output values in C is [0, 1].</p><p>In the following, we show how to embed the geometric information of human bodies into the proposed P network. We observe that, when a part of human body is occluded, the prediction of the un-occluded parts are typically not affected. This may be due to the DCNN's strong ability to learn local features.</p><p>In previous works on image translation using GANs, the discriminative network is learned with real samples being labeled 1 and 0 for fake samples. For the problem of human pose estimation, we found the network to be difficult to converge by simply setting 0 or 1 as ground truth for a sample. Instead, we designed a novel strategy for pose estimation.</p><p>The ground truth p real of a real sample is a 16 ? 1 unit vector. For the fake samples, if a predicted body part is far from the ground truth location, the pose is clearly implausible for the body configuration in this image. Therefore, the (0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1) 0: Wrong location; 1: Right location ? Discriminator on pose(P) (0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1) 0: Low confidence; 1: High confidence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator on confidence(C)</head><p>? ? ? ? <ref type="figure">Figure 4</ref>: Architectures of the discriminator network P and C.</p><p>ground truth p fake is:</p><formula xml:id="formula_7">p i fake = 1 if d i &lt; ? 0 if d i ? ,</formula><p>where ? is the threshold parameter and d i is the normalized distance between the predicted and ground-truth location of the i-th body part. The range of the output values in P is also [0, 1]. To deceive P, G will be trained towards the direction to generate heatmaps which satisfy the joints constraints of human bodies. Previous approaches to conditional GANs have found it beneficial to mix the GAN objective with a traditional loss, such as 2 distance <ref type="bibr" target="#b19">[20]</ref>. For our task, it is clear that we also need to supervise G in the training process with the ground truth human poses. Thus, the discriminator still plays the original role, but the generator will not only fool the discriminator but also approximate the ground-truth output in an 2 sense as in Eq. <ref type="figure" target="#fig_4">(3)</ref>. Therefore, the final objective function is presented as follows.</p><p>arg min</p><formula xml:id="formula_8">G max P,C L G (?) + ?L C (G, C) + ?L P (G, P ) . (4)</formula><p>In experiments, in order to make the different components of the final objective function have the same scale, the hyper parameters ? and ? are set to 1/220 and 1/180, respectively. Algorithm 1 demonstrates the whole training processing as the pseudo codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Datasets. We evaluate the proposed method on two widely used benchmarks on pose estimation, i.e.,  <ref type="bibr" target="#b28">[29]</ref>. Both datasets provide the visibility of body parts, which is used as the supervision occlusion signal in our method. Experimental Settings. According to the detection results, we crop the images with the target human centered at the images, and warp the image patch to the size of 256?256 pixels. We follow the data augmentation in <ref type="bibr" target="#b18">[19]</ref> by rotation (+/-30 degrees), and scaling (0.75-1.25) to make the network more robust to different scales and directions. During training for LSP, we use the MPII dataset to augment the training data of LSP, which is a regular routine as done in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>. During testing on the MPII dataset, we follow the standard routine to crop image patches with the given rough position and scale. The input resolution to the G net is 64?64 pixels. The network starts with a 7?7 convolutional layer with stride 2, followed by a residual modules and a max pooling to drop the resolution down from 256 to 64. Then two residual modules are followed before sending the feature into G. Across the entire network all residual modules contain three convolution layers and a skip connection with output of 512 feature maps. The generator is stacked four times if not specially indicated in our experiment. For implementation, we train our model with the Torch7 toolbox <ref type="bibr" target="#b6">[7]</ref>. The network is trained using the RMSprop algorithm with initial learning rate of 2.5 ? 10 ?4 . The model on the MPII dataset was trained for 230 epochs (about 1 day on a Tesla M40 GPU) and the LSP dataset for 250 epochs (about 1.5 days on a Tesla M40 GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quantitative Results</head><p>We use the Percentage Correct Keypoints (PCK@0.2) <ref type="bibr" target="#b35">[36]</ref> metric for comparison on the LSP dataset which reports the percentage of detection that falls within a normalized distance of the ground-truth for comparisons. For MPII, the distance is normalized by a fraction of the head size <ref type="bibr" target="#b0">[1]</ref> (referred to as PCKh@0.5). LSP Human Pose. <ref type="table" target="#tab_1">Table 1</ref> shows the PCK performance of our method and previous methods at a normalized distance of 0.2. Our approach outperforms the state-of-the-art across all the body joints, and obtains 2.4% improvement in average. MPII Human Pose. <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_6">Fig. 5</ref> reports the PCKh performance of our method and previous methods at a normalized distance of 0.5. "Ours (-)" refers to our baseline results by using a four-stacked single-task network without multi-task and discriminators. This network has similar structure but half of stacked layers and parameter numbers compared to <ref type="bibr" target="#b18">[19]</ref>. Our method achieves the best PCKh score of 92.1% on this data set.</p><p>In particular, for the most challenging body parts, e.g., wrist and ankle, our method achieves 1.5% and 1.9% improvement compared with the closest competitor respectively, which is significant. Our baseline using single-task network without multi-task and discriminators</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quanlitative Comparisons</head><p>To gain insights on how the proposed method accomplish the goal of setting the pose estimations within the geometric constraints, we visualize the predicted poses on the MPII test set compared with a 2-stacked hourglass network (HG) <ref type="bibr" target="#b18">[19]</ref>, as demonstrated in <ref type="figure">Fig. 6</ref>. For fare comparison, we also use a 2-stacked network in this section. We can see that our method gains a better understanding of the human body image which leads to less strange locations.</p><p>In (a), the human body is highly twisted or partly occluded, which results in some invisible body limbs in the image. In these cases, HG fails to understand some poses while our method succeeds. This may be because of the ability of occlusion prediction and shape prior learned the in the training process. In (b), HG locates some body parts to the nearby positions with the most salient features. This indicates that HG has learned excellent features about body parts. However, without human body structure awareness, this may locate some body parts to the surrounding area instead of the right one. In (c), due to lack of body configuration constraints, HG produces poses with deviated twisting across body limbs. As we have implicitly embedded the body constraints into our discriminator, our network succeeds in predicting the correct body location even under some difficult situations.</p><p>On the other hand, we also show some failure examples of our method on the MPII test set in <ref type="figure">Fig. 7</ref>. As shown in <ref type="figure">Fig. 7</ref>, our method may fail in some challenging cases with twisted limbs at the edge, overlapping people and occluded body parts. In some cases, human may also fail to figure out the correct pose at a glance. However, even when our method fails in this situations, our method also achieves more reasonable poses compared to previous method. In these complicated cases, previous method may generate some poses which are not within the manifold of human pose structure as shown in the first row of <ref type="figure">Fig. 7</ref>. When the previous network fails to find high-confidence locations around the centered person, it shifts to the surrounding area where the local features matches the trained feature best. Lacking of shape constrain finally results in some absurd poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Occlusion Analysis</head><p>Here we present a detailed analysis of the outputs of the networks when input images are occluded.</p><p>First, two examples with some body parts occluded are given in <ref type="figure">Fig. 8</ref>. In the first sample, two legs of the person are totally occluded by the table. In the corresponding occlusion maps, the occluded part are well predicted. Despite of the occlusions, the pose heatmaps generated by our method are mostly clear and Gaussian centered. This results in high scores in both pose prediction and confidence evaluation despite of occlusions.</p><p>In the second image, half part of the person is overlapped by the person ahead of him. Our method also succeeds to yield the correct pose locations with clear heatmaps. Occlusion information is also well predicted for the occluded parts. As shown in the columns in red, although the confidence scores of the occluded body parts are comparatively low, they remain an overall high level. This shows that as our network has learned some human body priors during training. Thus it has the ability to predict reasonable poses even under some occlusions. This verifies our motivation of designing the discriminators with GANs.</p><p>Next, we compare the performance of our method under occlusions with a stacked hourglass network <ref type="bibr" target="#b18">[19]</ref> as the strong baseline. In the validation set of MPII, about 25% of the elbows and wrists with annotations are labeled invisible. We show the results of elbows and wrists with visible samples and invisible samples in <ref type="table" target="#tab_3">Table 3</ref>. For body parts without occlusions, our method improves the baseline by about 0.8% of detection rate. However, our method improves the baseline by 3.5% and 3.6% of detection rates on the invisible twists and elbows. This shows the advantage of our method in dealing with body parts with occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ablation Study</head><p>To investigate the efficacy of the proposed multi-task generator network and the discriminators designed for learning human body priors, we conduct ablation experiments on the validation set of the MPII Human Pose dataset. A fourstacked single-task generator without occlusion is used as the baseline. The overall result is shown in <ref type="figure">Fig. 9</ref>. We give analysis to two components in our method: the multi-task manner and discriminators.</p><p>Multi-task. We compare the four-stacked multi-task generator with the baseline. The networks are trained simply by removing the generators (i.e., no GANs). By using the occlusion information, the performance on the MPII validation set increases 0.5% compared to the baseline model. This shows that the multi-task structure helps the network to understand the poses.</p><p>Discriminator with Single-task. We also compare the four-stacked single-task generator trained with discriminators with the baseline. The networks are trained by removing the part for the occlusion heatmaps. Discriminators also receive inputs without occlusion heatmaps. By using the body-structure-aware GANs, the performance on the MPII validation set increases by 0.6% compared to the baseline model. This shows that the discriminators contribute in pushing the generator to produce more reliable pose predictions.</p><p>In general, individually adding the multi-task or discriminator both increase the accuracy of location. But using them separately results in 0.6% and 0.5% improvement respectively, while using both produces an improvement of 1.5%. The may be due to the reliability of P and C on sufficient feature to discriminate the results. Occlusion features obviously can help to understand the image and the generated pose for the discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we proposed a novel conditional adversarial network for pose estimation, termed Adversarial PoseNet, which trains a multi-task pose generator with two discriminator networks. The two discriminators function like an expert who distinguishes reasonable poses from unreasonable ones. By training the multi-task pose generator to deceive the expert that the generated pose is real, our network is more robust to occlusions, overlapping and twisting of human bodies. In contrast to previous work using DCNNs in human pose estimation, our network is able to alleviate the risk of locating the human body part onto the matched features without consideration of human body priors.</p><p>Although we need to train three sub-networks (G, P, C), we only need to use G net during testing. With a small computation overhead, we achieve considerably better results on two popular benchmark datasets. We have also verified that our network can produce human poses which are mostly within the manifold of human body shape. The method developed here can be immediately applied to other shape estimation problems such as face landmark detection using DCNNs. More significantly, we believe that the use of GANs as a tool to predict structured output or enforcing output dependency can be further developed to much more general structured output learning.   <ref type="figure">Figure 6</ref>: Prediction samples on the MPII test set. The first row: original images. The second row: results by stacked hourglass network (HG) <ref type="bibr" target="#b18">[19]</ref>. The third row: results by our method. (a)-(c) stand for three kinds of failure with HG. <ref type="figure">Figure 7</ref>: Failure cases caused by body part at the edge (the first-second columns), overlapping people (the third column) and invisible limbs (the fourth column). The results on the first and second rows are generated by our method and HG <ref type="bibr" target="#b18">[19]</ref>, respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Motivation. We show the importance of strongly enforcing priors about the human body structure during training of DCNNs for pose estimation. Learning without using such priors generates inaccurate results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Output of G updated by the gradient from C . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Output of G updated by the gradient from C and P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview of the proposed Structure-aware Convolutional Network for human pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the multi-task generative network G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>The training process of our method.Require: Training images: x, the corresponding ground-truth heatmaps {y,z}; 1: Forward G by {?,?} = G(x), and optimize G according to Eq. (1); 2: Forward P by {p fake } = P(x, G(x)), and optimize P net by maximizing the second term in Eq. (2); 3: Forward P by {p real } = P(x, y, z), and optimize P by maximizing the first term in Eq. (2); 4: Forward C by {? fake } = C(G(x)), and optimize C by maximizing the second term in Eq. (3); 5: Forward C by {? real } = C(x, y, z), and optimize C by maximizing the first term in Eq. (3); 6: Optimize G by Eq. (4); 7: Go back to Step 1 until the accuracy of the validation set stop increasing; 8: return G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>PCKh comparison on MPII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>(a) Input images with predicted poses; (b) Predicted pose heatmaps of four occluded body parts; (c) Predicted occlusion heatmaps of four occluded body parts; (d) Outputs values of P (in blue) and C(in green). Red columns in the output of C correspond to values of the four occluded body parts. Ablation study: PCKh scores at the threshold of 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of PCK performance with normalized distance being 0.2 on the LSP dataset. Methods Head Sho. Elb. Wri. Hip Knee Ank. Mean</figDesc><table><row><cell>[2]</cell><cell>95.2 89.0 81.5 77.0 83.7 87.0 82.8 85.2</cell></row><row><cell>[16]</cell><cell>96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7</cell></row><row><cell>[21]</cell><cell>97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1</cell></row><row><cell>[14]</cell><cell>97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1</cell></row><row><cell>[22]</cell><cell>97.8 92.5 87.0 83.9 91.5 89.9 87.2 90.1</cell></row><row><cell>[33]</cell><cell>97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5</cell></row><row><cell>[4]</cell><cell>97.2 92.1 88.1 85.2 92.2 91.4 88.7 90.7</cell></row><row><cell>Ours</cell><cell>98.5 94.0 89.8 87.5 93.9 94.1 93.0 93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on MPII Human Pose (PCKh@0.5). Methods Head Sho. Elb. Wri. Hip Knee Ank. Mean</figDesc><table><row><cell>[30]</cell><cell>95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6</cell></row><row><cell>[5]</cell><cell>95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3</cell></row><row><cell>[29]</cell><cell>96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0</cell></row><row><cell>[13]</cell><cell>95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4</cell></row><row><cell>[21]</cell><cell>94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4</cell></row><row><cell>[16]</cell><cell>97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0</cell></row><row><cell>[10]</cell><cell>96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1</cell></row><row><cell>[24]</cell><cell>97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3</cell></row><row><cell>[14]</cell><cell>96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5</cell></row><row><cell>[33]</cell><cell>97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5</cell></row><row><cell>[4]</cell><cell>97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7</cell></row><row><cell>[19]</cell><cell>98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9</cell></row><row><cell cols="2">Ours (-) 1 98.2 96.2 90.9 86.7 89.8 87.0 83.2 90.6</cell></row><row><cell>Ours</cell><cell>98.6 96.4 92.4 88.6 91.5 88.6 85.7 92.1</cell></row><row><cell>1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Detection rates of visible and invisible elbows and wrists.</figDesc><table><row><cell>Methods</cell><cell cols="4">Visible Twist Elbow Twist Elbow Invisible</cell></row><row><cell>[19]</cell><cell>93.6</cell><cell>95.1</cell><cell>67.2</cell><cell>74.0</cell></row><row><cell>Ours</cell><cell>94.5</cell><cell>95.9</cell><cell>70.7</cell><cell>77.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Upper body detection and tracking in extended signing sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="197" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2D articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="214" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="728" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5600" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeeperCut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<title level="m">Conditional Generative Adversarial Nets. arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="1411" />
			<biblScope unit="volume">1784</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context encoders: feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepCut: joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv, 1511.06434:1-16</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vis. Conf</title>
		<meeting>British Machine Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MODEC: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascaded models for articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="406" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepPose: human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end jearning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixellevel domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Energy-based generative adversarial network. arXiv preprint arXiv, 1609</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">03126</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning temporal transformations from time-lapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
