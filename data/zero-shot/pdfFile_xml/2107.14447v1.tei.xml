<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihuang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
							<email>xjia@dlut.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>3 Huawei Technologies</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
							<email>jianzhong.he@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
							<email>huqinghua@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing domain adaptation methods focus on adaptation from only one source domain, however, in practice there are a number of relevant sources that could be leveraged to help improve performance on target domain. We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decomposition (T-SVD) into a neural network's training pipeline. Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at capturing consistent data structure across different domains. Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weighting strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty estimation. Extensive experiments conducted on public benchmarks demonstrate the superiority of our model in addressing the task of MDA compared to state-of-the-art methods. Code is available at https://github.com/lslrh/ T-SVDNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods have shown superior performance with huge amounts of training data as rocket fuel. However, directly transferring knowledge learned on a certain visual domain to other domains with different distributions would degrade the performance significantly due to * Work partly done during an internship at Noah's Ark Lab ? Corresponding Author the existence of domain shift <ref type="bibr" target="#b42">[43]</ref>. To handle this problem, the prominent approaches such as transfer learning and unsupervised domain adaptation (UDA) endeavor to extract domain-invariant features. Discrepancy-based methods reduce the domain gap by minimizing the discrepancy between source and target distributions, such as Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b25">[26]</ref>, correlation alignment <ref type="bibr" target="#b35">[36]</ref>, and contrastive domain discrepancy <ref type="bibr" target="#b16">[17]</ref>. Adversarial methods attempt to align source and target domains through adversarial training <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref> or GAN-based loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref>. These methods only focus on domain adaptation with only single source. However, in many practical application scenarios, there are a number of relevant sources collected in different ways available, which could be used to help improve performance on target domain. Naively combining various sources into one is not an effective way to fully exploit abundant information within multiple sources, and might even perform worse than single-source methods, because domain gap among multiple sources causes confusion in the learning process <ref type="bibr" target="#b44">[45]</ref>. Some Multi-Source Domain Adaptation (MDA) approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13]</ref> focus on aligning multiple source domains and a target domain by projecting them into a domain-invariant feature space. This is done by either explicitly minimizing the discrepancy of different domains <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> or learning an adversarial discriminator to align distributions of different domains <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>. However, eliminating distribution discrepancy of data has the risk of sacrificing discrimination ability. Moreover, these methods only achieve pair-wise matching, neglecting underlying high-order relations among all domains. Another widely used way in MDA is distribution-weighted combining rule <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>, which takes a weighted combination of pre-trained source classifiers as the classifier for target domain. In spite of reasonable performance on MDA task, they do not take into consideration intra-domain weightings among different training samples, so that underlying noisy source data may hurt the performance of learning in the target, which is referred to as "negative transfer" <ref type="bibr" target="#b30">[31]</ref>.</p><p>To address the aforementioned limitations, we propose a novel method named T-SVDNet which incorporates tensor singular value decomposition into a neural network's training pipeline. In MDA tasks, although there is large domain gap between different domains, data belonging to the same category do share essential semantic information across domains. Therefore, we assume that data from different domains should follow a certain kind of category-wise structure. Based on this assumption, we explore high-order relationships among multiple domains and categories in order to enforce the alignment of source and target at the prototypical correlation level. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor which is obtained by stacking up a set of prototypical similarity matrices, so that the relationships between categories are enforced to be consistent across domains by pursuing the lowest-rank structure of tensor. Furthermore, to avoid negative transfer <ref type="bibr" target="#b30">[31]</ref> caused by noisy training data, we propose a novel uncertainty-aware weighting strategy to guide the adaptation process. It could dynamically assign weights to different domains and training samples based on the result of uncertainty estimation. To train the whole framework with both classification loss and low-rank regularizer, we adopt an alternative optimization strategy, that is, optimizing network parameters with the low-rank tensor fixed and optimizing the low-rank tensor with network parameters unchanged. We conduct extensive evaluations on several public benchmark datasets, where a significant improvement over existing MDA methods has been achieved. Overall, the main contributions of this paper can be summarized as follows:</p><p>? We propose the T-SVDNet to explore high-order relationships among multiple domains and categories from the perspective of tensor, which facilitates both domain-invariance and category-discriminability.</p><p>? We devise a novel uncertainty-aware weighting strategy to balance different source domains and samples, so that clean data are fully exploited while negative transfer led by noisy data is avoided.</p><p>? We propose an alternative optimization method to train the deep model along with low-rank regularizer. Extensive evaluations on benchmark datasets demonstrate the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-source Domain Adaptation (SDA). SDA aims to generalize a model learned from a labeled source domain to a related unlabeled domain with different data distribution. Existing SDA methods usually incorporate two terms: one term is task loss like cross-entropy loss which helps learn a model on the labeled source; the other adaptation term aims to align the distributions of source and target domains. These SDA methods can be roughly categorized into three groups according to the alignment strategies: (1) discrepancy-based methods aim to minimize the discrepancy which is explicitly measured on corresponding layers, including Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b25">[26]</ref>, correlation alignment <ref type="bibr" target="#b35">[36]</ref>, and contrastive domain discrepancy <ref type="bibr" target="#b16">[17]</ref>; (2) Some adversarial-based methods align different data distributions by confusing a well-trained domain discriminator <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>. In addition, adversarial generative methods aggregate domains at pixel level by generating adapted fake data <ref type="bibr" target="#b46">[47]</ref>; (3) Reconstruction-based methods propose to reconstruct the target domain from latent representation by using the source task model <ref type="bibr" target="#b11">[12]</ref>.</p><p>Multi-source Domain Adaptation (MDA). In practical applications, data may be collected from multiple related domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>, which involve more abundant information but also bring the difficulty in handling the domain shift. Thus MDA methods become more and more popular. The earlier MDA methods mainly focus on weighted combination of source classifiers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> based on the assumption that target distribution can be approximated by the mixture of source distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>. Hoffman et al. <ref type="bibr" target="#b13">[14]</ref> cast distribution combination as a DC-programming and derived a tighter domain generalization bound. Besides classification losses, various domain assignment constraints are devised to reduce the domain gap. In addition to minimizing domain discrepancy between the target and each source domain, Li et al. <ref type="bibr" target="#b24">[25]</ref> also took into consideration the relationships between pairwise source domains and proposed a tighter bound on the discrepancy among multiple sources. Many explicit measures of discrepancy have been used in MDA methods, such as MMD <ref type="bibr" target="#b12">[13]</ref>, L 2 distance <ref type="bibr" target="#b33">[34]</ref>, and moment distance <ref type="bibr" target="#b32">[33]</ref>. Some approaches also focus on prototype-based alignment between different domains <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>. As for the adversarial MDA methods which aim to confuse the discriminator so that domaininvariant features are extracted, the optimized objective can be H-divergence <ref type="bibr" target="#b43">[44]</ref>, Wasserstein distance <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Uncertainty Estimation. Quantifying and measuring uncertainty is of great theoretical and practical significance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. In Bayesian modeling, there are two main categories of uncertainty <ref type="bibr" target="#b17">[18]</ref>: epistemic uncertainty and aleatoric uncertainty. The former is often referred to as model uncertainty, which captures uncertainty in the model parameters, while the latter accounts for noise inherent from the observations. There have been many methods proposed to estimate uncertainty in deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. Resorting to these techniques, the robustness and interpretability of many computer vision tasks are improved, such as object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref> and face recognition <ref type="bibr" target="#b5">[6]</ref>.  Given M labeled source domains S1, ? ? ? , SM and an unlabeled target domain T , we first extract features for input images and compute prototype f D c for each category and each domain in an online fashion. Furthermore, the relations between pairwise prototypes are modeled by a group of similarity matrices G S 1 , ? ? ? , G S M , G T . Then we stack these prototypical similarity matrices into a 3-order tensor G ? R C?C?(M +1) on which tensor-low-rank constraint is imposed in order to explore highorder relationships among different domains. Finally, together with low-rank regularizer, the model is effectively trained in an alternative optimization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In the MDA setting, there are M labeled source domains S 1 , S 2 , ? ? ? , S M and an unlabeled target domain</p><formula xml:id="formula_0">T . Each source domain S m contains N m observations {(x Sm i , y Sm i )} Nm i=1 ,</formula><p>where y i is the desired label, while in the target domain T , the label y is not available. Most existing MDA models can be formulated as the following mapping function:</p><formula xml:id="formula_1">M mda : X S1 ? ? ? ? X S M ? X T ? Y S1 ? ? ? ? Y S M , (1)</formula><p>where M mda is trained on both labeled samples (X S , Y S ) in the source domain and unlabeled samples X T in the target domain.</p><p>In this section, we propose the T-SVDNet which fully explores high-order relationships among all domains by exploiting the tensor obtained by stacking up a set of prototypical similarity matrices (see <ref type="figure" target="#fig_1">Fig. 1</ref>). In addition, we propose a novel uncertainty-aware weighting strategy to achieve both inter-and intra-domain weightings so that negative transfer is reduced (see <ref type="figure" target="#fig_2">Fig. 2</ref>). This section is organized as follows: we first construct prototypical similarity matrix in Sec. 3.1. Then we propose the tensor-lowrank constraint and uncertainty-aware weighting strategy in Sec. 3.2 and Sec. 3.3, respectively. Finally, we formulate the total objective function in Sec. 3.4 and propose a novel alternative optimization method in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prototypical similarity matrix</head><p>In the proposed T-SVDNet, we first map input image into latent space through a feature extractor denoted by f (?), then we update the centroid of each category (prototype) based on the feature embeddings of a mini-batch <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>. For domain D ? {S 1 , ? ? ? , S M , T }, the prototype of the c-th category denoted by f D c is computed by:</p><formula xml:id="formula_2">f D c = 1 |? D c | (xi,yi)?? D c f (x i ),<label>(2)</label></formula><p>where ? D c is the set of training samples belonging to the cth category in domain D, i.e., ? D c = {(x i , y i ) ? D|y i = c}. It is noteworthy that for unlabeled target domain, we first assign pseudo labels? i to samples with high classification confidence. Specifically, we first map each image in target domain x T i into a classification probability vector p i , then we set a threshold ? for selecting confident predictions as pseudo labels?</p><formula xml:id="formula_3">(k) i , i.e., y (k) * i = 1, if k = argmax c p (c) i and p (k) i &gt; ? 0, otherwise .<label>(3)</label></formula><p>In order to reduce the randomness in sampling of each mini-batch and stabilize the training process, the category prototypes are updated according to exponential moving average (EMA) method:</p><formula xml:id="formula_4">f D c | I := ?f D c | I + (1 ? ?)f D c | I?1 ,<label>(4)</label></formula><p>where ? is the exponential decay rate and I denotes current iteration.</p><p>Then we employ Gaussian kernel to model inter-class relationships and construct a series of prototypical similarity matrices G S1 , ? ? ? , G S M , G T :</p><formula xml:id="formula_5">G D ci,cj = K(f D ci , f D cj ) = exp(? f D ci ? f D cj 2 2 2? 2 ),<label>(5)</label></formula><formula xml:id="formula_6">Algorithm 1: T-SVD Input : G ? R n1?n2?n3 ; Output: U, S, V ; 1 G f = FFT(G, 3); 2 for k = 1 : n 3 do 3 [U (k) f , S (k) f , V (k) f ] = SVD(G (k) f ); 4 end 5 U, S, V = IFFT(U f , 3), IFFT(S f , 3), IFFT(V f , 3) ;</formula><p>where f D ci and f D cj are a pair of category centroids from domain D, and ? is the deviation parameter which is set as 0.05 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tensor-low-rank constraint via T-SVD</head><p>Unlike conventional methods only considering pairwise matching, we achieve high-order alignment of all domains at the prototypical correlation level. Specifically, we stack prototypical similarity matrices into a 3-order tensor G ? R C?C?(M +1) along the third dimension, where C and M denote the number of classes and domains, respectively. Then we impose the Tensor-Low-Rank (TLR) constraint on the assembled tensor in order to explore high-order correlations among domains and enforce the relationships between categories to be consistent across domains. Here we first give definitions of T-SVD and tensor rank as follows: definition 1 (T-SVD) Given tensor G ? R n1?n2?n3 , the tensor singular value decomposition of G is defined as a finite sum of outer product of matrices <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_7">G = min(n1,n2) i=1 U(:, i, :) * S(i, i, :) * V(:, i, :) T , (6)</formula><p>where U and V are orthogonal tensors with size n 1 ?n 1 ?n 3 and n 2 ? n 2 ? n 3 , respectively. S is a tensor with the size n 1 ? n 2 ? n 3 , each frontal slice of which is a diagonal matrix. * denotes tensor product (T-product).</p><p>T-SVD also can be computed more efficiently in the Fourier domain. Specifically, it can be replaced by conducting fast Fourier transformation (FFT) along the third dimension of G to get G f , and performing matrix SVDs on each frontal slice of G f :</p><formula xml:id="formula_8">G (k) f = U (k) f ? S (k) f ? V (k)T f , k = 1, ? ? ? , n 3<label>(7)</label></formula><p>where ? means matrix product. We use G definition 2 (Tensor rank) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> The rank of G ? R n1?n2?n3 is a vector p ? R n3?1 with the k-th element equal to the rank of the k-th frontal slice of G</p><formula xml:id="formula_9">(k) f .</formula><p>However, we need an adequate convex relaxation to 1 norm of tensor rank in optimization process. To this end we formulate it as tensor nuclear norm, which is defined as the sum of the singular values of all frontal slices S (k) f :</p><formula xml:id="formula_10">G T N N = n3 k=1 min(n1,n2) i=1 |S (k) f (i, i)|.<label>(8)</label></formula><p>Tensor rotation. In view of each frontal slice G (k) of tensor G only contains information from single domain, we rotate it horizontally (or vertically) to obtain G Rot (see <ref type="figure" target="#fig_1">Fig. 1</ref>). In this way, each frontal slice G (k)</p><p>Rot will involve information from different domains. In the end, imposing tensorlow-rank constraint on the rotated tensor G Rot benefits the exploration of high-order relationships among different domains. The inter-category data structure is enforced to be consistent across domains by pursuing the lowest rank of each frontal slice G </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Uncertainty-aware weighting strategy</head><p>Instead of equally treating each source domain and sample, we propose a novel uncertainty-aware weighting strategy to adaptively balance different sources and alleviate negative transfer led by noisy data. Considering that data uncertainty could capture the noise inherent in the data, i.e., it reflects the reliability of output <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref>, we could weigh different sources and samples based on the result of uncertainty estimation. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, g ? (?) and g ? (?) serve on a classifier and uncertainty predictor, respectively. The output of network is modeled as a Gaussian distribution parameterized by mean ? and variance ?. Specifically, the mean is acted by original feature vector, while the variance quantifies uncertainty of training samples. For regression tasks, the Gaussian likelihood is defined as:</p><formula xml:id="formula_11">p(y i |x i ) = N (? i , ? 2 i ),<label>(9)</label></formula><formula xml:id="formula_12">with ? i = h(x i ) = f (x i ) ? g ? (x i ) and ? i = f (x i ) ? g ? (x i ).</formula><p>For classification task, we often squash the model output through a softmax function and obtain a scaled classification likelihood:</p><formula xml:id="formula_13">p(y i |x i , ? i ) = Sof tmax 1 ? 2 i h(x i ) .<label>(10)</label></formula><p>This can be interpreted as a Boltzmann distribution (Gibbs distribution) and ? 2 i works as temperature for re-scaling input. The log likelihood of output is:</p><formula xml:id="formula_14">log p(yi = c|xi, ?i) = 1 ? 2 i hc(xi) ? log c =c exp( 1 ? 2 i h c (xi)),<label>(11)</label></formula><p>where h c (x i ) denotes the c-th element of vector h(x i ).</p><p>Then the total classification loss is defined as:</p><formula xml:id="formula_15">L cls (?) = ? 1 M M m=1 1 Nm Nm i=1 log p(yi = c|xi, ?i) = 1 M M m=1 1 Nm Nm i=1 1 ? 2 i LCE(?) + log c exp 1 ? 2 i h c (xi) c exp h c (xi) 1 ? 2 i ? 1 M M m=1 1 Nm Nm i=1 1 ? 2 i LCE(?) + log?i,<label>(12)</label></formula><p>where L CE (?) denotes classification cross entropy loss with h(x i ) not scaled. M and N m denote the number of domains and samples, respectively. log? i prevents ? from getting too large. Noisy data with large uncertainty would be assigned less weights, i.e., 1/? 2 . The derivation process of Eq. 12 is provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective function</head><p>The overall objective function of the proposed model is as follows:</p><formula xml:id="formula_16">L total = L cls (?) + ? G , s.t. G = ? R (G S1 , ? ? ? , G S M , G T ),<label>(13)</label></formula><p>where denotes two operations: tensor rotation and tensor nuclear norm, and ? R represents the operation of stacking up all the domain-specific prototypical similarity matrices into a tensor. ? denotes neural network parameters. The first term is classification loss and the second term imposes TLR constraint on the stacked tensor, aiming at achieving high-order alignment of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization of T-SVDNet</head><p>The optimization of T-SVDNet is presented in Alg. 2. In order to make the problem tractable, we introduce an auxiliary variable and alternatively update it along with the network parameters till convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Optimization of T-SVDNet</head><p>Input : Training data S 1 , ? ? ? , S M , T ; Output: Model parameters ? of T-SVDNet ;</p><formula xml:id="formula_17">1 for iter = 1 to max iter do 2 ? Updating ? 3 ? f ? ? f ? ?(L cls + ? 2 A ? G 2 F )/?? f ; 4 ? ? ? ? ? ? ?L cls /?? ? ; ? ? ? ? ? ? ?L cls /?? ? ; 5 ? Updating A 6 G Rot = Rotate(G); 7 G f = FFT(G Rot , 3); 8 for k = 1 : n 3 do 9 [U (k) f , S (k) f , V (k) f ] = SVD(G (k) f ); 10 A (k) f = U (k) f ? D ? ? (S (k) f ) ? V (k)T f ; 11 end 12 A Rot = IFFT(A f , 3); 13 A = Rotate(A Rot ) ; 14 ? = min(??, ? max ) 15 end</formula><p>Auxiliary variable. To optimize the objective function in Eq. 13, we first introduce an auxiliary tensor A to replace G, which converts the original optimization problem into the following one:</p><formula xml:id="formula_18">min ?,A L cls + ? A + ? 2 A ? G 2 F ,<label>(14)</label></formula><p>where ? is a penalty parameter. It starts from a small initial positive scalar ? 0 , and gradually increases to the maximum truncated value ? max with iterations, i.e., it is updated by ? = min(??, ? max ), where ? represents the rate of increase which is set as 1.1 in all experiments. The reason why we update ? in such a incremental fashion is that randomly initialized A may lead to the wrong direction of gradient descent at the beginning of training process. Update of network parameters ?. The parameters of feature extractor ? f , classifier ? ? , and uncertainty predictor ? ? are updated through gradient descent with auxiliary tensor A fixed.</p><p>Update of auxiliary variable A. When network parameters are fixed, we optimize the subproblem associated with A as follows:</p><formula xml:id="formula_19">min A ? A + ? 2 A ? G 2 F .<label>(15)</label></formula><p>We solve this problem in Fourier domain with basic procedure similar to Alg. </p><formula xml:id="formula_20">A (k) f = U (k) f ? D ? ? (S (k) f ) ? V (k)T f ,<label>(16)</label></formula><formula xml:id="formula_21">where D ?/? (S (k) f ) = S (k) f ? J (k) f is singular value shrinkage operator. J (k) f is a diagonal matrix with the i-th diagonal element to be J (k) f (i, i) = (1 ? ? ?S (k) f (i,i) )+. Finally, updated</formula><p>A is obtained by inverse fast Fourier transform from A f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we perform extensive evaluations on several benchmark datasets with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Digits-Five <ref type="bibr" target="#b15">[16]</ref> contains 5 different domains including MNIST (mt), MNIST-M (mm), SVHN (sv), USPS (up), and Synthetic Digits (syn). Each domain consists of 10 numerals from '0' to '9'. Previous methods only use a subset of samples in each domain, i.e., 25000 training data and 9000 testing data. But we find that there will be further performance gain if all data are employed for training. For a fair comparison, we report the results on both settings (T-SVDNet part and T-SVDNet all in Tab. 1).</p><p>PACS <ref type="bibr" target="#b23">[24]</ref> is a small-scale multi-domain dataset containing 9991 images from 4 domains: photo (P), artpainting (A), cartoon (C), sketch (S) whose styles are different. These domains share the same seven categories.</p><p>DomainNet <ref type="bibr" target="#b32">[33]</ref> is a large-scale dataset for Multi-Source Domain Adaptation. Due to the great number of categories and samples (345 categories, around 0.6 million images) and large domain shift. DomainNet is by far the most difficult dataset which contains 6 different domains: clipart (clp), infograph (inf), painting (pnt), quickdraw (qdr), real (rel), and sketch (skt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared methods</head><p>For all experiments, we compare our method with stateof-the-art single-source and multi-source domain adapta-  tion algorithms. Specifically, two strategies are adopted to train the single-source model: Single Best and Source Combination. The former reports the best result among all domains, while the latter simply combines all source domains together. Overall, these compared methods can be roughly categorized into two main groups: (1) adversarial-based methods include Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b10">[11]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b38">[39]</ref>, Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b34">[35]</ref>, Deep Cocktail Network (DCTN) <ref type="bibr" target="#b41">[42]</ref>, Adversarial Multiple Source Domain Adaptation (MDAN) <ref type="bibr" target="#b43">[44]</ref> and Multi-Source Distilling Domain Adaptation (MDDA) <ref type="bibr" target="#b45">[46]</ref>; (2) another typical strategy is discrepancy minimization, the representative methods involve Deep Adaptation Network (DAN) <ref type="bibr" target="#b25">[26]</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b27">[28]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b26">[27]</ref>, Correlation Alignment (CORAL) <ref type="bibr" target="#b35">[36]</ref>, and Moment Matching for Multi-Source Domain Adaptation (M 3 SDA) <ref type="bibr" target="#b32">[33]</ref>. Source-Only directly transfers the model trained in source domain to target domain. For a fair comparison, we use the same model architecture and data pre-processing routines as compared methods in all experiments. More implementation details are provided in supplementary materials.</p><formula xml:id="formula_22">Methods ? A ? C ? S ? P Avg</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results</head><p>The results on Digits-Five are shown in Tab.  average accuracy, around 5.25% higher than the second best method MDDA. In particular, a performance improvement about 11.42% and 5.21% over MDDA is achieved on '? mm' and '? syn' tasks, respectively. The performance will be further boosted to 93.94% if all training data is used, outperforming other algorithms by a large margin.</p><p>The results on PACS are shown in Tab. 2. Our method T-SVDNet achieves the best performance on all domains and gets 91.25% average accuracy, outperforming the second best method MDDA by 5.14%. Especially on '? S' task, our method achieves a 7.93% performance gain over MDDA.</p><p>The experimental results on DomainNet are reported in Tab. 3. Overall, T-SVDNet achieves the best performance on five out of six tasks. It obtains average accuracy of 47.0% on six domains and ranks the first in the list, with 3.8% performance improvement over MDDA, which is mainly attributed to the thorough exploration of high-order relations between different domains and categories. It is noteworthy that the performances of many MDA methods drop obviously compared to Single Best on '? qdr' task due to negative transfer, while our method still attains better performance because of uncertainty-aware weighting strategy. Negative transfer is avoided by filtering out noisy source samples near decision boundaries for training, while clean data with low noise intensity are fully exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>Ablation study. We further validate the effects of some key components in our framework. Tab. 4 shows the re- sults of controlled experiments on Digits-Five dataset. As a reference, we report the performance of Source-Only that directly transfers the model trained on source domains to target domain. For convenience, '+E', '+T', '+U' denote entropy minimization constraint on target domain, tensorlow-rank constraint, and uncertainty-aware weighting, respectively. In general, we have the following observations according to Tab. 4: (1) Entropy minimization boosts performance obviously due to the exploitation of target domain; (2) It is remarkable that Tensor-Low-Rank constraint significantly improves the performance by 14.91% on '? mm' task. This is attributed to the high-order alignment between different domains and the extraction of domaininvariant features; (3) Uncertainty-aware weighting strategy further improves the performance by 1.26% on average, which suggests that our model is able to learn more transferable features across domains.   The effect of TLR constraint. We compute tensor nuclear norm (TNN) which is usually used as an approximate measure of tensor rank. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, we compare the TNN curves w/ and w/o TLR constraint. We find that TNN w/ TLR drops significantly during the first several epochs and stabilizes at around 46, while the baseline w/o TLR drops slowly and becomes stable earlier. This demonstrates that our proposed TLR constraint is effective and brings large performance improvement.</p><p>Feature visualization. To demonstrate the transfer ability of our model, we visualize the feature embeddings of different models on '? C' task on PACS. As shown in <ref type="figure" target="#fig_6">Fig. 4 (a)</ref>, the target features learned by Source-Only almost mismatch with source domain and different classes in target domain are entirely mixed up. Compared to M3SDA and Source-Only, our method produces clusters with clearer boundaries, which suggests that T-SVDNet possesses better transfer ability on target and is able to eliminate domain dis- crepancy without sacrificing discrimination ability.</p><p>Visualizations of similarity matrices. To further validate the effect of TLR constraint, we visualize the prototypical similarity matrices of three domains on Digits-Five dataset in <ref type="figure" target="#fig_7">Fig. 5</ref>. Compared to the baseline without TLR constraint (the top row), our method (the bottom row) could capture clearer category-wise data structure. Specifically, matrices in the bottom row contain less domain-specific noise, because we search for a lowest-rank structure of tensor and enforce prototypical correlations to be consistent across domains. Especially on the target domain (MNIST-M), the noise is reduced by a large margin compared to Source-Only. These results indicate the effectiveness of TLR constraint on aligning source and target domains.</p><p>Uncertainty estimation. We conduct qualitative and quantitative experiments to demonstrate the ability of model to measure noise intensity (data uncertainty).</p><p>(1) Inter-domain weighting. The uncertainty distributions of different domains on '?mm' task are shown in <ref type="figure" target="#fig_8">Fig. 6 (a)</ref>. Overall, the estimated uncertainty is highly correlated with domain quality. e.g., the uncertainty distribution of high-quality domain MNIST (blue curve) is more concentrated than low-quality domain SVHN (green curve). This validates that our model is able to measure the quality of domain, and guide the combination of data distributions.</p><p>(2) Intra-domain weighting. To demonstrate the ability of our model to capture noise inherent in data, we add different proportions of Gaussian noise to images and plot the estimated uncertainty distributions in <ref type="figure" target="#fig_8">Fig. 6 (b)</ref>. Specifically, we add noise sampled from Gaussian distribution N (0, I) to original images, i.e.,x i = x i + r i , where denotes noise and r controls the intensity of noise. According to <ref type="figure" target="#fig_8">Fig. 6 (b)</ref>, when noise intensity is small (r = 0.1), the curves of noisy and clean samples (r = 0) are highly overlapped. However, with the increase of noise intensity (r = 0.5, 1), the uncertainty distributions get more dispersed. This demonstrates that our model could accurately evaluate intra-domain data quality, so that noisy samples would be assigned less weights and negative transfer would be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose the T-SVDNet for multi-source domain adaptation, which is featured by incorporating tensor singular value decomposition into neural network training process. Category-wise relations are modeled by prototypical similarity matrix, aiming at capturing complex data structure. Furthermore, high-order relations between different domains are fully explored by imposing tensor-low-rank constraint on the tensor stacked by domain-specific similarity matrices. In addition, a novel uncertainty-aware weighting strategy is proposed to combine data distributions of different domains, which reduces negative transfer led by noisy data. We adopt alternative optimization algorithm to train T-SVDNet efficiently. Extensive experiments on three public benchmark datasets demonstrate the favorable performance against state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FFT</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The framework of T-SVDNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>(k) f to denote the k-th frontal slice of G, i.e., G (k) f = G(:, :, k). The result of T-SVD is finally obtained by taking the inverse FFT on U f , S f , V f along the third dimension (see Alg. 1). Uncertainty-aware weighting strategy. Each sample is modeled as a Gaussian distribution parameterized by mean ? and variance ?. The classification loss is weighted by estimated data uncertainty ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Tensor nuclear norm and classification accuracy curves on "? mm" task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>The t-SNE visualizations of feature embeddings on '?C' task on PACS. The top row represents category information (each color denotes a class). The bottom row represents domain information (red: source domain; purple: target domain).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visualizations of similarity matrices on '?mm' task on Digits-Five. The top row denotes the model w/o TLR constraint, and the bottom row is T-SVDNet. Blue and green represent source and target domain, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>(a) Uncertainty distributions of different domains on Digits-Five. (b) Uncertainty distribution of single domain varies with the increasing noise intensity r on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>1. We first transform tensor G Rot to Fourier domain G f , and perform matrix SVD on each k-th frontal slice G SVDNet part 90.05 ? 0.91 99.24 ? 0.08 98.61 ? 0.16 84.03 ? 1.22 94.92 ? 0.17 93.37 T-SVDNet all 91.22 ? 0.74 99.28 ? 0.11 98.63 ? 0.22 84.86 ? 1.47 95.71 ? 0.30 93.94 Classification results on Digits-Five. The top value is highlighted in blue bold font and the second best in green bold font.</figDesc><table><row><cell>(k) f</cell><cell>and obtain the U</cell><cell cols="2">(k) f , S f , V (k) f . Then, (k)</cell></row><row><cell cols="4">each frontal slice A (k) f of auxiliary variable can be updated</cell></row><row><cell cols="3">by shrinkage operation [5, 21] on G</cell><cell>(k) f</cell><cell>in the Fourier do-</cell></row><row><cell cols="2">main defined as follows:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification results on PACS. The top value is highlighted in blue bold font and the second best in green bold font.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1. Overall, our method tops the list in all domains and achieves 93.37% ? 0.6 16.2 ? 0.4 42.1 ? 0.7 29.7 ? 0.9 28.6</figDesc><table><row><cell>Standards</cell><cell>Methods</cell><cell>?clp</cell><cell>?inf</cell><cell>?pnt</cell><cell>?qdr</cell><cell>?rel</cell><cell>?skt</cell><cell>Avg</cell></row><row><cell></cell><cell cols="2">Source-Only 39.6 ? 0.6</cell><cell>8.2 ? 0.8</cell><cell>33.9 ? 0.6</cell><cell>11.8 ? 0.7</cell><cell>41.6 ? 0.8</cell><cell cols="2">23.1 ? 0.7 26.4</cell></row><row><cell>Single-Best</cell><cell cols="4">DAN 33.3 RTN 39.1 ? 0.5 11.4 ? 0.8 35.3 ? 0.7 10.7 ? 0.6 31.7 ? 0.8 JAN 35.3 ? 0.7 9.1 ? 0.6 32.5 ? 0.7 ADDA 39.5 ? 0.8 14.5 ? 0.7 29.1 ? 0.8</cell><cell>13.1 ? 0.7 14.3 ? 0.6 14.9 ? 0.5</cell><cell>40.6 ? 0.6 43.1 ? 0.8 41.9 ? 0.8</cell><cell cols="2">26.5 ? 0.8 26.3 25.7 ? 0.6 26.7 30.7 ? 0.7 28.4</cell></row><row><cell></cell><cell>DANN</cell><cell>37.9 ? 0.7</cell><cell>11.4 ? 0.9</cell><cell>33.9 ? 0.6</cell><cell>13.7 ? 0.6</cell><cell>41.5 ? 0.7</cell><cell cols="2">28.6 ? 0.6 27.8</cell></row><row><cell></cell><cell>MCD</cell><cell>42.6 ? 0.3</cell><cell>19.6 ? 0.8</cell><cell>42.6 ? 1.0</cell><cell>3.8 ? 0.6</cell><cell>50.5 ? 0.4</cell><cell cols="2">33.8 ? 0.9 32.2</cell></row><row><cell></cell><cell cols="2">Source-Only 47.6 ? 0.5</cell><cell>13.0 ? 0.4</cell><cell>38.1 ? 0.5</cell><cell>13.3 ? 0.4</cell><cell>51.9 ? 0.9</cell><cell cols="2">33.7 ? 0.5 32.9</cell></row><row><cell></cell><cell>DAN</cell><cell>45.4 ? 0.5</cell><cell>12.8 ? 0.9</cell><cell>36.2 ? 0.6</cell><cell>15.3 ? 0.4</cell><cell>48.6 ? 0.7</cell><cell cols="2">34.0 ? 0.5 32.1</cell></row><row><cell>Source</cell><cell>RTN</cell><cell>44.2 ? 0.6</cell><cell>12.6 ? 0.7</cell><cell>35.3 ? 0.6</cell><cell>14.6 ? 0.8</cell><cell>48.4 ? 0.7</cell><cell cols="2">31.7 ? 0.7 31.1</cell></row><row><cell>Combination</cell><cell>ADDA</cell><cell>47.5 ? 0.8</cell><cell>11.4 ? 0.7</cell><cell>36.7 ? 0.5</cell><cell>14.7 ? 0.5</cell><cell>49.1 ? 0.8</cell><cell cols="2">33.5 ? 0.5 32.2</cell></row><row><cell></cell><cell>JAN</cell><cell>40.9 ? 0.4</cell><cell>11.1 ? 0.6</cell><cell>35.4 ? 0.5</cell><cell>12.1 ? 0.7</cell><cell>45.8 ? 0.6</cell><cell cols="2">32.3 ? 0.6 29.6</cell></row><row><cell></cell><cell>MCD</cell><cell>54.3 ? 0.6</cell><cell>22.1 ? 0.7</cell><cell>45.7 ? 0.6</cell><cell>7.6 ? 0.5</cell><cell>58.4 ? 0.7</cell><cell cols="2">43.5 ? 0.6 38.5</cell></row><row><cell></cell><cell>MDAN</cell><cell>52.4 ? 0.6</cell><cell>21.3 ? 0.8</cell><cell>46.9 ? 0.4</cell><cell>8.6 ? 0.6</cell><cell>54.9 ? 0.6</cell><cell cols="2">46.5 ? 0.7 38.4</cell></row><row><cell></cell><cell>MDDA</cell><cell cols="4">59.4 ? 0.6 23.8 ? 0.8 53.2 ? 0.6 12.5 ? 0.6</cell><cell>61.8 ? 0.5</cell><cell cols="2">48.6 ? 0.8 43.2</cell></row><row><cell>Multi-</cell><cell>DCTN</cell><cell>48.6 ? 0.7</cell><cell>23.5 ? 0.6</cell><cell>48.8 ? 0.6</cell><cell>7.2 ? 0.5</cell><cell>53.5 ? 0.6</cell><cell cols="2">47.3 ? 0.5 38.2</cell></row><row><cell>Source</cell><cell>M 3 SDA</cell><cell cols="3">58.6 ? 0.5 26.0 ? 0.9 52.3 ? 0.6</cell><cell>6.3 ? 0.6</cell><cell cols="3">62.7 ? 0.5 49.5 ? 0.8 42.6</cell></row><row><cell></cell><cell>T-SVDNet</cell><cell cols="7">66.1 ? 0.4 25.0 ? 0.8 54.3 ? 0.7 16.5 ? 0.9 65.4 ? 0.5 54.6 ? 0.6 47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Classification results on DomainNet. The top value is highlighted in blue bold font and the second best in green bold font. ? 0.41 99.16 ? 0.26 98.09 ? 0.14 82.94 ? 0.90 94.47 ? 0.62 92.68 5.85 ? T-SVDNet (+E+T+U) 91.22 ? 0.74 99.28 ? 0.11 98.63 ? 0.22 84.86 ? 1.47 95.71 ? 0.30 93.94 7.11 ? Ablation study on key components of model on Digits-Five.</figDesc><table><row><cell>Methods</cell><cell>? mm</cell><cell>? mt</cell><cell>? up</cell><cell>? sv</cell><cell>? syn</cell><cell>Avg</cell><cell>Gain</cell></row><row><cell>Source-Only</cell><cell cols="6">67.25 ? 0.81 98.88 ? 0.49 97.87 ? 0.43 77.76 ? 0.92 92.41 ? 0.57 86.83</cell><cell>?</cell></row><row><cell>T-SVDNet (+E)</cell><cell cols="7">73.85 ? 0.84 98.96 ? 0.35 97.87 ? 0.65 77.86 ? 0.84 92.44 ? 0.39 88.19 1.36 ?</cell></row><row><cell>T-SVDNet (+E+T)</cell><cell>88.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary</head><p>7.1. Relevant Definitions definition 3 (Tensor product) The tensor product C of A ? R n1?n2?n3 and B ? R n2?n4?n3 , i.e., C = AB, is a tensor of size n 1 ? n 4 ? n 3 , each (i, j)-th tube of which denoted by C(i, j, :) with i = 1, 2, ? ? ? , n 1 and j = 1, 2, ? ? ? , n 4 is given by:</p><p>where ? denotes the circular convolution between two vectors. Tensor product in the original domain can be replaced by matrix multiplication of frontal slices in the Fourier domains as follows:</p><p>definition 4 (Tensor transpose) For A ? R n1?n2?n3 , the transpose of A denoted by A T ? R n2?n1?n3 can be obtained by transposing each frontal slice of A and reversing the order of the transposed slices along the third dimension.</p><p>definition 5 (Identity tensor) The identity tensor I ? R n1?n1?n3 is a tensor whose first frontal slice is the n 1 ?n 1 identity matrix and all other frontal slices are zeros.</p><p>where * is the tensor product.</p><p>definition 7 (f-diagonal tensor) The f-diagonal tensor is a tensor each frontal slice of which is diagonal matrix. The tensor product of two f-diagonal tensors with the same size n 1 ? n 2 ? n 3 is also a tensor with the same size, each i-th (i = 1, ? ? ? , min(n 1 , n 2 )) diagonal tube of which is: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Derivation of Eq. 12</head><p>First, the cross entropy loss L CE is denoted as:</p><p>then we obtain the following derivation process pf Eq. 12:</p><p>here we introduce a simplifying assumption in the last transition:</p><p>which becomes equality when ? i ? 1. This assumption simplifies the optimization objective, while the performance is improved empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Implementation Details</head><p>Overall, for fair comparisons, we use the same model architecture and data pre-processing routines as compared methods in all experiments. Specifically, we present the detailed parameter settings on three datasets in Tab. 5. Parameters ? and ? are set as 0.05 and 1.1 in all experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multisource iterative adaptation for cross-domain classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Himanshu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3691" to="3697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5710" to="5719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the treatment of uncertainties and probabilities in engineering decision analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael Havbro</forename><surname>Faber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Offshore Mechanics and Arctic Engineering-transactions of The Asme</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;16 Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Algorithms and theory for multiple-source adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningshan</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8246" to="8256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="550" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17 Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5580" to="5590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><forename type="middle">Elena</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">S</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">C</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="172" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Aleatory or epistemic? does it matter? Structural Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ove</forename><surname>Dalager Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10285" to="10295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Extracting relationships by multi-domain matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawson</forename><surname>Murias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6798" to="6809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tensor robust principal component analysis with a new tensor nuclear norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="925" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An order-p tensor factorization with applications in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Betsy</forename><surname>Larue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2239" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised multi-source domain adaptation driven by deep adversarial ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Rakshit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biplab</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasis</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="485" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Correlation alignment for unsupervised domain adaptation. Domain Adaptation in Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A survey of multi-source domain adaptation. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to combine: Knowledge aggregation for multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="727" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5423" to="5432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">Paulo</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multisource domain adaptation in the deep learning era: A systematic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12169</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-source distilling domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12975" to="12983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
