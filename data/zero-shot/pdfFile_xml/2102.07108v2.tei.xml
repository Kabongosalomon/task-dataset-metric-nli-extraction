<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CATE: Computation-aware Neural Architecture Encoding with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">CATE: Computation-aware Neural Architecture Encoding with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works <ref type="bibr" target="#b65">(White et al., 2020a;</ref><ref type="bibr" target="#b73">Yan et al., 2020)</ref> demonstrate the importance of architecture encodings in Neural Architecture Search (NAS). These encodings encode either structure or computation information of the neural architectures. Compared to structure-aware encodings, computation-aware encodings map architectures with similar accuracies to the same region, which improves the downstream architecture search performance <ref type="bibr" target="#b65">White et al., 2020a)</ref>. In this work, we introduce a Computation-Aware Transformer-based Encoding method called CATE. Different from existing computation-aware encodings based on fixed transformation (e.g. path encoding), CATE employs a pairwise pre-training scheme to learn computation-aware encodings using Transformers with cross-attention. Such learned encodings contain dense and contextualized computation information of neural architectures. We compare CATE with eleven encodings under three major encoding-dependent NAS subroutines in both small and large search spaces. Our experiments show that CATE is beneficial to the downstream search, especially in the large search space. Moreover, the outside search space experiment demonstrates its superior generalization ability beyond the search space on which it was trained. Our code is available at: https://github.com/MSU-MLSys-Lab/CATE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) has recently drawn considerable attention <ref type="bibr" target="#b14">(Elsken et al., 2019)</ref>. While majority of the prior work focuses on either constructing new search spaces <ref type="bibr" target="#b46">Radosavovic et al., 2020;</ref><ref type="bibr">Ru et al., 1</ref> Michigan State University 2 University of Central Florida 3 Tencent AI Lab.</p><p>Correspondence to: Shen Yan &lt;yan-shen6@msu.edu&gt;.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 2020) or designing efficient architecture search and evaluation methods <ref type="bibr" target="#b35">(Luo et al., 2018b;</ref><ref type="bibr" target="#b53">Shi et al., 2020;</ref><ref type="bibr">White et al., 2021)</ref>, some of the most recent work <ref type="bibr" target="#b65">(White et al., 2020a;</ref><ref type="bibr" target="#b73">Yan et al., 2020)</ref> sheds light on the importance of architecture encoding on the subroutines in the NAS pipeline as well as on the overall performance of NAS.</p><p>While existing NAS methods use diverse architecture encoders such as LSTM <ref type="bibr" target="#b35">Luo et al., 2018b)</ref>, SRM <ref type="bibr" target="#b1">(Baker et al., 2018)</ref>, MLP <ref type="bibr" target="#b61">Wang et al., 2020)</ref>, GNN <ref type="bibr" target="#b64">(Wen et al., 2020;</ref><ref type="bibr" target="#b53">Shi et al., 2020;</ref><ref type="bibr" target="#b73">Yan et al., 2020)</ref> or adjacency matrix itself <ref type="bibr" target="#b19">(Kandasamy et al., 2018;</ref><ref type="bibr" target="#b50">Real et al., 2019;</ref><ref type="bibr" target="#b66">White et al., 2020b)</ref>, these encoders encode either structures <ref type="bibr" target="#b35">(Luo et al., 2018b;</ref><ref type="bibr" target="#b75">Ying et al., 2019;</ref><ref type="bibr" target="#b61">Wang et al., 2020;</ref><ref type="bibr" target="#b64">Wen et al., 2020;</ref><ref type="bibr" target="#b53">Shi et al., 2020;</ref><ref type="bibr" target="#b73">Yan et al., 2020)</ref> or computations <ref type="bibr" target="#b40">Ning et al., 2020b;</ref><ref type="bibr">White et al., 2021)</ref> of the neural architectures. Compared to structure-aware encodings, computation-aware encodings are able to map architectures with different structures but similar accuracies to the same region. This advantage contributes to a smooth encoding space with respect to the actual architecture performance instead of structures, which improves the efficiency of the downstream architecture search <ref type="bibr" target="#b65">White et al., 2020a)</ref>.</p><p>We argue that current architecture encoders limit the power of computation-aware architecture encoding for NAS. The major limitations lie in their representation power and the effectiveness of their pre-training objectives. Specifically,  uses shallow GRUs to encode computation, which is not sufficient to capture deep contextualized computation information. Moreover, their decoder is trained with the reconstruction loss via asynchronous message passing. This is very challenging in practice because directly learning the generative model based on a single architecture is not trivial. As a result, its pre-training is less effective and the downstream NAS performance is not as competitive as state-of-the-art structure-aware encoding methods. <ref type="bibr" target="#b65">(White et al., 2020a)</ref> proposes a computation-aware encoding method based on a fixed transformation called path encoding, which shows outstanding performance under the predictor-based NAS subroutine. However, path encoding scales exponentially without truncation and it inevitably causes information loss with truncation. Moreover, path encoding exhibits worse generalization performance in outside search space compared to the adjacency matrix encoding arXiv:2102.07108v2 <ref type="bibr">[cs.</ref>LG] 11 Jun 2021 since it could not generalize to unseen paths that are not included in the training search space.</p><p>In this work, we propose a new computation-aware neural architecture encoding method named CATE (Computation-Aware Transformer-based Encoding) that alleviates the limitations of existing computation-aware encoding methods. As shown in <ref type="figure">Figure 1</ref>, CATE takes paired computationally similar architectures as its input. Similar to BERT, CATE trains the Transformer-based model <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> using the masked language modeling (MLM) objective <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. Each input architecture pair is corrupted by replacing a fraction of their operators with a special mask token. The model is trained to predict those masked operators from the corrupted architecture pair. CATE differs from BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> in two aspects. First, each prediction in LMs has its inductive bias given the contextual information from different positions. This, however, is not the case in architecture representation learning since the prediction distribution is uniform for any valid graph, making it difficult to directly learn the generative model from a single architecture. Therefore, we propose a pairwise pre-training scheme that encodes computationally similar architecture pairs through two Transformers with shared parameters. The two individual encodings are then concatenated, and the concatenated encoding is fed into another Transformer with a cross-attention encoder to encode the joint information of the architecture pair. Second, the fully-visible attention mask <ref type="bibr" target="#b47">(Raffel et al., 2020)</ref> could not be used for architecture representation learning because it does not reflect the single-directional flow (e.g. directed, acyclic, single-in-single-out) of neural architectures <ref type="bibr" target="#b69">(Xie et al., 2019a;</ref><ref type="bibr" target="#b76">You et al., 2020a)</ref>. Therefore, instead of using a bidirectional Transformer encoder as in BERT, we directly use the adjacency matrix to compute the causal mask <ref type="bibr" target="#b47">(Raffel et al., 2020)</ref>. The adjacency matrix is further augmented with the Floyd algorithm <ref type="bibr" target="#b16">(Floyd, 1962)</ref> to encode the longrange dependency of different operations. Together with the MLM objective, CATE is able to encode the computation of architectures and learn dense and deep contextualized architecture representations that contain both local and global computation information in neural architectures. This is important for architecture encodings to be generalized to outside search space beyond the training search space.</p><p>We compare CATE with eleven structure-aware and computation-aware architecture encoding methods under three major encoding-dependent subroutines as well as eight NAS algorithms on NAS-Bench-101 <ref type="bibr" target="#b75">(Ying et al., 2019)</ref> (small), NAS-Bench-301 <ref type="bibr">(Siems et al., 2020) (large)</ref>, and an outside search space <ref type="bibr" target="#b65">(White et al., 2020a)</ref> to evaluate the effectiveness, scalability, and generalization ability of CATE. Our results show that CATE is beneficial to the downstream architecture search, especially in the large search space. Specifically, we found the strongest NAS performance in all search spaces using CATE with a Bayesian optimization-based predictor subroutine together with a novel computation-aware search. Moreover, the outside search space experiment shows its superior generalization capability beyond the search space on which it was trained. Finally, our ablation studies show that the quality of CATE encodings and downstream NAS performance are non-decreasingly improved with more training architecture pairs, more cross-attention Transformer blocks and larger dimension of the feed-forward layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Architecture Search (NAS). NAS has been started with genetic algorithms <ref type="bibr" target="#b37">(Miller et al., 1989;</ref><ref type="bibr" target="#b22">Kitano, 1990;</ref><ref type="bibr" target="#b57">Stanley &amp; Miikkulainen, 2002)</ref> and recently becomes popular when <ref type="bibr" target="#b82">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b0">Baker et al., 2017)</ref> gain significant attention. Since then, various NAS methods have been explored including sampling-based and gradient-based methods. Representative sampling-based methods include random search <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019)</ref>, evolutionary algorithms <ref type="bibr" target="#b32">Lu et al., 2020)</ref>, local search <ref type="bibr" target="#b41">(Ottelander et al., 2020;</ref><ref type="bibr" target="#b66">White et al., 2020b)</ref>, reinforcement learning <ref type="bibr" target="#b58">Tan et al., 2019)</ref>, Bayesian optimization <ref type="bibr" target="#b19">(Kandasamy et al., 2018;</ref><ref type="bibr" target="#b81">Zhou et al., 2019)</ref>, Monte Carlo tree search <ref type="bibr" target="#b38">(Negrinho &amp; Gordon, 2017;</ref><ref type="bibr" target="#b61">Wang et al., 2020)</ref> and Neural predictor <ref type="bibr" target="#b1">(Baker et al., 2018;</ref><ref type="bibr" target="#b27">Liu et al., 2018a;</ref><ref type="bibr" target="#b64">Wen et al., 2020;</ref><ref type="bibr" target="#b59">Tang et al., 2020;</ref><ref type="bibr" target="#b39">Ning et al., 2020a;</ref><ref type="bibr" target="#b36">Luo et al., 2020;</ref><ref type="bibr" target="#b53">Shi et al., 2020;</ref><ref type="bibr" target="#b73">Yan et al., 2020;</ref><ref type="bibr">White et al., 2021;</ref><ref type="bibr">Ru et al., 2021)</ref>. Weight-sharing methods <ref type="bibr" target="#b2">(Bender et al., 2018;</ref><ref type="bibr" target="#b44">Pham et al., 2018)</ref> have become popular due to their computation efficiency. Based on weight-sharing, gradient-based methods are proposed to optimize the architecture selection with gradient decent <ref type="bibr" target="#b35">(Luo et al., 2018b;</ref><ref type="bibr" target="#b29">Liu et al., 2019a;</ref><ref type="bibr" target="#b70">Xie et al., 2019b;</ref><ref type="bibr" target="#b72">Yan et al., 2019;</ref><ref type="bibr" target="#b77">You et al., 2020b;</ref><ref type="bibr" target="#b42">Peng et al., 2020;</ref><ref type="bibr" target="#b78">Zela et al., 2020;</ref><ref type="bibr" target="#b6">Chen &amp; Hsieh, 2020)</ref>. For comprehensive surveys, we suggest referring to <ref type="bibr" target="#b14">(Elsken et al., 2019;</ref><ref type="bibr" target="#b68">Xie et al., 2020)</ref>.</p><p>Neural Architecture Encoding. Majority of existing NAS work use one-hot adjacency matrix to encode the structures of neural architectures. However, adjacency matrix-based encoding grows quadratically as the search space scales up. <ref type="bibr" target="#b75">(Ying et al., 2019)</ref> proposes categorical adjacency matrixbased encoding to ensure fixed length encodings. They also propose continuous adjacency matrix-based encoding that is similar to DARTS <ref type="bibr" target="#b29">(Liu et al., 2019a)</ref>, where the architecture is created by taking fixed number of edges with the highest continuous values. However, this approach is not easily applicable to some NAS algorithms such as regularized evolution <ref type="bibr" target="#b49">(Real et al., 2017)</ref> without major changes. Tabular encoding in the form of ConfigSpace <ref type="bibr" target="#b26">(Lindauer et al., 2019)</ref> is often used for hyperparameter optimization (Li et al., <ref type="figure">Figure 1</ref>. Overview of CATE. CATE takes computationally similar architecture pairs as the input. The model is trained to predict masked operators given the pairwise computational information. Apart from the cross-attention blocks, the pretrained Transformer encoder is used to extract architecture encodings for the downstream encoding-dependent NAS subroutines.</p><p>2018 <ref type="bibr">;</ref><ref type="bibr" target="#b15">Falkner et al., 2018)</ref> and recently adopted by NAS-Bench-301 <ref type="bibr" target="#b54">(Siems et al., 2020)</ref> to represent architectures by introducing categorical hyperparameters for each operation along each potential edge. Recent NAS methods <ref type="bibr" target="#b34">(Luo et al., 2018a;</ref><ref type="bibr" target="#b61">Wang et al., 2020;</ref><ref type="bibr" target="#b64">Wen et al., 2020;</ref><ref type="bibr" target="#b53">Shi et al., 2020)</ref> use adjacency matrix as the input to LSTM/MLP/GNN to encode the structures of neural architectures in the latent space. <ref type="bibr" target="#b73">(Yan et al., 2020)</ref> validates that pre-training architecture representations without using accuracies can better preserve the local structural relationship of neural architectures in the latent space. <ref type="bibr" target="#b63">(Wei et al., 2020b)</ref> proposes to learn architecture representations using contrastive learning to find low-dimensional embeddings. <ref type="bibr" target="#b7">(Choi et al., 2021)</ref> studies various locality-based self-supervised objectives on the effect of architecture representations. One disadvantage of these methods is that they rely on a prior where the edit distance closeness between different architectures is a good indicator of the relative performance; however, structureaware encodings may not be computationally unique unless some certain graph hashing is applied <ref type="bibr" target="#b75">(Ying et al., 2019;</ref><ref type="bibr" target="#b40">Ning et al., 2020b)</ref>. <ref type="bibr">(White et al., 2021;</ref><ref type="bibr" target="#b62">Wei et al., 2020a)</ref> use path encoding and its categorical and continuous variants, which encode computation of architectures so that isomorphic cells are mapped to the same encoding.  uses GRU-based asynchronous message pass-ing to encode computation of architectures and the model is trained with the VAE loss. <ref type="bibr">(Lukasik et al., 2021)</ref> proposes a two-sided variational encoder-decoder GNN to learn smooth embeddings in various NAS search spaces. CATE is inspired by the advantage of computation encoding and addresses the drawbacks of <ref type="bibr">White et al., 2021)</ref>. Another line of work is based on the intrinsic properties of the architectures. <ref type="bibr" target="#b17">(Hesslow &amp; Poli, 2021)</ref> generates architecture representations by using contrastive learning over data Jacobian matrix values computed based on different initializations, and the generated embeddings are independent of the parameterization of the search space.</p><p>Context Dependency. Our work is close to self-supervised learning in language models (LMs) . In particular, ELMo <ref type="bibr" target="#b43">(Peters et al., 2018)</ref> uses two shallow unidirectional LSTMs <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref> to encode bidirectional text information, which is not sufficient for modeling deep interactions between the two directions. GPT-2 <ref type="bibr" target="#b45">(Radford et al., 2019)</ref> proposes an autoregressive language modeling method with Transformer <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> to cover the left-to-right dependency and is further generalized by XLNet  which encodes bidirectional context. (Ro)BERT/BART/T5 <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b30">Liu et al., 2019b;</ref><ref type="bibr" target="#b23">Lewis et al., 2020;</ref><ref type="bibr" target="#b47">Raffel et al., 2020)</ref> use bidirectional Transformer encoder to encode both left and right context. In architecture representation learning, however, the attention mask in the encoder cannot be used to attend to all the operators because it does not reflect the single-directional flow of the computational graphs <ref type="bibr" target="#b69">(Xie et al., 2019a;</ref><ref type="bibr" target="#b76">You et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CATE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>We restrict our search space to the cell-based architectures. Following the configuration in <ref type="bibr" target="#b75">(Ying et al., 2019)</ref>, each cell is a labeled directed acyclic graph (DAG) G = (V, E), with V as a set of N nodes and E as a set of edges that connect the nodes. Each node v i ? V, i ? [1, N ] is associated with an operation selected from a predefined set of V operations, and the edges between different nodes are represented as an upper triangular binary adjacency matrix A ? {0, 1} N ?N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computation-aware Neural Architecture Encoder</head><p>Our proposed computation-aware neural architecture encoder is built upon the Transformer encoder architecture which consists of a semantic embedding layer and L Transformer blocks stacked on top. Given G, each operation v i is first fed into a semantic embedding layer of size d e :</p><formula xml:id="formula_0">Emb i = Embedding(v i )<label>(1)</label></formula><p>The embedded vectors are then contextualized at different levels of abstract. We denote the hidden state after l-th layer as H l = [H l 1 , ..., H l N ] of size d h , where H l = T (H l?1 ) and T is a transformer block containing n head heads. The l-th Transformer block is calculated as:</p><formula xml:id="formula_1">Q k = H l?1 W l qk , K k = H l?1 W l kk , V k = H l?1 W l vk (2) H l k = softmax( Q k K T k ? d h + M)V k (3) H l = concatenate(? l 1 ,? l 2 , . . . ,? l n head )<label>(4)</label></formula><formula xml:id="formula_2">H l = ReLU(? l W 1 + b 1 )W 2 + b 2<label>(5)</label></formula><p>where the initial hidden state H 0 i is Emb i , thus d e = d h . Q k , K k , V k stand for "Query", "Key" and "Value" in the attention operation of the k-th head respectively. M is the attention mask in the Transformer, where M i,j ? {0, ??} indicates whether operation j is a dependent operation of operation i. W 1 ? R dc?d f f and W 2 ? R d f f ?dc denote the weights in the feed-forward layer.</p><p>Direct/Indirect Dependency Mask. A pair of nodes (operations) within an architecture are dependent if there is either a directed edge that directly connects them (local Algorithm 1 Floyd Algorithm 1: Input: the node set V, the adjacent matrix</p><formula xml:id="formula_3">A 2:? ? A 3: for k ? V do 4: for i ? V do 5: for j ? V do 6:? i,j | =? i,k &amp;? k,j 7: Output:?</formula><p>dependency) or a path made of a series of such edges that indirectly connects them (long-range dependency). We create dependency masks for such pairs of nodes for both direct and indirect cases and use these dependency masks as the attention masks in the Transformer. Specifically, the direct dependency mask M Direct and the indirect dependency mask M Indirect can be created as follows:</p><formula xml:id="formula_4">M Direct i,j = 0, if A i,j = 1 ??, if A i,j = 0 M Indirect i,j = 0, if? i,j = 1 ??, if? i,j = 0</formula><p>where A is the adjacency matrix and? = Floyed(A) is derived using Floyd algorithm in Algorithm 1.</p><p>Uni/Bidirectional Encoding. Finally, the final hidden vector H l N is used as the unidirectional encoding for the architecture. We also considered encoding the architecture in a bidirectional manner, where both the output node hidden vector from the original DAG and the input node hidden vector from the reversed one are extracted and then concatenated together. However, our experiments show that bidirectional encoding performs worse than unidirectional encoding. We include this result in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-training CATE</head><p>Architecture Pair Sampling. We split the dataset into 95% training and 5% held-out test sets for our pairwise pre-training. To ensure that it does not scale with quadratic time complexity, we first sort the architectures based on their computational attributes P (e.g. number of parameters, FLOPs). We then employ a sliding window for each architecture x i and its neighborhood r(x i ) = {y : |P(x i ) ? P(y)| &lt; ?}, where ? is a hyperparameter for the pairwise computation constraint. Finally, we randomly select K dis-</p><formula xml:id="formula_5">tinct architectures Y = {y 1 , . . . , y K }, x i / ? Y, Y ? r(x i ) within the neighborhood to compose K architecture pairs {(x i , y 1 ), . . . , (x i , y K )} for architecture x i .</formula><p>Pairwise Pre-training with Cross-Attention. Once the computationally similar architecture pair is composed, we randomly select 20% operations from each architecture within the pair for masking, where 80% of them are re-placed with a [M ASK] token and the remaining 20% are replaced with a random token chosen from the predefined operation set. We apply padding to architectures that have nodes less than the maximum number of nodes N in one batch to handle variable length inputs. The joint representation H L XY is derived by concatenating H L X and H L Y followed by the summation of the corresponding segment embedding. Segment embedding acts as an identifier of different architectures during pre-training. We set it to be trainable and randomly initialized. The joint representation H L XY is then contextualized with another L c -layer Transformer with the cross-attention mask M c such that segments from the two architectures can attend to each other given the pairwise information. For example, given two architectures X with three nodes and Y with four nodes in <ref type="figure">Figure 1</ref>, X has access to the non-padded nodes of Y and itself, and same for Y . The cross-attention dimension of the encoder is denoted as d c . The joint representation of the last layer is used for prediction. The model is trained by minimizing the cross-entropy loss computed using the predicted operations and the original operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Encoding-dependent NAS Subroutines</head><p>(White et al., 2020a) identifies three major encodingdependent subroutines included in existing NAS algorithms: sample random architecture, perturb architecture, and train predictor model. The sample random architecture subroutine includes random search <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019)</ref>. The perturb architecture subroutine includes regularized evolution (REA)  and local search (LS) <ref type="bibr" target="#b66">(White et al., 2020b)</ref>. The train predictor model subroutine includes neural predictor <ref type="bibr" target="#b64">(Wen et al., 2020;</ref><ref type="bibr" target="#b53">Shi et al., 2020;</ref><ref type="bibr">White et al., 2021)</ref>, Bayesian optimization with Gaussian process (GP) <ref type="bibr" target="#b48">(Rasmussen &amp; Williams, 2006)</ref>, and Bayesian optimization with neural networks (DNGO) <ref type="bibr" target="#b55">(Snoek et al., 2015)</ref> which is much faster to fit compared to GP and scales linearly with large datasets rather than cubically.</p><p>Inspired by <ref type="bibr" target="#b41">(Ottelander et al., 2020;</ref><ref type="bibr" target="#b66">White et al., 2020b)</ref>, we found that LS (perturb architecture) can be combined with DNGO (train predictor model). We thus propose a DNGO-based computation-aware search using CATE called CATE-DNGO-LS. Specifically, we maintain a pool of sampled architectures and take iterations to add new ones. In each iteration, we pass all architecture encodings to the predictor trained 30 epochs with samples in the current pool. We select new architectures with top-5 predicted accuracy and add them to the pool. Assume there are M new architectures which become the new top-5 in the updated pool. We then select the nearest neighbors of the other (5-M) top-5 architectures in L2 distance in latent space and add them to the pool. Hence, there will be 5 to 10 new architectures added to the pool in each iteration. The search stops when the number of samples reaches a pre-defined budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We describe two NAS benchmarks used in our experiments.</p><p>NAS-Bench-101. The NAS-Bench-101 search space <ref type="bibr" target="#b75">(Ying et al., 2019)</ref> consists of 423, 624 architectures. Each architecture has its pre-computed validation and test accuracies on CIFAR-10. The cell includes up to 7 nodes and at most 9 edges with the first node as input and the last node as output. The intermediate nodes can be either 1?1 convolution, 3?3 convolution, or 3?3 max pooling. We use the number of network parameters as the computational attribute P for architecture pair sampling. We set ? to 2, 000, 000 and K to 2. The ablation studies on ? and K are summarized in Section 4.4. We split the dataset into 95% training and 5% held-out test sets for pre-training.</p><p>NAS-Bench-301. NAS-Bench-301 <ref type="bibr" target="#b54">(Siems et al., 2020)</ref> is a new surrogate benchmark on the DARTS <ref type="bibr" target="#b29">(Liu et al., 2019a)</ref> search space that is much larger than NAS-Bench-101. It was created by fully training 60, 000 architectures that is stratified by the NAS methods 1 with a good coverage and then fitting a surrogate model that can estimate the accuracy (with noise) at epoch 100 and the training time for any of the remaining 10 18 architectures. To convert the DARTS search space into one with the same input format as NAS-Bench-101, we add a summation node to make nodes represent operations and edges represent data flow. Following , we use the same cell for both normal and reduction cell, allowing roughly 10 9 DAGs without considering graph isomorphism. More details about the DARTS/NAS-Bench-301 and a cell transformation example are included in Appendix D. We randomly sample 1, 000, 000 architectures in this search space, and use the same data split used in NAS-Bench-101 for pre-training. We use network FLOPs as the computational attribute P for architecture pair sampling. We set ? to 5, 000, 000 and K to 1. Since some NAS methods we compare against use the same GIN  surrogate model used in NAS-Bench-301, to ensure fair comparison, we thus followed <ref type="bibr" target="#b54">(Siems et al., 2020)</ref> to use XGB-v1.0 and LGB-runtime-v1.0 which utilizes gradient boosted trees <ref type="bibr" target="#b5">(Chen &amp; Guestrin, 2016;</ref><ref type="bibr" target="#b20">Ke et al., 2017</ref>) as the regression model.</p><p>Model and Training. We use a L = 12 layer Transformer encoder and a L c = 24 layer cross-attention Transformer encoder, each has 8 attention heads. The hidden state size is d h = d c = 64 for all the encoders. The hidden dimension is d f f = 64 for all the feed-forward layers. We employ AdamW <ref type="bibr" target="#b31">(Loshchilov &amp; Hutter, 2019)</ref> as our optimizer. The initial learning rate is 1e-3. The momentum parameters are set to 0.9 and 0.999. The weight decay is 0.01 for regular layer and 0 for dropout and layer normalization. We trained our model with batch size of 1024 on NVIDIA Quadro RTX 8000 GPUs. It takes around 4GB GPU memory for NAS-Bench-101 and 9GB GPU memory for NAS-Bench-301. The validation loss converges well after 10 epochs of pretraining, which takes 1.2 hours on NAS-Bench-101 and 7.5 hours on NAS-Bench-301.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Different Encoding Schemes</head><p>In our first experiment, we compare CATE with eleven architecture encoding schemes under three major encodingdependent subroutines described in Section 3.4 on NAS-Bench-101. These encoding schemes include (1-3) onehot/categorical/continuous adjacency matrix encoding <ref type="bibr" target="#b75">(Ying et al., 2019)</ref>, (4-6) one-hot/categorical/continuous path encoding and (7-9) their corresponding truncated counterparts <ref type="bibr">(White et al., 2021)</ref>, (10) D-VAE , and (11) arch2vec <ref type="bibr" target="#b73">(Yan et al., 2020)</ref>. For continuous encodings, we use L2 distance as the distance metric. To examine the effectiveness of the encoding schemes themselves, we compare different encoding schemes under the same search subroutine. <ref type="figure">Figure 2</ref> illustrates our results. For each subroutine, we show the top-five best-performing encoding schemes. Overall, despite there is no overall best encoding, we found that CATE is among the top five across all the subroutines.</p><p>Specifically, for sample random architecture subroutine, random search using adjacency matrix encoding performs the best. The random search using continuous encodings performs slightly worse than the adjacency encodings possibly due to the discretization loss from vector space into a fixed number of bins of same size before the random sampling.</p><p>For perturb architecture subroutine, CATE is on par with or outperforms adjacency encoding and path encoding because it is pre-trained to preserve strong computation locality information. This advantage allows the evolution or local search to find architectures with similar performance in local neighborhood more easily. Interestingly, we observe very small deviation using local search with CATE. This indicates that it always converges to some certain local minimums across different initial seeds. Since NAS-Bench-101 already exhibits locality in edit distance, encoding computation makes architectures even closer in terms of accuracy and thus benefits the local search.</p><p>For train predictor model subroutine, we have four observations: 1) Adjacency matrix encodings perform less effective with neural predictor and DNGO. It is possibly that edit distance cannot fully reflect the closeness of architectures w.  with neural predictor but worse than other encodings with Bayesian optimization. 3) D-VAE and arch2vec, two encodings learned via variational autoencoding, perform well only with some certain NAS methods. It could be attributed to their challenging training objective which easily leads to overfitting. 4) CATE is competitive with neural predictor and outperforms all the other encodings with Bayesian optimization. This is because neighboring computation-aware encodings correspond with similar accuracies. Moreover, the training objective in CATE is more efficient compared to the standard VAE loss <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2014)</ref> used by D-VAE and arch2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Different NAS Methods</head><p>In our second experiment, we compare the neural architecture search performance based on CATE encodings with state-of-the-art NAS algorithms on NAS-Bench-101 and NAS-Bench-301. Existing NAS algorithms contain one or more encoding-dependent subroutines. We consider six NAS algorithms that contain one encoding-dependent subroutine: random search (RS) <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019)</ref>  LS: a NAS algorithm based on CATE encodings with the combination of DNGO and LS subroutines (train predictor, perturb arch.) as described in Section 3.4. <ref type="figure" target="#fig_0">Figure 3</ref> and <ref type="table">Table 1</ref> summarize our results. We have three major findings from <ref type="figure" target="#fig_0">Figure 3</ref>: 1) Architecture encoding matters especially in the large search space. The right plot shows that CATE-DNGO and CATE-DNGO-LS in DARTS search space not only converge faster but also lead to better final search performance given the same budgets. 2) Local search (LS) is a strong baseline in both small and large search spaces. As mentioned in Section 4.1, performing LS using CATE leads to better results compared to other encodings. 3) NAS algorithms that use more than one encoding-dependent subroutine in general perform better than NAS algorithms with just one subroutine. Specifically, BOGCN and BA-NANAS that have multiple subroutines perform better than the single-subroutine NAS algorithms such as REA, DNGO, and BOHAMIANN. Moreover, CATE-DNGO-LS leads to the best performing result in both NAS-Bench-101 and NAS-Bench-301 search spaces. Meanwhile, the improvement of CATE-DNGO-LS versus CATE-DNGO shrinks in larger search space, indicating that the larger search space is more challenging to encode. NAS-Bench-301 uses a surrogate model trained on 60k architectures to predict the performance of all the other architectures in the DARTS search space. The performance of the other architectures, however, can be inaccurate. Given that, we further validate the effectiveness of CATE-DNGO-LS in the actual DARTS search space by training the queried architectures from scratch. We set the budget to 100 and 300 queries, separately. Each queried architecture is trained for 50 epochs with a batch size of 96, using 32 initial channels and 8 cell layers. The average validation error of the last 5 epochs is computed as the label. These values are chosen to be close to the proxy model used in DARTS. It takes about 3.3 GPU days to finish the search with 100 quries and 10.3 GPU days with 300 queries. See <ref type="figure" target="#fig_1">Figure 4</ref> for the best found cells. To ensure fair comparison, we compare CATE-DNGO-LS to methods <ref type="bibr" target="#b29">(Liu et al., 2019a;</ref><ref type="bibr" target="#b24">Li &amp; Talwalkar, 2019;</ref><ref type="bibr" target="#b73">Yan et al., 2020;</ref><ref type="bibr">White et al., 2021)</ref> that use the common test evaluation script which is to train for 600 epochs with cutout and auxiliary tower.  <ref type="bibr" target="#b73">(Yan et al., 2020;</ref><ref type="bibr">White et al., 2021)</ref> in the actual DARTS search space. This is consistent with our observation in NAS-Bench-301. We report the transfer learning results on ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization to Outside Search Space</head><p>In our third experiment, inspired by <ref type="bibr" target="#b65">(White et al., 2020a)</ref>, we evaluate the generalization ability of CATE beyond the search space on which it was trained. The training search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS Methods</head><p>Avg. Test Error Params Search Cost (%) (M) (GPU days)</p><p>RS <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019)</ref> 3.29 ? 0.15 3.2 4 DARTS <ref type="bibr" target="#b29">(Liu et al., 2019a)</ref> 2.76 ? 0.09 3.3 4 BANANAS <ref type="bibr">(White et al., 2021)</ref> 2.67 ? 0.07 3.6 11.8 arch2vec-BO <ref type="bibr" target="#b73">(Yan et al., 2020)</ref> 2.56 ? 0.05 3.6 9.2 CATE-DNGO-LS (small budget) 2.55 ? 0.08 3.5 3.3 CATE-DNGO-LS (large budget)</p><p>2.46 ? 0.05 4.1 10.3  space is designed as a subset of NAS-Bench-101, where each included architecture has 2 to 6 nodes and 1 to 7 edges. The test search space is disjointed from the training search space and includes architectures with 6 nodes and 7 to 9 edges. There are 10, 026 and 60, 669 non-isomorphic graphs in the training and test space respectively. The CATE encodings are pre-trained using the training space and are used to conduct architecture search in the test space. We compare CATE with the adjacency matrix encoding because it was shown in <ref type="bibr" target="#b65">(White et al., 2020a)</ref> to have the best generalization capability compared to other encodings. A simple 2-layer MLP with hidden size 128 is used as the neural predictor for both encodings. <ref type="figure" target="#fig_2">Figure 5</ref> shows the validation error curve of the test search space given the number of 150 sample budget across 500 independent runs. As shown, CATE outperforms adjacency matrix encoding by a large margin. This indicates that CATE can better contextualize the computation information compared to fixed encodings, which generalizes better when adapting to outside search space. Moreover, the padding scheme in our encoder allows us to handle architectures with different numbers of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Finally, we conduct ablation studies on different hyperparameters involved in CATE. We use CATE-DNGO as the NAS method and report the final NAS test error [%] given 150 queried architectures on NAS-Bench-101. The result is averaged over 200 independent runs.</p><p>Architecture Pair Sampling Hyperparameters. We plot the histogram of model parameters on NAS-Bench-101 in <ref type="figure" target="#fig_3">Figure 6</ref>. As shown, the architectures are neither nor-  mally nor uniformly distributed in this search space in terms of model parameters. This motivates us to use a sliding window-based architecture pair selection to avoid the unbalanced sampling as proposed in Section 3.3. The choice of ? and K and their effects on the downstream NAS are summarized in <ref type="table" target="#tab_6">Table 4</ref>. We found that strong computation locality (i.e. small ?) usually leads to better results. The choice of neighborhood size K does not have a significant effect on NAS performance. Therefore, we choose small K for faster pretraining. For NAS-Bench-301, we use the FLOPs as the computational attributes P and observe the same trend as in NAS-Bench-101 on the selection of ? and K. We report the results in Appendix B.</p><p>Transformer Hyperparameters. We studied the effect of the number of cross-attention Transformer blocks L c and the hidden dimension of the feed-forward layer d f f on CATE. We fix ? and K for pre-training as mentioned in Section 4. The downstream NAS result is summarized in  Choice of Mask Type. We studied pretraining CATE with direct/indirect dependency mask and summarize its downstream NAS results in <ref type="table">Table 6</ref>. CATE trained with indirect dependency mask outperforms the direct one in both benchmarks, indicating that capturing long-range dependency helps preserve computation information in the encodings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented CATE, a new computation-aware architecture encoding method based on Transformers. Unlike encodings with fixed transformations, we show that the computation information of neural architectures can be contextualized through a pairwise learning scheme trained with MLM. Our experimental results show its effectiveness and scalability along with three major encoding-dependent NAS subroutines in both small and large search spaces. We also show its superior generalization capability outside the training search space. We anticipate that the methods presented in this work can be extended to encode even larger search spaces (e.g. TuNAS ) to study the effectiveness of different downstream NAS algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Uni/Bidirectional Encoding</head><p>As mentioned in Section 3.2, we also considered encoding the architecture in a bidirectional manner where both the output node hidden vector from the original DAG and the input node hidden vector from the reversed one are extracted and then concatenated together. Note that d c in the crossattention Transformer encoder will be doubled due to the concatenation. We compare the results of unidirectional and bidirectional encodings in <ref type="table" target="#tab_9">Table 7</ref>. As shown, bidirectional encoding does not necessarily improve the results. Therefore, we keep unidirectional encoding in other experiments due to its simplicity and better performance.</p><p>Encoding NAS-Bench-101 NAS-Bench-301</p><p>Unidirectional 5.88 5.28 Bidirectional 5.89 5.30 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture Pair Sampling Hyperparameters</head><p>As mentioned in Section 4.4, we randomly sample 1,000,000 architectures in NAS-Bench-301 for pretraining. We use the same proxy model configuration (i.e. 100 training epochs, 32 initial channels, 8 cell layers) as used in NAS-Bench-301 to compute the model FLOPs. We plot the histogram of model FLOPs of the sampled architectures in <ref type="figure">Figure 7</ref>. Given that, we experiment different ? and K and summarize the downstream NAS results in <ref type="table">Table 8</ref>. Similar to our reported results on NAS-Bench-101, we find that strong locality leads to better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Corruption Rate</head><p>By default, we randomly select 20% operations from each architecture within the pair for masking in the pairwise pretraining. We also experimented corruption rates of 15% and 30%. As shown in <ref type="table" target="#tab_10">Table 9</ref>, overall, we find that the corruption rate has a limited effect on the NAS performance. Note that the number of nodes in our search space is much smaller compared to the number of tokens in the sequence modeling tasks. Given that, using larger corruption rate may slow down the training convergence and result in degraded performance. Based on these results, we use 20% corruption rate for other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. NAS-Bench-301 Benchmark</head><p>NAS-Bench-301 <ref type="bibr" target="#b54">(Siems et al., 2020)</ref> is the first surrogate NAS benchmark to cover the large-scale DARTS search space <ref type="bibr" target="#b29">(Liu et al., 2019a)</ref>. The DARTS search space consists of two cells: a convolutional cell and a reduction cell, each with six nodes. For each cell, the first two nodes are the outputs from the previous two cells. The next four nodes contain two edges as input, creating a DAG. In total, there are roughly 10 18 DAGs without considering graph isomorphism, which is a much larger search space compared to NAS-Bench-101 <ref type="bibr" target="#b75">(Ying et al., 2019)</ref> and NAS-Bench-201 <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref>.</p><p>NAS-Bench-301 is fully trained on around 60k architectures collected by unbiased architecture sampling using random search as well as biased and dense architecture sampling in high-performance regions using different NAS methods and training hyperparameters (including training time, number of parameters, and number of multiply-adds). It trains various regression models such as Random Forest (RF) <ref type="bibr" target="#b4">(Breiman, 2001)</ref>, Support Vector Regression (SVR) <ref type="bibr" target="#b13">(Drucker et al., 1997)</ref>, Graph Isomorphism Network (GIN)   and Tree-based gradient boosting model (e.g. XGBoost (XGB) <ref type="bibr" target="#b5">(Chen &amp; Guestrin, 2016)</ref>, LGBoost (LGB) <ref type="bibr" target="#b20">(Ke et al., 2017)</ref>) to predict the accuracies of unseen architectures. The three best-performing models (GIN, XGB, LGB) are used to predict the search trajectories in the benchmark API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Cell Transformation</head><p>To transform the DARTS search space into one with the same input format as NAS-Bench-101, we additionally add a summation node to make nodes to represent operations and edges to represent data flow. For example, if there is an edge from node A to node B with operation O, we create an additional node P, remove the edge A, B , and add 2 edges A, P and P, B . The operation on node P is set to be O. Given that, a 15 ? 15 upper-triangular binary matrix is used to encode edges and a 15 ? 11 operation matrix is used to encode operations with the order of {c k?2 , c k?1 , 3 ? 3 maxpool, 3 ? 3 average-pool, skip connect, 3 ? 3 separable conv, 5 ? 5 separable conv, 3 ? 3 dilated conv, 5 ? 5 dilated conv, sum, c k }. Following NAS-Bench-301 <ref type="bibr" target="#b54">(Siems et al., 2020)</ref>, we do not include zero operator. Following , we use the same cell for both normal and reduction cells. An example of cell transformation is shown in <ref type="figure" target="#fig_4">Figure 8</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Comparison between CATE and SOTA NAS methods on NAS-Bench-101 (left) and NAS-Bench-301 (right). It reports the test error of 200 independent runs. The error bars denote the variance of the test error. The number of queried architectures is set to 150 for NAS-Bench-101 and 100 for NAS-Bench-301.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Top: Best found cell from CATE-DNGO-LS given the budget of 100 samples. Bottom: Best found cell from CATE-DNGO-LS given the budget of 300 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance on the out-of-training search space. It reports the validation error of 500 independent runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Histogram of model parameters on NAS-Bench-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>A cell transformation example in DARTS search space. The top panel shows the cell. The bottom-left and bottom-right panels show its corresponding adjacency matrix and operation matrix respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>summarizes our results. As shown, CATE-DNGO-</cell></row><row><cell>LS (small budget) achieves competitive performance (2.55%</cell></row><row><cell>avg. test error) with much less search cost and CATE-</cell></row><row><cell>DNGO-LS (large budget) achieves superior performance</cell></row><row><cell>(2.46% avg. test error) with similar search cost compared</cell></row><row><cell>to other sampling-based search methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>NAS results in DARTS search space using CIFAR-10.</figDesc><table><row><cell>NAS Methods</cell><cell cols="3">Params Mult-Adds Top-1 Test Error</cell></row><row><cell></cell><cell>(M)</cell><cell>(M)</cell><cell>(%)</cell></row><row><cell>SNAS (Xie et al., 2019b)</cell><cell>4.3</cell><cell>522</cell><cell>27.3</cell></row><row><cell>DARTS (Liu et al., 2019a)</cell><cell>4.7</cell><cell>574</cell><cell>26.7</cell></row><row><cell>BayesNAS (Zhou et al., 2019)</cell><cell>4.0</cell><cell>440</cell><cell>26.5</cell></row><row><cell>arch2vec-BO (Yan et al., 2020)</cell><cell>5.2</cell><cell>580</cell><cell>25.5</cell></row><row><cell>BANANAS (ours)</cell><cell>5.1</cell><cell>576</cell><cell>26.3</cell></row><row><cell>CATE-DNGO-LS (small budget)</cell><cell>5.0</cell><cell>556</cell><cell>26.1</cell></row><row><cell>CATE-DNGO-LS (large budget)</cell><cell>5.8</cell><cell>642</cell><cell>25.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Transfer learning results on ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Effects of ? and K on architecture pair sampling.</figDesc><table><row><cell>?</cell><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="2">1 ? 10 6</cell><cell>6.02</cell><cell>5.95</cell><cell>5.99</cell><cell>5.95</cell></row><row><cell cols="2">2 ? 10 6</cell><cell>6.02</cell><cell>5.94</cell><cell>6.04</cell><cell>5.96</cell></row><row><cell cols="2">4 ? 10 6</cell><cell>5.94</cell><cell>6.03</cell><cell>6.05</cell><cell>5.99</cell></row><row><cell cols="2">8 ? 10 6</cell><cell>6.05</cell><cell>6.04</cell><cell>6.11</cell><cell>6.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>It shows that larger L c and d f f usually lead to better NAS performance, which indicates that deep contextualized representations are beneficial to downstream NAS.</figDesc><table><row><cell>d f f</cell><cell>L c</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>64</cell><cell></cell><cell>6.07</cell><cell>5.99</cell><cell>5.95</cell></row><row><cell>128</cell><cell></cell><cell>6.01</cell><cell>5.94</cell><cell>5.95</cell></row><row><cell>256</cell><cell></cell><cell>5.97</cell><cell>5.94</cell><cell>5.94</cell></row></table><note>Table 5. Effects of Lc and d f f on pretraining CATE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Unidirectional encoding vs. bidirectional encoding.</figDesc><table><row><cell>We</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>NAS results under different corruption rates.</figDesc><table><row><cell></cell><cell>140000</cell><cell></cell><cell cols="5">Histogram for model FLOPs on NASBench-301</cell></row><row><cell></cell><cell>120000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>frequency</cell><cell>60000 80000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.75</cell><cell>1.00</cell><cell>1.25</cell><cell>1.50 number of model FLOPs 1.75 2.00</cell><cell>2.25</cell><cell>2.50</cell><cell>1e8 2.75</cell></row><row><cell cols="9">Figure 7. Histogram of model FLOPs on the sampled 1, 000, 000</cell></row><row><cell cols="6">architectures of NAS-Bench-301.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We suggest referring to C.2 in<ref type="bibr" target="#b54">(Siems et al., 2020)</ref> for a detailed description on the data collection.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their helpful comments. We thank Yu Zheng, Colin White, and Frank Hutter for their help with this project. This work was partially supported by NSF Awards CNS-1617627, CNS-1814551, and PFI:BIC-1632051.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? an investigation with tunas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pretraining neural architecture search controllers with locality-based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08157</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithm 97: Shortest path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Contrastive embeddings for neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Poli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04208</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In JMLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Boah: A tool suite for multi-fidelity bayesian optimization and analysis of hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biedenkapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1908.06756</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nsganetv2: Evolutionary multi-objective surrogateassisted neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smooth variational graph embeddings for efficient neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Friede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<title level="m">Neural architecture optimization. In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">Neural architecture optimization. In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGA</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeparchitect</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03064</idno>
		<title level="m">A surgery of the neural architecture evaluators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generic graph-based neural architecture encoding scheme for predictor-based nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Local search is a remarkably strong baseline for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ottelander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dushatskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Virgolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bosman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cream of the crop: Distilling prioritized paths for one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural architecture generator optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esperanca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osborne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bridging the gap between sample-based and one-shot neural architecture search with bonas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nas-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A semi-supervised assessor of neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Npenas: Neural predictor guided evolution for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12857</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Self-supervised representation learning for evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2011.00186</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural predictor for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A study on encodings for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Local search is state of the art for neural architecture search benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02960</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Weight-sharing neural architecture search: A battle to shrink the optimization gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Snas: Stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Hm-nas: Efficient neural architecture search via hierarchical masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Does unsupervised architecture representation learning help neural architecture search? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">NAS-Bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Graph structure of neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Greedynas: Towards fast one-shot nas with greedy supernet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Autobss: An efficient algorithm for block stacking style search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">BayesNAS: A Bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
