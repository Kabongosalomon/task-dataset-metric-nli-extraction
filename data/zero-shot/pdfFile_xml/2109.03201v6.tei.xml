<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansen</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON MEDICAL IMAGING</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer</term>
					<term>Attention Mechanism</term>
					<term>Volu- metric Image Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer (i.e., not-another transFormer), a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in U-Net like architecture. Experiments show that nnFormer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable DSC results. Furthermore, we show that nnFormer and nnUNet are highly complementary to each other in model ensembling. Codes and models of nnFormer are available at https://git.io/JSf3i.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Transformer <ref type="bibr" target="#b0">[1]</ref>, which has become the de-facto choice for natural language processing (NLP) problems, has recently been widely exploited in vision-based applications <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The core idea behind is to apply the self-attention mechanism <ref type="bibr">(</ref> First two authors contributed equally.</p><p>to capture long-range dependencies. Compared to convolutional neural networks (i.e., convnets <ref type="bibr" target="#b5">[6]</ref>), transformer relaxes the inductive bias of locality, making it more capable of dealing with non-local interactions <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. It has also been investigated that the prediction errors of transformers are more consistent with those of humans than convnets <ref type="bibr" target="#b9">[10]</ref>. Given the fact that transformers are naturally more advantageous than convnets, there are a number of approaches trying to apply transformers to the field of medical image analysis. Chen et al. <ref type="bibr" target="#b10">[11]</ref> first time proposed TransUNet to explore the potential of transformers in the context of medical image segmentation. The overall architecture of TransUNet is similar to that of U-Net <ref type="bibr" target="#b11">[12]</ref>, where convnets act as feature extractors and transformers help encode the global context. In fact, one major characteristic of TransUNet and most of its followers <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> is to treat convnets as main bodies, on top of which transformers are further applied to capture long-term dependencies. However, such feature may cause a problem, which is the advantages of transformers are not fully exploited. In other words, we believe one-or two-layer transformers are not enough to entangle long-term dependencies with convolutional representations that often contain precise spatial information and provide hierarchical concepts.</p><p>To address the above issue, some researchers <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> started to use transformers as the main stem in segmentation models. Karimi et al. <ref type="bibr" target="#b16">[17]</ref> first time introduced a convolution-free segmentation model by forwarding flattened image representations to transformers, whose outputs are then reorganized into 3D tensors to align with segmentation masks. Recently, Swin Transformer <ref type="bibr" target="#b2">[3]</ref> showed that by referring to the feature pyramids used in convnets, transformers can learn hierarchical object concepts at different scales by applying appropriate down-sampling to feature maps. Inspired by this idea, SwinUNet <ref type="bibr" target="#b17">[18]</ref> utilized hierarchical transformer blocks to construct the encoder and decoder within a U-Net like architecture, based on which DS-TransUNet <ref type="bibr" target="#b18">[19]</ref> added one more encoder to accept different-sized inputs. Both SwinUNet and DS-TransUNet have achieved consistent improvements over TransUNet. Nonetheless, they did not explore how to appropriately combine convolution and self-attention for building an optimal medical segmentation network.</p><p>In contrast, nnFormer (i.e., not-another transFormer) uses a hybrid stem where convolution and self-attention are interleaved to give full play to their strengths. <ref type="figure" target="#fig_0">Figure 1</ref>   Former. Firstly, we put a light-weight convolutional embedding layer ahead of transformer blocks. In comparison to directly flattening raw pixels and applying 1D pre-processing in <ref type="bibr" target="#b16">[17]</ref>, the convolutional embedding layer encodes precise (i.e., pixellevel) spatial information and provides low-level yet highresolution 3D features. After the embedding block, transformer and convolutional down-sampling blocks are interleaved to fully entangle long-term dependencies with high-level and hierarchical object concepts at various scales, which helps improve the generalization ability and robustness of learned representations. The other contribution of nnFormer lies in proposing a computational-efficient way to leverage inter-slice dependencies. To be specific, nnFormer proposes to jointly use Local Volume-based Multi-head Self-attention (LV-MSA) and Global Volume-based Multi-head Self-attention (GV-MSA) to construct feature pyramids and provide sufficient receptive field for learning representations on both local and global 3D volumes, which are then aggregated to make predictions. Compared to the naive multi-head self-attention (MSA) <ref type="bibr" target="#b0">[1]</ref>, the proposed strategy can greatly reduce the computational complexity while producing competitive segmentation performance. Moreover, inspired by the attention mechanism used in the task of machine translation <ref type="bibr" target="#b0">[1]</ref>, we introduce skip attention to replace the atypical concatenation/summation operation in skip connections of U-Net like architecture, which further improves the segmentation results.</p><p>To sum up, our contributions can be summarized as follows:</p><p>? We introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer achieves significant improvements over previous transformer-based medical segmentation models on three well-established datasets. ? Technically, the contributions of nnFormer are three folds: i) an interleaved combination of convolution and selfattention operations. ii) the utilization of both local and global volume-based self-attention to build feature pyramids and provide large receptive fields, respectively. iii) skip attention is proposed to replace traditional concatenation/summation operations in skip connections. ? Thorough experiments have been conducted to validate the advantages of nnFormer over nnUNet. We show that nnFormer is significantly better than nnUNet in hausdorff distance and achieves slightly better performance in dice coefficient. Moreover, we found that nnFormer and nnUNet are highly complementary to each other as simply averaging their predictions can already greatly boost the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we mainly review methodologies that resort to transformers to improve segmentation results of medical images. Since most of them employ hybrid architecture of convolution and self-attention <ref type="bibr" target="#b0">[1]</ref>, we divide them into two categories based on whether the majority of the stem is convolutional or transformer-based.</p><p>Convolution-based stem. TransUNet <ref type="bibr" target="#b10">[11]</ref> first time applied transformer to improve the segmentation results of medical images. TransUNet treats the convnet as a feature extractor to generate a feature map for the input slice. Patch embedding is then applied to patches of feature maps in the bottleneck instead of raw images in ViT <ref type="bibr" target="#b1">[2]</ref>. Concurrently, similar to TransUNet, Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use a squeezed attention block to regularize the self-attention modules of transformers and an expansion block to learn diversified representations for fundus images, which are all implemented in the bottleneck within convnets. TransFuse <ref type="bibr" target="#b12">[13]</ref> introduced a BiFusion module to fuse features from the shallow convnet-based encoder and transformer-based segmentation network to make final predictions on 2D images. Compared to TransUNet, TransFuse mainly applied the self-attention mechanism to the input embedding layer to improve segmentation models on 2D images. Yun et al. <ref type="bibr" target="#b20">[21]</ref> employed transformers to incorporate spectral information, which are entangled with spectral information encoded by convolutional features to address the problem of hyperspectral pathology. Xu et al. <ref type="bibr" target="#b21">[22]</ref> extensively studied the trade-off between transformers and convnets and proposed a more efficient encoder named LeViT-UNet. Li et al. <ref type="bibr" target="#b22">[23]</ref> presented a new up-sampling approach and incorporated it into the decoder of UNet to model long-term dependencies and global information for better reconstruction results. TransClaw U-Net <ref type="bibr" target="#b14">[15]</ref> utilized transformers in UNet with more convolutional feature pyramids. TransAttUNet <ref type="bibr" target="#b15">[16]</ref> explored the feasibility of applying transformer self attention with convolutional global spatial attention. Xie et al.</p><p>[24] adopted transformers to capture long-term dependencies of multi-scale convolutional features from different layers of convnets. TransBTS <ref type="bibr" target="#b24">[25]</ref> first utilized 3D convnets to extract volumetric spatial features and down-sample the input 3D images to produce hierarchical representations. The outputs of the encoder in TransBTS are then reshaped into a vector (i.e. token) and fed into transformers for global feature modeling, after which an ordinary convolutional decoder is appended to up-sample feature maps for the goal of reconstruction. Different from these approaches that directly employ convnets as feature extractors, our nnFormer functionally relies on convolutional and transformer-based blocks, which are interleaved to take advantages of each other.</p><p>Transformer-based stem. Valanarasu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a gated axial-attention model (i.e., MedT) which extends the existing convnet architecture by introducing an summational control mechanism in the self-attention. Karimi et al. <ref type="bibr" target="#b16">[17]</ref> removed the convolutional operations and built a 3D segmentation model based on transformers. The main idea is to first split the local volume block into 3D patches, which are then flattened and embedded to 1D sequences and passed to a ViT-like backbone to extract representations. SwinUNet <ref type="bibr" target="#b17">[18]</ref> built a U-shape transformer-based segmentation model on top of transformer blocks in <ref type="bibr" target="#b2">[3]</ref>, where observable improvements were achieved. DS-TransUNet <ref type="bibr" target="#b18">[19]</ref> further extended Swin-UNet by adding one more encoder to handle multi-scale inputs and introduced a fusion module to effectively establish global dependencies between features of different scales through the self-attention mechanism. Compared to these transformerbased stems, nnFormer inherits the superiority of convolution in encoding precise spatial information and producing hierarchical representations that help model object concepts at various scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The overall architecture of nnFormer is presented in <ref type="figure">Figure  2</ref>, which maintains a similar U shape as that of U-Net <ref type="bibr" target="#b11">[12]</ref> and mainly consists of three parts, i.e., the encoder, bottleneck and decoder. Concretely, the encoder involves one embedding layer, two local transformer blocks (each block contains two successive layers) and two down-sampling layers. Symmetrically, the decoder branch includes two transformer blocks, two up-sampling layers and the last patch expanding layer for making mask predictions. Besides, the bottleneck comprises one down-sampling layer, one up-sampling layer and three global transformer blocks for providing large receptive field to support the decoder. Inspired by U-Net <ref type="bibr" target="#b11">[12]</ref>, we add skip connections between corresponding feature pyramids of the encoder and decoder in a symmetrical manner, which helps to recover fine-grained details in the prediction. However, different from atypical skip connections that often use summation or concatenation operation, we introduce skip attention to bridge the gap between the encoder and decoder.</p><p>In the following, we will demonstrate the forward procedure on Synapse. The forward pass on different datasets can be easily inferred based on the procedure on Synapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder</head><p>The input of nnFormer is a 3D patch X ? R H?W ?D (usually randomly cropped from the original image), where H, W and D denote the height, width and depth of each input scan, respectively.</p><p>The embedding layer. On Synapse, the embedding block is responsible for transforming each input scan X into a high-dimensional tensor</p><formula xml:id="formula_0">X e ? R H 4 ? W 4 ? D 2 ?C , where H 4 ? W 4 ? D 2</formula><p>represents the number of the patch tokens and C represents the sequence length (these numbers may slightly vary on different datasets). Different from ViT <ref type="bibr" target="#b1">[2]</ref> and Swin Transformer <ref type="bibr" target="#b2">[3]</ref> that use large convolutional kernels in the embedding block to extract features, we found that applying successive convolutional layers with small convolutional kernels bring more benefits in the initial stage, which could be explained from two perspectives, i.e., i) why applying successive convolutional layers and ii) why using small-sized kernels. For i), we use convolutional layers in the embedding block because they encode pixel-level spatial information, more precisely than patch-wise positional encoding used in transformers. For ii), compared to large-sized kernels, small kernel sizes help reduce computational complexity while providing equal-sized receptive field. As shown in <ref type="figure">Figure 2b</ref>, the embedding block consists of four convolutional layers whose kernel size is 3. After each convolutional layer (except the last one), one GELU <ref type="bibr" target="#b25">[26]</ref> and one layer normalization <ref type="bibr" target="#b26">[27]</ref> layers are appended. In practice, depending on the size of input patch, strides of convolution in the embedding block may accordingly vary.</p><p>Local Volume-based Multi-head Self-attention (LV-MSA). After the embedding layer, we pass the high-dimensional tensor X e to transformer blocks. The main point behind is to fully entangle the captured long-term dependencies with the hierarchical object concepts at various scales produced by the down-sampling layers and the high-resolution spatial information encoded by the initial embedding layer. Compared to Swin Transformer <ref type="bibr" target="#b2">[3]</ref>, we compute self-attention within 3D local volumes (i.e., LV-MSA, Local Volume-based Multi-head Self-attention) instead of 2D local windows.</p><p>Suppose that X LV ? R L?C represents the input of the local transformer block, X LV would be first reshaped toX LV ? R NLV?N T ?C , where N LV is a pre-defined number of 3D local volumes and N T = S H ? S W ? S D denotes the number of patch tokens in each volume. {S H , S W , S D } stand for the size of local volume.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 3a</ref>, we follow <ref type="bibr" target="#b2">[3]</ref> to conduct two successive transformer layers in each block, where the second layer can be regarded as a shifted version of the first layer (i.e., SLV-MSA). The main difference lies in that our computation is built on top of 3D local volumes instead of 2D local windows. The computational procedure can be summarized as follows:</p><formula xml:id="formula_1">X l LV = LV-MSA Norm X l?1 LV + X l?1 LV , X l LV = MLP Norm X l LV +X l LV , X l+1 LV = SLV-MSA Norm X l LV + X l LV , X l+1 LV = MLP Norm X l+1 LV +X l+1 LV .<label>(1)</label></formula><p>Here, l stands for the layer index. MLP is an abbreviation for multi-layer perceptron. The computational complexity of LV-MSA on a volume of h ? w ? d patches is:</p><formula xml:id="formula_2">?(LV-MSA) = 4hwdC 2 + 2S H S W S D hwdC.<label>(2)</label></formula><p>SLV-MSA displaces the 3D local volume used in LV-MSA by</p><formula xml:id="formula_3">S H 2 , S W 2 , S D 2</formula><p>to introduce more interactions between different local volumes. In practice, SLV-MSA has the similar computational complexity as that of LV-MSA.</p><p>The query-key-value (QKV) attention <ref type="bibr" target="#b0">[1]</ref> in each 3D local volume can be computed as follows:</p><formula xml:id="formula_4">Attention(Q, K, V ) = softmax QK T ? d k + B V,<label>(3)</label></formula><p>where Q, K, V ? R N T ?d k denote the query, key and value matrices. B ? R N T is the relative position encoding. In  Encoder Decoder Bottleneck <ref type="figure">Fig. 2</ref>: Architecture of nnFormer. In (a), we show the overall architecture of nnFormer. In (b), we present more details of the embedding layers on three publicly available datasets. In (c), (d), (e), we display how to implement the down-sampling, up-sampling and expanding layers, respectively. In practice, the architecture may slightly vary depending on the input scan size. In (b)-(e), K denotes the convolutional kernel size, DK stands for the deconvolutional kernel size and S represents the stride. Norm refers to the layer normalization strategy.  : Three types of attention mechanism in nnFormer. Norm denotes the layer normalization method. MLP is the abbreviation for multi-layer perceptron, which is a two-layer neural network in practice.</p><p>practice, we first initialize a smaller-sized position matrix B ? R (2S H ?1)?(2S W ?1)?(2S D ?1) and take corresponding values fromB to build a larger position matrix B.</p><p>The down-sampling layer. We found that by replacing the patch merging operation in <ref type="bibr" target="#b2">[3]</ref> with straightforward strided convolution, nnFormer can provide more improvements on volumetric image segmentation. The intuition behind is that  convolutional down-sampling produces hierarchical representations that help model object concepts at multiple scales. As displayed in <ref type="figure">Figure 2c</ref>, in most cases, the down-sampling layer involves a strided convolution operation where the stride is set to 2 in all dimensions. However, in practice, the stride with respect to specific dimension can be set to 1 as the number of slices is limited in this dimension and over-down-sampling (i.e., using a large down-sampling stride) can be harmful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bottleneck</head><p>The original vision transformer (i.e., ViT) <ref type="bibr" target="#b1">[2]</ref> employs the naive 2D multi-head self-attention mechanism. In this paper, we extend it to a 3D version (as shown in <ref type="figure" target="#fig_3">Figure 3b</ref>), whose computational complexity can be formulated as follows:</p><formula xml:id="formula_5">?(GV-MSA) = 4hwdC 2 + 2(hwd) 2 C.<label>(4)</label></formula><p>Compared to <ref type="bibr" target="#b1">(2)</ref>, it is obvious that GV-MSA requires much more computational resources when {h, w, d} are relatively larger (e.g., an order of magnitude larger) than {S H , S W , S D }.</p><p>In fact, this is exactly the reason why we use local transformer blocks in the encoder, which are designed to handle large-sized inputs efficiently with the local self-attention mechanism. However, in the bottleneck, {h, w, d} already become much smaller after several down-sampling layers, making the product of them, i.e. hwd, , have a similar size to that of S H S W S D . This creates the condition for applying GV-MSA, which is able to provide larger receptive field compared to LV-MSA and large receptive field has been proven to be beneficial in different applications <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>. In practice, we use three global transformer blocks (i.e., six GV-MSA layers) in the bottleneck to provide sufficient receptive field to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Decoder</head><p>The architecture of two transformer blocks in the decoder is highly symmetrical to those in the encoder. In contrast to the down-sampling blocks, we employ strided deconvolution to up-sample low-resolution feature maps to high-resolution ones, which in turn are merged with representations from the encoder via skip attention to capture both semantic and fine-grained information. Similar to up-sampling blocks, the last patch expanding block also takes the deconvolutional operation to produce final mask predictions.</p><p>Skip Attention. Atypical skip connections in convnets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> adapt either concatenation or summation to incorporate more information. Inspired by the machine translation task in <ref type="bibr" target="#b0">[1]</ref>, we propose to replace the concatenation/summation with an attention mechanism, which is named as Skip Attention in this paper. To be specific, the output of the l-th transformer block of the encoder, i.e., X l {LV,GV} , is transformed and split into a key matrix K l * and a value matrix V l * after the linear projection (i.e, a one-layer neural network):</p><formula xml:id="formula_6">K l * , V l * = LP(X l {LV,GV} ),<label>(5)</label></formula><p>where LP stands for the linear projection. Accordingly, X l * UP , the output feature maps after the l * -th up-sampling layer of the decoder, is treated as the query Q l * . Then, we can conduct LV/GV-MSA on Q l * , K l * and V l * in the decoder like what we have done in (3), i.e.,</p><formula xml:id="formula_7">Attention(Q l * , K l * , V l * ) = softmax ? ? Q l * (K l * ) T d l * k + B l * ? ? V l * ,<label>(6)</label></formula><p>where l * denotes the layer index. d l * k and B l * have the same meaning as those in <ref type="formula" target="#formula_4">(3)</ref>, whose sizes can be easily inferred, accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>For thoroughly comparing nnFormer to previous convnetand transformer-based architecture, we conduct experiments on three datasets/tasks: the brain tumor segmentation task in Medical Segmentation Decathlon (MSD) <ref type="bibr" target="#b35">[36]</ref>, Synapse multiorgan segmentation <ref type="bibr" target="#b36">[37]</ref> and Automatic Cardiac Diagnosis Challenge (ACDC) <ref type="bibr" target="#b37">[38]</ref>. For each experiment, we repeat it for ten times and report their average results. We also calculate p-values to demonstrate the significance of nnFormer.</p><p>Brain tumor segmentation using MRI scans. This task consists of 484 MRI images, each of which includes four channels, i.e., FLAIR, T1w, T1gd and T2w. The data was acquired from 19 different institutions and contained a subset of the data used in the 2016 and 2017 Brain Tumor Segmentation (BraTS) challenges <ref type="bibr" target="#b38">[39]</ref>. The corresponding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Average    </p><note type="other">WT ET TC HD95</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>We run all experiments based on Python 3.6, PyTorch 1.8.1 and Ubuntu 18.04. All training procedures have been performed on a single NVIDIA 2080 GPU with 11GB memory. The initial learning rate is set to 0.01 and we employ a "poly" decay strategy as described in <ref type="bibr">Equation 7</ref>. The default optimizer is SGD where we set the momentum to 0.99. The weight decay is set to 3e-5. We utilize both cross entropy loss and dice loss by simply summing them up. The number of training epochs (i.e., max epoch in <ref type="bibr">Equation 7</ref>) is 1000 and one epoch contains 250 iterations. The number of heads of multi-head self-attention used in different encoder stages is <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">48]</ref> on Synapse. In the rest two datasets, the number of heads becomes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>.</p><formula xml:id="formula_8">lr = initial lr ? (1 ? epoch id max epoch ) 0.9 .<label>(7)</label></formula><p>Pre-processing and augmentation strategies. All images will be first resampled to the same target spacing. Augmentations such as rotation, scaling, gaussian noise, gaussian blur, brightness and contrast adjust, simulation of low resolution, gamma augmentation and mirroring are applied in the given order during the training process.</p><p>Deep supervision. We also add deep supervision during the training stage. Specifically, the output of each stage in the decoder is passed to the final expanding block, where cross entropy loss and dice loss would be applied. In practice, given the prediction of one typical stage, we down-sample the ground truth segmentation mask to match the prediction's resolution. Thus, the final training objective function is the sum of all losses at three resolutions:</p><formula xml:id="formula_9">L all = ? 1 L {H, W, D} + ? 2 L { H 4 , W 4 , D 2 } + ? 3 L { H 8 , W 8 , D 4 } .<label>(8)</label></formula><p>Here, ? {1, 2, 3} denote the magnitude factors for losses in different resolutions. In practice, ? {1, 2, 3} halve with each decrease in resolution, leading to ? 2 = ?1 2 and ? 3 = ?1 4 . Finally, all weight factors are normalized to 1. <ref type="table" target="#tab_4">Table I</ref>, we display network configurations of experiments on all three datasets. Compared to nnUNet, in nnFormer, better segmentation results can be achieved with smaller-sized input patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network configurations. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with transformer-based methodologies</head><p>Brain tumor segmentation. <ref type="table" target="#tab_4">Table II</ref> presents experimental results of all models on the task of brain tumor segmentation. Our nnFormer achieves the lowest HD95 and the highest DSC scores in all classes. Moreover, nnFormer is able to surpass the second best method, i.e., UNETR, by large margins in <ref type="bibr" target="#b1">2</ref> Similar to Synapse, we also follow the evaluation setting of TransUNet.  <ref type="figure">Fig. 4</ref>: Visualization of segmentation results on three wellestablished datasets. We mainly compare nnFormer against nnUNet and UNETR. In addition to segmentation results, we also provide ground truth masks for better comparison.</p><p>both evaluation metrics. For instance, nnFormer outperforms UNETR by over 4.5 mm in average HD95 and nearly 10 percents in DSC of each class. In comparison to previous transformer-based methods, nnFormer shows more strength in HD95 than in DSC.</p><p>Multi-organ segmentation (Synapse). As shown in <ref type="table" target="#tab_4">Table  III</ref>, we make experiments on Synapse and to compare our nnFormer against a variety of transformer-based approaches. As we can see, the best performing methods are LeViT-UNet-384s <ref type="bibr" target="#b21">[22]</ref> and TransUNet <ref type="bibr" target="#b10">[11]</ref>. LeViT-UNet-384s achieves an average HD95 of 16.84 mm while TransUNet produces    <ref type="bibr" target="#b2">[3]</ref>. Conv. Embed. and Conv. Down. represent our convolutional embedding and down-sampling layers, respectively. Skip Att. refers to the proposed skip attention mechanism. 1?LV-MSA in lines 0-2 means that each transformer block contains one transformer layer and each layer consists of one LV-MSA. 1?GV-MSA in lines 3-4 denotes that we replace LV-MSA in the bottleneck with GV-MSA. 1?SLV-MSA and 2?GV-MSA in line 5 mean that we increase the number of transformer layers in each transformer block from one to two. To be specific, in the encoder/decoder, each transformer block contains 1?LV-MSA and 1?SLV-MSA while in the bottleneck, there are 2?GV-MSA in each block.</p><p>an average DSC of 84.36%. In comparison, our nnFormer is able to outperform LeViT-UNet-384s and TransUNet by over 6 mm and 2 percents in average HD95 and DSC, respectively, which are quite impressive improvements on Synapse. To be specific, nnFormer achieves the highest DSC in six organs, including aotra, kidney (left), kidney (right), liver, pancreas and stomach. Compared to previous transformer-based methods, nnFormer is more advantageous in segmentation pancreas and stomach, both of which are difficult to delineate using past segmentation models.</p><p>Automated cardiac diagnosis (ACDC). Statistical significance. In <ref type="table" target="#tab_4">Table II</ref>, III and IV, we employ independent two-sample t-test to calculate p-values between the average performance of our nnFormer and the best performing baseline in both HD95 and DSC. The null hypothesis is that our nnFormer has no advantage over the best performing baseline. As we can see, on all three public datasets, nnFormer produces p-values smaller than 1e-2 under both HD95 and DSC, which indicate strong evidence against the null hypothesis. Thus, nnFormer shows significant improvements over previous transformer-based methods on three different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with nnUNet and Discussion</head><p>In this section, we compare nnFormer with nnUNet, which has been recognized as one of the most powerful 3D medical image segmentation models <ref type="bibr" target="#b39">[40]</ref>.</p><p>Results. In <ref type="table" target="#tab_10">Table V</ref>, we display the class-specific results in both HD95 and DSC metrics to make a thorough comparison. To be specific, from the perspective of the class-specific HD95 results, nnFormer outperforms nnUNet in 11 out of 16 categories. In the class-specific DSC, nnFormer outperforms nnUNet in 9 out of 16 categories. Thus, it seems that nnFormer is more advantageous under HD95, which means nnFormer may better delineate the object boundary. From the view of the average performance, we can see that nnFormer often achieves better average performance. For example, nnFormer outperforms nnUNet on all three public datasets with lower HD95 results, while performing better than nnUNet on two out of three datasets with higher DSC results.</p><p>Statistical significance. To further verify the significance of nnFormer over nnUNet, we also calculate the p-values between the average performance of nnFormer and nnUNet. Similar to what we have done in <ref type="table" target="#tab_4">Table II</ref>, we provide two p-values based on HD95 and DSC on three public datasets, respectively. The most obvious observation is that nnFormer achieves p-values smaller than 0.05 in HD95 on three public datasets. These results suggest that nnFormer is the first choice when HD95 is treated as the primary evaluation metric. Besides, the p-values based on DSC on tumor and multi-organ segmentation (&gt; 0.05) imply that nnFormer is a model comparable to nnUNet, while the results on ACDC demonstrate the significance of nnFormer. In conclusion, nnFormer has slight advantages over nnUNet under DSC.</p><p>Model ensembling. Besides single model performance, we also investigate the diversity between nnFormer and nnUNet, which is a crucial factor in model ensembling. Somewhat surprisingly, we found that by simply averaging the predictions of nnFormer and nnUNet (i.e., nnAvg in <ref type="table" target="#tab_10">Table V)</ref>, it can already boost the overall performance by large margins. For instance, nnAvg achieves the best results in all classes under HD95 and DSC on tumor segmentation. Moreover, nnAvg brings nearly 30% improvements on Synapse when the evaluation metric is HD95. These results indicate that nnFormer and nnUNet are highly complementary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>Table VI displays our ablation study results towards different modules in nnFormer. For simplicity, we made experiments on ACDC and used DSC as the default evaluation metric.</p><p>The most basic baseline in Table VI (line 0) consists of LV-MSA (but without SLV-MSA), the patch merging and embedding layers used in <ref type="bibr" target="#b2">[3]</ref>. We can see that such combination can already achieve a higher average DSC than LeViT-UNet-38 <ref type="bibr" target="#b21">[22]</ref>, which is the best performing baseline in <ref type="table" target="#tab_4">Table IV</ref>. We firstly replaced the patch embedding layer, which is implemented with large kernel size and convolutional stride, with our proposed volume embedding layer, i.e., successive convolutional layers with small kernel size and convolutional stride. We found that the introduced convolutional embedding layer improves the average DSC by approximate 0.4 percents.</p><p>Next, we removed the patch merging layer and added our convolutional down-sampling layer. We found such simple replacement can further boost the overall performance by 0.3 percents. Then, we replaced LV-MSA in the bottleneck with GV-MSA, where we observed 0.2-percent improvements. This phenomenon indicates that providing sufficient larger receptive field can be beneficial to the segmentation task. Afterwards, we use skip attention to replace traditional concatenation/summation operations. Somewhat surprisingly, we found that the skip attention is able to boost the overall performance by 0.4 percents, which demonstrates that the skip attention may serve as an alternative choice other than traditional skip connections. Last but not the least, we investigate adding more transformer layers to each transformer block by cascading an SLV-MSA layer with every LV-MSA layer as in Swin Transformer and doubling the number of global self-attention layers. We found that introducing more transformer layers does bring more improvements to the overall performance as it entangles more long-range dependencies into the learned volume representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of segmentation results</head><p>In <ref type="figure">Figure 4</ref>, we visualize some segmentation results of our nnFormer, nnUNet and UNETR on three public datasets. Compared to UNETR, our nnFormer can greatly reduce the number of false positive predictions. One typical example is the fifth example on ACDC. We can see that UNETR produces a large number of wrong right ventricle pixels outside the myocardium. In contrast, our nnFormer generates no prediction of right ventricle outside the myocardium, which demonstrates that nnFormer is more discriminative than UNETR on ACDC.</p><p>On the other hand, we observe that nnUNet displays very competitive segmentation results, much better than UNETR in nearly all examples. However, we still find that nnFormer maintains clear advantages over nnUNet, one of which is that nnFormer is better at dealing with the boundary. In fact, this phenomenon has been reflected in <ref type="table" target="#tab_4">Table VI</ref>, where nnFormer is significantly better than nnUNet when HD95 is the default evaluation metric. In <ref type="figure">Figure 4</ref>, we can also observe some evidences. For instance, in the second example on Synapse, nnFormer captures the shape of the left kidney and stomach better than nnUNet. Also, in the third example on brain tumor segmentation, nnUNet misses a major part of the non-enhancing tumor enclosed by the edema. These results verify that our nnFormer has the potential to be treated as an alternative to nnUNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present a 3D transformer, nnFormer, for volumetric image segmentation. nnFormer is constructed on top of an interleaved stem of convolution and self-attention.</p><p>Convolution helps encode precise spatial information and builds hierarchical object concepts. For self-attention, nn-Former employs three types of attention mechanism to entangle long-range dependencies. Specifically, local and global volume-based self-attention focus on constructing feature pyramids and providing large receptive field. Skip attention is responsible for bridging the gap between the encoder and decoder. Experiments show that nnFormer maintains great advantages over previous transformer-based models in both HD95 and DSC. Compared to nnUNet, nnFormer is significantly better in HD95 while producing comparable results in DSC. More importantly, we demonstrate that nnFormer and nnUNet can be beneficial to each other in model ensembling, where the simple averaging operation can already produce great improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The interleaved stem used in the encoder of nnFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3D</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3: Three types of attention mechanism in nnFormer. Norm denotes the layer normalization method. MLP is the abbreviation for multi-layer perceptron, which is a two-layer neural network in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Automatic cardiac diagnosis (ACDC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Corresponding author: Liansheng Wang and Yizhou Yu.) This work was done when Hong-Yu Zhou was a visiting student at Xiamen University. Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang and Liansheng Wang are with the Department of Computer Science, Xiamen University, Siming District, Xiamen, Fujian Province, P.R. China (email: whuzhouhongyu@gmail.com, jsguo@stu.xmu.edu.cn, zhangy-inghao@stu.xmu.edu.cn, lswang@xmu.edu.cn). Hong-Yu Zhou and Yizhou Yu are with the Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong (e-mail: yizhouy@acm.org). Xiaoguang Han is with the Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong (Shenzhen), Shenzhen, Guangdong Province, P.R. China (email: hanxiaoguang@cuhk.edu.cn). Lequan Yu is with the Department of Statistics and Actuarial Science, The University of Hong Kong, Pokfulam, Hong Kong (e-mail: lqyu@hku.hk).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>presents the effects of different components used in the encoder of nn-arXiv:2109.03201v6 [cs.CV] 4 Feb 2022</figDesc><table><row><cell>1. Precise spatial encoding.</cell><cell></cell><cell cols="2">Incorporating long-term</cell><cell>Modeling object concepts from high-</cell></row><row><cell cols="2">2. High-resolution low-level features.</cell><cell cols="2">dependencies into high-level features.</cell><cell>level features at multiple scales.</cell></row><row><cell>Convolutional</cell><cell cols="2">Transformer</cell><cell cols="2">Convolutional</cell><cell>Transformer</cell></row><row><cell>embedding</cell><cell></cell><cell>blocks</cell><cell cols="2">down-sampling</cell><cell>blocks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Network configurations of our nnFormer and nnUNet on three public datasets. We only report the down- sampling stride (abbreviated as DS Str.) as the correspond- ing up-sampling stride can be easily inferred according to symmetrical down-sampling operations. Note that the network configuration of nnUNet is automatically determined based on pre-defined hand-crafted rules (for self-adaptation).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Comparison with transformer-based models on brain tumor segmentation. The evaluation metrics are HD95 (mm) and DSC in (%). Best results are bolded while second best are underlined. Experimental results of baselines are from<ref type="bibr" target="#b33">[34]</ref>. We calculate the p-values between the average performance of our nnFormer and the best performing baseline in both metrics.</figDesc><table><row><cell>Methods</cell><cell cols="2">Average HD95 ? DSC ?</cell><cell>Aotra</cell><cell>Gallbladder</cell><cell>Kidney (Left)</cell><cell>Kidney (Right)</cell><cell>Liver</cell><cell>Pancreas</cell><cell>Spleen</cell><cell>Stomach</cell></row><row><cell>ViT [2] + CUP [11]</cell><cell>36.11</cell><cell>67.86</cell><cell>70.19</cell><cell>45.10</cell><cell>74.70</cell><cell>67.40</cell><cell>91.32</cell><cell>42.00</cell><cell>81.75</cell><cell>70.44</cell></row><row><cell>R50-ViT [2] + CUP [11]</cell><cell>32.87</cell><cell>71.29</cell><cell>73.73</cell><cell>55.13</cell><cell>75.80</cell><cell>72.20</cell><cell>91.51</cell><cell>45.99</cell><cell>81.99</cell><cell>73.95</cell></row><row><cell>TransUNet [11]</cell><cell>31.69</cell><cell>77.48</cell><cell>87.23</cell><cell>63.16</cell><cell>81.87</cell><cell>77.02</cell><cell>94.08</cell><cell>55.86</cell><cell>85.08</cell><cell>75.62</cell></row><row><cell>TransUNet [11]</cell><cell>-</cell><cell>84.36</cell><cell>90.68</cell><cell>71.99</cell><cell>86.04</cell><cell>83.71</cell><cell>95.54</cell><cell>73.96</cell><cell>88.80</cell><cell>84.20</cell></row><row><cell>SwinUNet [18]</cell><cell>21.55</cell><cell>79.13</cell><cell>85.47</cell><cell>66.53</cell><cell>83.28</cell><cell>79.61</cell><cell>94.29</cell><cell>56.58</cell><cell>90.66</cell><cell>76.60</cell></row><row><cell>TransClaw U-Net [15]</cell><cell>26.38</cell><cell>78.09</cell><cell>85.87</cell><cell>61.38</cell><cell>84.83</cell><cell>79.36</cell><cell>94.28</cell><cell>57.65</cell><cell>87.74</cell><cell>73.55</cell></row><row><cell>LeVit-UNet-384s [22]</cell><cell>16.84</cell><cell>78.53</cell><cell>87.33</cell><cell>62.23</cell><cell>84.61</cell><cell>80.25</cell><cell>93.11</cell><cell>59.07</cell><cell>88.86</cell><cell>72.76</cell></row><row><cell>MISSFormer [35]</cell><cell>18.20</cell><cell>81.96</cell><cell>86.99</cell><cell>68.65</cell><cell>85.21</cell><cell>82.00</cell><cell>94.41</cell><cell>65.67</cell><cell>91.92</cell><cell>80.81</cell></row><row><cell>UNETR [34]</cell><cell>22.97</cell><cell>79.56</cell><cell>89.99</cell><cell>60.56</cell><cell>85.66</cell><cell>84.80</cell><cell>94.46</cell><cell>59.25</cell><cell>87.81</cell><cell>73.99</cell></row><row><cell>Our nnFormer</cell><cell>10.63</cell><cell>86.57</cell><cell>92.04</cell><cell>70.17</cell><cell>86.57</cell><cell>86.25</cell><cell>96.84</cell><cell>83.35</cell><cell>90.51</cell><cell>86.83</cell></row><row><cell>P-values</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">&lt; 1e-2 (HD95), &lt; 1e-2 (DSC)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Comparison with transformer-based models on multi-organ segmentation (Synapse). The evaluation metrics are HD95 (mm) and DSC in (%). Best results are bolded while second best are underlined.denotes TransUNet uses larger inputs, whose size is 512?512. The p-values are calculated based on the average performance of our nnFormer and the best performing baseline in both metrics.</figDesc><table><row><cell>Methods</cell><cell>Average</cell><cell>RV</cell><cell>Myo</cell><cell>LV</cell></row><row><cell>VIT-CUP [2]</cell><cell>81.45</cell><cell cols="3">81.46 70.71 92.18</cell></row><row><cell>R50-VIT-CUP [2]</cell><cell>87.57</cell><cell cols="3">86.07 81.88 94.75</cell></row><row><cell>TransUNet [11]</cell><cell>89.71</cell><cell cols="3">88.86 84.54 95.73</cell></row><row><cell>SwinUNet [18]</cell><cell>90.00</cell><cell cols="2">88.55 85.62</cell><cell>95.83</cell></row><row><cell>LeViT-UNet-384s [22]</cell><cell>90.32</cell><cell>89.55</cell><cell>87.64</cell><cell>93.76</cell></row><row><cell>UNETR [34]</cell><cell>88.61</cell><cell cols="3">85.29 86.52 94.02</cell></row><row><cell>nnFormer</cell><cell>92.06</cell><cell cols="2">90.94 89.58</cell><cell>95.65</cell></row><row><cell>P-value</cell><cell></cell><cell cols="2">&lt; 1e-2 (DSC)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with transformer-based models on automatic cardiac diagnosis (ACDC). The evaluation metric is DSC (%). Best results are bolded while second best are underlined. The default evaluation metric is DSC, based on which we calculate the p-value. For the split of data, we follow the instruction of UNETR, where ratios of training/validation/test sets are 80%, 15% and 5%, respectively. As above, we use both HD95 and Dice score as evaluation metrics.Synapse for multi-organ CT segmentation. This dataset includes 30 cases of abdominal CT scans. Following the split used in<ref type="bibr" target="#b10">[11]</ref>, 18 cases are extracted to build the training set while the rest 12 cases are used for testing. We report the model performance evaluated with the 95% Hausdorff Distance (HD95) and Dice score (DSC) on 8 abdominal organs, which are aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas and stomach 1 .ACDC for automated cardiac diagnosis. ACDC involves 100 patients, with the cavity of the right ventricle, the myocardium of the left ventricle and the cavity of the left</figDesc><table /><note>target ROIs were the three tumor sub-regions, namely edema (ED), enhancing tumor (ET), and non-enhancing tumor (NET). To be consistent with those results reported in UNETR [34], we display the experimental results of the whole tumor (WT), enhancing tumor (ET) and tumor core (TC) when comparing our nnFormer with transformer-based models.ventricle to be segmented. Each case's labels involve left ventricle (LV), right ventricle (RV) and myocardium (MYO). The dataset is split into 70 training samples, 10 validation samples and 20 test samples. The evaluation metrics include both HD95 and Dice score 2 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ? HD95 ? DSC ?nnUNet [40] 10.78 86.99 5.91 93.01 15.19 71.77 18.60 85.57 6.44 88.18 1.62 97.23 4.52 83.01 24.34 91.86 9.58 85.26 Our nnFormer 10.63 86.57 11.38 92.04 11.55 70.17 18.09 86.57 12.76 86.25 2.00 96.84 3.72 83.35 16.92 90.51 8.58 86.83</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Average HD95 ? DSC ?</cell><cell cols="2">WT HD95 ? DSC ?</cell><cell cols="2">ET HD95 ? DSC ?</cell><cell cols="3">TC HD95 ? DSC ?</cell><cell cols="3">ED HD95 ? DSC ?</cell><cell>NET HD95 ? DSC ?</cell></row><row><cell>nnUNet [40]</cell><cell></cell><cell>4.60</cell><cell>81.87</cell><cell>3.64</cell><cell>91.99</cell><cell>4.06</cell><cell>80.97</cell><cell>4.91</cell><cell cols="2">85.35</cell><cell>4.26</cell><cell cols="2">84.39</cell><cell>6.14</cell><cell>66.65</cell></row><row><cell cols="2">Our nnFormer</cell><cell>4.42</cell><cell>82.02</cell><cell>3.80</cell><cell>91.26</cell><cell>3.87</cell><cell>81.80</cell><cell>4.49</cell><cell cols="2">86.02</cell><cell>4.17</cell><cell cols="2">83.76</cell><cell>5.76</cell><cell>67.29</cell></row><row><cell>P-values</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">&lt; 1e-2 (HD95), 8.8e-2 (DSC)</cell><cell></cell><cell></cell></row><row><cell>nnAvg</cell><cell></cell><cell>4.09</cell><cell>82.65</cell><cell>3.43</cell><cell>92.33</cell><cell>3.69</cell><cell>82.26</cell><cell>4.17</cell><cell cols="2">86.14</cell><cell>3.92</cell><cell cols="2">84.95</cell><cell>5.23</cell><cell>67.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Brain tumor segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Average HD95 ? P-values Methods</cell><cell>Aotra</cell><cell cols="2">Gallbladder</cell><cell cols="5">Kidney (Left) Kidney (Right) 2e-2 (HD95), 7.7e-2 (DSC) Liver</cell><cell>Pancreas</cell><cell></cell><cell>Spleen</cell><cell>Stomach</cell></row><row><cell>nnAvg</cell><cell cols="13">7.70 87.51 5.90 93.11 8.63 72.08 18.42 86.20 8.56 87.76 1.63 97.20 3.64 84.21 9.42 91.94 5.41 87.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Multi-organ segmentation (Synapse)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Methods</cell><cell cols="2">Average HD95 ? DSC ?</cell><cell>RV HD95 ?</cell><cell>DSC ?</cell><cell cols="2">Myo HD95 ?</cell><cell>DSC ?</cell><cell cols="2">LV HD95 ?</cell><cell>DSC ?</cell></row><row><cell></cell><cell></cell><cell cols="2">nnUNet [40]</cell><cell>1.15</cell><cell>91.61</cell><cell>1.31</cell><cell>90.24</cell><cell>1.06</cell><cell></cell><cell>89.24</cell><cell>1.09</cell><cell></cell><cell>95.36</cell></row><row><cell></cell><cell></cell><cell cols="2">Our nnFormer</cell><cell>1.12</cell><cell>92.06</cell><cell>1.23</cell><cell>90.94</cell><cell>1.04</cell><cell></cell><cell>89.58</cell><cell>1.09</cell><cell></cell><cell>95.65</cell></row><row><cell></cell><cell></cell><cell cols="2">P-values</cell><cell></cell><cell></cell><cell cols="5">2e-2 (HD95), &lt; 1e-2 (DSC)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>nnAvg</cell><cell>1.10</cell><cell>92.15</cell><cell>1.19</cell><cell>91.03</cell><cell>1.04</cell><cell></cell><cell>89.75</cell><cell>1.06</cell><cell></cell><cell>95.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) Automated cardiac diagnosis (ACDC)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V :</head><label>V</label><figDesc>Comparison with nnUNet on three public datasets. nnAvg means that we simply average the predictions of nnUNet and nnFormer. Color green denotes the target result of nnAvg is the best among all three approaches. Besides, we also highlight the best results between nnUNet and nnFormer in bold font. We calculate p-values between the average performance of nnUNet and our nnFormer in both metrics on three public datasets.</figDesc><table><row><cell>#</cell><cell>Models</cell><cell>Average</cell><cell>RV</cell><cell>Myo</cell><cell>LV</cell></row><row><cell>0</cell><cell>1?LV-MSA + PM [3] + PE [3]</cell><cell>90.55</cell><cell cols="2">88.59 88.47</cell><cell>94.60</cell></row><row><cell>1</cell><cell>1?LV-MSA + PM [3] + Conv. Embed.</cell><cell>90.97</cell><cell cols="2">88.94 88.84</cell><cell>95.13</cell></row><row><cell>2</cell><cell>1?LV-MSA + Conv. Down. + Conv. Embed.</cell><cell>91.26</cell><cell cols="2">89.70 89.04</cell><cell>95.04</cell></row><row><cell>3</cell><cell>1?LV-MSA + 1?GV-MSA + Conv. Down. + Conv. Embed.</cell><cell>91.46</cell><cell cols="2">89.82 89.17</cell><cell>95.39</cell></row><row><cell>4</cell><cell>1?LV-MSA + 1?GV-MSA + Conv. Down. + Conv. Embed. + Skip Att.</cell><cell>91.85</cell><cell cols="2">90.41 89.50</cell><cell>95.63</cell></row><row><cell>5</cell><cell>1?LV-MSA + 1?SLV-MSA + 2?GV-MSA + Conv. Down. + Conv. Embed. + Skip Att.</cell><cell>92.06</cell><cell cols="2">90.94 89.58</cell><cell>95.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VI :</head><label>VI</label><figDesc>Investigation of the impact of different modules used in nnFormer. PM and PE denote the patch merging and patch embedding strategies used in swin transformer</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell>of the left ventricle (LV). In contrast, nnFormer surpasses</cell></row><row><cell>LeViT-UNet-384s in all classes and by nearly 1.7 percents in</cell></row><row><cell>average DSC, which again verifies its advantages over past</cell></row><row><cell>transformer-based approaches.</cell></row><row><cell>displays</cell></row><row><cell>experimental results on ACDC. We can see that the best</cell></row><row><cell>transformer-based model is LeViT-UNet-384s, whose average</cell></row><row><cell>DSC is slightly higher than SwinUNet while TransUNet</cell></row><row><cell>and SwinUNet are more capable of handling the delineation</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here, we follow the evaluation setting of TransUNet.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ConvNets vs. Transformers: Whose visual representations are more transferable?,&quot; ICCV Workshop on Deep Multi-Task Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">M3net: A multi-scale multi-view framework for multiphase pancreas segmentation based on cross-phase non-local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102232</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crossmodality deep feature learning for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107562</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Are convolutional neural networks or transformers more like human vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07197</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TransUNet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">TransFuse: Fusing transformers and cnns for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08005</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Medical Transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10662</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">TransClaw U-Net: Claw u-net with transformers for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Menghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guangtao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao-Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05188</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TransAttUnet: Multi-level attention-guided u-net with transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05274</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolution-free medical image segmentation using transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasylechko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13645</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DS-TransUNet: Dual swin transformer u-net for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06716</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Medical image segmentation using squeeze-and-expansion transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09511</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SpecTr: Spectral transformer for hyperspectral pathology image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03604</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">LeViT-UNet: Make faster encoders with transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08623</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">More than encoder: Introducing transformer decoder to upsample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10637</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CoTr: Efficiently bridging cnn and transformer for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03024</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">TransBTS: Multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UNETR: Transformers for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MISSFormer: An effective medical image segmentation transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07162</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05735</idno>
		<title level="m">The medical segmentation decathlon</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Miccai multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI: Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI: Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: Is the problem solved?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cervenansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lekadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A G</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
