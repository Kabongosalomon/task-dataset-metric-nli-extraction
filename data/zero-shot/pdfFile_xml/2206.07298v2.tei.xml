<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S 2 -FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">A M</forename><surname>Elhassan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewodros</forename><surname>Legesse Munea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Huaqiao University</orgName>
								<address>
									<postCode>361021</postCode>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S 2 -FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>Scale-aware strip attention</term>
					<term>deep convolutional neural networks</term>
					<term>real-time semantic segmentation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern high-performance semantic segmentation methods employ a heavy backbone and dilated convolution to extract the relevant feature. Although extracting features with both contextual and semantic information is critical for the segmentation tasks, it brings a memory footprint and high computation cost for real-time applications. This paper presents a new model to achieve a trade-off between accuracy/speed for real-time road scene semantic segmentation. Specifically, we proposed a lightweight model named Scale-aware Strip Attention Guided Feature Pyramid Network (S 2 -FPN). Our network consists of three main modules: Attention Pyramid Fusion (APF) module, Scale-aware Strip Attention Module (SSAM), and Global Feature Upsample (GFU) module. APF adopts an attention mechanisms to learn discriminative multi-scale features and help close the semantic gap between different levels. APF uses the scale-aware attention to encode global context with vertical stripping operation and models the long-range dependencies, which helps relate pixels with similar semantic label. In addition, APF employs channel-wise reweighting block (CRB) to emphasize the channel features. Finally, the decoder of S 2 -FPN then adopts GFU, which is used to fuse features from APF and the encoder. Extensive experiments have been conducted on two challenging semantic segmentation benchmarks, which demonstrate that our approach achieves better accuracy/speed trade-off with different model settings. The proposed models have achieved a results of 76.2%mIoU/87.3FPS, 77.4%mIoU/67FPS, and 77.8%mIoU/30.5FPS on Cityscapes dataset, and 69.6%mIoU,71.0% mIoU,and 74.2% mIoU on Camvid dataset.The code for this work will be made available at https://github.com/mohamedac29/S2-FPN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic segmentation is an essential high-level topic in computer vision and has been widely used in various challenging problems such as, medical diagnosis, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, autonomous vehicles <ref type="bibr" target="#b2">[3]</ref>, and scene analysis <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Unlike image classification, which aims to classify the whole image, semantic segmentation predicts the per-pixel class for each image content. The current frontiers in semantic segmentation methods are driven by the success of deep convolution neural networks, specifically the fully convolution network (FCN) Framework <ref type="bibr" target="#b5">[6]</ref>. In the original FCN approach to obtain larger receptive fields, we increase the depth of the network with more convolutional and downsampling layers. However, simply increasing the number of downsampling operations leads to reduced feature resolution, which causes spatial information loss. On the other hand, the increased number of convolutional layers adds more challenges to network optimization.</p><p>Over the years, various approaches have been proposed to tackle the drawbacks of the typical FCN-based architecture. Here  we mention the following: (i) increasing the resolution of feature maps or maintaining a high-resolution feature across stages, e.g., through decoder network <ref type="figure">(Figure 2</ref> (a)) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, dilated convolutions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> or high resolution <ref type="bibr" target="#b10">[11]</ref>. (ii) improve the segmentation performance with the subsequent approaches, e.g., PSPNet <ref type="bibr" target="#b4">[5]</ref>, DeepLab <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, PAN <ref type="bibr" target="#b12">[13]</ref> DenseASPP <ref type="bibr" target="#b13">[14]</ref> , RefineNet <ref type="bibr" target="#b14">[15]</ref>, PPANet <ref type="bibr" target="#b15">[16]</ref>, and ParseNet <ref type="bibr" target="#b16">[17]</ref>. These previous networks enlarge the receptive field to exploit the rich contextual information. Nevertheless, the latter solution is infeasible for real-time applications because it requires heavy networks and expensive computation. (iii) some networks such as <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have added spatial encoding path to preserve the spatial details as shown in <ref type="figure">Figure 2</ref>.b. To perform fast segmentation with light computation cost, small size and satisfactory segmentation accuracy, there are certain design philosophy that could be followed: (i) incorporate a pre-trained lightweight classification networks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> as a backbone to construct segmentation architecture such as BiSeNet <ref type="bibr" target="#b18">[19]</ref>, STDC-Seg <ref type="bibr" target="#b23">[24]</ref>. (ii) design a building block that is suitable for low computation cost <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. (iii) put into consideration the downsampling strategy and the depth of the network <ref type="bibr" target="#b26">[27]</ref>. (iv) combine the low-level and high-level features using the multi-path framework <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>. in addition to that, restricting the input image size plays an important part in increasing the inference speed. Furthermore, many existing frameworks <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> have been developed based on FPN <ref type="bibr" target="#b32">[33]</ref> to exploit the inherent multi-scale feature representation of deep convolutional networks. More specifically, FPN-based architectures combine largereceptive field features, low-resolution-with small-receptive-field features, high-resolution to capture objects at different scales. For instance, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> utilize the lateral path to fuse adjacent features in a top-down manner. This way FPN strengthen the learning of multi-scale representation.Until now, feature pyramid network based approaches have achieved state-of-arts performance in object detection. Nevertheless, the fusion of adjacent features from different stages ignores the semantic gap between these features, which degrades the multi-scale feature representation (FPN-like model is shown in <ref type="figure">Figure 2</ref> (c). To alleviate this problem and achieve a better accuracy/speed trade-off, we propose Scale-aware Strip Attention guided Pyramid Network(S 2 -FPN), which can extract multi-scale/multilevel representations while maintaining high computation efficiency. <ref type="figure">Fig. 1</ref> illustrates the accuracy (mIoU) and inference speed (FPS) obtained by several state-of-the-art methods, including PSPNet <ref type="bibr" target="#b4">[5]</ref>, DeepLab <ref type="bibr" target="#b9">[10]</ref>, ENet <ref type="bibr" target="#b33">[34]</ref>,ICNet <ref type="bibr" target="#b34">[35]</ref>, DABNet <ref type="bibr" target="#b5">[6]</ref>, DFANet-A <ref type="bibr" target="#b35">[36]</ref>, DFANet-B <ref type="bibr" target="#b35">[36]</ref>, BiSeNet1 <ref type="bibr" target="#b18">[19]</ref>, BiSeNet2 <ref type="bibr" target="#b18">[19]</ref>, FasterSeg <ref type="bibr" target="#b36">[37]</ref>, TD4-Bise18 <ref type="bibr" target="#b37">[38]</ref>, FANet-18 <ref type="bibr" target="#b38">[39]</ref>, FANet-34 <ref type="bibr" target="#b38">[39]</ref>, LBN-AA <ref type="bibr" target="#b39">[40]</ref>, AGLNet <ref type="bibr" target="#b39">[40]</ref>, BiSeNetV2 <ref type="bibr" target="#b27">[28]</ref>, BiSeNetV2-L <ref type="bibr" target="#b27">[28]</ref>, HMSeg <ref type="bibr" target="#b40">[41]</ref>, TinyHMSeg <ref type="bibr" target="#b40">[41]</ref>, STDC2-Seg50 <ref type="bibr" target="#b23">[24]</ref>, STDC2-Seg50 <ref type="bibr" target="#b23">[24]</ref>, and our proposed method, on the Cityscapes test dataset.</p><p>The main contributions are summed as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Efficient Network Designs</head><p>In recent literature, real-time model design has become an essential part of computer vision research development. Several lightweight architectures were introduced as a trade-off between accuracy and latency SqueezeNet <ref type="bibr" target="#b19">[20]</ref>, MobileNet <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b41">[42]</ref>, ShuffleNet <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b42">[43]</ref>,and Xception <ref type="bibr" target="#b22">[23]</ref>. SqueezeNet reduces the number of parameters through the squeeze and expansion module. MobileNetV1 implemented depth-wise separable convolutions to reduce the number of parameters. MobileNetV2 proposed an inverted residual block to alleviate the effect of depth-wise separable convolutions. ShuffleNet and Xception utilize group convolution to reduce FLOPs. ResNet <ref type="bibr" target="#b43">[44]</ref> adopted residual blocks to obtain outstanding performance. These models were designed primarily for image classification tasks. Later on, incorporated to work as backbones for semantic segmentation networks to achieve realtime semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Real-time Segmentation</head><p>Real-time semantic segmentation methods aim to predict dense pixels accurately while maintaining high inference speed. Over the years there are different approaches have been proposed in this regard. ENet <ref type="bibr" target="#b44">[45]</ref>, ERFNet <ref type="bibr" target="#b25">[26]</ref>, and ESPNet <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> proposed compact semantic segmentation networks that utilize lightweight backbones. LEDNet <ref type="bibr" target="#b47">[48]</ref>, DABNet <ref type="bibr" target="#b24">[25]</ref> introduced a lightweight model to address the computation burden that limits the usage of deep convolutional neural networks in mobile devices. The former used asymmetric encoder-decoder-based architecture and implemented channel split and shuffle operations to reduce the computation cost. The latter structured a depth-wise symmetric building block to jointly extract local and global context information. FDDWNet <ref type="bibr" target="#b48">[49]</ref> utilized factorized depth-wise separable convolutions to reduce the number of parameters and the dilation to learn multi-scale feature representations. ICNet <ref type="bibr" target="#b49">[50]</ref> proposes a compressed version of PSPNet based on an image cascade network to increase the semantic segmentation speed. DFANet <ref type="bibr" target="#b26">[27]</ref> obtains the multi-scale feature propagation by utilizing a substage and sub-network aggregation.Tow-pathway networks such as BiSeNetV2 <ref type="bibr" target="#b27">[28]</ref> and DSANet <ref type="bibr" target="#b17">[18]</ref> Fast-SCNN <ref type="bibr" target="#b50">[51]</ref> introduce a shallow detailed branch to extract the spatial details and a deep semantic branch to extract the semantic context. This paper propose a lightweight model that utilizes (ResNet18 or ResNet34) as backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Attention Mechanism</head><p>Recent studies have shown that attention mechanisms can be successfully applied to different tasks such as machine translation <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, image classification <ref type="bibr" target="#b53">[54]</ref>, object detection <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> and semantic segmentation <ref type="bibr" target="#b56">[57]</ref>. The attention mechanisms perform better than convolutional neural networks when used to model the long-range dependencies. SENet <ref type="bibr" target="#b57">[58]</ref> proposed the squeeze and excitation module to re-calibrate the channel dependency dynamically. Motivated by self-attention, DANet <ref type="bibr" target="#b58">[59]</ref> improves feature representation by applying channel attention and position attention, while OCNet <ref type="bibr" target="#b59">[60]</ref> uses self-attention to learn the object context. In this paper, to construct an efficient and effective approach for modelling the long-range dependencies, we introduce scale-aware attention with vertical striping operation to reduce the computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-scale Feature Fusion</head><p>One of the main difficulties in semantic segmentation is effectively processing and representing multi-scale features. Earlier semantic segmentation methods perform multilevel feature representations fusion <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Multi-branch architectures have been adopted to tackle the issue of multi-scale fusion, e.g., BiSiNetV1 <ref type="bibr" target="#b18">[19]</ref>, BiSiNetV2 <ref type="bibr" target="#b27">[28]</ref> and DSANet <ref type="bibr" target="#b17">[18]</ref>, add a shallow branch to preserve the spatial details. ICNet fused branches with different input resolutions. SPFNet <ref type="bibr" target="#b60">[61]</ref> proposed a method for learning separate spatial pyramid fusion for each feature subspace. Feature pyramid network <ref type="bibr" target="#b32">[33]</ref> is widely adopted in the object detection task <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b61">[62]</ref> to address the problem of multi-scale feature representation. The feature pyramid network architecture introduces a top-down routing to fuse features. Based on FPN, NAS-FPN <ref type="bibr" target="#b28">[29]</ref> utilizes the neural architecture search to obtain optimal topology. EfficientDet <ref type="bibr" target="#b30">[31]</ref> acquires more higher-level feature fusion using a repeated bidirectional path. PANet <ref type="bibr" target="#b29">[30]</ref> proposes a method that facilitates the information flow through a bottom-up path augmentation. This work proposes the Attention Pyramid Fusion module, which adopts a lightweight scale-aware attention strategy to bridge the semantic gap between the different levels more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>This section describes the proposed Scale-aware Strip Attention Guided Feature Pyramid Network (S 2 -FPN). We first give an overview of S 2 -FPN in Section 3.1. Then, we introduce the different components of S 2 -FPN from Section 3.2 to Section 4. Finally, we present the experimental results and analysis in Section 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scale-aware Strip Attention Module</head><p>In order to capture the long-range dependencies and reduce the computation cost, we introduce the Scale-aware Strip Attention module. In particular, inspired by the results in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b62">[63]</ref>; and to maintain the contextual consistency, we apply striping operation to collect long-range context along the vertical axis. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the details structure of Scale-aware Strip Attention </p><formula xml:id="formula_0">module. Let F Ref ine ? R C?H?W be an input feature maps (F Ref ine is the output of feature refinement block (FRB) in APF Figure 5) where C is</formula><formula xml:id="formula_1">Z avg = G pool (F R )<label>(1)</label></formula><formula xml:id="formula_2">Z max = G pool (F R )<label>(2)</label></formula><p>Where G pool (.) represents max or average pooling operations. Then SSAM applies 1 ? 1 convolution layer with shared weights to Z max and Z avg (Equations 3 and 3) to transfer the height-wise context information of every feature within F R , and employs a softmax function to generate an attention map that highlights the importance of corresponding feature maps in F R (Equation 5).</p><formula xml:id="formula_3">F 1 = f 1s (Z avg )<label>(3)</label></formula><formula xml:id="formula_4">F 2 = f 1s (Z max )<label>(4)</label></formula><formula xml:id="formula_5">A = ?(F 1 F 2 )<label>(5)</label></formula><p>Where f 1s (.) represents a convolutional layer with shared weights. The attention mechanism can select the appropriate scale feature dynamically and fuse feature of different scale by selflearning. Next, we have performed element-wise multiplication operations ( ) between attention maps and F 1 ,F 2 to generate the scaled feature map F scale . Finally, an element-wise sum operation between F scale and the input feature map F is employed with learnable parameters ? to obtain the final output F SSAM as in Equation 7 </p><formula xml:id="formula_6">F scale = A F 2 + A F 2<label>(6)</label></formula><formula xml:id="formula_7">F SSAM = ?F scale + (? ? 1)F<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Pyramid Fusion (APF) Module</head><p>Maintaining detailed information of an object when passing an image through the encoder and decoder is a challenging problem. Feature pyramid network <ref type="bibr" target="#b32">[33]</ref>, which was first introduced for object detection, has been used to leverage the multi-level feature from the backbone (see <ref type="figure">Figure 2</ref> c). However, inherent defects in FPN inhibit it from extracting sufficient discriminative features. For instance, the extracted feature by the lateral connection in FPN could suffer information loss in the high-level layers. Another issue is that using bilinear interpolation operation and simple element-wise addition of adjacent features ignores the semantic gaps of different depths. Based on these observations, we propose the Attention Pyramid Fusion (APF) module, which can learn discriminative multi-scale features. The detailed structure of our Attention Pyramid Fusion block is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. In APF, the scale-aware strip attention guides the deeper stages to aggregate high-resolution features, while channel-wise attention employs channel reweighting. By combining these attention mechanisms, APF is able to extract more detailed information in the final segmentation map. In order to combine the coarse F i and low-level F i?1 adjacent feature maps, we first apply a 1x1 convolution on F i?1 followed by batch normalization and ReLU operation. Meanwhile, we upsample the coarse feature map generated with CFGB to match the low-level feature in terms of channel and spatial dimensions. Then the coarse and the low-level feature maps are concatenated channel-wise to produce F concat , and the output of this operation is fed into a feature refinement block (FRB) to reduce the aliasing effect. FRB is constructed by stacking two convolutional layers, 1 ? 1 and 3 ? 3 each, followed by batch normalization and ReLU. Furthermore, we have developed two modules to reduce the gap between different levels and enhance semantic consistency. Specifically, We first apply a 3 ? 3 convolution followed by batch normalization and ReLU on the low-level path, then multiplied by channel attention <ref type="figure" target="#fig_5">(Fig 5.b )</ref> to generate a feature reweighting (CRB) that filters the irrelevant information (see <ref type="figure" target="#fig_5">Fig 5)</ref>. CAM employs global average pooling to extract global context and learn the weights for the channel sub-module. The final output of feature reweighting branch X A is given in Equation <ref type="bibr" target="#b7">8</ref>.</p><formula xml:id="formula_8">X A = f 3 (f 1 (F i?1 )) F C (F R )<label>(8)</label></formula><p>where f 3 (?) represents the convolution layer whose kernel size is 3 ? 3, f 1 (?) represents the convolution layer whose kernel size is 1?1. Secondly, we introduce scale-aware attention to aggregate discriminative contextual information. The output of SSAM (F S ) is multiplied by the output of the CFGB branch. The output of feature reweighting branch X B is given in <ref type="bibr">Equation 9</ref>.</p><formula xml:id="formula_9">X B = f 3 (UP(F i )) F S (F R )<label>(9)</label></formula><p>where UP(?) represents the bilinear upsample operation. The two branches X A and X B are fused with an element-wise summation to produce F out . Finally, the output feature is refined using a 3 ? 3 convolution, dropout, and projection layer to produce the deep supervision in the four-level hierarchy. Similarly, the output is passed into a feature refinement block consisting of 3 ? 3 convolution to generate the output for attention pyramid fusion of the next stage.</p><formula xml:id="formula_10">F out = X A + X B<label>(10)</label></formula><p>Where F out represents the fused coarse and low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GLOBAL FEATURE UPSAMPLE (GFU)</head><p>In typical U-Net or encoder-decoder architectures, the extracted high-level feature maps are directly upsampled in the decoding   part to restore the original resolution. However, if the highlevel feature can be fully propagated, it will obtain an accurate segmentation map. We applied a depthwise convolution with stride to F 5 to obtain enough information to produce a coarse semantic segmentation. On the other hand, the features from the attentionbased feature pyramid network are rich in details information about different objects, which are beneficial in recognizing the boundaries between objects. We can get an accurate semantic map by combining the details and coarse semantic information. The proposed GFU module is shown in <ref type="figure" target="#fig_6">Fig.6</ref>. For the high-level features, the proposed global feature upsampling module firstly performs a bilinear upsampling on the input feature to sample it to the same dimensions of AFPN features. We then applied 1 ? 1 convolution and non-linearity followed by global average pooling to generate a global context and sample it to the same channels of AFPN. In the meantime, we performed 1 ? 1 convolution with batch normalization and relu on the feature from AFPN and added high-level features. Finally, the output feature is obtained by applying 1 ? 1 convolution with batch normalization and relu. The proposed GFU can process and integrate features maps from different stages more efficiently. The output of this block X GF U can be summarized as follows:</p><formula xml:id="formula_11">BiSeNetV2/BiSeNetV2_L [28] - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_12">X GF U = f 1 (G pool (f 1 (ReLU (UP(X F )))) + f 1 (X P )) (11)</formula><p>where ReLU is the Rectified Linear Unit, X F denotes the feature map from the feature adaptation block (FAB), X P represents the feature from the attention pyramid fusion (APF) module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We evaluate the effectiveness of our proposed segmentation network on two datasets: Cityscapes <ref type="bibr" target="#b64">[65]</ref> and Camvid <ref type="bibr" target="#b65">[66]</ref>. We first introduce the training details. Then, we provide our full report of S 2 -FPN compared to other state-of-the-arts methods on the two benchmarks. The following criteria are used for the performance evaluation, i.e. accuracy,speed,backbone,input resolution and parameters in Cityscapes. Experimental results demonstrate that S 2 -FPN obtains a trade-off between accuracy and speed on both Camvid and Cityscapes datasets. In the following subsection, we provide further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training setting details</head><p>All the experiments are implemented with one Nvidia GTX 1080Ti GPU, on the PyTorch platform <ref type="bibr" target="#b44">[45]</ref>. We use ResNet with weights pre-trained on ImageNet <ref type="bibr" target="#b66">[67]</ref> as our backbone. The Adam optimizer <ref type="bibr" target="#b67">[68]</ref> with initial learning rate of 3e ? 4 is used to train the model on both Camvid and Cityscapes datasets. We use weight decay of 5e-6. Following the previous methods settings <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we decay the learning rate using polynomial learning rate scheduling using equation (1 ? iter maxiter ) power . The model has been trained for 500 epochs on Cityscapes dataset . For Camvid dataset we have train for 180 epochs when the model configured with ResNet18 and 150 when configured modified ResNet34. All BatchNorm layers in our architecture replace by InPlaceABN-Sync <ref type="bibr" target="#b68">[69]</ref>. To avoid over-fitting, We apply a number of data augmentations techniques as follows: random resize with scale range [0.75,1.0,1.25,1.5,1.75, 2.0], random horizontal flipping and random cropping images to a resolution of 512 ? 1024 for Cityscapes and 360 ? 360 for Camvid. We adopt a well-known metrics to evaluate our model. Frame persecond (FPS) and mean Intersection over Union (mIoU), which measure the semantic segmentation latency and accuracy, respectively. Moreover, we evaluate the computational complexity and memory consumption based on floating point operations (FLOPs) and model parameters (Params), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cityscapes Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Ablation Study</head><p>In this subsection, we conduct a series of experiments to explore the effect of each component (GFU, APF, SSAM, Supervision) in the proposed method. SS-FPN is implemented with a different backbone setting (ResNet18, ResNet34) and we have tested the model with the different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Ablation Study With Modules</head><p>In this section, we conduct an empirical evaluation to ensure the effectiveness of the different components in our architecture. We show the importance of the scale-aware strip attention module   <ref type="figure" target="#fig_2">Fig 3 by</ref> attaching it to the encoder <ref type="figure" target="#fig_2">(Fig 3 (a)</ref>. Essentially, the GFU is designed to attain category information without using complex decoder blocks. The result in <ref type="table" target="#tab_7">Table 4</ref> shows that adding GFM obtains 71.9 % mIoU, which add 6.2%mIoU improvement to the baseline. Moreover, we add APF without supervision or scaleaware strip attention module (replaced with 1 ? 1 convolution), it brings 3.7 %mIoU performance gain. Using deep supervision has added 0.2 % mIoU improvement. To capture the long-range dependencies, We further evaluate the significance of the scaleaware attention module by adding it into APF with all the other modules. <ref type="table" target="#tab_7">Table 4</ref>, shows this module improves the performance by 0.5%mIoU. The ablation study and analyses highlighted the importance of each component in our network. S 2 -FPN has improved the baseline by 10.7 % mIoU according to our training settings. Thus, it presents an important strategy to design a network with accuracy/speed trade-off. Online Bootstrapping: Camvid and Cityscapes datasets contain easy and hard pixels. One efficient way to train these hard pixels is to automate their selection process during training. Following the previous works[], we utilize OHEM or online hard example mining for hard training pixels. The examples with a probability less than the threshold are considered hard training pixels. In our experiments, k depends on the image crop size and the threshold is set to 0.7. <ref type="table" target="#tab_6">Table 3</ref> shows that using OHEM increases the performance by 0.9 % mIoU over Cross-entropy on Cityscapes validation dataset.</p><p>Ablation study with different baselines: We further detailed the performance comparison of three settings for our proposed S 2 -FPN model on the Cityscapes validation dataset. Specifically, we train the model using the following baseline, ResNet18, ResNet34 and a modified version of ResNet34 as shown in <ref type="table" target="#tab_5">Table 2</ref>. S 2 -FPN obtains accuracy of 76.4% mIoU and 87.3 FPS, which has a better inference speed than using ResNet34 variants with minor inaccuracy. Using the modified ResNet34 (S 2 -FPN34M) achieve the highest accuracy with 0.4 % mIoU increase over S 2 -FPN34 with the same number of parameters, but with very high  Effectiveness of Attention Pyramid Fusion (APF) module:. To verify the effectiveness of the attention pyramid fusion module, we compare it with the conventional feature pyramid network. As shown in <ref type="table" target="#tab_8">Table 5</ref>, the attention feature pyramid module achieves 73.9% mIoU, which is 4.8 % higher than the feature pyramid network. The result shows that channel reweighing and contextual information encoding are effectively incorporated in the proposed attention pyramid fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with State-of-the-Art Methods</head><p>In <ref type="table" target="#tab_9">Table 6</ref>, we present the comparisons between our S 2 -FPN and the state-of-the-arts real-time semantic segmentation methods. Our method is tested on a single GTX 1080Ti GPU with image resolution of 512 ? 1024. We tested the speed without any accelerating strategy and used only fine-data to train the model. As stated in <ref type="table" target="#tab_9">Table 6</ref>, our proposed network setting get 76.2% mIoU with 102 FPS by using ResNet18 as the backbone network. As can be observed, the accuracy is better than all other methods except for STD-Seg75, which has 0.2 % mIoU better than our small model. The inference speed is significantly faster, even though we tested it using GTX1080 ti, this accuracy and inference speed prove that even with light backbones our approach still obtains better performance than other approaches. Besides, S 2 -FPN with ResNet34 base model obtains 85 FPS inference speed with 77.4 % mIoU accuracy, which is state of the art on speed/accuracy tradeoffs on the Cityscapes benchmark and exceed the best model STD-Seg75 by a margin of 0.6 % mIoU. Finally, we have modified the ResNet34 backbone by changing the stride in stage 2 from 2 to 1 and run the model (we name it S 2 -FPN34M), this modification increases the accuracy by about 0.4 % mIoU but reduced the speed significantly to 30.5 FPS. We give a detail of per-class accuracy of our three configurations <ref type="table" target="#tab_3">Table 1</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on Camvid Dataset</head><p>As illustrated in <ref type="table" target="#tab_10">Table 7</ref>, we evaluate the segmentation accuracy of our proposed method on the Camvid dataset. We trained with an input image size of 360x480, with no use of external data. The training images used to train the model and validation images to evaluate our models while the testing samples are used to get the result for comparison with other methods. Overall, our model has better accuracy. Our S 2 -FPN model with ResNet18 as a backbone achieves a 69.6 % mIoU on the test set, while S 2 -FPN34 obtains accuracy of 71.0 % mIoU, which is a significant trade-off between speed and accuracy. The last model configuration S 2 -FPN34M which replaces the stride at stage 2 from 2 to 1 achieves 74.2% mIoU . This further shows the validity and effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we designed architecture for semantic segmentation that achieves a better trade-off accuracy/speed and presents Scaleaware Strip Attention S 2 -FPN for scene parsing in real-time. First, we introduce Scale-aware Strip Attention and channel attention modules to model the long-range dependencies. Specifically, by using scale-aware and vertical strip operations, our network drastically reduced the computation cost of the attention mechanism. Furthermore, we propose the Attention Pyramid Fusion module, which enables and facilitates acquiring contextual information by utilizing the attention mechanism. The ablation studies show the effectiveness of scale-ware strip attention and attention pyramid fusion modules. Experimental results demonstrate that S 2 -FPN achieves accuracy/speed trade-offs on Camvid and Cityscapes.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>E-mail addresses: mohammedac29@stu.xmu.edu.cn (Mohammed A. M. Elhassan). ? E-mail addresses: chyang@xmu.edu.cn ( Chenhui Yang). ? E-mail addresses: supermonkeyxi@xmu.edu.cn ( Chenxi Huang) ? E-mail addresses: teddylegessemunea@gmail.com ( Tewodros Legesse Munea) ? E-mail addresses: xinhong@hqu.edu.cn ( Xin Hong) ? * Correspondence should be addressed to Chenhui Yang</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 : 2 FPNFig. 2 :</head><label>122</label><figDesc>Accuracy/Speed performance comparison on the Cityscapes test set. Our methods are presented in red dots while other methods are presented in blue dots. Our approaches achieve state-of-the-art speed-accuracy trade-off arXiv:2206.07298v2 [cs.CV] 16 Jun 2022 (a) Encoder-decoder (b) Two Pathway (c) FPN (d) S A comparison of an important semantic segmentation architectures,(a) encoder-decoder model,(b) two-pathway model,(c) feature pyramid model,(d) Scale-aware feature pyramid model (Ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The detailed architecture of the proposed Scale-aware Strip Attention Guided Pyramid Fusion model (S 2 -FPN). The model constructs from the following modules: (a) Encoder, which incorporates ResNet18 or ResNet34, (b) Attention Pyramid Fusion module, (c) illustrates the Global Feature Upsample(GFU) module, (e) Feature adaptation block (FAB), and (e) Components of Coarse Feature Generator block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The illustration of Scale-Aware Strip Attention module SSAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>illustrates the proposed S 2 -FPN, which follows the feature pyramid network and encoder-decoder architectures. S 2 -FPN consists of three main parts: Feature Extractor or Encoder, Attention Pyramid Fusion (APF), and Global Feature Upsample (GFU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>An overview of the Attention Pyramid Fusion Module. (a) APF module architecture. (b) Components of the channel attention module (CAM). (c) Components of the feature refinement block (FRB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of global feature upsampling module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>(a) Original (b) S 2 -FPN18 (c) S 2 -FPN34 (d) S 2 -FPN34M (e) Ground Truth Visual results of our method S 2 -FPN on Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Visual results of our method S 2 -FPN on Camvid dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>FPN takes ResNet18 and ResNet34<ref type="bibr" target="#b43">[44]</ref> as base models for feature extraction while removing the global average pooling and the softmax. We utilize {F 1 , F 2 , F 3 , F 4 , F 5 } as a feature hierarchy from the base model with strides {2, 4, 8, 16, 32}. Then, we attached a coarse feature generator block and feature adaptation block in parallel after F 5 . CFGB is constructed from a depthwise convolution layer with stride 2 to generate coarse features for the attention pyramid fusion, while FAB is produced through a depth-wise convolutional layer with stride 1 to project the feature maps of the encoder into the global feature upsample module. Note that the attention pyramid features have strides of {4, 8, 16, 23} pixel w.r.t the input image. {AP F 2 , AP F 3 , AP F 4 , AP F 5 } are the top-down features generated using the Attention Feature Pyramid (APF) module (Section 3.3).</figDesc><table /><note>). In particular, Coarse Feature Generator Block (CFGB), Feature Adaptation Block (FAB), and Scale-aware Strip Attention Module (SSAM). S 2 -</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the number of channels, W and H are the spatial dimensions. SSAM first extracts height-wise contextual information from each row of the refined feature maps F Ref ine by aggregating the C ? H ? W input representation into a Z max ? R C?H?1 , Z avg ? R C?H?1 using Max Poling or Average Pooling operation in parallel. Z avg and Z max are illustrated in Equation 1 and 2, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>THE PER-CLASS, CLASS, AND CATEGORY IOU EVALUATION ON THE CITYSCAPES TEST SET."</figDesc><table><row><cell>Method</cell><cell cols="9">Road S.Walk Build Wall Fence Pole T-Light T-Sign Veg</cell><cell cols="2">Terrain Sky</cell><cell cols="3">Person Rider Car</cell><cell cols="2">Truck Bus</cell><cell>Tra</cell><cell cols="2">Motor Bic</cell><cell>mIoU</cell></row><row><cell>CRF-RNN [34]</cell><cell>96.3</cell><cell>73.9</cell><cell>88.2</cell><cell>47.6</cell><cell>41.3</cell><cell cols="2">35.2 49.5</cell><cell>59.7</cell><cell cols="2">90.6 66.1</cell><cell cols="2">93.5 70.4</cell><cell>34.7</cell><cell>90.1</cell><cell>39.2</cell><cell cols="3">57.5 55.4 43.9</cell><cell>54.6</cell><cell>62.5</cell></row><row><cell>FCN [6]</cell><cell>97.4</cell><cell>78.4</cell><cell>89.2</cell><cell>34.9</cell><cell>44.2</cell><cell cols="2">47.4 60.1.5</cell><cell>65.0</cell><cell cols="2">91.4 69.3</cell><cell cols="2">93.9 77.1</cell><cell>51.4</cell><cell>92.6</cell><cell>35.3</cell><cell cols="3">48.6 46.5 51.6</cell><cell>66.8</cell><cell>65.3</cell></row><row><cell>DeepLabv2 [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.4</cell></row><row><cell>Dilation10 [36]</cell><cell>97.6</cell><cell>79.2</cell><cell>89.9</cell><cell>37.3</cell><cell>47.6</cell><cell cols="2">53.2 58.6</cell><cell>65.2</cell><cell cols="2">91.8 69.4</cell><cell cols="2">93.7 78.9</cell><cell>55.0</cell><cell>3.3</cell><cell>45.5</cell><cell cols="3">53.4 47.7 52.2</cell><cell>66.0</cell><cell>67.1</cell></row><row><cell>AGLNet [64]</cell><cell>97.8</cell><cell>80.1</cell><cell>91.0</cell><cell>51.3</cell><cell>50.6</cell><cell cols="2">58.3 63.0</cell><cell>68.5</cell><cell cols="2">92.3 71.3</cell><cell cols="2">94.2 80.1</cell><cell>59.6</cell><cell>93.8</cell><cell>48.4</cell><cell cols="3">68.1 42.1 52.4</cell><cell>67.8</cell><cell>70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">COMPARISON BETWEEN S 2 -FPN16, S 2 -FPN34,</cell></row><row><cell cols="5">AND S 2 -FPN34M IN TERMS OF PARAMETERS, FRAME PER</cell></row><row><cell cols="5">SECOND AND ACCURACY ON CITYSCAPES VALIDATION</cell></row><row><cell>SET</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Parameters(M) Flops FPS mIoU</cell></row><row><cell>S 2 -FPN18</cell><cell>17.8</cell><cell>29.1</cell><cell>87.3</cell><cell>76.4</cell></row><row><cell>S 2 -FPN34</cell><cell>27.9</cell><cell>48.4</cell><cell>67</cell><cell>77.1</cell></row><row><cell>S 2 -FPN34M</cell><cell>27.9</cell><cell cols="2">190.0 30.5</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: COMPARISON CORSSENTROPY AND OHEM</cell></row><row><cell cols="4">LOSS ON CITYSCAPES VALIDATION DATASET</cell><cell></cell></row><row><cell cols="5">Method Params Flops FPS mIoU</cell></row><row><cell>CE</cell><cell>17.5</cell><cell>29.1</cell><cell>87</cell><cell>75.6</cell></row><row><cell>OHEM</cell><cell>17.5</cell><cell>29.1</cell><cell>87</cell><cell>76.4</cell></row><row><cell cols="5">(SSAM), attention pyramid fusion module (APF), Global Feature</cell></row><row><cell cols="5">(GFU) module, and Deep Supervision. The training set is used to</cell></row><row><cell cols="5">train the model, while the validation set is used for evaluation. We</cell></row><row><cell cols="5">first conduct the experiment with ResNet18 as a baseline model.</cell></row><row><cell cols="5">As illustrated in Table4, the baseline network obtains 65.7% mIoU</cell></row><row><cell cols="5">. We further evaluate the effect of global feature upsample module,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>ABLATION STUDY FOR THE PROPOSED MOD-ULES ON THE CITYSCAPES VALIDATION SET,WHERE RESNET18 ARCHITECTURE SERVES AS THE BASELINE. WE SHOW THE EFFECTIVENESS OF GFM: GLOBAL FU-SION MODULE, AFP: ATTENTION PYRAMID FUSION , SSAM: SCALE-AWARE STRIP ATTENTION, AND SUPERVI-SION</figDesc><table><row><cell cols="3">Backbone GFU APF Supervision SSAM Params Flops FPS</cell><cell>mIoU(%)</cell></row><row><cell>11.2</cell><cell>19</cell><cell>187</cell><cell>65.7</cell></row><row><cell>11.5</cell><cell>19.7</cell><cell cols="2">156.9 71.9</cell></row><row><cell>17.8</cell><cell>29.2</cell><cell>93.8</cell><cell>75.6</cell></row><row><cell>17.8</cell><cell>29.2</cell><cell>88</cell><cell>75.9</cell></row><row><cell>17.8</cell><cell>29.1</cell><cell>87</cell><cell>76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: COMPARISON BETWEEN FEATURE PYRAMID</cell></row><row><cell cols="5">NETWORK (FPN) AND ATTENTION PYRAMID FUSION</cell></row><row><cell cols="5">(APF) ON THE CITYSCAPES VALIDATION DATASET</cell></row><row><cell cols="5">Method Params Flops FPS mIoU</cell></row><row><cell>baseline</cell><cell>11.2</cell><cell>19</cell><cell>187</cell><cell>65.7</cell></row><row><cell>FPN</cell><cell>11.4</cell><cell>20.9</cell><cell>152</cell><cell>69.1</cell></row><row><cell>AFPN</cell><cell>17.5</cell><cell>28.6</cell><cell>94.3</cell><cell>73.9</cell></row><row><cell cols="5">floating points. S 2 -FPN18 and S 2 -FPN34 seem to obtain a better</cell></row><row><cell cols="2">accuracy/speed trade-offs.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>COMPARISON BETWEEN THE PROPOSED METHOD S 2 -FPN AND THE OTHER SOTA METHODS ON THE CITYSCAPES TEST DATASET. WE REPORT THE BACKBONE, INPUT RESOLUTION, GPU TYPE, NUMBER OF PARAM-ETERS (M), FLOPS (G), EVALUATION SPLIT (SET), ACHIEVED ACCURACY (MIOU), AND THE INFERENCE SPEED (FPS)</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Resolution</cell><cell>GPU</cell><cell cols="4">Parameters(M) Flops (G) test set mIoU FPS</cell></row><row><cell>PSPNet [5]</cell><cell>ResNet101</cell><cell>713 ?713</cell><cell>-</cell><cell>250.8</cell><cell>412.2</cell><cell>81.2</cell><cell>0.78</cell></row><row><cell>DeepLab [10]</cell><cell>VGG16</cell><cell>512?1024</cell><cell>Titan X</cell><cell>262.1</cell><cell>457.8</cell><cell>63.1</cell><cell>0.25</cell></row><row><cell>ENet [34]</cell><cell>No</cell><cell>640 ?360</cell><cell>TitanX</cell><cell>0.4</cell><cell>3.8</cell><cell>58.3</cell><cell>135.4</cell></row><row><cell>ICNet [35]</cell><cell>PSPNet50</cell><cell cols="2">1024?2048 TitanX</cell><cell>26.5</cell><cell>28.3</cell><cell>69.5</cell><cell>30.3</cell></row><row><cell>DABNet [6]</cell><cell>No</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>0.76</cell><cell>10.4</cell><cell>70.1</cell><cell>104</cell></row><row><cell>DFANet-A [36]</cell><cell>XceptionA</cell><cell cols="2">1024?1024 Titan X</cell><cell>7.8</cell><cell>3.4</cell><cell>71.3</cell><cell>100</cell></row><row><cell>DFANet-B [36]</cell><cell>XceptionB</cell><cell cols="2">1024?1024 Titan X</cell><cell>4.8</cell><cell>2.1</cell><cell>67.1</cell><cell>120</cell></row><row><cell>BiSeNet1 [19]</cell><cell>Xception39</cell><cell>768?1536</cell><cell cols="2">NVIDIA Titan X 5.8</cell><cell>14.8</cell><cell>68.4</cell><cell>105.8</cell></row><row><cell>BiSeNet2 [19]</cell><cell>ResNet18</cell><cell>768?1536</cell><cell cols="2">NVIDIA Titan X 49</cell><cell>54.02</cell><cell>74.8</cell><cell>65.5</cell></row><row><cell>FasterSeg [37]</cell><cell>No</cell><cell cols="2">1024?2048 GTX 1080Ti</cell><cell>-</cell><cell></cell><cell>71.5</cell><cell>163.9</cell></row><row><cell>TD4-Bise18 [38]</cell><cell>BiseNet18</cell><cell cols="2">1024?2048 Titan Xp</cell><cell>-</cell><cell></cell><cell>74.9</cell><cell>-</cell></row><row><cell>FANet-18 [39]</cell><cell>ResNet18</cell><cell cols="2">1024?2048 Titan X</cell><cell>-</cell><cell>49</cell><cell>74.4</cell><cell>72</cell></row><row><cell>FANet-34 [39]</cell><cell>ResNet34</cell><cell cols="2">1024?2048 Titan X</cell><cell>-</cell><cell>65</cell><cell>75.5</cell><cell>58</cell></row><row><cell>LBN-AA [40]</cell><cell cols="2">LBN-AA+MobileNetV2 488?896</cell><cell>Titan X</cell><cell>6.2</cell><cell>49.5</cell><cell>73.6</cell><cell>51.0</cell></row><row><cell>AGLNet [40]</cell><cell>No</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>1.12</cell><cell>13.88</cell><cell>71.3</cell><cell>52.0</cell></row><row><cell>BiSeNetV2 [28]</cell><cell>No</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>49.0</cell><cell>21.2</cell><cell>72.6</cell><cell>156</cell></row><row><cell>BiSeNetV2-L [28]</cell><cell>No</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell></cell><cell>118.5</cell><cell>75.3</cell><cell>47.3</cell></row><row><cell>HMSeg [41]</cell><cell>No</cell><cell>768?1536</cell><cell>GTX 1080Ti</cell><cell>2.3</cell><cell></cell><cell>74.3</cell><cell>83.2</cell></row><row><cell>TinyHMSeg [41]</cell><cell>No</cell><cell>768?1536</cell><cell>GTX 1080Ti</cell><cell>0.7</cell><cell></cell><cell>71.4</cell><cell>172.4</cell></row><row><cell cols="2">STDC1-Seg50 [24] STDC1</cell><cell>512?1924</cell><cell>GTX 1080Ti</cell><cell>8.4</cell><cell>-</cell><cell>71.9</cell><cell>250.4</cell></row><row><cell cols="2">STDC2-Seg50 [24] STDC2</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>12.5</cell><cell>-</cell><cell>73.4</cell><cell>188.6</cell></row><row><cell cols="2">STDC1-Seg75 [24] STDC1</cell><cell>768?1536</cell><cell>GTX 1080Ti</cell><cell>8.4</cell><cell>-</cell><cell>75.3</cell><cell>126.7</cell></row><row><cell cols="2">STDC2-Seg75 [24] STDC2</cell><cell>768?1536</cell><cell>GTX 1080Ti</cell><cell>12.5</cell><cell>-</cell><cell>76.8</cell><cell>97.0</cell></row><row><cell>S 2 -FPN18</cell><cell>ResNet18</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>17.8</cell><cell>29.1</cell><cell>76.2</cell><cell>87.3</cell></row><row><cell>S 2 -FPN34</cell><cell>ResNet34</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>27.9</cell><cell>48.4</cell><cell>77.4</cell><cell>67</cell></row><row><cell>S 2 -FPN34M</cell><cell>ResNet34</cell><cell>512?1024</cell><cell>GTX 1080Ti</cell><cell>27.9</cell><cell>190</cell><cell>77.8</cell><cell>30.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>INDIVIDUAL CATEGORY RESULTS ON CAMVID TEST SET IN TERMS OF MIOU FOR 11 CLASSES 82.1 20.5 97.2 57.1 49.3 27.5 84.4 30.7 55.6 ENet [45] 74.7 77.8 95.1 82.4 51.0 95.1 67.2 51.7 35.4 86.7 34.1 51.3 BiSeNet1 [19] 82.2 74.4 91.9 80.8 42.8 93.3 53.8 49.7 25.4 77.3 50.0 65.6</figDesc><table><row><cell>Method</cell><cell>Bui</cell><cell cols="2">Tree Sky</cell><cell cols="2">Car Sig</cell><cell cols="2">Roa Ped</cell><cell>Fen</cell><cell>Pol</cell><cell cols="2">Side Bic</cell><cell>mIoU</cell></row><row><cell cols="13">SegNet [7] 92.4 BiSeNet2 [19] 88.8 87.3 83.0 75.8 92.0 83.7 46.5 94.6 58.8 53.6 31.9 81.4 54.0 68.7</cell></row><row><cell>NDNet45-FCN8-LF [70]</cell><cell cols="2">85.5 84.6</cell><cell cols="10">94.8 82.6 39.2 97.4 60.1 37.3 17.6 86.8 53.7 57.5</cell></row><row><cell>LBN-AA [40]</cell><cell cols="2">83.2 70.5</cell><cell cols="10">92.5 81.7 51.6 93.0 55.6 53.2 36.3 82.1 47.9 68.0</cell></row><row><cell>AGLNet [64]</cell><cell cols="2">82.6 76.1</cell><cell cols="10">91.0 87.0 45.3 95.4 61.5 39.5 39.0 83.1 62.7 69.4</cell></row><row><cell cols="2">BiSeNetV2/BiSeNetV2L [28] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.4/73.2</cell></row><row><cell>S 2 -FPN18</cell><cell>83.</cell><cell>77.2</cell><cell cols="10">91.8 88.9 48.2 95.7 56.4 43.4 32.4 84.8 62.5 69.6</cell></row><row><cell>S 2 -FPN34</cell><cell cols="2">85.3 77.4</cell><cell cols="10">91.7 91.2 49.6 95.7 59.1 46.8 33.2 85.4 66.5 71.0</cell></row><row><cell>S 2 -FPN34M</cell><cell cols="2">86.0 78.8</cell><cell cols="10">92.6 92.2 56.2 96.0 67.1 47.3 42.1 86.8 70.7 74.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 :</head><label>8</label><figDesc>RESULTS OF THE MODEL ON CAMVID DATASET. * INDICATES THE MODELS PRE-TRAINED ON CITYSCAPES</figDesc><table><row><cell>Method</cell><cell cols="3">Year Params M Speed (FPS)</cell><cell>mIoU%</cell></row><row><cell>Deeplab [10]</cell><cell>2017</cell><cell>262.1</cell><cell>4.9</cell><cell>61.6</cell></row><row><cell>PSPNet [5]</cell><cell>2017</cell><cell>250.8</cell><cell>5.4</cell><cell>69.1</cell></row><row><cell>SegNet [7]</cell><cell>2015</cell><cell>29.5</cell><cell>46</cell><cell>55.6</cell></row><row><cell>ENet [45]</cell><cell>2016</cell><cell>0.36</cell><cell>-</cell><cell>61.3</cell></row><row><cell>DFANet-A [27]</cell><cell>2019</cell><cell>7.8</cell><cell>120</cell><cell>64.7</cell></row><row><cell>DFANet-B [27]</cell><cell>2019</cell><cell>4.8</cell><cell>160</cell><cell>59.3</cell></row><row><cell>BiSeNet1 [19]</cell><cell>2018</cell><cell>5.8</cell><cell>175</cell><cell>65.7</cell></row><row><cell>BiSeNet2 [19]</cell><cell>2018</cell><cell>49.0</cell><cell>116.3</cell><cell>68.7</cell></row><row><cell>ICNet [50]</cell><cell>2018</cell><cell>26.5</cell><cell>27.8</cell><cell>67.1</cell></row><row><cell>DABNet [25]</cell><cell>2019</cell><cell>0.76</cell><cell></cell><cell>66.4</cell></row><row><cell>AGLNet [64]</cell><cell>2020</cell><cell>1.12</cell><cell>90.1</cell><cell>69.4</cell></row><row><cell>NDNet45-FCN8-LF [70]</cell><cell>2020</cell><cell>1.1</cell><cell>-</cell><cell>57.5</cell></row><row><cell>LBN-AA [40]</cell><cell>2020</cell><cell>6.2</cell><cell>39.3</cell><cell>68.0</cell></row><row><cell cols="2">BiSeNetV2/BiSeNetV2L [28] 2021</cell><cell>-</cell><cell>-</cell><cell>72.4/73.2</cell></row><row><cell>STDC1-Seg75 [24]</cell><cell>2021</cell><cell>8.4</cell><cell>197.6</cell><cell>73.0</cell></row><row><cell>STDC2-Seg75 [24]</cell><cell>2021</cell><cell>12.5</cell><cell>152.2</cell><cell>73.9</cell></row><row><cell>S 2 -FPN18</cell><cell></cell><cell>17.8</cell><cell>124.2</cell><cell>69.5</cell></row><row><cell>S 2 -FPN34</cell><cell></cell><cell>27.9</cell><cell>107.2</cell><cell>71.0</cell></row><row><cell>S 2 -FPN34M</cell><cell></cell><cell>27.9</cell><cell>55.5</cell><cell>74.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Her2net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparative study of real-time semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation,&quot; in Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ppanet: Point-wise pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elhassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wireless Communications and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dsanet: Dilated spatial attention for real-time semantic segmentation in urban street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elhassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Munea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">115090</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9716" to="9725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time high-performance semantic image segmentation of urban street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3258" to="3274" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">When humans meet machines: Towards efficient segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lednet: A lightweight encoder-decoder network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1860" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fddwnet: a lightweight convolutional neural network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2373" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02344</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Spfnet: Subspace pyramid fusion network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elhassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Munea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01278</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Augfpn: Improving multi-scale feature learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="595" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Aglnet: Towards real-time semantic segmentation of self-driving images via attention-guided lightweight network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106682</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Small object augmentation of urban scenes for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5175" to="5190" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
