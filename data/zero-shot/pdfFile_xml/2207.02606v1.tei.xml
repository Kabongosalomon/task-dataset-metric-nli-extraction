<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseHybrid: Hybrid Anomaly Detection for Dense Open-set Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Grci?</surname></persName>
							<email>matej.grcic@fer.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
							<email>petra.bevandic@fer.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
							<email>sinisa.segvic@fer.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DenseHybrid: Hybrid Anomaly Detection for Dense Open-set Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dense anomaly detection</term>
					<term>Dense open-set recognition</term>
					<term>Out- of-distribution detection</term>
					<term>Semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection can be conceived either through generative modelling of regular training data or by discriminating with respect to negative training data. These two approaches exhibit different failure modes. Consequently, hybrid algorithms present an attractive research goal. Unfortunately, dense anomaly detection requires translational equivariance and very large input resolutions. These requirements disqualify all previous hybrid approaches to the best of our knowledge. We therefore design a novel hybrid algorithm based on reinterpreting discriminative logits as a logarithm of the unnormalized joint distributionp(x, y). Our model builds on a shared convolutional representation from which we recover three dense predictions: i) the closedset class posterior P (y|x), ii) the dataset posterior P (din|x), iii) unnormalized data likelihoodp(x). The latter two predictions are trained both on the standard training data and on a generic negative dataset. We blend these two predictions into a hybrid anomaly score which allows dense open-set recognition on large natural images. We carefully design a custom loss for the data likelihood in order to avoid backpropagation through the untractable normalizing constant Z(?). Experiments evaluate our contributions on standard dense anomaly detection benchmarks as well as in terms of open-mIoU -a novel metric for dense open-set performance. Our submissions achieve state-ofthe-art performance despite neglectable computational overhead over the standard semantic segmentation baseline. Official implementation: https://github.com/matejgrcic/DenseHybrid</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High accuracy, fast inference and small memory footprint of modern neural networks steadily expand the horizon of downstream applications. Many exciting applications require advanced image understanding functionality provided by semantic segmentation <ref type="bibr" target="#b16">[17]</ref>. These models associate each pixel with a class from a predefined taxonomy. They can accurately segment two megapixel images in real-time on low-power embedded hardware <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref>. However, the standard training procedures assume the closed-world setup which may raise serious safety issues in real-world deployments. For example, if a segmentation model missclassifies an unknown object (e.g. lost cargo) as road, the autonomous car may experience a serious accident. Such hazards can be alleviated by complementing semantic segmentation with dense anomaly detection. The resulting dense open-set recognition models are more suitable for real-world applications due to ability to decline the decision in anomalous pixels.</p><p>Previous approaches for dense anomaly detection either use a generative or a discriminative perspective. Generative approaches are based on density estimation <ref type="bibr" target="#b5">[6]</ref> or image resynthesis <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref>. Discriminative approaches use classification confidence <ref type="bibr" target="#b22">[23]</ref>, a binary classifier <ref type="bibr" target="#b1">[2]</ref> or Bayesian inference <ref type="bibr" target="#b28">[29]</ref>. These two perspectives exhibit different failure modes. Generative detectors inaccurately disperse the probability volume <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref> or rely on risky image resynthesis. On the other hand, discriminative detectors assume training on full span of the input space, even including unknown unknowns <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work we combine the two perspectives into a hybrid anomaly detector. The proposed approach complements a standard semantic segmentation model with two additional predictions: i) unnormalized dense data likelihoodp(x) <ref type="bibr" target="#b5">[6]</ref>, and ii) dense data posterior P (d in |x) <ref type="bibr" target="#b1">[2]</ref>. Both predictions require training with negative data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. Joining these two outputs yields an accurate yet efficient dense anomaly detector which we refer to as DenseHybrid.</p><p>We summarize our contributions as follows. We propose the first hybrid anomaly detector which allows end-to-end training and operates at pixel level. Our approach combines likelihood evaluation and discrimination with respect to an off-the-shelf negative dataset. Our experiments reveal accurate anomaly detection despite minimal computational overhead. We complement semantic segmentation with DenseHybrid to achieve dense open-set recognition. We report state-of-the-art dense open-set recognition performance according to a novel performance metric which we refer to as open-mIoU. <ref type="figure">Fig. 1</ref>. Qualitative performance of the proposed DenseHybrid approach on standard datasets. Top: input images. Bottom: dense maps of the proposed anomaly score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMIYC-ObstacleTrack LostAndFound Fishyscapes Static StreetHazards</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Detecting samples which deviate from the generative process of the training data is a decades old problem <ref type="bibr" target="#b21">[22]</ref>. In the machine learning community this task is also known as anomaly detection or out-of-distribution (OOD) detection <ref type="bibr" target="#b23">[24]</ref>. Early image-wide approaches utilize max-softmax probability <ref type="bibr" target="#b23">[24]</ref>, input perturbations <ref type="bibr" target="#b33">[34]</ref> ensembling <ref type="bibr" target="#b30">[31]</ref> or Bayesian uncertainty <ref type="bibr" target="#b39">[40]</ref>. More encouraging performance has been reported by discriminative training against a broad negative dataset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37]</ref> or an appropriately trained generative model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b53">54]</ref>. Another line of work detects anomalies by estimating the likelihood with a generative model. Surprisingly, this research revealed that anomalies may give rise to higher likelihood than inliers <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>. Further works suggest that better performance can be hoped for group-wise anomaly detection <ref type="bibr" target="#b26">[27]</ref>, however, this case has less practical importance. Generative models can be encouraged to assign low likelihood in negative training data <ref type="bibr" target="#b24">[25]</ref>. This practice may mitigate sub-optimal dispersion of the probability volume <ref type="bibr" target="#b37">[38]</ref>.</p><p>Image-wide anomaly detection approaches can be adapted for dense prediction with variable success. None of the existing generative approaches can deliver dense likelihood estimates. On the other hand, concepts such as max-softmax and discriminative training with negative data are easily ported to dense prediction. Many dense anomaly detectors are trained on mixed-content images obtained by pasting negatives (e.g. ImageNet, COCO, ADE20k) over regular training images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref>. Discriminative anomaly detections may be produced by a dedicated OOD head which shares features with the standard classification head. Shared features improve OOD performance and incur neglectable computational overhead with respect to the baseline semantic segmentation model <ref type="bibr" target="#b1">[2]</ref>. Recent approach <ref type="bibr" target="#b9">[10]</ref> encourages large softmax entropy in negative pixels.</p><p>Anomalies can also be recognized in feature space <ref type="bibr" target="#b5">[6]</ref>. However, this approach complicates the detection of small objects due to subsampled feature represenations and feature collapse <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref>. Orthogonally to previous approaches, anomaly detector can be implemented according to dissimilarity between the input and a resynthesised image <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50]</ref>. The resynthesis is performed by a generative model conditioned on the predicted labels. However, this approach is suitable only for uniform backgrounds such as roads <ref type="bibr" target="#b35">[36]</ref>. Furthermore, it adds significant computational overhead making it inapplicable for real-time applications.</p><p>Our approach to dense anomaly detection is a hybrid combination of discriminative detection and likelihood evaluation. Discriminative OOD detection has been introduced in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>. Contrary to all these approaches, we improve discriminative OOD detection through synergy with likelihood testing. Dense likelihood evaluation has been accomplished by fitting a generative model to discriminative features <ref type="bibr" target="#b5">[6]</ref>. However, their approach is vulnerable to feature collapse <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref> due to two-phase training. Moreover, detection of small outliers is jeopardized due to subsampling. Contrary to their approach, our method allows joint training with the standard dense prediction model and anomaly detection at full resolution.</p><p>We perform dense likelihood evaluation by reinterpreting logits as unnormalized joint likelihood <ref type="bibr" target="#b19">[20]</ref>. However, the method <ref type="bibr" target="#b19">[20]</ref> is completely unsuitable for dense prediction due to intractability of Langevin sampling at large resolutions. We reformulate their method in order to allow training on mixed-content images and show that such adaptation dramatically simplifies the training by precluding backpropagation through intractable normalizing constant Z(?). To the best of our knowledge, the proposed design offers the first approach for dense likelihood evaluation that is suitable for end-to-end training.</p><p>We build an open-set recognition model by thresholding our hybrid anomaly score and combining it with the standard semantic segmentation predictions <ref type="bibr" target="#b6">[7]</ref>. The resulting model is suitable for simultaneous anomaly detection and recognition of inlier scenery. We note that standard metrics for dense recognition performance <ref type="bibr" target="#b15">[16]</ref> do not take into account the accuracy in anomalous samples. This is not surprising since outlier pixels have been introduced only in recent dense prediction benchmarks <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. Also, previous work on discrimination in presence of anomalous pixels was more focused on robustness of algorithms rather than on recognition performance <ref type="bibr" target="#b51">[52]</ref>. Hence, we propose a novel anomaly-aware metric (open-mIoU) which measures the prediction quality both in inliers and the outliers, similarly to previous image-wide metrics <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b45">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dense Recognition with Hybrid Anomaly Detector</head><p>We propose a hybrid algorithm for dense anomaly detection based on unnormalized data likelihood and dataset posterior (Sec. 3.1). The proposed hybrid anomaly detector extends the standard dense classifier to form dense open-set recognition model (Sec. 3.2). The resulting recognition model trains on mixed content images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hybrid Anomaly Detection for Dense Prediction</head><p>We represent RGB images with a random variable x. Variable y denotes the corresponding pixel-level predictions, while the binary random variable d models whether a given pixel belongs to the inliers or outliers. We denote a realization of a random variable without the underline. Thus, P (y|x) is a shortcut for P (y = y|x = x). We write d in for inliers and d out for outliers. Thus, P (d in |x) denotes a dense posterior probability that a given pixel is an inlier <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref>. Conversely, p(x) denotes dense likelihoods of patches centered at a given pixel.</p><p>We build upon reinterpretation of logits s produced by a discriminative model P (y|x) = softmax(f ?2 (q ?1 (x))) <ref type="bibr" target="#b19">[20]</ref>. We reinterpret the logits as unnormalized joint log-density of input and labels:</p><formula xml:id="formula_0">p(y, x) = 1 Zp (y, x) := 1 Z exp s, s = f ?2 (q ?1 (x)).<label>(1)</label></formula><p>Note that q ?1 produces pre-logits t based on which f ?2 computes logits s. Hence, q ?1 and f ?2 form the standard discriminative model.p(y, x) denotes unnormalized joint density across data x and labels y, while Z denotes the corresponding normalization constant. As usual, computing Z is intractable since it requires evaluating the unnormalized distribution for all realizations of y and x. Throughout this work we conveniently eschew the evaluation of Z in order to enable efficient training and inference. Standard discriminative predictions are easily obtained through Bayes rule:</p><formula xml:id="formula_1">P (y|x) = p(y, x) y p(y, x) = exp s i exp s i = softmax(s).<label>(2)</label></formula><p>Hence, we can recover the unnormalized joint density (1) through the standard closed-world discriminative learning over K classes. Moreover, we can share the logits with the primary discriminative task and even exploit pretrained classifiers. We can express the dense likelihood p(x) by marginalizing out y:</p><formula xml:id="formula_2">p(x) = y p(y, x) = 1 Z yp (y, x) = 1 Z i exp s i .<label>(3)</label></formula><p>One could argue for detecting anomalies with p(x) directly: if a given input is unlikely under the p(x), it should likely be an anomaly. However, this approach may not work very well in practice due to tendency of maximum likelihood optimization towards over-generalization <ref type="bibr" target="#b37">[38]</ref>. In simple words, some outliers will have higher likelihood than the inliers <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b40">41]</ref>. We discourage such behaviour by minimizing the likelihood of negatives during the training <ref type="bibr" target="#b24">[25]</ref>.</p><p>Besides logit reinterpretation, we define the dataset posterior P (d in |x) as a non-linear transformation based on pre-logit activations q ?1 (x) <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_3">P (d in |x) := ?(g ? (q ?1 (x))).<label>(4)</label></formula><p>In our case, the function g is BN-ReLU-Conv1x1 of pre-logits, followed by a sigmoid non-linearity. Anomalies can be detected solely with P (d in |x) <ref type="bibr" target="#b12">[13]</ref>: inlier samples should give rise to high posterior of the inlier dataset. However, our experiments show that this is suboptimal compared to our hybrid approach. <ref type="figure">Fig. 2</ref> illustrates shortcomings of generative and discriminative anomaly detectors on a toy problem. Blue dots designate inlier data. Green triangles designate the negative data used for training. Red squares denote anomalous test data. Discriminative detectors which model P (d in |x) can't differentiate inliers if the negative data seen during the training insufficiently covers the sample space (left). On the other hand, generative detectors which model p(x) tend to inaccurately distribute probability volume over sample space <ref type="bibr" target="#b37">[38]</ref> (center). Joining discriminative and generative approach into a hybrid detector we mitigate the aforementioned limitations (right).</p><p>We build our hybrid anomaly detector upon the discriminative dataset posterior P (d in |x) and the generative data likelihood p(x). We express a novel hybrid anomaly score as log-ratio between P (d out |x) = 1 ? P (d in |x) and p(x):  <ref type="bibr" target="#b37">[38]</ref>. The hybrid approach achieves a synergy between discriminative and generative modelling</p><formula xml:id="formula_4">s(x) := ln P (d out |x) p(x) = ln P (d out |x) ? lnp(x) + ln Z (5) ? = ln P (d out |x) ? lnp(x).<label>(6)</label></formula><p>We can neglect Z since ranking performance <ref type="bibr" target="#b23">[24]</ref> is invariant to monotonic transformations such as taking a logarithm or adding a constant. Other formulations of s(x) may also be effective which is an interesting direction for future work.  The developed hybrid model aims at achieving a synergy between generative and discriminative modelling. However, the proposed hybrid interpretation requires specific training objectives. Dense class posteriors require a discriminative loss over the inlier data D in :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dense Open-set Recognition based on Hybrid Anomaly Detection</head><formula xml:id="formula_5">L cls (?) = E x,y?Din [? ln P (y|x)] (7) = ? E x,y?Din [s y ] + E x,y?Din [ln i exp s i ].<label>(8)</label></formula><p>The discriminative loss <ref type="formula">(7)</ref> corresponds to the standard training in the closed world. We introduce the negative data D out into the training procedure to ensure the desired behaviour of P (d in |x) and p(x) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref>. Both distributions should yield low probability in negative pixels. We propose to train p(x) to maximize the likelihood in inliers and to minimize the likelihood in outliers. We derive the upper bound of the desired loss as follows:</p><formula xml:id="formula_6">L x (?) = E x?Din [? ln p(x)] ? E x?Dout [? ln p(x)] (9) = E x?Din [? lnp(x)] + ln Z ? E x?Dout [? lnp(x)] ? ln Z (10) = ? E x?Din ln i exp(s i ) + E x?Dout ln i exp(s i ) (11) ? ? E x,y?Din [s y ] + E x?Dout [ln i exp(s i )].<label>(12)</label></formula><p>Note that we eschew the backpropagation into the normalization constant Z, and derive the upper bound according to the following inequality:</p><formula xml:id="formula_7">ln i exp s i ? max i s i ? s y .<label>(13)</label></formula><p>Proof of inequality <ref type="bibr" target="#b12">(13)</ref> can be easily derived by recalling that log-sum-exp is a smooth upper bound of the max function. By comparing the standard classification loss <ref type="bibr" target="#b6">(7)</ref> and the upper bound <ref type="bibr" target="#b11">(12)</ref> we realize that minimizing the standard classification loss increases p(x) for inlier pixels. Indeed, minimizing the negative logarithm of softmax output increases the value of logit for the correct class. Alternatively, p(x) could be trained only on inliers <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. This would require sample hallucination via MCMC sampling and back-propagation into the corresponding approximation of Z. Such procedure is infeasible for large images. Consequently, we choose to deal with negative samples instead of hallucinated ones and optimize the proposed loss L x (?).</p><p>We train the dataset posterior P (d in |x) with the standard discriminative loss <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_8">L d (?, ?) = E x?Din [? ln P (d in |x)] + E x?Dout [? ln(P (d out |x))].<label>(14)</label></formula><p>By joining losses L cls , L x and L d we obtain the final loss:</p><formula xml:id="formula_9">L(?, ?) = ?E x,y?Din [ln P (y|x) + ln P (d in |x)] ? ? ? E x?Dout [ln(P (d out |x)) ? lnp(x)].<label>(15)</label></formula><p>Hyperparameter ? controls the impact of negative data to the primary classification task. Note that the final loss (15) omits the first term from L x (12) in positive pixels. We choose to do so sincep(x) is implicitly optimized through L cls . <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the described training procedure of the proposed open-set recognition model. We prepare the training images by pasting the negative instances atop the standard training images. The resulting mixed-content image <ref type="bibr" target="#b1">[2]</ref> is fed to the hybrid model. We obtain the classification output P (y|x) with softmax. The unnormalized likelihoodp(x) is obtained through sum-exp operator. We recover p(d in |x) by branching from pre-logit activations. The model outputs are trained by applying the dicriminative loss L cls <ref type="bibr" target="#b6">(7)</ref>, likelihood loss L x <ref type="bibr" target="#b11">(12)</ref> and dataset posterior loss L d <ref type="bibr" target="#b13">(14)</ref>. As proposed, these losses are conveniently joined into a single loss L(?, ?) <ref type="bibr" target="#b14">(15)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Measuring Dense Open-set Performance</head><p>Test datasets for anomaly segmentation either exclusively measure the performance of anomaly detectors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9]</ref> or simply report the classification performance <ref type="bibr" target="#b4">[5]</ref>. In the latter case, the reported drop in segmentation performance is usually negligible and is explained away by allocation of model capacity for the anomaly detection. We will show that the real impact of anomaly detector on the segmentation performance can be clearly seen only in the open world. Also, the impact is more severe than the small performance drop visible in the closed world.</p><p>To properly measure open-set recognition performance, we first select threshold at which the anomaly detector achieves TPR of 95%. This ensures high safety standards for the recognition model. Then, we override the classification in pixels which raise concern according to the thresholded anomaly map. The resulting recognition map has K + 1 labels. We </p><formula xml:id="formula_10">open-IoU k = TP k TP k + FP ow k + FN ow k , FP ow k = K+1 i? =k i=1 FP i k , FN ow k = K+1 i? =k i=1 FN i k<label>(16)</label></formula><p>Different that the standard IoU formulation, open-IoU also takes into account false positives and false negatives caused by imperfect anomaly detector. However, we still average open-IoU over K inlier classes. This means that a recognition model which uses a perfect anomaly detector would match segmentation performance in the closed world. This property would not be preserved if we averaged IoU over K+1 classes. <ref type="figure" target="#fig_6">Figure 5 (right)</ref> shows the open world confusion matrix. Imperfect anomaly detection impacts recognition performance through increased false positives (designated in yellow) and false negatives (designated in red). Difference between closed mIoU and averaged open-IoU over K inlier classes reveals the performance hit due to inaccurate anomaly detection.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We report dense anomaly detection and open-set recognition performance of the proposed DenseHybrid approach, and compare them with the state of the art.</p><p>We also explore influence of distance, show computational requirements of the proposed module, and ablate the design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmarks and Datasets</head><p>We evaluate performance on standard benchmarks for dense anomaly detection. Fishyscapes <ref type="bibr" target="#b4">[5]</ref> considers urban scenarios on a subset of LostAndFound <ref type="bibr" target="#b43">[44]</ref> and on Cityscapes validation images with pasted anomalies (FS Static). SegmentMeIfYouCan (SMIYC) <ref type="bibr" target="#b8">[9]</ref> moves away from anomaly injection. Instead, appropriate images are collected from the real world and grouped based on the anomaly size into AnomalyTrack (large) and ObstacleTrack (small). Additionally, the benchmark encapsulates all LostAndFound images. Unfortunately, both benchmarks only have binary labels which makes them insufficient for measuring the recognition performance as proposed in Sec. 4. StreetHazards <ref type="bibr" target="#b22">[23]</ref> is a synthetic dataset created by CARLA virtual environment. The simulated environment enables smooth anomaly injection and low-cost label extraction. Consequently, the dataset contains K + 1 labels which makes it suitable for measuring both anomaly detection and dense recognition. <ref type="table" target="#tab_1">Table 1</ref> shows performance of the proposed hybrid anomaly detector on the SMIYC benchmark <ref type="bibr" target="#b8">[9]</ref>. DenseHybrid outperforms contemporary approaches on both AnomalyTrack and ObstacleTrack by a wide margin. Also, the proposed anomaly detector achieves the best FPR on LostAndFound.  <ref type="table" target="#tab_2">Table 2</ref> shows performance of the proposed DenseHybrid on Fishyscapes <ref type="bibr" target="#b4">[5]</ref>. Our anomaly detector achieves the best results on FS LostAndFound, and the best FPR on FS Static. We achieve these results while having negligible impact on classification task in closed-world. However, in the next section we show that the impact of anomaly detection to recognition performance is much more significant than in the closed world.  <ref type="table" target="#tab_3">Table 3</ref> explores sensitivity of anomaly detection with respect to distance from the camera. We perform all these experiments on LostAndFound since it includes disparity maps. Still, due to errors in available disparities, we limit our analysis to the first 50 meters from the camera. The proposed DenseHybrid approach achieves accurate results even at large distances from the vehicle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dense Anomaly Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dense Open-set Recognition</head><p>By fusing a properly thresholded anomaly detector with the dense classifier, we obtain a dense open-set recognition model <ref type="figure" target="#fig_0">(Fig. 3</ref>). The resulting model detects anomalous scene parts, while correctly classifying the rest of the scene.</p><p>To measure the dense recognition performance, we create two test folds based on towns t5 and t6 from StreetHazards test. Then, we select anomaly threshold on t6 and use it to measure the proposed open-mIoU on t5. We switch the folds and repeat the procedure. We compute the weighted average based on image count to obtain the final test set open-mIoU. <ref type="table" target="#tab_4">Table 4</ref> shows performance of our dense recognition models on StreetHazards. The left part of the table considers anomaly detection where DenseHybrid achieves the best performance. The right part of the table considers dense recognition performance. Our model outperforms other contemporary approaches despite lower classification performance in the closed world. Note that the performance drop between the closed and the open set is significant. The models achieve over 60% mIoU in closed world while the open world performance peeks at 46%. Hence, we conclude that even the best anomaly detectors are still insufficient for matching the closed world performance in open-world. Researchers should strive to close this gap in order to improve the safety of recognition systems in the real world.  <ref type="figure" target="#fig_7">Figure 6</ref> visualises dense anomaly and recognition maps on StreetHazards. Our recognition model significantly outperforms the max-logit baseline <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Input</head><p>DenseHybrid-anomaly DenseHybrid-recognition Max logit Ground truth DenseHybrid significantly outperforms the max-logit baseline <ref type="bibr" target="#b22">[23]</ref> 5.4 Inference speed <ref type="table" target="#tab_5">Table 5</ref> shows computational overhead of the proposed DenseHybrid anomaly detector over the baseline segmentation model on two megapixels images. Dense-Hybrid has negligible computational overhead of 0.1 GFLOPs and 2.8ms. Our results are averaged over 200 runs on NVIDIA RTX3090. These experiments also suggest that image resynthesis is not applicable for real-time inference.  <ref type="table" target="#tab_6">Table 6</ref> compares the proposed DenseHybrid approach with its generative and discriminative components -p(x) and P (d in |x). The hybrid anomaly score based on the ratio of these two distributions outperforms each of the two components. The results are averaged over the last three epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impact of anomaly detector design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Implementation details</head><p>We adapt the standard segmentation networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55]</ref> to enable co-operation with our hybrid anomaly detector. We append an additional branch g ? which is in our case BN-ReLU-Conv1x1. The additional branch computes the discriminative anomaly output. We obtain generative anomaly output by computing sum of exponentiated logits. We build our recognition models based on dense classifiers. We fine-tune all our models on mixed content images with pasted negative instances from ADE20k. In the case of SMIYC we fine-tune LDN-121 <ref type="bibr" target="#b29">[30]</ref> for 10 epochs on images from Cityscapes <ref type="bibr" target="#b11">[12]</ref>, Vistas <ref type="bibr" target="#b41">[42]</ref> and Wilddash2 <ref type="bibr" target="#b51">[52]</ref>. In the case of Fishyscapes we use DeepLabV3+ with WideResNet38 <ref type="bibr" target="#b54">[55]</ref>. We fine-tune the model for 10 epochs on Cityscapes. We train LDN-121 on Street-Hazards for 120 epochs in closed world and then fine-tune the recognition model on mixed-content images. Other details are available in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Discriminative and generative approaches to dense anomaly detection assume different failure modes. We propose to achieve a synergy of these two approaches by fusing the data posterior and the data likelihood derived from the standard discriminative model. The proposed hybrid setup relies on unnormalized distributions. Hence, we try to eschew evaluation of the intractable normalization constant both during training and inference. The proposed DenseHybrid architecture yields state-of-the-art performance on the standard anomaly segmentation benchmarks as well as competitive dense recognition performance in the open world. The latter is measured with the novel open-mIoU score which takes into account classification in both inliers and anomalous pixels. Future work should focus on reducing the revealed performance gap between closedworld and open-world recognition in order to improve the progress toward safe autonomous driving systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Supplement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Limitations</head><p>We term our method hybrid since it optimizes two different training objectives: i) dense discrimination between inliers and negatives, and ii) high likelihood of inliers and low likelihood of negative data. It may seem that our method can generate samples due to likelihood evaluation being a standard feature of generative models (except GANs). However, our formulation is not suitable for sample generation due to dealing with unnormalized distributions. This would require MCMC sampling which can not be performed at large resolutions, at least not with known techniques. Even if sample generation was feasible, the resulting approach would likely be too slow for real-time inference as shown by other image resynthesis approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Impact of Known Inliers on Anomaly Detection</head><p>Many real-world deployments of autonomous systems work in environment with limited variety (e.g. warehouses or industrial plants). Such environments usually have perfectly aligned training and test distributions. Still, anomalous objects can occur. We show that anomaly detection performance in such cases is significantly more easier. <ref type="table" target="#tab_7">Table 7</ref> shows such setup. We used two DLv3+ segmentation networks. The first one is trained on Cityscapes train while the second one is trained on Cityscapes train and val splits. Since the Fishyscapes Static dataset is created by pasting negative objects atop Cityscapes val images, we can measure anomaly detection performance when the inlier instances from test set are known. We see that the average precision of anomaly detection is drastically improved from 60% to 89%. This indicates that the DenseHybrid anomaly detector is feasible for scenarios with limited scene variety. 7.</p><p>3 More results <ref type="table" target="#tab_8">Table 8</ref> shows the dense open-set recognition performance with generative and discriminative anomaly detectors on the StreetHazards dataset. The proposed hybrid anomaly score based on the ratio of these two distributions outperforms each of the two components. Note that generative and discriminative outputs are trained using the same mixed-content images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Implementation Details</head><p>We create mixed content training samples by pasting negative data atop the inlier crops. The inlier crops are obtained by jittering images in range [0.5, 2], applying random horizontal flip, and taking random square crop of size 768. We use ADE20k instances as negative content. We resize the negative image to 768 pixels, take a random jittered crop of size 384. Then, we paste two instances from the negative crop at random position atop the inlier crop. For LDN-121, we use batch size 16. We use Adam optimizer with the initial learning rate 10 ?5 . The learning rate is decayed through a cosine schedule down to 10 ?7 . We set the loss modulation hyper-parameter ? to 0.03. For DLv3+ we use batch size 8 and inlier crops of 512. We use Adam optimizer with learning rate 10 ?6 and do not decay it. We set the loss modulation hyper-parameter ? to 0.01. In the case of Fishyscapes, we fine-tune DLv3+ with a WRN38 backbone pre-trained by NVIDIA <ref type="bibr" target="#b54">[55]</ref>. However, due to hardware limitations we could not train it from scratch to achieve the desired robustness required for SMIYC. Hence, we opted for LDN-121 as an efficient alternative which can be trained on a single GPU. Using the bigger DLv3+ would additionally improve results on SMIYC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Visualizations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>illustrates the inference with the proposed open-set recognition setup. RGB input is fed to a hybrid dense model which produces pre-logit activations t and logits s. Then, we obtain the closed-set class posterior P (y|x) = softmax(s) (designated in yellow) and the unnormalized data likelihoodp(x) (designated in green). A distinct head g transforms pre-logits t into the dataset posterior P (d out |x). The anomaly score s(x) is a log-ratio between latter two distributions. The resulting anomaly map is thresholded and fused with the discriminative output into the final dense open-set recognition map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed dense open-set recognition approach. Our anomaly score is a log-ratio of outputs derived from the hybrid model. We fuse the thresholded anomaly score with the closed-set segmentation map to obtain the open-set segmentation map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The training procedure of the proposed open-set recognition model. Mixedcontent images are fed to the open-set model with three outputs. Each output is optimized according to the compound loss<ref type="bibr" target="#b14">(15)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>compute the recognition performance in open-world using open intersection over union (open-IoU). For the k-th class we can compute the proposed open-IoU as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The proposed open intersection over union (open-IoU) takes into account missclassifications in anomalous pixels to accurately measure dense recognition performance in open world Measuring performance using the proposed open-IoU requires datasets with K+1 labels. Creating such taxonomy requires substantial resources. Currently, only StreetHazards [23] offers appropriate taxonomy for measuring open-IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Visualisation of dense open-set recognition performance on StreetHazards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 Fig. 7 .</head><label>77</label><figDesc>visualizes anomaly detection performance of DenseHybrid on SMIYC-AnomalyTrack and SMIYC-ObstacleTrack. Anomalies detected with DenseHybrid are highlighted above the input image. The corresponding ground-truth has anomalies designated in orange. Inlier pixels are designated in white, while the ignore pixels are designated in black. Anomaly detection performance of DenseHybrid on SMIYC validation subsets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Anomaly detection on a toy dataset. The discriminative approach (left) models P (din|x). It fails if the negative training dataset does not cover all modes of the test anomalies. The generative approach (middle) models p(x). It often assigns high likelihoods to test anomalies due to over-generalization</figDesc><table><row><cell>Discriminative</cell><cell>FPR=11.0%</cell><cell>Generative</cell><cell>FPR=24.0%</cell><cell>Hybrid</cell><cell>FPR=9.5%</cell></row><row><cell></cell><cell>Inlier data</cell><cell>Negative training data</cell><cell>Outlier test data</cell><cell></cell><cell></cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance evaluation on the SMIYC benchmark<ref type="bibr" target="#b8">[9]</ref>. DenseHybrid outperforms contemporary approaches on Anomaly and Obstacle track by a wide margin, while also achieving the best FPR on LostAndFound</figDesc><table><row><cell>Method</cell><cell cols="8">AnomalyTrack ObstacleTrack LAF-noKnown data rsyn. AP FPR95 AP FPR95 AP FPR95 Aux Img</cell></row><row><cell>SynBoost [4]</cell><cell>?</cell><cell cols="2">? 56.4</cell><cell>61.9</cell><cell>71.3</cell><cell>3.2</cell><cell>81.7</cell><cell>4.6</cell></row><row><cell>Image Resyn. [36]</cell><cell>?</cell><cell cols="2">? 52.3</cell><cell>25.9</cell><cell>37.7</cell><cell>4.7</cell><cell>57.1</cell><cell>8.8</cell></row><row><cell>JSRNet [50]</cell><cell>?</cell><cell cols="2">? 33.6</cell><cell>43.9</cell><cell>28.1</cell><cell>28.9</cell><cell>74.2</cell><cell>6.6</cell></row><row><cell>Road Inpaint. [35]</cell><cell>?</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>54.1</cell><cell cols="2">47.1 82.9</cell><cell>35.8</cell></row><row><cell>Embed. Dens. [5]</cell><cell>?</cell><cell cols="2">? 37.5</cell><cell>70.8</cell><cell>0.8</cell><cell>46.4</cell><cell>61.7</cell><cell>10.4</cell></row><row><cell>ODIN [34]</cell><cell>?</cell><cell cols="2">? 33.1</cell><cell>71.7</cell><cell>22.1</cell><cell>15.3</cell><cell>52.9</cell><cell>30.0</cell></row><row><cell>MC Dropout [29]</cell><cell>?</cell><cell cols="2">? 28.9</cell><cell>69.5</cell><cell>4.9</cell><cell>50.3</cell><cell>36.8</cell><cell>35.6</cell></row><row><cell>Max softmax [24]</cell><cell>?</cell><cell cols="2">? 28.0</cell><cell>72.1</cell><cell>15.7</cell><cell>16.6</cell><cell>30.1</cell><cell>33.2</cell></row><row><cell>Mahalanobis [33]</cell><cell>?</cell><cell cols="2">? 20.0</cell><cell>87.0</cell><cell>20.9</cell><cell>13.1</cell><cell>55.0</cell><cell>12.9</cell></row><row><cell>Void Classifier [5]</cell><cell>?</cell><cell cols="2">? 36.6</cell><cell>63.5</cell><cell>10.4</cell><cell>41.5</cell><cell>4.8</cell><cell>47.0</cell></row><row><cell cols="2">DenseHybrid (ours) ?</cell><cell cols="2">? 78.0</cell><cell>9.8</cell><cell>87.1</cell><cell>0.2</cell><cell>78.7</cell><cell>2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance evaluation on the Fishyscapes benchmark<ref type="bibr" target="#b4">[5]</ref>. DenseHybrid achieves the best performance on FS LostAndFound and the best FPR on FS Static</figDesc><table><row><cell>Method</cell><cell cols="6">LostAndFound data rsyn. AP FPR95 AP FPR95 Cityscapes mIoU Static Closed world Aux Img</cell></row><row><cell>SynBoost [4]</cell><cell>?</cell><cell cols="2">? 43.2</cell><cell>15.8</cell><cell>72.6 18.8</cell><cell>81.4</cell></row><row><cell>Image Resyn. [36]</cell><cell>?</cell><cell cols="2">? 5.7</cell><cell>48.1</cell><cell>29.6 27.1</cell><cell>81.4</cell></row><row><cell>Standardized ML [28]</cell><cell>?</cell><cell cols="2">? 31.1</cell><cell>21.5</cell><cell>53.1 19.6</cell><cell>80.3</cell></row><row><cell>Embed. Dens. [5]</cell><cell>?</cell><cell>?</cell><cell>4.7</cell><cell>24.4</cell><cell>62.1 17.4</cell><cell>80.3</cell></row><row><cell>Max softmax [24]</cell><cell>?</cell><cell cols="2">? 1.77</cell><cell>44.9</cell><cell>12.9 39.8</cell><cell>80.3</cell></row><row><cell>Dirichlet prior [39]</cell><cell>?</cell><cell cols="2">? 34.3</cell><cell>47.4</cell><cell>84.6 30.0</cell><cell>70.5</cell></row><row><cell>OOD Head [2]</cell><cell>?</cell><cell cols="2">? 30.9</cell><cell>22.2</cell><cell>84.0 10.3</cell><cell>77.3</cell></row><row><cell>Void Classifier [5]</cell><cell>?</cell><cell cols="2">? 10.3</cell><cell>22.1</cell><cell>45.0 19.4</cell><cell>70.4</cell></row><row><cell cols="2">Mutual information [40] ?</cell><cell>?</cell><cell>9.8</cell><cell>38.5</cell><cell>48.7 15.5</cell><cell>73.8</cell></row><row><cell>DenseHybrid (ours)</cell><cell>?</cell><cell cols="2">? 43.9</cell><cell>6.2</cell><cell>72.3 5.5</cell><cell>81.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Anomaly detection performance at different distances from camera.</figDesc><table><row><cell>Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance evaluation on StreetHazards [23]. DenseHybrid achieves the best anomaly detection performance. The corresponding open-set recognition model yields the best performance measured by open-mIoU (Sec. 4)</figDesc><table><row><cell>Method</cell><cell cols="3">Aux. Anomaly detection Closed world data AP FPR95 AUC IoU</cell><cell cols="3">Open world o-IoU-t5 o-IoU-t6 o-IoU</cell></row><row><cell>SynthCP [51]</cell><cell>?</cell><cell>9.3 28.4 88.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dropout [29][51]</cell><cell>?</cell><cell>7.5 79.4 69.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TRADI [19]</cell><cell>?</cell><cell>7.2 25.3 89.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OVNNI [18]</cell><cell cols="2">? 12.6 22.2 91.2</cell><cell>54.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SO+H [21]</cell><cell cols="2">? 12.7 25.2 91.7</cell><cell>59.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DML [8]</cell><cell cols="2">? 14.7 17.3 93.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MSP [24]</cell><cell>?</cell><cell>7.5 27.9 90.1</cell><cell>65.0</cell><cell>32.7</cell><cell>40.2</cell><cell>35.1</cell></row><row><cell>ML [23]</cell><cell cols="2">? 11.6 22.5 92.4</cell><cell>65.0</cell><cell>39.6</cell><cell>44.5</cell><cell>41.2</cell></row><row><cell>ODIN [34]</cell><cell>?</cell><cell>7.0 28.7 90.0</cell><cell>65.0</cell><cell>26.4</cell><cell>33.9</cell><cell>28.8</cell></row><row><cell>ReAct [49]</cell><cell cols="2">? 10.9 21.2 92.3</cell><cell>62.7</cell><cell>33.0</cell><cell>36.2</cell><cell>34.0</cell></row><row><cell>Energy [37]</cell><cell cols="2">? 12.9 18.2 93.0</cell><cell>63.3</cell><cell>41.7</cell><cell>44.9</cell><cell>42.7</cell></row><row><cell cols="3">Outlier Exposure [25] ? 14.6 17.7 94.0</cell><cell>61.7</cell><cell>43.7</cell><cell>44.1</cell><cell>43.8</cell></row><row><cell>OOD-Head [2]</cell><cell cols="2">? 19.7 56.2 88.8</cell><cell>66.6</cell><cell>33.7</cell><cell>34.3</cell><cell>33.9</cell></row><row><cell>OH*MSP [3]</cell><cell cols="2">? 18.8 30.9 89.7</cell><cell>66.6</cell><cell>43.3</cell><cell>44.2</cell><cell>43.6</cell></row><row><cell>DenseHybrid (ours)</cell><cell cols="2">? 30.2 13.0 95.6</cell><cell>63.0</cell><cell>46.1</cell><cell cols="2">45.3 45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Computational overhead of the proposed DenseHybrid anomaly detector when inferring with RTX3090 on two megapixel images</figDesc><table><row><cell>Method</cell><cell cols="4">Resynth. Infer. time (ms) Frames per sec. GFLOPs</cell></row><row><cell>SynBoost [4]</cell><cell>?</cell><cell>1055.5</cell><cell>&lt;1</cell><cell>-</cell></row><row><cell>SynthCP [51]</cell><cell>?</cell><cell>146.9</cell><cell>&lt;1</cell><cell>4551.1</cell></row><row><cell>LDN-121 [30]</cell><cell>?</cell><cell>60.9</cell><cell>16.4</cell><cell>202.3</cell></row><row><cell>LDN-121 + SML [28]</cell><cell>?</cell><cell>75.4</cell><cell>13.3</cell><cell>202.6</cell></row><row><cell>LDN-121 + DenseHybrid (ours)</cell><cell>?</cell><cell>63.7</cell><cell>15.7</cell><cell>202.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Validation of DenseHybrid components on Fishyscapes validation set</figDesc><table><row><cell>Anomaly detector</cell><cell>FS LostAndFound AP FPR95</cell><cell>FS Static AP FPR95</cell></row><row><cell cols="3">Discriminative (1 ? P (din|x)) 42.9 ? 4.2 42.1 ? 7.0 47.8 ? 5.0 41.6 ? 8.3</cell></row><row><cell>Generativep(x)</cell><cell cols="2">60.5 ? 2.6 7.4 ? 0.8 54.2 ? 2.1 6.2 ? 0.7</cell></row><row><cell cols="3">Hybrid (1 ? P (din|x))/p(x) 63.8 ? 2.9 6.1 ? 0.7 60.0 ? 2.0 4.9 ? 0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Impact of known inlier instances on anomaly detection performance. Results indicate that DenseHybrid is feasible for scenarios with limited scene variety</figDesc><table><row><cell>Training splits</cell><cell cols="2">FS Static AP FPR95 Cityscapes val mIoU Closed world</cell></row><row><cell>train</cell><cell>60.0?2.0 4.9?0.6</cell><cell>81.0</cell></row><row><cell>train+val</cell><cell>88.5?0.8 1.1?0.1</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Performance evaluation on StreetHazards. DenseHybrid anomaly score based on the ratio of generative and discriminative distributions outperforms each of the two components DenseHybrid (ours) 30.2 13.0 95.6 65.6 61.6 63.0 46.1 45.3 45.8</figDesc><table><row><cell>Method</cell><cell cols="3">Anomaly detection AP FPR95 AUC IoU-t5 IoU-t6 IoU o-IoU-t5 o-IoU-t6 o-IoU Closed world Open world</cell></row><row><cell>Generative</cell><cell>30.0 13.3 95.5 65.6 61.6 63.0 45.6</cell><cell>45.2</cell><cell>45.5</cell></row><row><cell>Discriminative</cell><cell>23.3 20.5 93.1 65.6 61.6 63.0 36.9</cell><cell>35.2</cell><cell>36.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by Croatian Science Foundation grant IP-2020-02-5851 ADEPT, as well as by European Regional Development Fund grants KK.01.2.1.02.0119 DATACROSS and KK.01.2.1.02.0119 A-Unit co-funded by Gideon Brothers ltd. We thank Marin Or?i? for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11409</idno>
		<title level="m">On feature collapse and deep kernel learning for single forward pass uncertainty</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-33676-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-33676-93" />
	</analytic>
	<monogr>
		<title level="m">41st DAGM German Conference, DAGM GCPR 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dense outlier detection and open-set recognition based on training with noisy negative images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno>abs/2101.09193</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixel-wise anomaly detection in complex driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Biase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3119" to="3135" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fishyscapes: A benchmark for safe semantic segmentation in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2019.00294</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2019.00294" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning and the unknown: Surveying steps toward open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henrydoss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="9801" to="9807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep metric learning for open world semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="15333" to="15342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Segmentmeifyoucan: A benchmark for anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<idno>abs/2104.14812</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gottschalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00365</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00365" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3551" to="3560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR abs/1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="3603" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">One versus all for deep neural network incertitude (OVNNI) quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aldea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
		<idno>abs/2006.00954</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TRADI: tracking deep neural network weight distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aldea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense open-set recognition with synthetic outliers generated by real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identification of Outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-015-3994-4</idno>
		<ptr target="https://doi.org/10.1007/978-94-015-3994-4" />
	</analytic>
	<monogr>
		<title level="j">Monographs on Applied Probability and Statistics</title>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep dual-resolution networks for realtime and accurate semantic segmentation of road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/2101.06085</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting flow generative models for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient ladder-style densenets for semantic segmentation of large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
		<imprint>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting outof-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Detecting road obstacles by erasing them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno>abs/2012.13633</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive density estimation for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>abs/1811.12709</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient semantic segmentation with pyramidal fusion. Pattern Recognit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107611</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107611" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107611</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IROS</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Open set recognition for automatic target classification with rejection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Scherreik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Rigling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="632" to="642" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Input complexity and out-of-distribution detection with likelihood-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">React: Out-of-distribution detection with rectified activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Road anomaly detection by partial image reconstruction with segmentation coupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?ipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chumerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Reino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wilddash -creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding failures in out-ofdistribution detection with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Revealing distributional vulnerability of explicit discriminators by implicit generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/2108.09976</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
