<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting ImageNet-scale models to complex distribution shifts with self-learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting ImageNet-scale models to complex distribution shifts with self-learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While self-learning methods are an important component in many recent domain adaptation techniques, they are not yet comprehensively evaluated on ImageNetscale datasets common in robustness research. In extensive experiments on ResNet and EfficientNet models, we find that three components are crucial for increasing performance with self-learning: (i) using short update times between the teacher and the student network, (ii) fine-tuning only few affine parameters distributed across the network, and (iii) leveraging methods from robust classification to counteract the effect of label noise. We use these insights to obtain drastically improved state-of-the-art results on ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error). Our techniques yield further improvements in combination with previously proposed robustification methods. Self-learning is able to reduce the top-1 error to a point where no substantial further progress can be expected. We therefore re-purpose the dataset from the Visual Domain Adaptation Challenge 2019 and use a subset of it as a new robustness benchmark (ImageNet-D) which proves to be a more challenging dataset for all current state-of-the-art models (58.2% error) to guide future research efforts at the intersection of robustness and domain adaptation on ImageNet scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) can reach human-level performance in many complex cognitive tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">48]</ref> if the distribution of the training data is sufficiently similar to the test data. However, DNNs are known to struggle if the distribution of the test data is shifted relative to the training data <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b8">9]</ref>. Even relatively simple distribution shifts induced through JPEG compression artefacts, low-* Equal contribution. ? Work done during an internship at Amazon T?bingen. ? Joint senior authors. ? ResNeXt+DeepAug+AugMix <ref type="bibr" target="#b40">[39]</ref> 38.0 34.8 -3.2 EfficientNet-L2, Noisy Student <ref type="bibr" target="#b54">[52]</ref> 28.3 22.0 -6 pass filtering, or salt-and-pepper noise (often referred to as common corruptions <ref type="bibr" target="#b21">[21]</ref>) can drastically reduce the accuracy of models trained on, e.g., ImageNet (IN; <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b7">8]</ref>). A whole subfield of machine learning is now concerned with the robustness of DNNs to common corruptions. Besides the standard benchmark dataset ImageNet-C (IN-C; <ref type="bibr" target="#b21">[21]</ref>), which tests object recognition performance on a range of 15 image corruptions, several other benchmark datasets have been proposed, including ObjectNet (ON; natural images with unusual perspectives <ref type="bibr" target="#b2">[3]</ref>), ImageNet-R (IN-R; artistic renditions for IN classes <ref type="bibr" target="#b20">[20]</ref>), ImageNet-A (IN-A; natural images that are challenging to ResNet50 models <ref type="bibr" target="#b23">[23]</ref>) or COCO-C (object detection on corrupted images <ref type="bibr" target="#b32">[31]</ref>).</p><p>In this subfield, robustness is almost exclusively evaluated in the setting of inductive inference where models are fixed and have no way of adapting to individual corruptions. In contrast, unsupervised domain adaptation operates in the realm of transductive inference <ref type="bibr" target="#b12">[13]</ref> where models have access to unlabeled data from the target domain which they can use for adaptation <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b41">40]</ref>. First works have started viewing robustness in the transductive inference setting and thus, utilizing methods from unsupervised domain adaptation for evaluating robustness. <ref type="bibr">Sun</ref>   <ref type="bibr" target="#b52">[50]</ref> and show stronger results compared to <ref type="bibr" target="#b46">[45]</ref>. Schneider et al. <ref type="bibr" target="#b40">[39]</ref> demonstrate that a simple domain adaptation technique, namely adapting batch normalization statistics to corrupted images, substantially increases classification accuracy on IN-C. A natural extension of this line of work is to apply and evaluate more potent, yet simple, domain adaptation techniques on common corruption and other robustness benchmarks. Self-learning is an essential component of many successful domain adaptation techniques (cf. <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b10">11]</ref>). In this paper, we perform an extensive study of several variants of self-learning, and analyze their properties on several robustness benchmarks. We use the umbrella term self-learning to describe the variants of pseudo-labeling and entropy-minimization. We leverage insights from label noise robustness to enable pseudo-labeling approaches with hard-labels in typical robustness evaluation scenarios. Results are compared to the three relevant baselines: Test-Time Training based on a self-supervised auxiliary objective <ref type="bibr" target="#b46">[45]</ref>, test time entropy minimization (TENT, <ref type="bibr" target="#b52">[50]</ref>) and batch norm adaptation <ref type="bibr" target="#b40">[39]</ref>). Our contributions are as follows:</p><p>? We obtain state-of-the-art adaptation performance on all common robustness datasets (IN-C: <ref type="bibr" target="#b22">22</ref>.0% mCE, IN-A: 14.8% top-1 error, IN-R: 17.4% top-1 error) and improve upon existing strategies for increasing model robustness for all tested model types ( <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_0">Table 1</ref>).</p><p>? We find that self-learning with short update intervals and a limited number of both adaptable and distributed parameters is crucial for success. We leverage label noise robustness methods to enable adaptation with hard labels and a limited number of images, a problem not typically present in smaller scale domain adaptation.</p><p>? Given the huge performance boost on all robustness datasets, we re-purpose the closest candidate to an ImageNet-scale domain adaptation dataset-the dataset used in the Visual Domain Adaptation Challenge 2019-and propose a subset of it as an additional robustness benchmark for the robustness community. We refer to it as ImageNet-D (IN-D, see example images in <ref type="figure" target="#fig_1">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Self-learning for Domain Adaptation</head><p>We consider the unsupervised domain adaptation setting for classification tasks. In this setting, we have access to labeled samples from the source domain (e.g., clean IN samples from the training set) as well as unlabeled samples from the target dataset (e.g., corrupted IN samples). We first review several popular variants of self-learning for domain adaptation before introducing a new robust variant.</p><p>A standard variant of self-learning for unsupervised domain adaptation works as follows: initially, a network f 0 (the teacher) trained on the source domain predicts labels on the target domain. Then, a second model f 1 (the student) is fine-tuned for one epoch on the predicted labels for all samples for which the teacher's confidence was above a given threshold. For the next epoch, the fine-tuned model f 1 (now being the new teacher) predicts the labels on the target domain, which are then used for fine-tuning another model f 2 (the new student).</p><p>In the following, let F j (x) denote the probability for class j for the teacher model on image x, and let f j (x) denote the probability for class j for the student model f . For all techniques, we can optionally only admit samples where the probability max j F j (x) exceeds some threshold. We consider three popular variants of self-learning for unsupervised domain adaptation: hard-and soft-labeling and entropy minimization. For all variants, the model architectures for the teacher and the student are chosen to be the same.</p><p>Hard Pseudo-Labeling <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b11">12]</ref> In this setting, we generate labels using a teacher model and train the student model on the pseudo labels using the standard cross-entropy loss,?</p><formula xml:id="formula_1">= argmax j F j (x), H (x,?) := ? log f?(x).<label>(1)</label></formula><p>Usually, only samples with a confidence above a certain threshold are considered for training the student. We test several thresholds but note that thresholding means discarding a large portion of the data which lead to a performance decrease in itself. The teacher is updated after each epoch.</p><p>Soft Pseudo-Labeling <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b11">12]</ref> In contrast to the hard pseudo-labeling variant, we here train the student on class probabilities predicted by the teacher,</p><formula xml:id="formula_2">S (x) := ? j F j (x) log f j (x).<label>(2)</label></formula><p>Soft pseudo-labeling is typically not used in conjunction with thresholding, since it already incorporates the certainty of the model. The teacher is updated after each epoch.</p><p>Entropy Minimization (ENT) <ref type="bibr" target="#b17">[17]</ref> This variant is similar to soft pseudo-labeling, but we no longer differentiate between a teacher and student network. It corresponds to an "instantaneous" update of the teacher. The training objective becomes</p><formula xml:id="formula_3">E (x) := ? j f j (x) log f j (x).<label>(3)</label></formula><p>Intuitively, self-training with entropy minimization leads to a sharpening of the output distribution for each sample, making the model more confident in its predictions.</p><p>Robust Pseudo-Labeling (RPL) Virtually all introduced self-training variants rely on the standard cross-entropy classification objective at their core. Standard cross-entropy loss has been shown to be sensitive to label noise <ref type="bibr" target="#b57">[55,</ref><ref type="bibr" target="#b56">54]</ref>.</p><p>In the setting of domain adaptation, inaccuracies in the teacher predictions and thus, the labels for the student, are inescapable, with severe repercussions for training stability and hyperparameter sensitivity as we show in the results. As a straight-forward solution to this problem, we propose to replace the cross-entropy loss by a robust classification loss designed to withstand certain amounts of label noise <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b57">55]</ref>. A popular candidate is the Generalized Cross Entropy (GCE) loss which combines the noise-tolerant Mean Absolute Error (MAE) loss <ref type="bibr" target="#b16">[16]</ref> with the CE loss. We combine GCE with hard-labeling and short update intervals into a robust variant of pseudo-labeling.</p><p>We only consider the hard labels and use the robust GCE as the training loss for the student,</p><formula xml:id="formula_4">GCE (x,?) = q ?1 (1 ? f?(x) q ),<label>(4)</label></formula><p>with q ? (0, 1]. For the limit case q ? 0, the GCE loss approaches the CE loss and for q = 1, the GCE loss becomes the MAE loss <ref type="bibr" target="#b57">[55]</ref>. We test updating the teacher both after every update step of the student (RPL) and once per epoch (RPL ep ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">New robustness benchmark: ImageNet-D</head><p>We re-purpose the dataset from the Visual Domain Adaptation Challenge 2019 <ref type="bibr" target="#b39">[38]</ref> as an additional robustness benchmark. This dataset has different image styles: Clipart, Real, Infograph, Painting, Quickdraw and Sketch and 345 Performance evaluation on IN-D The domains in IN-D differ in terms of their difficulty for the studied models. Therefore, we propose normalizing the error rates by the error achieved by AlexNet on the respective domains to calculate the mean error, following the approach in Hendrycks et al. <ref type="bibr" target="#b21">[21]</ref> for IN-C. This way, we obtain the aggregate score mean Domain Error (mDE) by calculating the mean over different domains,</p><formula xml:id="formula_5">DE f d = E f d E AlexNet d , mDE = 1 D D d=1 E f d ,<label>(5)</label></formula><p>where E f d is the top-1 error of a classifier f on domain d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment design</head><p>Datasets Hyperparameters The different self-learning variants have a range of hyperparameters like the learning rate or the stopping criterion. Our goal is to quantify the performance that we can hope to achieve in practice. To this end, we optimize hyperparameters for each variant of pseudolabeling on a hold-out set of IN-C that contains four types of image corruptions ("speckle noise", "Gaussian blur", "saturate" and "spatter") with five different strengths each. We refer to the hold-out set of IN-C as the dev set in the following. We use the found hyperparameters for all experiments on all datasets. Our experiments rely on two types of critical hyperparameters. The first type concerns the adaptation mechanism, i.e. which parameters of the student model are adapted. We consider three choices: (i) the parameters of the last network layer, (ii) the affine batch normalization parameters or (iii) all trainable parameters. The second type concerns our optimization strategy, in particular the optimizer, the number of total gradient updates and the learning rate. We use an SGD optimizer with a momentum of 0.9 and weight decay of 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We consider three popular model architectures: ResNet50, ResNeXt101 and EfficientNet-L2 (see Appendix A.2 for details on the used models). For ResNet50 and ResNeXt101, we include a simple baseline version trained on IN only.</p><p>For ResNeXt101, we additionally include a state-of-the-art robust version trained with DeepAugment and Augmix (DAug+AM; <ref type="bibr" target="#b20">[20]</ref>) and a version that was trained on 3.5 billion weakly labeled images (IG-3.5B; <ref type="bibr" target="#b29">[28]</ref>). Finally, for EfficientNet-L2 we select the current state of the art on IN-C which was trained on 300 million images from JFT-300M <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">24]</ref> using a noisy student-teacher protocol <ref type="bibr" target="#b54">[52]</ref>. For all models except EfficientNet-L2, we adapt the batch norm statistics to the test domains following <ref type="bibr" target="#b40">[39]</ref>. We do not expect significant gains for combining EfficientNet-L2 with batch norm adaptation: as demonstrated in <ref type="bibr" target="#b40">[39]</ref>, models trained with large amounts of weakly labeled data do not seem to benefit from batch norm adaptation. We validate the IN and IN-C performance of all considered models and match the originally reported scores <ref type="bibr" target="#b40">[39]</ref>. For EfficientNet-L2, we match IN top-1 accuracy up to 0.1% points, and IN-C up to 0.6% mCE. All details considering all hyperparameters that have been tested for all models and all methods can be found in Appendix A.1. For the EfficientNet-L2, we follow the procedure in <ref type="bibr" target="#b54">[52]</ref> and rescale all inputs to a resolution of 507 ? 507 px and then center-crop them to 475 ? 475 px.</p><p>The GCE loss The GCE loss introduces another single hyperparameter, its q-value. According to <ref type="bibr" target="#b57">[55]</ref>, for a noise rate around 0.2, q = 0.8 is optimal. On our dev set, the EfficientNet-L2 model has an average top-1 error of 18.8% (which fits the 'around 0.2' criterion). For higher noise rates, we could increase q at the cost of a possibly lower classification accuracy and/or slower convergence.We experiment with q = 0.7 and q = 0.8 for all models. We additionally show an ablation study over q for our ResNet50 model (cf. Appendix <ref type="table" target="#tab_0">Table 16</ref>).</p><p>Infrastructure The EfficientNet-L2 was adapted on 8 parallel NVIDIA Tesla V100 GPUs (16GB RAM) with a batch size of 8. For validation, we used the same GPUs and a batch size of 16. The ResNeXt101 was adapted on single NVIDIA Tesla K80 GPUs (11GB RAM) with a batch size of 32. For validation, we used the same batch size of 128 for the ResNeXt101 model but only one NVIDIA Tesla K80 GPU. The ResNet50 was adapted and evaluated on single NVIDIA Tesla K80 GPUs with batch size 128. Some ablation studies are performed on single NVIDIA RTX 2080 cards (11GB RAM) with a batch size of 96 and linear learning rate scaling to match the effective learning rate of other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the following subsections, we compare the performance and hyperparameter sensitivity of all selflearning methods, and perform extensive ablation studies to find optimal hyperparameters and self-learning paradigms.</p><p>For all datasets and models, we perform model selection based on the mCE obtained on the IN-C development set (IN-C dev; four corruptions) following previous methodology <ref type="bibr" target="#b40">[39]</ref>. While this set is conceptually similar to the IN-C test set (15 corruptions), we note that IN-A (natural images), IN-R (renditions) and IN-D (various domains) contain very different domain shifts.</p><p>We proceed in three steps: First, we show the effectiveness of self-learning on various models and datasets in Section 5.1. Second, we discuss additional insights into the optimal setting through a range of ablations in Section 5.2. Finally, given the substantial gains on all considered robustness datasets, we propose a new, more challenging dataset (IN-D) in Section 5.3 along with improvements obtained by self-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Self-learning improves various models on ImageNet-C, -R and -A</head><p>We first report results for the two best self-learning setups, robust pseudo-labeling (RPL) and entropy minimization (ENT), and compare our results to several recently published state-of-the-art models. For all models in ResNet50 <ref type="bibr" target="#b19">[19]</ref> ResNeXt101 32?8d <ref type="bibr" target="#b55">[53]</ref> EfficientNet-L2 <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b54">52]</ref>   <ref type="table" target="#tab_8">Table 2</ref>. mCE ( ) on IN-C in %. Entropy minimization (ENT) and pseudo-labeling paired with a robust loss function (RPL) reduce the mean Corruption Error (mCE) on IN-C for different models. We report the dev score on the holdout corruptions that were used for hyperparameter tuning and the "test" score on the 15 test corruptions, evaluated with the best hyper-parameters found on the dev set. We compare vanilla trained (Baseline) and the best known robust variants of different architectures. * For the EfficientNet-L2 model, we evaluate the mCE on dev on the severities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> to save computational resources. For the ResNet50 model, we show results averaged over three seeds as "mean (unbiased std)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>number of Model</head><p>Method learning rate epochs vanilla ResNet50 <ref type="table">Table 3</ref>. The best hyperparameters for all models that we found on IN-C. For all models, we fine-tune only the affine batch normalization parameters and use q = 0.8 for RPL.</p><formula xml:id="formula_6">ENT 1 ? 10 ?3 1 vanilla ResNet50 RPL 1 ? 10 ?3 5 vanilla ResNeXt101 ENT 2.5 ? 10 ?4 1 vanilla ResNeXt101 RPL 2.5 ? 10 ?4 4 IG-3.5B ResNeXt101 ENT 2.5 ? 10 ?4 4 IG-3.5B ResNeXt101 RPL 2.5 ? 10 ?3 2 DAug+AM ResNeXt101 ENT 2.5 ? 10 ?4 1 DAug+AM ResNeXt101 RPL 2.5 ? 10 ?4 4 EfficientNet-L2 ENT 4.6 ? 10 ?5 1 EfficientNet-L2 RPL 4.6 ? 10 ?4 1</formula><p>this section, only the affine batch normalization parameters were fine-tuned and the teacher was updated after every student gradient update. For robust pseudo-labeling, we used q = 0.8. The learning rate and the number of epochs varied between different models (but not between datasets since we used the same parameter values on all datasets). We provide an overview over the learning rates and number of training epochs used to produce our best results for different models in <ref type="table">Table 3</ref>.    <ref type="bibr" target="#b52">[50]</ref> in <ref type="table" target="#tab_5">Table 4</ref>. RPL outperforms TENT by 3% points. While TENT also uses entropy minimization, they did not tune hyperparameters and use a different learning rate (2.5 ? 10 ?4 ). The current state of the art on IN-C for a ResNet50 model is the model trained with powerful data augmentation (DeepAugment and AugMix) which reaches an mCE of 53.6% with a baseline evaluation <ref type="bibr" target="#b20">[20]</ref> and an mCE of 45.4% with batch norm adaptation <ref type="bibr" target="#b40">[39]</ref>. Thus, this model is superior to a vanilla model fine-tuned with RPL when batch norm adaptation is allowed. Since we were able to substantially improve the performance of the ResNeXt101 model trained with DeepAugment and AugMix (see next paragraph), we expect that the performance of the ResNet50 model trained with DeepAugment and AugMix can also be further improved with RPL and will add results in the near future.</p><p>Test-Time Training (TTT; <ref type="bibr" target="#b46">[45]</ref>) is another relevant baseline.   <ref type="table" target="#tab_0">Table 19</ref> in Appendix B.5. We ran this ablation without tuning hyperparameters besides the early stopping; we found one epoch to be optimal for hard labeling, four epochs optimal for RPL and besides use the same hyperparameters that we found for our vanilla ResNet50 model, see <ref type="table">Table 3</ref>. We expect even better results with hyperparameter tuning. For larger ResNeXt101 models (   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies: Update Interval, Adaptation Mechanisms, Label Noise Robustness</head><p>We now discuss variations of the previously discussed best self-learning setup in a set of ablation studies. We generally sweep across learning rates and number of epochs for all models (cf. Appendix B for all results).</p><p>To study the trade-off between loss functions and adaptation mechanisms, we carry out ablations for a baseline ResNet50. As before, we perform all evaluations that inform model and hyperparameter choices solely on the IN-C dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short updates outperform epoch-wise updates on ResNet50</head><p>We present results for soft-and hard-labeling and RPL for different update intervals in <ref type="table" target="#tab_9">Table 6</ref>. We differentiate between "no update": never updating the teacher, "epoch": updating the teacher after each epoch, and "instant": updating the teacher after every student update. We note that soft-labeling with instant teacher updates is equivalent to entropy minimization. We observe that RPL performs better than both soft-and hard-labeling for all update intervals. Updating the teacher after each student update ("instant") performs better than updating the teacher once per epoch ("epoch"). This large gap motivates us to focus on ENT and RPL with instant updates for the rest of our experiments. We show a learning rate ablation for soft-and hard-labeling and RPL ep in Appendix B.3. RPL performs better than hard-labeling with thresholding Since hard-labeling is usually used in conjuction with thresholding <ref type="bibr" target="#b27">[26]</ref>, we test both hardlabeling and RPL for different thresholds. For hard-labeling, using a threshold improves performance since the pseudo labels are cleaner. However, for RPL is robust to label noise and does not benefit from thresholding. Thresholding is even detrimental for RPL since the model is trained on less data. Still, RPL outperforms hard-labeling for all tested thresholds. Fine-tuning affine parameters outperforms lastlayer and full model fine-tuning Adapting all model parameters or only the parameters in the last layer is detrimental compared to only adapting the affine batch normalization parameters for both ENT and RPL <ref type="table">(Table 8)</ref>.</p><p>Additionally, adapting only the affine batch normalization parameters is considerably more resource efficient than adapting all model parameters. <ref type="bibr" target="#b0">1</ref> We hence only adapt batch normalization parameters for the rest of this section.</p><p>Additional ablations RPL has an additional hyperparameter q. We perform an ablation study over q and show the results in Appendix B.4, demonstrating that RPL is robust to the choice of q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ImageNet-D is a challenging new robustness dataset even after adaptation</head><p>We saw that combinations of models trained on large datasets (EfficientNet-L2) paired with pseudo labeling gives another performance boost over iid performance. Indeed, given the comparable improvements on all considered datasets (IN-C, -A, -R), it is questionable how many additional insights about model robustness could be leveraged by additional incremental improvements on these datasets. We therefore propose ImageNet-D (IN-D) as a new, more challenging dataset to include in the robustness benchmarking suite.</p><p>Similar More robust models perform better on IN-D We first evaluate the performance of various state-of-the-art robust ResNet50 models (cf. We find that the hyperparameters chosen on the IN-C dev set work well for all domains except for "Infograph" and "Quickdraw" where both RPL and ENT increase the top-1 error. We note that both domains have very high error rates in the beginning and thus hypothesize that the produced pseudo labels are of low quality from the very start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error analysis on IN-D To understand better what kind</head><p>of errors a ResNet50 model makes on IN-D, we analyze the most frequently predicted classes for different domains in order to find systematic errors indicative of the encountered distribution shifts. We find most errors interpretable: e.g., it makes sense that the classifier assigns the label "comic book" to images from the "Clipart" or "Painting" domains, or "website" to images from the "Infograph" domain, or "envelope" to images from the "Sketch" domain. We find no systematic errors on the "Real" domain which is expected since this domain should be similar to IN. Detailed results on the top-3 most frequently predicted classes for different domains can be found in <ref type="figure" target="#fig_9">Fig. 6</ref>, Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The IN-C benchmark <ref type="bibr" target="#b21">[21]</ref> has grown popular in the robustness community and has since been extended to other datasets and vision tasks <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b26">25]</ref>. Methods for improving model robustness to common corruptions often include data augmentation like Gaussian noise <ref type="bibr" target="#b9">[10]</ref>, combinations of diverse parametrical <ref type="bibr" target="#b22">[22]</ref> or DNN-based <ref type="bibr" target="#b20">[20]</ref> augmentations, training on stylized images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32]</ref> or against adversarial noise distributions <ref type="bibr" target="#b37">[36]</ref>.</p><p>This work builds upon one technique for unsupervised domain adaptation, self-learning, but future work could build on top of many other techniques currently deployed for domain adaptation. Examples include <ref type="bibr" target="#b10">[11]</ref> who use a mean teacher approach where the teacher weights are an exponential moving average of the student weights. They enforce consistent predictions between the student and the mean teacher. Another possibility might be building upon <ref type="bibr" target="#b44">[43]</ref> who combine Virtual Adversarial Domain Adaptation (VADA) with Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T).</p><p>The crucial aspect of RPL is the robust classification objective that lends robustness to the unavoidable label noise present in pseudo-labeling. DNNs are prone to memorization <ref type="bibr" target="#b56">[54]</ref> and tend to overfit to noisy labels when trained with the cross-entropy loss <ref type="bibr" target="#b16">[16]</ref>. Several recent works propose loss functions that are more robust to label noise compared to cross-entropy, e.g., <ref type="bibr" target="#b57">[55,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b53">51]</ref>. In this work, we focused on the Generalized Cross Entropy loss (GCE) as a proof of principle. A future extension might be to investigate other robust loss functions in this context. <ref type="bibr" target="#b35">[34]</ref> use GCE for multi-source domain adaptation in the Visual Domain Adaptation Challenge 2019 <ref type="bibr" target="#b39">[38]</ref>. They show better results with GCE compared to CE when training on pseudo-labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>We consider a transductive evaluation setting, assuming access to a sufficient amount of unlabeled test samples. This setting is justified for those robustness scenarios where the distribution varies slowly over time. For example, weather changes in autonomous driving occur only slowly over time, or data from a particular image acquisition system is available in batches.</p><p>Compared to simple techniques like batch norm adaptation <ref type="bibr" target="#b40">[39]</ref> which work even on single or small batches of samples, self-learning techniques are more applicableand also give larger performance boosts-when a sufficient amount of data is available.</p><p>Both RPL and ENT require hyperparameter tuning for the learning rate, number of training epochs and the choice of trained model parameters. While we showed that the IN-C dev set yields hyperparameters that can be transferred to domains very different from IN-C ( <ref type="figure">like IN-A, IN-R and IN-D)</ref>, it is conceivable that other validation sets are needed for more drastic shifts. The "Quickdraw" and "Infograph" domains in IN-D are examples for such failure cases.</p><p>We achieve our best results on all datasets with a (pre-trained) EfficientNet-L2 model that requires large computational resources, especially when trained from scratch. However, RPL and ENT also beat all other stateof-art baselines on smaller models, e.g., we achieve the strongest results reported for ResNeXt101 models and the ResNet50 model trained on IN only. Our methods bring additional improvements on top of known robustification methods. Additional evidence could be gathered in future work by evaluating additional ResNet50 based models <ref type="bibr" target="#b20">[20]</ref> with RPL and ENT.</p><p>We deliberately evaluated the simplest variants of selflearning and benchmark their performance in isolation. While popular in domain adaptation, self-learning is typically used in conjunction with additional techniques (cf. Section 6). It is well conceivable that state-of-the-art domain adaptation techniques developed on small datasets can be adapted to ImageNet-scale as well and will yield additional improvements over RPL, ENT and other selflearning techniques.</p><p>We would like to stress that our work does not propose a new domain adaptation technique. Rather, we show the conditions under which a simple technique is effective for test-time adaptation in robustness benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We evaluated and analysed how self-learning, an essential component in many unsupervised domain adaptation techniques, can be applied to larger scale image recognition problems common in robustness research.</p><p>Three ingredients are important to increase model robustness to complex distribution shifts with self-learning: (i) updating the teacher after every student gradient update, (ii) fine-tuning only the affine batch normalization parameters, and (iii) mitigating the effects of label noise by utilizing methods from robust classification. Guided by these insights, we report a new state of the art of 22% mCE on IN-C using a EfficientNet-L2 model. We show that the result is also transferable to visually different domains: Using hyperparameters tuned on the IN-C dev set, we obtain additional state of the art results on IN-R and IN-A. Hence, the methodology is viable for computer vision applications where extensive hyperparameter tuning is not feasible or practicable. We demonstrated our new robustness benchmark IN-D to be a challenging dataset for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional information on used models</head><p>A.1. Details on all hyperparameters we tested for different models ResNet50 models We use a vanilla ResNet50 model and compare soft-and hard-labeling against entropy minimization and robust pseudo-labeling. To find optimal hyperparameters for all methods, we perform an extensive evaluation and test (i) three different adaptation mechanisms (ii) several learning rates 1.0 ? 10 ?4 , 1.0 ? 10 ?3 , 1.0 ? 10 ?2 and 5.0 ? 10 ?2 , (iii) the number of training epochs and (iv) updating the teacher after each epoch or each iteration. For all experiments, we use a batch size of 128. The hyperparameter search is performed on IN-C dev. We then use the optimal hyperparameters to evaluate the methods on the IN-C test set.</p><p>ResNeXt101 models The ResNeXt101 model is considerably larger than the ResNet50 model and we therefore limit the number of ablation studies we perform for this architecture. Besides a baseline, we include a state-of-the-art robust version trained with DeepAugment+Augmix (DAug+AM; <ref type="bibr" target="#b20">[20]</ref>) and a version that was trained on 3.5 billion weakly labeled images (IG-3.5B; <ref type="bibr" target="#b29">[28]</ref>). We only test the two leading methods on the ResNeXt101 models (ENT and RPL). We vary the learning rate in same interval as for the ResNet50 model but scale it down linearly to account for the smaller batch size of 32. We only train the affine batch normalization parameters because adapting only these parameters leads to the best results on ResNet50 and is much more resource efficient than adapting all model parameters. Again, the hyperparameter search is performed only on the development corruptions of IN-C. We then use the optimal hyperparameters to evaluate the methods on the IN-C test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EfficientNet-L2 models The current state of the art on IN, IN-C, IN-R and IN-A is an</head><p>EfficientNet-L2 trained on 300 million images from JFT-300M <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">24]</ref> using a noisy student-teacher protocol <ref type="bibr" target="#b54">[52]</ref>. We adapt this model for only one epoch due to resource constraints. During the hyperparameter search, we only evaluate three corruptions on the IN-C development set <ref type="bibr" target="#b1">2</ref> and test the learning rates 4.6 ? 10 ?2 , 4.6 ? 10 ?3 , 4.6 ? 10 ?4 and 4.6 ? 10 ?5 . We use the optimal hyperparameters to evaluate ENT and RPL on the full IN-C test set (with all severity levels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Full list of used models</head><p>ImageNet trained models (ResNet50, ResNeXt) are taken directly from torchvision <ref type="bibr" target="#b30">[29]</ref>. The model variants trained with DeepAugment and AugMix augmentations <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b20">20]</ref> are taken from https://github.com/hendrycks/imagenet-r. The weaklysupervised ResNeXt101 model is taken from the PyTorch Hub. For EfficientNet <ref type="bibr" target="#b47">[46]</ref>, we use the PyTorch re-implementation available at https://github.com/rwightman/gen-efficientnet-pytorch. This is a verified re-implementation of the original work by <ref type="bibr" target="#b54">[52]</ref>. We verify the performance on ImageNet, yielding a 88.23% top-1 accuracy and 98.546% top-5 accuracy which is within 0.2% points of the originally reported result <ref type="bibr" target="#b54">[52]</ref>. On ImageNet-C, our reproduced baseline achieves 28.9% mCE vs. 28.3% mCE originally reported by <ref type="bibr" target="#b54">[52]</ref>. As noted in the re-implementation, this offset is possible due to minor differences in the pre-processing. It is possible that our adaptation results would improve further when applied on the original codebase by Xie et al..</p><p>The following <ref type="table">Table contains</ref> all models we evaluated on various datasets with references and links to the corresponding source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Source</head><p>ResNet50 <ref type="bibr" target="#b19">[19]</ref> https://github.com/pytorch/vision/tree/master/torchvision/models ResNeXt101, 32?8d <ref type="bibr" target="#b55">[53]</ref> B. Detailed and additional Results on IN-C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Detailed results for tuning epochs and learning rates</head><p>We tune the learning rate for all models and the number of training epochs for all models except the EfficientNet-L2. In this section, we present detailed results for tuning these hyperparameters for all considered models. The best hyperparameters that we found in this analysis, are summarized in <ref type="table">Table 3</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Detailed results for all IN-C corruptions</head><p>We outline detailed results for all corruptions and models in <ref type="table" target="#tab_0">Table 15</ref>. Performance across the severities in the dataset is depicted in <ref type="figure" target="#fig_6">Figure 4</ref>. All detailed results presented here are obtained by following the model selection protocol outlined in the main text.    <ref type="table" target="#tab_8">Table 2</ref> in the main paper. We show (unnormalized) top-1 error rate averaged across 15 test corruptions along with the mean corruption error (mCE: which is normalized). Hyperparameter selection for both ENT and RPL was carried out on the dev corruptions as outlined in the main text. Mismatch in baseline mCE for EfficientNet-L2 can be most likely attributed to pre-processing differences between the original tensorflow implementation <ref type="bibr" target="#b54">[52]</ref> and the PyTorch reimplementation we employ. We start with slightly weaker baselines for ResNet50 and ResNext101 than <ref type="bibr" target="#b40">[39]</ref>: ResNet50 and ResNext101 results are slightly worse than previously reported results (typically 0.1% points) due to the smaller batch size of 128 and 32. Smaller batch sizes impact the quality of re-estimated batch norm statistics when computation is performed on the fly <ref type="bibr" target="#b40">[39]</ref>, which is of no concern here due to the large gains obtained by pseudo-labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Ablation over the learning rate for soft-and hard-labeling and RPL ep</head><p>We show an ablation over the learning rate for soft-and hard-labeling in <ref type="table" target="#tab_0">Table 17</ref>. In <ref type="table">Table 3</ref> of the main paper, we show the results obtained when training with the best learning rate (0.001 for hard-labeling and 0.05 for soft-labeling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Ablation over the hyperparameter q for RPL</head><p>For RPL, we must choose the hyperparameter q. We performed an ablation study over q and show results in <ref type="table" target="#tab_0">Table 16</ref>, demonstrating that RPL is robust to the choice of q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Comparison to Test-Time Training [45]</head><p>Sun et al. <ref type="bibr" target="#b46">[45]</ref> use a ResNet18 for their experiments on ImageNet and only evaluate their method on severity 5 of IN-C. To enable a fair comparison, we trained a ResNet18 with both hard labeling and RPL and compare the efficacy of both methods to Test-Time Training in <ref type="table" target="#tab_0">Table 19</ref>. For both hard labeling and RPL, we use the hyperparameters we found for the vanilla ResNet50 model and thus, we expect even better results for hyperparameters tuned on the vanilla ResNet18 model and following our general hyperparameter search protocol.</p><p>While all methods (self-learning and TTT) improve the performance over a simple vanilla ResNet18, we note that even the very simple baseline using hard labeling already outperfoms Test-Time Training; further gains are possible with RPL. The result highlights the importance of simple baselines (like self-learning) when proposing new domain adaptation schemes. It is likely that many established DA techniques more complex than the basic self-learning techniques considered in this work will even further improve over TTT and other adaptation approaches developed exclusively in robustness settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPLq</head><p>IN-C* q=0.5 48.5* q=0. <ref type="bibr" target="#b5">6</ref> 48.5* q=0. <ref type="bibr" target="#b7">8</ref> 48.3* q=0. <ref type="bibr" target="#b8">9</ref> 48.2*   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed and additional Results on IN-D C.1. Evaluation protocol on IN-D</head><p>We tried a different model selection scheme on IN-D as a control experiment with "Leave one out cross-validation" (L1outCV): with a round-robin procedure, we choose the hyperparameters for the test domain on all other domains. We select the same hyperparameters as when tuning on the "dev" set: For the ResNet50 model, we select over the number of training epochs (with a maximum of 7 training epochs) and search for the optimal learning rate in the set [0.01, 0.001, 0.0001]. For the EfficientNet-L2 model, we train only for one epoch as before and select the optimal learning rate in the set <ref type="bibr">[</ref>  <ref type="table" target="#tab_8">(Table 23</ref>) and ENT <ref type="table" target="#tab_5">(Table 24</ref>). For RPL q=0.8 and ENT, we use the same hyperparameters that we chose on our IN-C 'dev' set. This means we train the models for 5 epochs with RPL q=0. <ref type="bibr" target="#b7">8</ref>  We show the top-1 error for the different IN-D domains versus training epochs for a vanilla ResNet50 in <ref type="figure" target="#fig_8">Fig. 5</ref>. We indicate the epochs 1 and 5 at which we extract the errors with dashed black lines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Detailed results on the error analysis on IN-D</head><p>Frequently predicted classes We analyze the most frequently predicted classes on IN-D by a vanilla ResNet50 and show the results in <ref type="figure" target="#fig_9">Fig. 6</ref>. We make several interesting observations: First, we find most errors interpretable: it makes sense that a ResNet50 assigns the label "comic book" to images from the "clipart" or "painting" domains, or "website" to images from the "infograph" domain, or "envelope" to images from the "sketch" domain. Second, on the hard domain "quickdraw", the ResNet50 mostly predicts non-sensical classes that are not in IN-D, mirroring its almost chance performance on this domain. Third, we find no systematic errors on the "real" domain which is expected since this domain should be similar to IN.</p><p>Filtering predictions on IN-D that cannot be mapped to ImageNet-D We perform a second analysis: We filter the predicted labels according to whether they can be mapped to IN-D and report the filtered top-1 errors as well as the percentage of filtered out inputs in <ref type="table" target="#tab_7">Table 25</ref>. We note that for the domains "infograph" and "quickdraw", the ResNet50 predicts labels that cannot be mapped to IN-D in over 70% of all cases, highlighting the hardness of these two domains.</p><p>Filtering labels and predictions on IN that cannot be mapped to ImageNet-D To test for possible class-bias effects, we test the performance of a ResNet50 model on IN classes that can be mapped to IN-D and report the results in <ref type="table" target="#tab_7">Table 25</ref>.</p><p>First, we map IN labels to IN-D to make the setting as similar as possible to our experiments on IN-D and report the top-1 error (12.1%). This error is significantly lower compared to the top-1 error a ResNet50 obtains following the standard evaluation protocol (23.9%). This can be explained by the simplification of the task: While in IN there are 39 bird classes, these are all mapped to the same hierarchical class in IN-D. Therefore, the classes in IN-D are more dissimilar from each other than in IN. Additionally, there are only 164 IN-D classes compared to the 1000 IN classes, raising the chance level prediction.</p><p>If we further only accept predictions that can be mapped to IN-D, the top-1 error is slightly increased to 13.4%. In total, about 52.7% of all images in the IN validation set cannot be mapped to IN-D. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Robust pseudo labeling (RPL ) achieves a new state of the art on ImageNet-C across various model architectures. ImageNet-C (mCE [%] ) Baseline RPL(ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example images from the different domains of ImageNet-D. classes in total, with 164 overlapping IN classes. For our robustness benchmark, we filter out the classes that cannot be mapped to IN and refer to the smaller version as ImageNet-D (IN-D). We map 463 classes in IN to these 164 IN-D classes; e.g., for an image from the "bird" class in IN-D, we accept all 39 bird classes in IN as valid predictions. We show example images from IN-D in Fig. 2. The detailed evaluation protocol along with justifications for our design choices and additional analysis are outlined in Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Preprocessing</head><label></label><figDesc>For IN, IN-R, IN-A and IN-D, we resize all images to 256 ? 256 px and take the center 224 ? 224 px crop. The IN-C images are already rescaled and cropped. We center and re-scale the color values with ? RGB = [0.485, 0.456, 0.406] and ? RGB = [0.229, 0.224, 0.225].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>to IN-C, we use baseline results of testing an AlexNet model on IN-D to normalize the top-1 error on different domains in the dataset (cf. C.4) for the proposed mean domain error (mDE) metric. To exclude possible class-bias effects, we repeat our IN-D evaluation protocol on IN for a vanilla ResNet50 model and discuss the results in Appendix C.3. We use the same hyperparameters we obtained on IN-C dev for all our IN-D experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Severity-wise mean corruption error (normalized using the average AlexNet baseline error for each corruption) for ResNet50 (RN50), ResNext101 (RNx101) variants and the Noisy Student L2 model. Especially for more robust models (DeepAugment+Augmix and Noisy Student L2), most gains are obtained across higher severities 4 and 5. For weaker models, the baseline variant (Base) is additionally substantially improved for smaller corruptions. g h t c o n t r a s t e l a s t i c p i x e l a t e j p e g m C E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>g h t c o n t r a s t e l a s t i c p i x e l a t e j p e gA v g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Top-1 error for the different IN-D domains for a ResNet50 and training with RPLq=0.8 and ENT. We indicate the epochs at which we extract the test errors by the dashed black lines (epoch 1 for ENTand epoch 5 for RPLq=0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Systematic predictions of a vanilla ResNet50 on IN-D for different domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Robust pseudo labeling (RPL) achieves a new state of the art on ImageNet-C, ImageNet-R and ImageNet-A.</figDesc><table><row><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. propose a new adaptation technique (Test-Time Training) by adding an auxiliary self-supervised loss to the training objective and by continuing training the model on the auxiliary loss during inference [45]. Wang et al. adapt the batch normalization parameters on IN-C with entropy minimization</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>mCE ( ) in % on IN-C for different ResNet50 models that have been trained on IN only, with information whether they have been adapted at test-time. We note that allowing adaptation at test-time improve robustness.IN-CWe improve upon the best results for vanilla and robust models of all sizes, and achieve a new state of the art of 22% mCE on IN-C with the largest EfficientNet-L2 model (Table 2, right). Both ENT and RPL improve the baseline score consistently as more unlabeled samples become available during adaptation(Figure 3 i).For the vanilla ResNet50, ENT and RPL improve over batch norm adaptation<ref type="bibr" target="#b40">[39]</ref> by 10.6% and 11.7% points mCE, respectively (Table 2, left). We include a comparison to TENT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The authors limit their IN-C experiments to a ResNet18 evaluated on IN-C severity 5 corruptions. For</figDesc><table><row><cell></cell><cell></cell><cell>Number of</cell></row><row><cell>Model</cell><cell cols="2">top-1 err IN only parameters</cell></row><row><cell>ResNet50 (vanilla)</cell><cell>63.8</cell><cell>2.6 ? 10 7</cell></row><row><cell>ResNet50 (BN adapt, [39])</cell><cell>59.9</cell><cell>2.6 ? 10 7</cell></row><row><cell>ResNet50 (ENT, ours)</cell><cell>56.1</cell><cell>2.6 ? 10 7</cell></row><row><cell>ResNet50 (RPL, ours)</cell><cell>54.1</cell><cell>2.6 ? 10 7</cell></row><row><cell>EfficientNet-L2 [52]</cell><cell>23.5</cell><cell>4.8 ? 10 8</cell></row><row><cell>EfficientNet-L2 (ENT, ours)</cell><cell>19.7</cell><cell>4.8 ? 10 8</cell></row><row><cell>EfficientNet-L2 (RPL, ours)</cell><cell>17.4</cell><cell>4.8 ? 10 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Top-1 error ( ) on IN-R in %. RPL reaches a new state of the art on IN-R (200 classes) after adaptation.a fair comparison, we adapt a ResNet18 and show results for hard pseudo-labeling and RPL, both with instant teacher updates. TTT reduces mean top-1 error from 68.8% to 66.26%; a simple hard-labeling baseline trained for a single epoch already improves to 62.45% and RPL enables a further improvement to 61.85 % when trained for four epochs. We were able to improve over TTT in all but the contrast corruption, cf.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2</head><label>2</label><figDesc>, middle), RPL improves the performance of the vanilla model by<ref type="bibr" target="#b12">13</ref>.6% mCE over batch norm adaptation. Applying RPL to a model trained with DeepAugment and Augmix<ref type="bibr" target="#b20">[20]</ref> still yields a 3.3% points mCE improvement. Interestingly, while Schneider et al. found that batch norm adaptation yields almost no gains on the ResNeXt101 trained on 3.5 billion weakly labeled samples (IG-3.5B)<ref type="bibr" target="#b40">[39]</ref>, RPL is able to improve performance by 11% mCE over the baseline.We show more detailed results in Appendix B.2 for all IN-C test corruptions for the best hyperparameters. We improve over the baseline result with both ENT and RPL in all corruptions, but RPL performs consistently better across all but one corruption type. We show the full hyperparameter sweep across learning rates and training epochs inTables 11, 12, 13 and 14, Appendix B.1.IN-R ENT and RPL improve performance on IN-R.We use the hyperparameters obtained on IN-C dev and adapt the EfficientNet-L2 and the vanilla ResNet50 models on the renditions in the IN-R dataset. We improve the state-of-theart top-1 error from 23.5% to 17.4% (cf. Table 5,Figure 3ii) with the EfficientNet-L2 model. For a vanilla ResNet50, we improve the top-1 error from 63.8%<ref type="bibr" target="#b20">[20]</ref> to 56.1% with ENT and to 54.1% with RPL.</figDesc><table><row><cell></cell><cell cols="3">Update Interval</cell></row><row><cell cols="4">Method no update epoch instant</cell></row><row><cell>hard</cell><cell>57.1</cell><cell>53.8</cell><cell>50.2</cell></row><row><cell>soft</cell><cell>59.2</cell><cell>60.1</cell><cell>50.0</cell></row><row><cell>RPL</cell><cell>54.0</cell><cell>49.7</cell><cell>49.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Ablation over update interval (mCE, IN-C dev).</figDesc><table><row><cell></cell><cell cols="2">Pseudo-Label Threshold</cell></row><row><cell></cell><cell>0.0 0.5</cell><cell>0.9</cell></row><row><cell>hard</cell><cell>53.8 51.9</cell><cell>52.4</cell></row><row><cell cols="2">RPL ep 49.7 49.9</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Ablation over thresholds (mCE, IN-C dev).IN-AAdapting the EfficientNet-L2 model using ENT or RPL yields improvements on IN-A. We again use the optimal hyperparameters obtained on IN-C dev.</figDesc><table /><note>ENT improves the top-1 error from 16.5% [52] to 15.5% and we obtain a new state-of-the art of 14.8% top-1 error with RPL, with a 10.3% relative reduction in error.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table 8. mCE ( ) in % on IN-C dev for ENT and RPL over different learning rates and adaptation mechanisms.</figDesc><table><row><cell></cell><cell cols="2">ENT</cell><cell></cell><cell>RPL</cell></row><row><cell>lr</cell><cell>10 ?4</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>10 ?3</cell></row><row><cell>affine</cell><cell>51.0</cell><cell>50.0</cell><cell>52.9</cell><cell>48.9</cell></row><row><cell>full</cell><cell>51.3</cell><cell>60.2</cell><cell>50.5</cell><cell>51.5</cell></row><row><cell>last layer</cell><cell>60.2</cell><cell>60.9</cell><cell>60.2</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 )Table 10</head><label>910</label><figDesc>. With RPL and ENT, we improve the performance of all robust ResNet50 models,</figDesc><table><row><cell></cell><cell></cell><cell cols="3">mDE on IN-D ( )</cell></row><row><cell>Model</cell><cell cols="5">Baseline BN adapt RPL q=0.8 ENT</cell></row><row><cell>vanilla</cell><cell cols="2">88.2</cell><cell>80.2</cell><cell>76.1</cell><cell>77.3</cell></row><row><cell>SIN [14]</cell><cell cols="2">85.6</cell><cell>79.6</cell><cell>76.8</cell><cell>75.5</cell></row><row><cell>ANT [36]</cell><cell cols="2">86.9</cell><cell>80.7</cell><cell>78.1</cell><cell>76.5</cell></row><row><cell>ANT+SIN [36]</cell><cell cols="2">83.1</cell><cell>77.8</cell><cell>76.1</cell><cell>75.2</cell></row><row><cell>AugMix [22]</cell><cell cols="2">85.4</cell><cell>78.4</cell><cell>74.6</cell><cell>74.4</cell></row><row><cell>DAug [20]</cell><cell cols="2">85.6</cell><cell>78.8</cell><cell>74.8</cell><cell>73.3</cell></row><row><cell cols="3">DAug+AM [20] 83.4</cell><cell>74.9</cell><cell>72.6</cell><cell>72.7</cell></row><row><cell cols="6">Table 9. mDE on IN-D in % as obtained by robust ResNet50</cell></row><row><cell cols="6">models with a baseline evaluation, batch norm adaptation,</cell></row><row><cell>RPLq=0.8 and ENT.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain</cell><cell></cell><cell cols="3">Base ENT RPL</cell></row><row><cell>Clipart</cell><cell></cell><cell>45.0</cell><cell>39.8</cell><cell>37.9</cell></row><row><cell cols="2">Infograph</cell><cell>77.9</cell><cell>91.3</cell><cell>94.3</cell></row><row><cell>Painting</cell><cell></cell><cell>42.7</cell><cell>41.7</cell><cell>40.9</cell></row><row><cell cols="2">Quickdraw</cell><cell>98.4</cell><cell>99.4</cell><cell>99.4</cell></row><row><cell>Real</cell><cell></cell><cell>29.2</cell><cell>28.7</cell><cell>27.9</cell></row><row><cell>Sketch</cell><cell></cell><cell>56.4</cell><cell>48.0</cell><cell>51.5</cell></row><row><cell>mDE</cell><cell></cell><cell>67.2</cell><cell>66.8</cell><cell>67.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-C and</cell></row><row><cell cols="6">IN-R. This indicates good generalization capabilities of</cell></row><row><cell cols="6">the techniques combined for these models, given the</cell></row><row><cell cols="6">large differences between the three considered datasets.</cell></row><row><cell cols="6">However, even the best models perform 20 to 30 percentage</cell></row><row><cell cols="6">points worse on IN-D compared to their performance on</cell></row><row><cell cols="6">IN-C or IN-R, indicating that IN-D might be a more</cell></row><row><cell cols="2">challenging benchmark.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">EfficientNet-L2 The EfficientNet-L2 model obtains an</cell></row><row><cell cols="6">mDE on IN-D of 67.2% which is reduced slightly with</cell></row><row><cell cols="6">ENT to 66.8% (cf. Table 10). We note that the</cell></row><row><cell cols="6">mDE is surprisingly high compared to the model's strong</cell></row><row><cell cols="6">performance on the other considered datasets (IN-A: 14.8%</cell></row><row><cell cols="6">top-1 error, IN-R: 17.4% top-1 error, IN-C: 22.0% mCE).</cell></row><row><cell cols="6">Even on the "Real" domain closest to clean IN where</cell></row><row><cell cols="6">the EfficientNet-L2 model has a top-1 error of 11.6%, the</cell></row><row><cell cols="5">model only reaches a top-1 error of 29.2%.</cell></row></table><note>. Top-1 error ( ) on IN-D in % for EfficientNet-L2 with DAug+AM having the best mDE of 72.6% with RPL q=0.8 , see Table 9. We show detailed results for all domains, all models and all methods, and compare the IN- D results to results on IN-C and IN-R in Appendix C.2. The best performing models on IN-D (ANT+SIN and DAug+AM) are also the strongest ones on IN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>in the main part.</figDesc><table><row><cell>criterion</cell><cell></cell><cell>ENT</cell><cell></cell><cell></cell><cell>RPL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>lr</cell><cell>1e-4</cell><cell>1e-3</cell><cell cols="2">1e-2 1e-4</cell><cell cols="2">1e-3 1e-2</cell><cell></cell><cell></cell></row><row><cell>epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>60.2</cell><cell>60.2</cell><cell>60.2</cell><cell>60.2</cell><cell>60.2</cell><cell>60.2</cell><cell cols="3">lr (4.6 ?) base 10 ?3 10 ?4 10 ?5 10 ?6</cell></row><row><cell>1</cell><cell cols="2">54.3 50.0</cell><cell>72.5</cell><cell>57.4</cell><cell>51.1</cell><cell>52.5</cell><cell>ENT</cell><cell cols="2">25.5 87.8 25.3 22.2 24.1</cell></row><row><cell>2</cell><cell>52.4</cell><cell>50.9</cell><cell>96.5</cell><cell>55.8</cell><cell>49.6</cell><cell>57.4</cell><cell cols="2">RPL q=0.7 25.5 60.3 21.3 23.3</cell><cell>n/a</cell></row><row><cell>3</cell><cell>51.5</cell><cell cols="2">51.0 112.9</cell><cell>54.6</cell><cell>49.2</cell><cell>64.2</cell><cell cols="2">RPL q=0.8 25.5 58.2 21.4 23.4</cell><cell>n/a</cell></row><row><cell>4</cell><cell>51.0</cell><cell cols="2">52.4 124.1</cell><cell>53.7</cell><cell>49.0</cell><cell>71.0</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>50.7</cell><cell cols="2">53.5 131.2</cell><cell cols="2">52.9 48.9</cell><cell>76.3</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>50.7</cell><cell cols="2">53.5 131.2</cell><cell>52.9</cell><cell>48.9</cell><cell>76.3</cell><cell></cell><cell></cell></row><row><cell cols="7">Table 11. mCE in % on the IN-C dev set for ENT and RPL for</cell><cell></cell><cell></cell></row><row><cell cols="7">different numbers of training epochs when adapting the affine</cell><cell></cell><cell></cell></row><row><cell cols="5">batch norm parameters of a ResNet50 model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc></figDesc><table><row><cell>mCE ( ) in % on the IN-C dev set for different</cell></row><row><cell>learning rates for EfficientNet-L2. We favor q = 0.8 over</cell></row><row><cell>q = 0.7 due to slightly improved robustness to changes in the</cell></row><row><cell>learning rate in the worst case error setting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 .</head><label>13</label><figDesc></figDesc><table><row><cell></cell><cell>RPL</cell><cell>Baseline</cell><cell>IG-3.5B</cell><cell>DAug+AM</cell></row><row><cell></cell><cell cols="4">lr 2.5? 1e-4 1e-3 5e-3 1e-4 1e-3 5e-3 1e-4 1e-3 5e-3</cell></row><row><cell></cell><cell>epoch</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BASE</cell><cell cols="3">53.6 53.6 53.6 47.4 47.4 47.4 37.4 37.4 37.4</cell></row><row><cell></cell><cell>1</cell><cell cols="3">43.4 51.3 div. 45.0 39.9 43.6 35.3 35.1 79.1</cell></row><row><cell></cell><cell>2</cell><cell cols="3">42.3 63.2 div. 43.4 39.3 48.2 34.9 35.6 121.2</cell></row><row><cell></cell><cell>3</cell><cell cols="3">42.0 72.6 div. 42.4 39.4 52.9 34.7 40.1 133.5</cell></row><row><cell></cell><cell>4</cell><cell cols="3">42.0 72.6 div. 42.4 39.4 52.9 34.7 40.1 133.5</cell></row><row><cell>mCE in % on IN-C dev for entropy minimization for</cell><cell cols="4">Table 14. mCE in % on IN-C dev for robust pseudo-</cell></row><row><cell>different learning rates and training epochs for ResNeXt101.</cell><cell cols="4">labeling for different learning rates and training epochs for</cell></row><row><cell>(div.=diverged)</cell><cell cols="2">ResNeXt101. (div.=diverged)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Baseline (ours) 57.2 59.5 60.0 61.4 62.3 51.3 49.5 54.6 54.1 39.3 29.1 46.7 41.4 38.2 41.8 62.8 ENT 45.5 45.5 46.8 48.4 48.7 40.0 40.3 42.0 46.6 33.2 28.1 42.4 35.2 32.2 35.1 51.6 RPL 44.2 44.4 45.5 47.0 47.4 38.8 39.2 40.7 46.2 32.5 27.7 42.7 34.6 31.6 34.4 50.5 28.1 27.8 28.3 29.1 30.1 26.3 27.4 28.8 29.8 25.9 22.7 25.6 27.9 23.2 25.4 34.8 Baseline (ours) 21.6 22.0 20.5 23.9 40.5 19.8 23.2 22.8 26.9 21.0 15.2 21.2 24.8 17.9 18.6 28.9 ENT 18.5 18.7 17.4 18.8 23.4 16.9 18.8 17.1 19.6 16.8 14.1 16.6 19.6 15.8 16.5 23.0 RPL 17.8 18.0 17.0 18.1 21.4 16.4 17.9 16.4 18.7 15.7 13.6 15.6 19.2 15.0 15.6 22.0 Table 15. Detailed results for each corruption along with mean corruption error (mCE) as reported in</figDesc><table><row><cell>ResNet50</cell><cell></cell></row><row><cell>Baseline [39]</cell><cell>62.2</cell></row><row><cell cols="2">ResNeXt101 Baseline</cell></row><row><cell>Baseline [39]</cell><cell>56.7</cell></row><row><cell cols="2">Baseline (ours) 52.8 54.1 54.0 55.4 56.8 46.7 46.6 48.5 49.4 36.6 25.4 42.8 37.8 32.5 36.7 56.8</cell></row><row><cell>ENT</cell><cell>40.5 39.5 41.4 41.6 43.0 34.1 34.5 35.0 39.4 28.5 24.0 33.8 30.3 27.2 30.5 44.3</cell></row><row><cell>RPL</cell><cell>39.4 38.9 39.8 40.3 41.0 33.4 33.8 34.6 38.7 28.0 23.7 31.4 29.8 26.8 30.0 43.2</cell></row><row><cell cols="2">ResNeXt101 IG-3.5B</cell></row><row><cell>Baseline [39]</cell><cell>51.6</cell></row><row><cell cols="2">Baseline (ours) 50.7 51.5 53.1 54.2 55.5 45.5 44.7 41.7 42.0 28.1 20.1 33.8 35.4 27.8 33.9 51.8</cell></row><row><cell>ENT</cell><cell>38.6 38.3 40.4 41.4 41.5 33.8 33.6 32.2 34.6 24.1 19.7 26.3 27.6 24.2 27.9 40.8</cell></row><row><cell>RPL</cell><cell>39.1 39.2 40.8 42.1 42.4 33.7 33.5 31.8 34.7 23.9 19.6 26.1 27.5 23.8 27.5 40.9</cell></row><row><cell cols="2">ResNeXt101 DeepAug+Augmix</cell></row><row><cell>Baseline [39]</cell><cell>38.0</cell></row><row><cell cols="2">Baseline (ours) 30.0 30.0 30.2 32.9 35.5 28.9 31.9 33.3 32.8 29.5 22.6 28.4 31.2 23.0 26.5 38.1</cell></row><row><cell>ENT</cell><cell>28.7 28.5 29.0 29.8 30.9 26.9 28.0 29.3 30.5 26.2 23.2 26.3 28.5 23.7 26.0 35.5</cell></row><row><cell>Noisy Student L2</cell><cell></cell></row><row><cell>Baseline [52]</cell><cell>28.3</cell></row></table><note>RPL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 .</head><label>16</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>LR</cell><cell>CE-HARD</cell><cell>CE-SOFT</cell><cell>LR</cell><cell>ENT</cell><cell>RPL</cell><cell>ep q=0.8</cell><cell>RPL q=0.8</cell></row><row><cell cols="2">mCE in % on a random subsample of severities of the IN-C "test" corruptions for different values of q for a vanilla ResNet50</cell><cell cols="3">0.001 0.010 0.050 Table 17. mCE on the IN-C dev 57.144 61.266 58.034 59.462 59.887 59.217 corruptions for different learning rates for CE-HARD and CE-SOFT.</cell><cell cols="4">1e-4 1e-3 1e-2 5e-2 133.9 50.7 50.0 72.5 Table 18. mCE in % on IN-C dev for ENT, -52.9 54.4 48.9 54.0 52.5 55.0 111.0 RPL ep q=0.8 and RPLq=0.8 over different learning rates and update intervals.</cell></row><row><cell>model.</cell><cell>Explorative result: Numbers are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparable amongst each other, but not</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparable to results in other tables.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>64.7 55.7 52.2 55.7 66.26 hard labeling, single epoch 73.2 70.8 73.6 76.5 75.6 63.9 56.1 59.0 65.9 48.4 39.7 85.2 50.4 47.0 51.5 62.45 RPL (ours), four epochs 71.3 68.3 71.7 76.2 75.6 61.5 54.4 56.9 67.1 47.3 39.3 93.2 48.9 45.7 50.4 61.85 Table 19. Comparison to Test-Time Training [45]: Top-1 error for a ResNet18 and severity 5 for all corruptions.</figDesc><table><row><cell>vanilla ResNet18</cell><cell>98.8 98.2 99.0 88.6 91.3 88.8 82.4 89.1 83.5 85.7 48.7 96.6 83.2 76.9 70.4 85.41</cell></row><row><cell>Test-Time Training</cell><cell>73.7 71.4 73.1 76.3 93.4 71.3 66.6 64.4 81.3 52.4 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>4.6 ? 10 ?3 , 4.6 ? 10 ?4 , 4.6 ? 10 ?5 , 4.6 ? 10 ?6 ]. This model selection leads to worse results both for the ResNet50 and the EfficientNet-L2 models, highlighting the robustness of our model selection process, seeTable 20.Table 20. mDE in % on IN-D for different model selection strategies.C.2. Detailed results for robust ResNet50 models on IN-DWe show detailed results for all models on IN-D for vanilla evaluation(Table 21)BN adaptation(Table 22), RPL q=0.8</figDesc><table><row><cell>model</cell><cell cols="2">model selection</cell></row><row><cell></cell><cell cols="2">L1outCV IN-C dev</cell></row><row><cell>ResNet50 RPLq=0.8</cell><cell>81.3</cell><cell>76.1</cell></row><row><cell>ResNet50 ENT</cell><cell>82.4</cell><cell>77.3</cell></row><row><cell>EfficientNet-L2 ENT</cell><cell>69.2</cell><cell>66.8</cell></row><row><cell>EfficientNet-L2 RPLq=0.8</cell><cell>69.1</cell><cell>67.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 21 .</head><label>21</label><figDesc>and for one epoch with ENT. Top-1 error on IN-D in % as obtained by robust ResNet50 models. For reference, we also show the mCE on IN-C and the top-1 error on IN-R.</figDesc><table><row><cell>Model</cell><cell cols="7">Clipart Infograph Painting Quickdraw Real Sketch mDE IN-C IN-R</cell></row><row><cell>vanilla</cell><cell>76.0</cell><cell>89.6</cell><cell>65.1</cell><cell>99.2</cell><cell>40.1</cell><cell>82.0</cell><cell>88.2 76.7 63.9</cell></row><row><cell>SIN [14]</cell><cell>71.3</cell><cell>88.6</cell><cell>62.6</cell><cell>97.5</cell><cell>40.6</cell><cell>77.0</cell><cell>85.6 69.3 58.5</cell></row><row><cell>ANT [36]</cell><cell>73.4</cell><cell>88.9</cell><cell>63.3</cell><cell>99.2</cell><cell>39.9</cell><cell>80.8</cell><cell>86.9 62.4 61.0</cell></row><row><cell>ANT+SIN [36]</cell><cell>68.4</cell><cell>88.6</cell><cell>60.6</cell><cell>95.5</cell><cell>40.8</cell><cell>70.3</cell><cell>83.1 60.7 53.7</cell></row><row><cell>AugMix [22]</cell><cell>70.8</cell><cell>88.6</cell><cell>62.1</cell><cell>99.1</cell><cell>39.0</cell><cell>78.5</cell><cell>85.4 65.3 58.9</cell></row><row><cell>DeepAugment [20]</cell><cell>72.0</cell><cell>88.8</cell><cell>61.4</cell><cell>98.9</cell><cell>39.4</cell><cell>78.5</cell><cell>85.6 60.4 57.8</cell></row><row><cell>DeepAug+Augmix [20]</cell><cell>68.4</cell><cell>88.1</cell><cell>58.7</cell><cell>98.2</cell><cell>39.2</cell><cell>75.2</cell><cell>83.4 53.6 53.2</cell></row><row><cell>Model</cell><cell cols="7">Clipart Infograph Painting Quickdraw Real Sketch mDE</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For reference, a ResNet50 has 2.6 ? 10 7 trainable parameters, out of which 5.3 ? 10 4 (0.2%) are affine batch normalization parameters and 2.0 ? 10 6 (7.7%) are parameters in the last network layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ResNet50+DeepAugment+AugMix<ref type="bibr" target="#b20">[20]</ref> https://github.com/hendrycks/imagenet-r ResNext101<ref type="bibr" target="#b20">[20]</ref> https://github.com/hendrycks/imagenet-r ResNext101 32?8d IG-3.5B<ref type="bibr" target="#b29">[28]</ref> https://github.com/facebookresearch/WSL-Images/blob/master/hubconf.py Noisy Student EfficientNet-L2<ref type="bibr" target="#b54">[52]</ref> https://github.com/rwightman/gen-efficientnet-pytorch<ref type="bibr" target="#b1">2</ref> We compare the results of computing the dev set on the 1, 3 and 5 severities versus the 1, 2, 3, 4 and 5 severities on our ResNeXt101 model in the Supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Roland S. Zimmermann, Yi Zhu, Raghavan Manmatha, Matthias K?mmerer, Matthias Tangemann, Bernhard Sch?lkopf, Shubham Krishna, Berkay Kicanaoglu and Mohammadreza Zolfaghari for helpful discussions on pseudo labeling and feedback on our paper draft. We thank Yasser Jadidi and Alex Smola for support in the setup of our compute infrastructure.</p><p>We </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dota 2 with large scale deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemyslaw</forename><surname>Debiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christy</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quirin</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shariq</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<idno>abs/1912.06680</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. Language models are few-shot learners. 2020. 1</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning with instance-dependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2010.02347</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Communications and Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples are a natural consequence of test error in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nic</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogus</forename><surname>Cubuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empirical comparison of hard and soft label propagation for relational classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th international conference on Inductive logic programming</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth Conference on Uncertainty in Artificial Intelligence UAI</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet-trained CNNs are biased towards texture</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">increasing shape bias improves accuracy and robustness</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalisation in humans and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<biblScope unit="page" from="7538" to="7550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Robust loss functions under label noise for deep neural networks. In Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Dawn Song. Natural adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Benchmarking the robustness of semantic segmentation models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop : Challenges in Representation Learning (WREPL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Curriculum loss: Robust learning and generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Docker: Lightweight linux containers for consistent development and deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Merkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux J</title>
		<imprint>
			<biblScope unit="issue">239</biblScope>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Benchmarking robustness in object detection: Autonomous driving when winter is coming. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Miko?ajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Grochowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Interdisciplinary PhD Workshop (IIPhDW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MNIST-C: A robustness benchmark for computer vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation and semi-supervised domain adaptation with focus on visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<idno>abs/1910.03548</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Increasing the robustness of dnns against image corruptions by playing the game of noise. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Visual Domain Adaptation Challenge</title>
		<imprint>
			<date type="published" when="2019-11-17" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2118" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning adaptive loss for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno>abs/2002.06482, 2020. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transductive domain adaptation with affinity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Shu And Longin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2007.08199, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Test-time training for out-ofdistribution generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Gnu parallel -the command-line power tool. ;login: The USENIX Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tange</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Starcraft ii: A new challenge for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno>abs/1708.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Vand Erplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Paul van Mulbregt, and SciPy 1. 0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fully test-time adaptation by entropy minimization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Top1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and batch norm adaptation, with a batch size of 128</title>
	</analytic>
	<monogr>
		<title level="j">Model Clipart Infograph Painting Quickdraw Real Sketch mDE</title>
		<imprint/>
	</monogr>
	<note>Table 22</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and RPLq=0.8. Model Clipart Infograph Painting Quickdraw Real Sketch mDE</title>
	</analytic>
	<monogr>
		<title level="m">Table 23</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and ENT. Dataset top-1 error in % top-1 error on filtered labels in % percentage of rejected inputs</title>
	</analytic>
	<monogr>
		<title level="m">Table 24</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">Table 25. top-1 error on IN and different IN-D domains for different settings: left column: default evaluation</title>
		<imprint/>
	</monogr>
	<note>middle column: predicted labels that cannot be mapped to IN-D are filtered out, right column: percentage of filtered out labels</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<title level="m">Top-1 error on IN-D for AlexNet We report the top-1 error numbers on different IN-D as achieved by AlexNet in Table 26. We used these numbers for</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Software stack We use different open source software packages for our experiments, most notably Docker [30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>scipy and numpy [49], GNU parallel [47], Tensorflow [1], PyTorch [35], and torchvision [29</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
