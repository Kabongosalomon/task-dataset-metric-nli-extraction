<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Temporal Alignment for Video Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<email>kunzhou@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzheng)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Smartmore Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liying</forename><surname>Lu</surname></persName>
							<email>lylu@cse.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<email>hanxiaoguang@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzheng)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
							<email>jiangbo.lu@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Smartmore Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Temporal Alignment for Video Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-range temporal alignment is critical yet challenging for video restoration tasks. Recently, some works attempt to divide the long-range alignment into several subalignments and handle them progressively. Although this operation is helpful in modeling distant correspondences, error accumulation is inevitable due to the propagation mechanism. In this work, we present a novel, generic iterative alignment module which employs a gradual refinement scheme for sub-alignments, yielding more accurate motion compensation. To further enhance the alignment accuracy and temporal consistency, we develop a non-parametric re-weighting method, where the importance of each neighboring frame is adaptively evaluated in a spatial-wise way for aggregation. By virtue of the proposed strategies, our model achieves state-of-the-art performance on multiple benchmarks across a range of video restoration tasks including video super-resolution, denoising and deblurring. Our project is available in https://github. com / redrock303 / Revisiting -Temporal -Alignment-for-Video-Restoration.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Frame alignment plays an essential role in aggregating temporal information in video restoration tasks, e.g., video super-resolution (Video SR), video deblurring, and video denoising. In recent years, great attempts have been made to study this problem. Especially, the deep learningbased methods are successful in building temporal correspondences and achieve promising results.</p><p>The existing alignment methods can be roughly categorized into two classes: (i) independent alignment that conducts frame-to-frame alignments totally independently (see <ref type="figure">Fig. 2(a)</ref>) and (ii) progressive alignment that performs temporally consecutive alignments sequentially in a recur-  <ref type="figure">Figure 1</ref>. Performance and efficiency comparison on Vimeo-90K-T <ref type="bibr" target="#b32">[33]</ref>. Besides high PSNR and fast inference, our alignment algorithm can be easily integrated into existing frameworks (e.g., IconVSR <ref type="bibr" target="#b2">[3]</ref>) to further improve performance. Circle sizes are set proportional to the numbers of parameters. sive manner (see <ref type="figure" target="#fig_7">Fig. 2(b)</ref>). Those independent alignment approaches typically focus on designing effective feature descriptors and motion estimation modules to improve the performance. For example, EDVR <ref type="bibr" target="#b27">[28]</ref> develops pyramid, cascading and deformable convolutions (PCD) for more accurate alignment. Whereas, without exploiting the correlations between multiple alignments, this strategy is still facing challenges to estimate the long-range motion fields. The second line typically adopts a recurrent framework for gradual alignment. Taking BasicVSR <ref type="bibr" target="#b2">[3]</ref> for example, the authors propose an optical-flow-based recurrent architecture for video super-resolution. They predict the bidirectional optical flow between two neighboring frames and then conduct a bidirectional propagation, where the temporal information is aggregated by warping image features produced by previous steps. This kind of methods is mainly proposed to model long-range dependencies since it only needs to handle relatively small motion between neighboring frames in one step. However, such chain-rule-based propagation has no chance to correct the misalignment caused by previ-  <ref type="figure">Figure 2</ref>. Three alignment strategies in video restoration tasks. (a) Independent alignment that estimates frame-to-frame correspondences in isolation. (b) Progressive alignment that performs multiple alignments sequentially. (c) Our proposed iterative alignment scheme that performs gradual refinement for shared sub-alignments. A k refers to the k-th temporal alignment and ai is the i-th sub-alignment. ous steps and may suffer from the error accumulation issue.</p><p>As illustrated in <ref type="figure">Fig. 2</ref>(c), we observe that different long-range alignments (A i ) actually share some subalignments (a i ), e.g., a 1 is shared among A 1 , A 2 and A 3 , so as a 2 in A 2 and A 3 . How can we utilize this property to improve the accuracy of the shared sub-alignments? In this work, we propose an iterative alignment module (IAM) built upon the progressive alignment strategy to gradually refine the shared sub-alignments. For a specific shared subalignment (e.g., a 2 in A 2 and A 3 ), the previously estimated result (a 2 in A 2 ) is used as a prior in the current iteration (a 2 in A 3 ). Our IAM has two merits over the progressive alignment scheme. First, the progressive alignment only conducts a single prediction for each sub-alignment so that misalignment can not be corrected. In contrast, our IAM refines each sub-alignment iteratively, yielding more accurate alignment. Second, the progressive alignment performs multi-frame aggregation based on a chain-like propagation so that misalignment will be propagated to the end. In our IAM, each neighboring frame is aligned through individual propagation, making it more reliable. Furthermore, to reduce the computational complexity, we elaborate a simple yet efficient alignment unit for temporal sub-alignments. From <ref type="figure">Fig. 1</ref>, it is observed that our alignment algorithm yields high inference efficiency and superior performance compared with state-the-of-art video SR methods. Particularly, our IAM can be easily plugged into existing deep models. For example, by replacing the original independent alignment module of IconVSR <ref type="bibr" target="#b2">[3]</ref> with our "IAM" (denoted as "IconVSR+IAM" in <ref type="figure">Fig. 1</ref>), the PSNR is boosted from 37.47dB to 37.56dB on Vimeo-90K-T <ref type="bibr" target="#b32">[33]</ref>, while reducing the number of parameters from 8.7M to 7.8M.</p><p>Besides, the aggregation of multiple aligned frames remains an essential step, for the purpose of preserving details while eliminating alignment errors. Modern restoration systems either employ a sequential of convolutions to directly fuse the aligned features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> or adopt spatial-temporal adaptive aggregation strategies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. However, all these methods solely rely on the learned parameters, raising the risk of overfitting on a specific domain. In this work, we strive a non-parametric re-weighting module, where two strategies are designed to explicitly evaluate the spatially-adaptive importance of different frames. First, we explore the accuracy of alignments. Patches in the aligned frames are compared with the counterparts in the reference frame, and those of high similarity are assigned with larger weights during fusion. Second, to evaluate the consistency of alignments, we compute the pixel-wise L2 distances of the aligned frames with their average. Pixels with smaller distances are considered to be more consistent with other frames and hence assigned with larger weights. The proposed re-weighting module is parameterless and hence can be plugged into other models.</p><p>The main contributions are summarized as:</p><p>? We rethink issues of the progressive alignment and accordingly propose an iterative alignment scheme, yielding more accurate estimation, especially over long-range correspondences.</p><p>? We propose a non-parametric re-weighting module that simultaneously evaluates the alignment accuracy and temporal consistency.</p><p>? The quantitative and qualitative results justify the state-of-the-art performance of our method across several video restoration tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal Alignment. Many video restoration approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> perform independent temporal alignment between neighboring frames with the central frame. Various strategies have been proposed to improve the performance. For example, to fill the domain gap between op-  tical flow estimation and video SR tasks, TOF <ref type="bibr" target="#b32">[33]</ref> integrates a task-oriented flow module into their VSR framework for end-to-end training. Pan et al. <ref type="bibr" target="#b20">[21]</ref> develop CNNs to estimate the optical flow and the latent frame simultaneously. Later on, some methods start to develop adaptive kernel-based schemes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> to perform the alignment and process the occlusion simultaneously. EDVR <ref type="bibr" target="#b27">[28]</ref> proposes a coarse-to-fine alignment algorithm to tackle the large displacement. However, these independent alignment models only focus on exploring correlations between two frames in isolation. It is still challenging to handle long-range alignments.</p><p>Another line of work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> begins to explore a progressive alignment strategy for video restoration tasks. To alleviate the challenges of long-range alignment, they typically split multiple long-range alignments into several sub-alignments. Those sub-alignments are subsequently processed progressively. In BasicVSR <ref type="bibr" target="#b2">[3]</ref>, a pre-trained SPyNet <ref type="bibr" target="#b21">[22]</ref> is utilized to estimate motion fields of each sub-alignment between adjacent frames. Then, they progressively aggregate the temporal information by warping image features produced by previous steps. The progressive alignment scheme makes it effective in handling long-range alignment. Based on BasicVSR, BasicVSR++ <ref type="bibr" target="#b3">[4]</ref> presents a second-order propagation and motion field residual learning method to improve the accuracy of sub-alignments. However, inaccurately estimated motion fields of some subalignments will wrongly warp the image features. The misaligned information is subsequently propagated and aggregated in the later steps, resulting in error accumulation. In this work, we propose an iterative alignment algorithm built upon the progressive alignment scheme. Each subalignment is estimated and refined gradually, largely improving the accuracy of alignment.</p><p>Feature Fusion. The majority of video restoration methods fuse the aligned frames for temporal information aggregation by feature concatenation followed by a convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. For example, FastDVD <ref type="bibr" target="#b24">[25]</ref> divides con- secutive frames into different groups and designs a twostage convolutional neural network for multi-frame fusion. In addition, more effective aggregation strategies have been proposed by applying spatial or temporal attention-based mechanism <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. Isobe et al. <ref type="bibr" target="#b8">[9]</ref> design a framerate-aware group attention, which can handle various levels of motions. In <ref type="bibr" target="#b29">[30]</ref>, a motion robustness analysis is adopted to fuse temporal information, where different confidence scores are assigned to the local neighbors of each pixel for merging. Inspired by this work, we design an adaptive reweighting module for information aggregation, considering both the accuracy and consistency of the alignment. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the proposed framework. Our goal is to reconstruct a high-quality image I hq 0 from 2N +1 consecutive low-quality images {I lq ?N , ? ? ? , I lq 0 , ? ? ? , I lq N }. In the feature extraction module, the input frames are first downsampled with strided convolutions for video deblurring/denoising, while being processed under the same resolution for video SR. Then we utilize the proposed IAM to align input frames referring to the central frame. For simplicity, we only consider the one-side alignment in the following as the other side is processed symmetrically. Afterwards, an adaptive re-weighting module is designed to fuse the aligned features. Finally, the I hq 0 is obtained by adding the predicted residue to the original (for video deblurring/denoising) or upsampled (for video SR) input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>, we conduct feature extraction to transform a RGB frame I lq k to high-dimensional feature maps F k . We first utilize two convolutions with strides of 2 to downsample the feature resolutions for video deblurring and denoising (highlighted in blue dotted box in <ref type="figure" target="#fig_4">Fig. 4</ref>) for computational efficiency, while keeping the same resolution for video SR (highlighted in green dotted box in <ref type="figure" target="#fig_4">Fig. 4</ref>). Then we utilize another two convolutions with stride of 2 to obtain the pyramid representations of the input frames. At <ref type="figure">Figure 5</ref>. PSNR differences of four SOTA video SR methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> compared to our method (dotted line) on REDS4 <ref type="bibr" target="#b19">[20]</ref> and REDS4-Fast. The smaller the value, the larger the gap. last, we fuse the pyramid features with a single convolution.</p><formula xml:id="formula_0">PSNR (dB) REDS4 REDS4-Fast EDVR MuCAN BasicVSR VSR-T Ours 0.0 -1.0 -0.5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Alignment</head><p>Temporal alignment aims to align multiple neighboring features {F ?N , ? ? ? , F ?1 , F 1 , ? ? ? , F N } to a reference F 0 . Let A k be the k-th temporal alignment between the neighboring frame F k and the reference frame F 0 , then we have</p><formula xml:id="formula_1">A k (F k , F 0 ) =F 0 k , k ? {?N, ? ? ? , ?1, 1, ? ? ? , N },<label>(1)</label></formula><p>whereF 0 k is the aligned result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Progressive Alignment</head><p>In order to facilitate the long-range alignment, some recent methods <ref type="bibr" target="#b3">[4]</ref> adopt a progressive alignment strategy. For the alignment A k , they divide it into sequential subalignments {a k , a k?1 , ? ? ? , a 1 } to gradually align the feature F k to the reference frame F 0 . We use a i to represent the sub-alginment from F i to F i?1 :</p><formula xml:id="formula_2">a i : F i ? F i?1 .<label>(2)</label></formula><p>As illustrated in <ref type="figure" target="#fig_7">Fig. 2(b)</ref>, all neighboring frames are processed through the chained sub-alignments, indicating that the latter sub-alignments strongly depend on the former predictions. Consequently, the error incurred by an intermediate inaccurate sub-alignment will be propagated and accumulated till the end, leading to inferior performance. To alleviate the issue of error accumulation and boost the restoration quality, we propose an iterative alignment algorithm to focus on improving the accuracy of each sub-alignment a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Iterative Alignment</head><p>Unlike the progressive alignment that conducts each subalignment only once, our algorithm iteratively refines the sub-alignments based on the previous estimation. As shown in <ref type="figure">Fig. 2</ref>(c), we start from the alignment A 1 , which only contains the sub-alignment a 1 : F 1 ? F 0 , described as:</p><formula xml:id="formula_3">A 1 : a 1 (F 1 , F 0 , t = 1) ?F 0 1 , h 1 1 ,<label>(3)</label></formula><formula xml:id="formula_4">+ Concatenation Conv. + ReLU Residual Blocks Deformable Conv. + + ! ! " "#$ " % " !#" " !&amp;$#" ! ! "#$ Eq. 6 ( = )</formula><p>Eq. 6 ( ? ) Sub-alignment Refinement <ref type="figure">Figure 6</ref>. Illustration of our iterative sub-alignment unit for the sub-alignment ai of A k .F i k is the source feature and Fi?1 is the target feature. The iterative refinement is highlighted in dashed box.F i?1 k is the aligned result and h k+1?i i is the refined motion field of sub-alignment ai.</p><p>whereF i?1 k refers to the aligned result of sub-alignment a i in A k . And h t i represents the estimated motion field of the sub-alignment a i after being refined t times.</p><p>After that, we consider the next alignment A 2 by sequentially performing two sub-alignments {a 2 , a 1 }:</p><formula xml:id="formula_5">A 2 : a 2 (F 2 , F 1 , t = 1) ?F 1 2 , h 1 2 , a 1 (F 1 2 , F 0 , h 1 1 , t = 2) ?F 0 2 , h 2 1 .<label>(4)</label></formula><p>For the sub-alignment a 1 in A 2 , it has already been carried out in A 1 once. Thus we take the pre-estimated motion field h 1 1 of a 1 in A 1 as the initialization and conduct a refinement, formulated as an iterative optimization.</p><p>For the subsequent alignment A 3 , two sub-alignments {a 2 , a 1 } will be refined as:</p><formula xml:id="formula_6">A 3 : ? ? ? ? ? a 3 (F 3 , F 2 , t = 1) ?F 2 3 , h 1 3 , a 2 (F 2 3 , F 1 , h 1 2 , t = 2) ?F 1 3 , h 2 2 , a 1 (F 1 3 , F 0 , h 2 1 , t = 3) ?F 0 3 , h 3 1 .<label>(5)</label></formula><p>It can be concluded that, apart from the first sub-alignment a k in A k , all other sub-alignments are optimized at least twice. There are two merits: (i) The sub-alignments will be more accurate through our iterative refinements. (ii) The sub-alignments not only rely on the pre-aligned features but also the pre-estimated motion field, making it more reliable.</p><p>To verify our claim, we evaluate our algorithm together with recent video SR models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> on REDS4 <ref type="bibr" target="#b19">[20]</ref> and REDS4-Fast 1 . As shown in <ref type="figure">Fig. 5</ref>, our model achieves the best performance over the competing methods. Particularly, our method brings about significant improvement in the context of large motion, demonstrating the effectiveness of our IAM in the long-range alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Sub-Alignment Unit</head><p>In Sec. 3.3.2, we describe the iterative alignment algorithm in detail. It is observed that for 2N neighboring frames, our method requires N (N + 1) sub-alignments. In contrast, the independent and progressive alignment schemes only need 2N (sub-)alignments. So it is critical to design a simple sub-alignment unit for computational efficiency. To this end, two improvements have been proposed. (i) While the previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> typically adopt a pyramid alignment scheme that performs multiple-scale processing in the alignment phase, we adopt an early multiscale fusion strategy in the feature extraction phase so that our IAM only performs single-scale alignments. (ii) We develop a lightweight sub-alignment unit with much fewer parameters than other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, we use a compact structure of residual blocks to reduce computational overhead (see details in supplementary materials.). <ref type="figure">Fig. 6</ref> shows the structure of our sub-alignment unit. Taking the i-th sub-alignment a i of A k for example, we first utilize two convolutions followed by ReLU activation to estimate the initialized motion field h c i from the concatenation of source featureF i k and target feature F i?1 . After that, there are two cases for the prediction h k+1?i i of a i :</p><formula xml:id="formula_7">h k+1?i i = h c i , i = k , ?(h c i , h k?i i ), others .<label>(6)</label></formula><p>If a i is the first sub-alignment of A k (i = k), then no historical prediction can be reused to refine the a i . As a result, we simply set h c i as the estimated motion field of a i . Otherwise, we will take the last estimation h k?i i together with the current estimation h c i as input and utilize a single convolution followed by two residual blocks (dubbed as ?) to refine the prediction. Finally, we adopt a deformable convolution <ref type="bibr" target="#b5">[6]</ref> to adaptively sample contents from the source featureF i k :</p><formula xml:id="formula_8">F i?1 k = DConv(F i k , F i?1 , h k+1?i i )<label>(7)</label></formula><p>Specially, if a i is the first sub-alignment in A k (i = k), Eq. 7 can be written as:</p><formula xml:id="formula_9">F k?1 k = DConv(F k , F k?1 , h 1 k )<label>(8)</label></formula><p>The sub-alignment unit is shared for all sub-alignments, largely reducing the number of learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptive Re-weighting</head><p>Although the temporal alignment module performs motion compensation for neighboring frames, it remains vital to fuse them in an effective way. Recently, convolutionbased attention mechanism becomes popular to aggregate multi-frame information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. By contrast, we present a non-parametric re-weighting module to explicitly evaluate the spatially-adaptive importance of aligned frames from two perspectives. First, we evaluate the accuracy of aligned frames with respect to the reference frame. Second, we measure the consistency of aligned neighboring frames. <ref type="figure" target="#fig_5">Fig. 7</ref> describes the pipeline of our re-weighting module.</p><p>Accuracy-Based Re-weighting. As shown in <ref type="figure" target="#fig_5">Fig. 7(a)</ref>, we measure the accuracy of aligned frames. For the reference frame F 0 , the feature vector at position (x, y) is denoted as v 0 , i.e., F 0 (x, y) = v 0 . We find its corresponding 3 ? 3 patch centered at the same position in the k-th aligned frameF 0 k . For each feature vector on this patch, we calculate its cosine similarity (normalized inner product) with respect to v 0 as:</p><formula xml:id="formula_10">S x,y k (?x, ?y) =F 0 k (x + ?x, y + ?y) F0 k (x + ?x, y + ?y) 2 ? v 0 v 0 2 ,<label>(9)</label></formula><p>where S x,y k is the 3 ? 3 similarity map at position (x, y) and ? represents the inner product. (x + ?x, y + ?y) is the coordinate of feature vector where ?x, ?y ? {?1, 0, 1}. Then a Softmax function is applied to S x,y k in the spatial dimension, yielding the pixel-wise weights as:</p><formula xml:id="formula_11">W x,y k = Softmax(S x,y k ) .<label>(10)</label></formula><p>Then W x,y k is used to fuse feature vectors on the 3?3 patch and the re-weighted resultF 0 k (x, y) is obtained as: Consistency-Based Re-weighting. We first calculate the average of aligned neighboring frames yieldingF 0 avg , as illustrated in <ref type="figure" target="#fig_5">Fig. 7(b)</ref>. For the k-th aligned frameF 0 k , we evaluate its consistency with other aligned frames as</p><formula xml:id="formula_12">F 0 k (x, y) = ?x,</formula><formula xml:id="formula_13">C k = exp(? ? F 0 k ?F 0 avg 2 2 ) ,<label>(12)</label></formula><p>where ? is set to ?1 in our experiments. It is noted that C k maintains the same shape asF 0 k .</p><p>Finally, we multiply the accuracy-based re-weighted fea-tureF 0 k to the consistency map C k and obtain the result:</p><formula xml:id="formula_14">F 0 k =F 0 k C k .<label>(13)</label></formula><p>The refined aligned featureF 0 k is passed to the reconstruction module for high-quality image regression (see <ref type="figure" target="#fig_3">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and Training Details</head><p>Configuration. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, our network consists of four modules: feature extraction, alignment, reweighting, and reconstruction. The feature extraction module in Sec. 3.2 contains 5 residual blocks for all tasks. <ref type="table">Table 1</ref> shows other detailed configurations, where M is the number of feature channels in the network and B is the number of residual blocks in the reconstruction module.</p><p>Training. We show the training settings in <ref type="table">Table 1</ref>. We use 2-6 NVIDIA GeForce RTX 2080 Ti GPUs to train our models for 900K iterations for all three video restoration tasks. We adopt random vertical or horizontal flipping or 90 ? rotation for data augmentation. The initial learning rate is set to 5 ? 10 ?4 and a cosine decay strategy is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Metrics</head><p>Video Super-Resolution. REDS <ref type="bibr" target="#b19">[20]</ref> and Vimeo-90K <ref type="bibr" target="#b32">[33]</ref> are two widely used datasets in Video SR. Vimeo-90K contains 64,612 training and 7,840 testing 7-frame sequences with resolution 448 ? 256. The testing set is denoted as Vimeo-90K-T. In REDS, there are 266 training and 4 testing video sequences. Each sequence consists of 100 consecutive frames with resolution 1280 ? 720. Following <ref type="bibr" target="#b27">[28]</ref>,  <ref type="table">Table 2</ref>. Quantitative comparison for ablation study. PSNR (dB) is reported. "Baseline" means the model without the proposed strategies. "IAM" and "ARW" denote the iterative alignment module and adaptive re-weighting, respectively. we denote the testing set as REDS4. Apart from these two testing datasets, we also give the quantitative results on Vid4 <ref type="bibr" target="#b15">[16]</ref>, which consists of 4 video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Deblurring.</head><p>We utilize the video deblurring dataset <ref type="bibr" target="#b23">[24]</ref> (short for VDB) to train and evaluate our models. There are a total of 61 training and 10 testing video pairs. Each pair contains a blurry and sharp videos. The testing subset is marked as VDB-T.</p><p>Video Denoising. In this task, we aim to remove Gaussian white noises with known noise levels (?). Our model is trained on DAIVS <ref type="bibr" target="#b10">[11]</ref>, which contains 87 training and 30 testing 540p videos. Set8 <ref type="bibr" target="#b24">[25]</ref> is also adopted for testing. Following <ref type="bibr" target="#b24">[25]</ref>, we keep a maximum of 85 frames for all training and testing sequences for a fair comparison.</p><p>Metrics. PSNR and SSIM <ref type="bibr" target="#b28">[29]</ref> are used to evaluate the performance of our models in the video SR and deblurring tasks. For video denoising, following the existing methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>, only PSNR is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>In this section, we perform a comprehensive analysis of our method. We abbreviate the iterative alignment module as IAM and the adaptive re-weighting as ARW for clarity.</p><p>IAM and ARW. To evaluate the performance of the proposed IAM and ARW designs, we perform a quantitative comparison in <ref type="table">Table 2</ref>. Starting from a baseline without these designs, we incrementally add the iterative alignment module (IAM) and adaptive re-weighting (ARW). As illus-   trated in <ref type="table">Table 2</ref>, the proposed IAM brings about 0.36dB, 2.31dB and 0.74dB improvement on PSNR in the video SR, deblurring and denoising tasks, respectively. Besides, we notice that the utilization of ARW further pushes the PSNR up to a new height. Especially, it brings more improvement in the denoising task. All these results manifest the effectiveness of our proposed IAM and ARW strategies.</p><p>Iterative Number in IAM. We assess the influence of iterative number in <ref type="table" target="#tab_2">Table 3</ref> on video SR. Compared to the baseline that performs a single prediction of each subalignment (identical to the progressive alignment), we gradually increase the number of refinements to 2 and 3 (denoted as R2 and R3) for sub-alignments, resulting in PSNR gains by 0.32dB and 0.36dB, respectively. It is noteworthy that the increase of running time is acceptable (13-16ms). Also, as illustrated in <ref type="figure" target="#fig_6">Fig. 8</ref>, the optical flow between the prediction and GT becomes smaller with the increase of refinements, indicating more accurate alignment. Both quantitative and qualitative results suggest that our IAM can significantly improve the alignment accuracy by reducing the Bicubic EDVR MuCAN BasicVSR Ours GT <ref type="figure">Figure 10</ref>. Qualitative comparison on Vimeo-90K-T <ref type="bibr" target="#b32">[33]</ref> and REDS4 <ref type="bibr" target="#b19">[20]</ref> in video SR. error accumulation during propagation.</p><p>Re-weighting Type in ARW. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we study the proposed accuracy-and consistency-based reweighting strategies for video SR. Compared with the baseline, the accuracy-based re-weighting leads to a 0.03dB gain while the consistency one obtains 0.07dB improvement, only costing extra 1-5ms. <ref type="figure" target="#fig_8">Fig. 9</ref> shows some examples to illustrate the improved accuracy and consistency of our ARW. It can be observed that the model with our reweighting module is able to restore more accurate textures while maintaining temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-The-Art Methods</head><p>We compare our method with state-of-the-art approaches quantitatively and qualitatively in the video SR, video deblurring and video denoising tasks.</p><p>Video Super-resolution. <ref type="table">Table 5</ref> and <ref type="table" target="#tab_6">Table 9</ref> exhibit the quantitative results of our method and existing video SR methods <ref type="bibr">[2-4, 7, 10, 14, 28, 33]</ref> on Vimeo-90K-T <ref type="bibr" target="#b32">[33]</ref>, REDS4 <ref type="bibr" target="#b19">[20]</ref> and Vid4 <ref type="bibr" target="#b15">[16]</ref> datasets. Compared to the representative independent <ref type="bibr" target="#b27">[28]</ref> and progressive <ref type="bibr" target="#b2">[3]</ref>   <ref type="table">Table 6</ref>. Set8 <ref type="bibr" target="#b24">[25]</ref> and DAVIS <ref type="bibr" target="#b10">[11]</ref> results in video denoising. PSNR(dB) results are reported.</p><p>formance with 0.23dB and 0.66dB improvement on Vimeo-90K-T, respectively. In addition, our model also surpasses the VSR-T [2] by 0.13dB, which has much more parameters. While BasicVSR++ <ref type="bibr" target="#b3">[4]</ref> uses an additional dataset for pretraining, our results are still the best. In terms of the Vid4 <ref type="bibr" target="#b15">[16]</ref> dataset, our method achieves significant improvement with 0.51dB on PSNR compared with IconVSR <ref type="bibr" target="#b2">[3]</ref>.</p><p>Besides, <ref type="figure">Fig. 10</ref> shows the visual comparison on Vimeo-90K-T and REDS4. Our model recovers much clearer text and more accurate structures compared to other methods.</p><p>Video Denoising. Following previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>, we adopt Set8 <ref type="bibr" target="#b24">[25]</ref> and DAVIS <ref type="bibr" target="#b10">[11]</ref> as our benchmarks in the video denoising task. The quantitative results are reported in <ref type="table">Table 6</ref>. Our model achieves the best results under most noise levels. Especially, compared with the secondbest approaches, our method largely improves the PSNR by 0.37dB and 0.65dB under the noise level ? = 50 on Set8 and DAVIS, respectively. <ref type="figure">Fig. 11</ref> presents some qualitative results. It is observed that our method restores richer and clearer textures compared with other approaches.</p><p>Video Deblurring. We compare our method with several recent video deblurring approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref> on VDB-T <ref type="bibr" target="#b23">[24]</ref>. As illustrated in <ref type="table">Table 1</ref>, two models with different sizes (10 or 40 residual blocks) are developed, denoted as "Ours-M" and "Ours". From <ref type="table">Table 7</ref>, compared to the second best ARVo <ref type="bibr" target="#b20">[21]</ref>, we see that our model achieves 0.12dB and 0.013 improvement on PSNR and SSIM, respectively. Some visual examples illustrated in <ref type="figure" target="#fig_9">Fig. 12</ref> also demonstrate that our model is able to handle soame challenging cases with complex motion blur.  <ref type="table">Table 7</ref>. VDB-T <ref type="bibr" target="#b23">[24]</ref> results in video deblurring. "Ours-M" and "Ours" denote our medium and standard models.</p><formula xml:id="formula_15">VNLNet Input ( ) = 50</formula><p>FastDVD Ours GT <ref type="figure">Figure 11</ref>. Qualitative comparison on Set8 <ref type="bibr" target="#b24">[25]</ref> in video denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STFA Pan</head><p>Ours GT Blurry Patch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitation</head><p>In this work, the proposed designs are mainly for improving the accuracy of the long-range alignment. There remains plenty of room to optimize the modeling of subtle motion. Besides, further improving the efficiency of the alignment pipeline is also our future target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a simple yet effective iterative alignment algorithm (IAM) and an efficient adaptive reweighting strategy (ARW) to better utilize multi-frame information. The quantitative and qualitative results of three video restoration tasks illustrate the effectiveness of our method. Besides, we show that our method is general to be deployed in existing video processing systems to further improve their performance. We will explore more videobased tasks in the future. The code will be publicly available to promote the development of the community.  <ref type="table">Table 10</ref>. Quantitative comparison on Vid4 <ref type="bibr" target="#b15">[16]</ref> under x4 setting for video super-resolution. We report the PSNR (dB)/SSIM results on both the RGB and the Y channel. Numbers in red and blue refer to the best and second-best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUF EDVR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuCAN BasicVSR</head><p>Ours GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUF EDVR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuCAN BasicVSR</head><p>Ours GT <ref type="figure" target="#fig_3">Figure 13</ref>. Visualization of temporal consistency on Vid4 <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Video Results</head><p>We also provide three videos for visual inspection. "city.mp4". This video illustrates the visual comparison between bicubic and our method on a Vid4 clip for video super-resolution. It can be observed that our method restores much clear image details (e.g., the finer structure of buildings). "IMG0030.mp4". This video demonstrates the visual results of our method on a testing sequence of VDB-T <ref type="bibr" target="#b23">[24]</ref> for the video deblurring task. The burry input and the generated frames are shown in it. "motorbike.mp4". This video shows the restoration results on a sequence of Set8 <ref type="bibr" target="#b24">[25]</ref> for video denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic Patch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCAN</head><p>DUF EDVR Ours GT <ref type="figure" target="#fig_4">Figure 14</ref>. Qualitative comparison on UDM10 <ref type="bibr" target="#b33">[34]</ref> and Vid4 <ref type="bibr" target="#b15">[16]</ref> for video SR.    <ref type="figure" target="#fig_5">Figure 17</ref>. Qualitative comparison on Set8 <ref type="bibr" target="#b24">[25]</ref>, DAIVS <ref type="bibr" target="#b10">[11]</ref> for video denoising. The values beneath images represent the PSNR (dB).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A general framework for video restoration tasks. There are four components including a frame feature extraction module, an iterative alignment module, an adaptive re-weighting module and a reconstruction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Overview of our feature extraction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Adaptive re-weighting module. There are two branches: (a) the accuracy-based re-weighting branch for measuring the accuracy of alignment, (b) the consistency-based re-weighting branch for evaluating the consistency of the aligned frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FlowFigure 8 .</head><label>8</label><figDesc>Magnitude 0.69 Flow Magnitude 0.46 Flow Magnitude 0.39 Analysis of the iterative number in IAM in video SR.The first line shows the predictions and the second line shows the optical flows (using RAFT<ref type="bibr" target="#b25">[26]</ref>) between predictions and GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( b )</head><label>b</label><figDesc>Visual analysis for tempoal profile on video SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Analysis of the ARW module in Video SR. (a) Visual comparison between without (w/o) and with (w/) ARW. (b) Temporal consistency comparison between without and with ARW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Visual results on VDB-T<ref type="bibr" target="#b23">[24]</ref> in video deblurring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 .</head><label>15</label><figDesc>Qualitative comparison on Vimeo-90K-T<ref type="bibr" target="#b32">[33]</ref> and REDS4<ref type="bibr" target="#b19">[20]</ref> for video SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different IAM and ARW settings for video SR. The running time of each model is also reported with an input size of 7 ? 112 ? 64.</figDesc><table><row><cell cols="2">Methods</cell><cell>PSNR (dB)</cell><cell cols="2">SSIM Runtime (ms)</cell></row><row><cell cols="2">Baseline</cell><cell>37.36</cell><cell>0.9468</cell><cell>153</cell></row><row><cell>IAM</cell><cell cols="3">R2 37.68 (+0.32) 0.9487 R3 37.72 (+0.36) 0.9490</cell><cell>166 169</cell></row><row><cell>ARW</cell><cell cols="3">Acc. 37.39 (+0.03) 0.9469 Con. 37.43 (+0.07) 0.9469</cell><cell>154 158</cell></row><row><cell cols="2">Full</cell><cell cols="2">37.84 (+0.48) 0.9498</cell><cell>170</cell></row><row><cell cols="2">Input 35.20 dB</cell><cell cols="2">w/o ARW 40.47 dB</cell><cell>w/ ARW 42.04 dB</cell></row><row><cell></cell><cell cols="4">(a) Visual comparison on video SR.</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o ARW</cell><cell>w/ ARW</cell><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.8684 37.61/0.9489 37.32/0.9465 37.18/0.9450 37.47/0.9476 37.79/0.9500 37.71/0.9494 37.84/0.9498Table 5. Vimeo-90K-T<ref type="bibr" target="#b32">[33]</ref> results in video SR. The PSNR(dB)/SSIM results are obtained under the ?4 setting. Numbers in red and blue refer to the best and second-best results. ' ?' means BasicVSR++ uses an additional REDS dataset for pretraining.</figDesc><table><row><cell cols="2">Methods</cell><cell>Bicubic</cell><cell cols="7">EDVR [28] MuCAN [14] BasicVSR [3] IconVSR [3]  ? BasicVSR++ [4] VSR-T [2]</cell><cell>Ours</cell></row><row><cell cols="2">nFrame</cell><cell>1</cell><cell>1</cell><cell></cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>Param.</cell><cell></cell><cell>-</cell><cell>20.6</cell><cell></cell><cell>19.8</cell><cell>6.3</cell><cell>8.7</cell><cell>7.3</cell><cell>43.8</cell><cell>17.0</cell></row><row><cell>RGB</cell><cell></cell><cell cols="3">29.79/0.8483 35.79/0.9374</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.88/0.9380 35.96/0.9389</cell></row><row><cell cols="7">Y 31.32/0Dataset ? VNLB [1] V-BM4D [19] VNLnet [5] FastDVD [25] Ours</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>37.26</cell><cell>36.05</cell><cell>37.10</cell><cell>36.44</cell><cell>37.25</cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>33.72</cell><cell>32.19</cell><cell>33.88</cell><cell>33.43</cell><cell>34.05</cell><cell></cell><cell></cell></row><row><cell>Set8</cell><cell>30</cell><cell>31.74</cell><cell>30.00</cell><cell>-</cell><cell>31.68</cell><cell>32.19</cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell>30.39</cell><cell>28.48</cell><cell>30.55</cell><cell>30.46</cell><cell>30.89</cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>29.24</cell><cell>27.33</cell><cell>29.47</cell><cell>29.53</cell><cell>29.90</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>38.85</cell><cell>37.58</cell><cell>35.83</cell><cell>38.71</cell><cell>39.75</cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>35.68</cell><cell>33.88</cell><cell>34.49</cell><cell>35.77</cell><cell>36.73</cell><cell></cell><cell></cell></row><row><cell>DAVIS</cell><cell>30</cell><cell>33.73</cell><cell>31.65</cell><cell>-</cell><cell>34.04</cell><cell>34.89</cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell>32.32</cell><cell>30.05</cell><cell>32.32</cell><cell>32.82</cell><cell>33.56</cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>31.13</cell><cell>28.80</cell><cell>31.43</cell><cell>31.86</cell><cell>32.51</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>alignment</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">methods, our method obtains superior Y-channel PSNR per-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>.7261 28.52/0.8034 25.41/0.7386 26.14/0.7292 RCAN [35] 1 26.17/0.7371 29.34/0.8255 31.85/0.8881 27.74/0.8293 28.78/0.8200 TOF [33] 7 26.52/0.7540 27.80/0.7858 30.67/0.8609 26.92/0.7953 27.98/0.7990 DUF [10] 7 27.30/0.7937 28.38/0.8056 31.55/0.8846 27.30/0.8164 28.63/0.8251 EDVR [28] 5 28.01/0.8250 32.17/0.8864 34.06/0.9206 30.09/0.8881 31.09/0.8800 Quantitative comparison on REDS4<ref type="bibr" target="#b19">[20]</ref> benchmark under ?4 setting for video super-resolution. Numbers in red and blue refer to the best and second-best results. All the results are evaluated in the RGB channel. ' * ' indicates the results are from<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell cols="2">Methods</cell><cell>nFrame</cell><cell cols="2">Clip 000</cell><cell></cell><cell>Clip 011</cell><cell>Clip 015</cell><cell>Clip 020</cell><cell>Average</cell></row><row><cell cols="9">Bicubic 26.06/0MuCAN [14] 1 24.55/0.6489 5 27.99/0.8219 31.84/0.8801 33.90/0.9170 29.78/0.8811 30.88/0.8750</cell></row><row><cell cols="2">*  BasicVSR [3]</cell><cell>5</cell><cell cols="2">27.67/0.8114</cell><cell cols="4">31.27/0.8740 33.58/0.9135 29.71/0.8803 30.56/0.8698</cell></row><row><cell cols="2">*  IconVSR [3]</cell><cell>5</cell><cell cols="2">27.83/0.8182</cell><cell cols="4">31.69/0.8798 33.81/0.9164 29.90/0.8841 30.81/0.8746</cell></row><row><cell cols="2">VSR-T [2]</cell><cell>5</cell><cell cols="2">28.06/0.8267</cell><cell cols="2">32.28/0.8883</cell><cell cols="2">34.15/0.9199 30.26/0.8912 31.19/0.8815</cell></row><row><cell>Ours</cell><cell></cell><cell>5</cell><cell cols="2">28.16/0.8316</cell><cell cols="2">32.24/0.8889</cell><cell cols="2">34.53/0.9275 30.26/0.8920 31.30/0.8850</cell></row><row><cell>Clip Name</cell><cell>Bicubic</cell><cell cols="2">DUF [10]</cell><cell cols="2">EDVR [28]</cell><cell cols="2">MuCAN [14] BasicVSR [3]</cell><cell>IconVSR [3]</cell><cell>VSR-T [2]</cell><cell>Ours</cell></row><row><cell>Calendar (Y)</cell><cell cols="3">20.39/0.5720 24.04/0.8110</cell><cell cols="2">24.05/0.8147</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.08/0.8125 24.65/0.8270</cell></row><row><cell>City (Y)</cell><cell cols="3">25.16/0.6028 28.27/0.8313</cell><cell cols="2">28.00/0.8122</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.94/0.8107 29.92/0.8428</cell></row><row><cell>Foliage (Y)</cell><cell cols="3">23.47/0.5666 26.41/0.7709</cell><cell cols="2">26.34/0.7635</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.33/0.7635 26.41/0.7652</cell></row><row><cell>Walk (Y)</cell><cell cols="3">26.10/0.7974 30.60/0.9141</cell><cell cols="2">31.02/0.9152</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.10/0.9163 31.15/0.9167</cell></row><row><cell>Average (Y)</cell><cell cols="3">23.78/0.6347 27.33/0.8318</cell><cell cols="2">27.35/0.8264</cell><cell>27.26/0.8215</cell><cell>27.24/0.8251</cell><cell>27.39/0.8279</cell><cell>27.36/0.8258 27.90/0.8380</cell></row><row><cell cols="4">Average (RGB) 22.37/0.6098 25.79/0.8136</cell><cell cols="2">25.83/0.8077</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.57/0.8235</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Figure 16. Qualitative comparison on VDB-T<ref type="bibr" target="#b23">[24]</ref> for video deblurring.</figDesc><table><row><cell>Burry Patch</cell><cell>STFA</cell><cell>Pan</cell><cell>Ours</cell><cell>GT</cell></row><row><cell>20.45dB/0.723 Noisy Patch</cell><cell>22.45dB/0.634 BM3D</cell><cell>24.44dB/0.723 FastDVD</cell><cell>27.72dB/0.840 Ours</cell><cell>-/-GT</cell></row><row><cell>17.37dB/0.377</cell><cell>18.54dB/0.447</cell><cell>21.65dB/0.678</cell><cell>23.35dB/0.753</cell><cell>-/-</cell></row><row><cell>19.00dB</cell><cell>29.18dB</cell><cell>34.33dB</cell><cell>34.46dB</cell><cell>-</cell></row><row><cell>28.95dB/0.883</cell><cell>33.14dB/0.938</cell><cell>34.24dB/0.953</cell><cell>34.39dB/0.950</cell><cell>-/-</cell></row><row><cell>18.85dB</cell><cell>31.52dB</cell><cell>35.78dB</cell><cell>36.73dB</cell><cell>-</cell></row><row><cell>33.10dB/0.959</cell><cell>35.97dB/0.966</cell><cell>35.78dB/0.969</cell><cell>36.51dB/0.969</cell><cell>-/-</cell></row><row><cell>19.35dB</cell><cell>28.24dB</cell><cell>35.89dB</cell><cell>36.65dB</cell><cell>-</cell></row><row><cell>26.84dB/0.835</cell><cell>31.47dB/0.915</cell><cell>32.64dB/0.935</cell><cell>35.10dB/0.952</cell><cell>-/-</cell></row><row><cell>19.29dB</cell><cell>27.43dB</cell><cell>32.04dB</cell><cell>32.07dB</cell><cell>-</cell></row><row><cell>21.65dB/0.643</cell><cell>27.45dB/0.855</cell><cell>28.10dB/0.972</cell><cell>29.61dB/0.892</cell><cell>-/-</cell></row><row><cell>21.32dB/0.629 19.24dB</cell><cell>26.00dB/0.827 26.99dB</cell><cell>26.75dB/0.861 30.95dB</cell><cell>27.15dB/0.865 31.20dB</cell><cell>-/--</cell></row><row><cell>21.18dB/0.571</cell><cell>26.12dB/0.807</cell><cell>28.24dB/0.882</cell><cell>28.83dB/0.882</cell><cell>-/-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">REDS4-Fast is a subset of REDS4<ref type="bibr" target="#b19">[20]</ref> with an average motion magnitude of 9.4 pixels, much larger than the average of REDS4 of 4.3 pixels. The optical flows are calculated by RAFT<ref type="bibr" target="#b25">[26]</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Residual Block</head><p>As illustrated in <ref type="table">Table 8</ref>, our residual block is comprised of two convolutions, where the first convolution is followed by ReLU activation. In previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28]</ref>, the channel numbers typically keep identical within the residual block. In contrast, we set the channel dimension of hidden representations (M 1 ) no more than 64 to reduce the parameters of our network and speed up the training and inference phases: We compare the temporal consistency of our method with several state-of-art video SR approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>. The visual results are illustrated in <ref type="figure">Figure 13</ref>. It is observed that other methods fail to restore the consistent textures clearly. While our method empowered with iterative alignment and two adaptively reweighting strategies is able to generate realistic image contents that are closest to the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Comparison with State-of-the-art</head><p>In <ref type="table">Table 9</ref> and 10, we give the detialed comparison with several state-of-the-art video SR approaches [2, 2, 3, 28] on REDS4 <ref type="bibr" target="#b19">[20]</ref> and Vid4 <ref type="bibr" target="#b15">[16]</ref>. The PSNR and SSIM of each video sequence are reported. For most video clips of both two validation sets, our model consistently achieves the best performance. Moreover, we provide extensive qualitative comparison on UDM10 <ref type="bibr" target="#b33">[34]</ref>, Vid4 <ref type="bibr" target="#b15">[16]</ref>, Vimeo-90K-T <ref type="bibr" target="#b32">[33]</ref>, and REDS4 <ref type="bibr" target="#b19">[20]</ref> for video SR (in <ref type="figure">Figure 14, 15</ref>), VDB-T <ref type="bibr" target="#b23">[24]</ref> for video deblurring (in <ref type="figure">Figure 16</ref>) and Set8 <ref type="bibr" target="#b24">[25]</ref>, DAIVS <ref type="bibr" target="#b10">[11]</ref> for video denoising (in <ref type="figure">Figure 17</ref>). All the qualitative results demonstrate that our method has the capacity to handle various challenging cases in these three video restoration tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video denoising via empirical bayesian estimation of space-time patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="93" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Video super-resolution transformer. arXiv, 2021. 4, 5, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02181</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Basicvsr++: Improving video superresolution with enhanced propagation and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13371</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep rnns for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXXIX</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9971</biblScope>
			<biblScope unit="page">99711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4038" to="4047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arvo: Learning all-range volumetric correspondence for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7721" to="7731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a deep dual attention network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihui</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lapar: Linearly-assembled pixel-adaptive regression network for single image super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2507" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image super-resolution via attention based back projection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui-Lam</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3043" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Raisr: rapid and accurate image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards real-time deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Handheld multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Wronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">David: Dual-attentional video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning deformable kernels for image and video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06903</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kernel modeling superresolution on real low-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2433" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
